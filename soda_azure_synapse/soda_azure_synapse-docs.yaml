resources:
- name: data_source
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: adventureworks
  endpoint:
    path: /data_source/adventureworks
    method: POST
    data_selector: data
    params: {}
- name: data_source
  endpoint:
    path: /services/data/vXX.X/sobjects/DataSource
    method: GET
    data_selector: records
- name: data_source
  endpoint:
    path: /data-sources
    method: GET
- name: datasets
  endpoint:
    path: /discover/datasets
    method: GET
    data_selector: datasets
    params: {}
- name: profile_columns
  endpoint:
    path: /profile/columns
    method: GET
    data_selector: columns
    params: {}
- name: automated_monitoring
  endpoint:
    path: /monitoring
    method: GET
    data_selector: datasets
    params: {}
- name: Soda Agent
  endpoint:
    path: /agents
    method: POST
- name: soda-agent
  endpoint:
    path: /soda/agent
    method: POST
- name: agent
  endpoint:
    path: /soda/agent
    method: POST
- name: scanlauncher
  endpoint:
    path: /soda/scanlauncher
    method: POST
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: POST
    data_selector: agent
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: POST
    data_selector: records
    params: {}
- name: agent
  endpoint:
    path: /agents
    method: POST
- name: data_source
  endpoint:
    path: /data-sources
    method: POST
- name: agent
  endpoint:
    path: /agent
    method: POST
    data_selector: agent
    params: {}
- name: scanlauncher
  endpoint:
    path: /scanlauncher
    method: POST
    data_selector: scanlauncher
    params: {}
- name: soda_agent
  endpoint:
    path: /soda-agent
    method: POST
    data_selector: agent
    params: {}
- name: soda_agent
  endpoint:
    path: /cloud.soda.io/v1/agents
    method: POST
    data_selector: agent
    params: {}
- name: agent
  endpoint:
    path: /soda/agent
    method: POST
    data_selector: resources
- name: scanlauncher
  endpoint:
    path: /soda/scanlauncher
    method: POST
    data_selector: resources
- name: agent
  endpoint:
    path: /soda-agent
    method: GET
    data_selector: resources
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: POST
    data_selector: records
- name: agent
  endpoint:
    path: /soda/agent
    method: POST
- name: scanlauncher
  endpoint:
    path: /soda/scanlauncher
    method: POST
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: GET
    data_selector: records
    params: {}
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: GET
- name: agent
  endpoint:
    path: /soda-agent
    method: POST
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: GET
- name: data_source
  endpoint:
    path: /data-source-reference
    method: GET
- name: soda_agent
  endpoint:
    path: /soda-agent
    method: POST
- name: agent
  endpoint:
    path: /soda/agent
    method: POST
    data_selector: agent
    params: {}
- name: scanlauncher
  endpoint:
    path: /soda/scanlauncher
    method: POST
    data_selector: scanlauncher
    params: {}
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: POST
    data_selector: records
    params: {}
- name: soda-agent
  endpoint:
    path: /soda-agent
    method: GET
- name: local_postgres_test
  endpoint:
    path: /data_source/local_postgres_test
    method: POST
    data_selector: records
- name: soda-agent
  endpoint:
    path: local/soda
    method: GET
    data_selector: POSTGRES_USERNAME
    params: {}
- name: soda-agent-password
  endpoint:
    path: local/soda
    method: GET
    data_selector: POSTGRES_PASSWORD
    params: {}
- name: soda-agent
  endpoint:
    path: /local/soda
    method: GET
    data_selector: POSTGRES_USERNAME
    params: {}
- name: soda-agent
  endpoint:
    path: /local/soda
    method: GET
    data_selector: POSTGRES_PASSWORD
    params: {}
- name: events
  endpoint:
    path: /events
    method: GET
    data_selector: data
- name: datasets
  endpoint:
    path: /api/v1/datasets
    method: GET
    data_selector: datasets
    params: {}
- name: incidents
  endpoint:
    path: /api/v1/incidents
    method: GET
    data_selector: incidents
    params: {}
- name: adventureworks
  endpoint:
    path: /data_source/soda-demo
    method: GET
    data_selector: records
    params: {}
- name: data_source
  endpoint:
    path: /data_source
    method: GET
    data_selector: data
- name: data_source
  endpoint:
    path: /data_source
    method: POST
    data_selector: data_source
    params: {}
- name: data_source
  endpoint:
    path: /data_source
    method: GET
    data_selector: records
    params: {}
- name: dim_product_category
  endpoint:
    path: soda/ingest-checks/dim_product_category.yml
- name: fact_internet_sales
  endpoint:
    path: soda/ingest-checks/fact_internet_sales.yml
- name: report_category_sales
  endpoint:
    path: soda/reports-checks/report_category_sales.yml
- name: postgres_data
  endpoint:
    path: /data_source/postgres_data
    method: POST
    data_selector: data
    params: {}
- name: azure_sql_data
  endpoint:
    path: /data_source/azure_sql_data
    method: POST
    data_selector: data
    params: {}
- name: trigger_scan
  endpoint:
    path: /api/v1/scans
    method: POST
    data_selector: scan_result
    params:
      scanDefinition: dagsterredshift_default_scan
- name: get_scan_status
  endpoint:
    path: /api/v1/scans/{scan_id}
    method: GET
    data_selector: scan_status
    params: {}
- name: employees
  endpoint:
    path: /data_source/employees
    method: POST
    data_selector: data
- name: employee_info
  endpoint:
    path: /employee_info
    method: GET
    data_selector: records
- name: employee_survey
  endpoint:
    path: /employee_survey
    method: GET
    data_selector: records
- name: manager_survey
  endpoint:
    path: /manager_survey
    method: GET
    data_selector: records
- name: login_logout
  endpoint:
    path: /login_logout
    method: GET
    data_selector: records
- name: input_data_attrition_model
  endpoint:
    path: /input_data_attrition_model
    method: GET
    data_selector: records
- name: attrition_model_output
  endpoint:
    path: /attrition_model_output
    method: GET
    data_selector: records
- name: fulfillment_apac_prod
  endpoint:
    path: /data_source/fulfillment_apac_prod
    method: POST
- name: fulfillment_apac_staging
  endpoint:
    path: /data_source/fulfillment_apac_staging
    method: POST
- name: fulfillment_apac1_staging
  endpoint:
    path: /data_source/fulfillment_apac1_staging
    method: POST
- name: my_datasource_name
  endpoint:
    path: /soda/configuration.yml
    method: POST
    data_selector: data
- name: datasets
  endpoint:
    path: /api/v1/datasets
    method: GET
    data_selector: content
    params:
      page: '0'
- name: checks
  endpoint:
    path: /api/v1/checks
    method: GET
    data_selector: content
    params:
      size: '100'
- name: checks
  endpoint:
    path: checks
    method: GET
    data_selector: results
- name: datasets
  endpoint:
    path: datasets
    method: GET
    data_selector: results
- name: scans
  endpoint:
    path: scans
    method: POST
    data_selector: id
- name: scan_state
  endpoint:
    path: scans/{scan_id}
    method: GET
    data_selector: state
- name: vault
  endpoint:
    path: /v1/secret/data
    method: GET
    data_selector: data
    params: {}
- name: nyc_bus_breakdowns_and_delays
  endpoint:
    path: /data_source
    method: POST
    data_selector: data_source
    params: {}
- name: data_source
  endpoint:
    path: /data-source
    method: POST
    data_selector: data_source
    params:
      type: postgres
- name: soda.pandas.example
  endpoint:
    path: /datasets/soda.pandas.example
    method: GET
    data_selector: check_results
- name: soda_pandas_example
  endpoint:
    path: /pandas_reference_example
    method: POST
    data_selector: results
    params: {}
- name: checks
  endpoint:
    path: /soda-cloud-api-v1/checks
    method: GET
    data_selector: checks
    params: {}
- name: datasets
  endpoint:
    path: /soda-cloud-api-v1/datasets
    method: GET
    data_selector: datasets
    params: {}
- name: Soda Checks
  endpoint:
    path: /soda-cloud-api-v1/checks
    method: GET
    data_selector: checks
- name: Soda Scans
  endpoint:
    path: /soda-cloud-api-v1/scans
    method: GET
    data_selector: scans
- name: dataset_name
  endpoint:
    path: /soda-cl-overview/quick-start-sodacl
    method: GET
    data_selector: checks
    params: {}
- name: dim_customer
  endpoint:
    path: /soda-checks/dim_customer
    method: POST
    data_selector: checks
    params: {}
- name: dataset_A
  endpoint:
    path: /soda/suggest
    method: POST
    data_selector: checks
- name: automated_monitoring
  endpoint:
    path: /automated_monitoring
    method: POST
    data_selector: datasets
    params: {}
- name: discover_datasets
  endpoint:
    path: /discover/datasets
    method: POST
    data_selector: datasets
    params: {}
- name: profile_columns
  endpoint:
    path: /profile/columns
    method: POST
    data_selector: columns
    params: {}
- name: sample_datasets
  endpoint:
    path: /sample/datasets
    method: POST
    data_selector: datasets
    params: {}
- name: dim_employee
  endpoint:
    path: /soda/v3/checks/dim_employee
    method: GET
    data_selector: records
- name: dim_customers_dev
  endpoint:
    path: /soda/v3/checks/dim_customers_dev
    method: GET
    data_selector: records
- name: dim_customers_prod
  endpoint:
    path: /soda/v3/checks/dim_customers_prod
    method: GET
    data_selector: records
- name: retail_customers
  endpoint:
    path: /soda/v3/checks/retail_customers
    method: GET
    data_selector: records
- name: retail_sfdc_customers
  endpoint:
    path: /soda/v3/checks/retail_sfdc_customers
    method: GET
    data_selector: records
- name: public.employee_dimension
  endpoint:
    path: /soda/v3/checks/public.employee_dimension
    method: GET
    data_selector: records
- name: online_sales.online_page_dimension
  endpoint:
    path: /soda/v3/checks/online_sales.online_page_dimension
    method: GET
    data_selector: records
- name: retail_customers_stage
  endpoint:
    path: /soda/v3/checks/retail_customers_stage
    method: GET
    data_selector: records
- name: retail_customers_prod
  endpoint:
    path: /soda/v3/checks/retail_customers_prod
    method: GET
    data_selector: records
- name: prod.staging.dmds_scores
  endpoint:
    path: /soda/v3/checks/prod.staging.dmds_scores
    method: GET
    data_selector: records
- name: prod.measurement.post_scores
  endpoint:
    path: /soda/v3/checks/prod.measurement.post_scores
    method: GET
    data_selector: records
- name: dim_product
  endpoint:
    path: /check/dim_product
    method: POST
    data_selector: checks
    params: {}
- name: failed_row_samples
  endpoint:
    path: /run-a-scan/failed-row-samples
    method: GET
    data_selector: samples
    params: {}
- name: dim_employee
  endpoint:
    path: public.dim_employee
    method: SELECT
    data_selector: failed_rows
    params:
      fail_condition: employee_key = 1
- name: anomaly_dashboard
  endpoint:
    path: /api/v1/anomaly_dashboard
    method: POST
- name: anomaly_dashboard
  endpoint:
    path: /datasets/{dataset_id}/anomalies
    method: GET
    data_selector: anomalies
    params: {}
- name: check_attributes
  endpoint:
    path: /collaborate/check-attributes
    method: GET
    data_selector: attributes
    params: {}
- name: attributes
  endpoint:
    path: /attributes
    method: POST
    data_selector: attributes
    params: {}
- name: datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: datasets
    params: {}
- name: incidents
  endpoint:
    path: /soda-cloud-api-v1/incidents
    method: POST
    data_selector: incident
    params: {}
- name: dataset_roles
  endpoint:
    path: /collaborate/roles-dataset
    method: GET
    data_selector: roles
    params: {}
- name: data_source
  endpoint:
    path: /data_sources
    method: GET
    data_selector: data_sources
    params: {}
- name: dataset
  endpoint:
    path: /datasets
    method: GET
    data_selector: datasets
    params: {}
- name: check
  endpoint:
    path: /checks
    method: GET
    data_selector: checks
    params: {}
- name: data_quality_checks
  endpoint:
    path: /api/v1/checks
    method: GET
    data_selector: checks
    params: {}
- name: incidents
  endpoint:
    path: /api/v1/incidents
    method: GET
    data_selector: incidents
    params: {}
- name: soda_scan
  endpoint:
    path: /run-a-scan
    method: POST
- name: soda_scan
  endpoint:
    path: /soda/checks.yaml
    method: POST
    data_selector: checks
- name: incident
  endpoint:
    path: /api/incidents
    method: POST
    data_selector: incident
    params: {}
- name: ms_teams_integration
  endpoint:
    path: /integrate-soda/integrate-msteams
    method: POST
- name: ServiceNow Webhook
  endpoint:
    path: /integrate-soda/integrate-webhooks
    method: POST
- name: Slack Integration
  endpoint:
    path: /integrate-soda/integrate-slack
    method: GET
    data_selector: notifications
    params: {}
- name: webhook_configuration
  endpoint:
    path: /webhook/configure
    method: POST
    data_selector: webhook_response
    params: {}
- name: incidentCreated
  endpoint:
    path: /integrate-soda/integrate-webhooks#incidentcreated
    method: POST
    data_selector: incident
    params: {}
- name: incidentUpdated
  endpoint:
    path: /integrate-soda/integrate-webhooks#incidentupdated
    method: POST
    data_selector: incident
    params: {}
- name: agreementCreated
  endpoint:
    path: /integrate-soda/integrate-webhooks#agreementcreated
    method: POST
    data_selector: agreement
    params: {}
- name: agreementContentsUpdated
  endpoint:
    path: /integrate-soda/integrate-webhooks#agreementcontentsupdated
    method: POST
    data_selector: agreement
    params: {}
- name: agreementDeleted
  endpoint:
    path: /integrate-soda/integrate-webhooks#agreementdeleted
    method: POST
    data_selector: agreement
    params: {}
- name: dim_customer
  endpoint:
    path: /checks/dim_customer
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /checks/orders
    method: GET
    data_selector: records
    params: {}
- name: dim_promotion
  endpoint:
    path: /checks/dim_promotion
    method: GET
    data_selector: records
    params: {}
- name: dim_reseller
  endpoint:
    path: /checks/dim_reseller
    method: GET
    data_selector: records
    params: {}
- name: dim_product
  endpoint:
    path: /checks/dim_product
    method: GET
    data_selector: records
    params: {}
- name: dim_promotion
  endpoint:
    path: checks for dim_promotion
    method: GET
    data_selector: anomaly detection for freshness(start_date)
- name: dim_customer
  endpoint:
    path: checks for dim_customer
    method: GET
    data_selector: anomaly detection for customers
- name: dim_reseller
  endpoint:
    path: checks for dim_reseller
    method: GET
    data_selector: avg_order_span between 5 and 10
- name: orders
  endpoint:
    path: checks for orders
    method: GET
    data_selector: anomaly detection for missing_count(id)
- name: anomaly detection for row_count
  endpoint:
    method: POST
    data_selector: model
    params: {}
- name: distribution_reference
  endpoint:
    path: /soda/update-dro
    method: POST
- name: checks
  endpoint:
    path: /soda/scan
    method: POST
- name: fact_internet_sales
  endpoint:
    path: /fact_internet_sales
    method: GET
    data_selector: records
    params: {}
- name: dim_customer
  endpoint:
    path: /dim_customer
    method: GET
    data_selector: records
    params: {}
- name: group_evolution_checks
  endpoint:
    path: /group/evolution/checks
    method: POST
    data_selector: checks
- name: missing_metrics
  endpoint:
    path: /soda-cloud-api-v1/checks
    method: GET
    data_selector: checks
    params: {}
- name: retail_products
  endpoint:
    path: /checks/retail_products
    method: POST
    data_selector: checks
- name: retail_orders_postgres
  endpoint:
    path: /checks/retail_orders_postgres
    method: POST
    data_selector: checks
- name: dim_reseller
  endpoint:
    path: /checks/dim_reseller
    method: POST
    data_selector: checks
- name: Production
  endpoint:
    path: /reconciliation
    method: GET
    data_selector: checks
    params: {}
- name: dim_department_group
  endpoint:
    path: /checks/dim_department_group
    method: POST
    data_selector: checks
    params: {}
- name: dim_employee
  endpoint:
    path: /checks/dim_employee
    method: POST
    data_selector: checks
    params: {}
- name: dim_customer_staging
  endpoint:
    path: /checks/dim_customer_staging
    method: POST
    data_selector: checks
    params: {}
- name: dim_customer_prod
  endpoint:
    path: /checks/dim_customer_prod
    method: POST
    data_selector: checks
    params: {}
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: checks
    params: {}
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: datasets
    params: {}
- name: dim_product
  endpoint:
    path: /checks/dim_product
    method: POST
    data_selector: checks
    params: {}
- name: dim_customer
  endpoint:
    path: /checks/dim_customer
    method: POST
    data_selector: checks
    params: {}
- name: user_defined_checks
  endpoint:
    path: /soda-cloud-api-v1/user-defined-checks
    method: POST
    data_selector: checks
    params: {}
- name: dim_product
  endpoint:
    path: /sodacl-reference/user-defined#example-with-check-name
    method: GET
    data_selector: checks
    params: {}
- name: FULFILLMENT
  endpoint:
    path: /sodacl-reference/optional-config#scan-a-portion-of-your-dataset
    method: GET
    data_selector: checks
    params: {}
- name: CUSTOMERS
  endpoint:
    path: /sodacl-reference/optional-config#customize-sampling-for-checks
    method: GET
    data_selector: checks
    params: {}
- name: product_b
  endpoint:
    path: /sodacl-reference/user-defined#example-with-column-parameter
    method: GET
    data_selector: checks
    params: {}
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: records
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: records
- name: my_datasource_name
  endpoint:
    path: /data-source-reference/connect-athena
    method: POST
    data_selector: results
    params: {}
- name: gcloud-credentials
  endpoint:
    volumeMounts:
    - name: gcloud-credentials
      mountPath: /opt/soda/etc
    volumes:
    - name: gcloud-credentials
      secret:
        secretName: gcloud-credentials
        items:
        - key: serviceaccount.json
          path: serviceaccount.json
- name: customers
  endpoint:
    path: /
    method: GET
    data_selector: records
    params:
      database: customers
- name: timeseries
  endpoint:
    path: /datasets/timeseries
    method: GET
    data_selector: records
- name: employee
  endpoint:
    path: /datasets/employee
    method: GET
    data_selector: records
- name: my_datasource_name
  endpoint:
    path: /
    method: GET
    data_selector: ''
    params: {}
- name: my_datasource_name
  endpoint:
    path: /data_source/my_datasource_name
    method: GET
    data_selector: records
    params: {}
- name: my_datasource_name
  endpoint:
    path: /data_source/my_datasource_name
    method: GET
    data_selector: records
    params:
      database: filename.db
      read_only: true
      schema_name: public
- name: dask
  endpoint:
    path: /data-source-reference/connect-dask
    method: GET
- name: sample_data
  endpoint:
    path: /sample_data
    method: GET
    data_selector: records
    params: {}
- name: my_datasource_name
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: fabric
  endpoint:
    path: /
    method: CONNECT
    data_selector: data
    params:
      host: host
      port: '1433'
      database: database
      schema: dbo
      trusted_connection: 'false'
      encrypt: 'false'
      trust_server_certificate: 'false'
      driver: ODBC Driver 18 for SQL Server
      scope: DW
      connection_parameters:
        multi_subnet_failover: 'true'
- name: customers
  endpoint:
    path: /customers
    method: GET
- name: data_source
  endpoint:
    path: /data_source/my_datasource_name
    method: POST
    data_selector: data
    params:
      type: oracle
      connectstring: ${USARBIG_HOST}:${UARBIG_PORT}/USARBIG_SID
- name: my_datasource_name
  endpoint:
    path: /
    method: CONNECT
    data_selector: data
    params:
      type: postgres
      host: db
      port: '5432'
      database: postgres
      schema: public
      sslmode: prefer
- name: my_datasource_name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: my_datasource_name
  endpoint:
    path: /
    method: POST
    data_selector: data
    params: {}
- name: my_datasource_name
  endpoint:
    path: /services/data/vXX.X/sobjects/my_datasource_name
    method: GET
    data_selector: records
    params: {}
- name: datalake
  endpoint:
    path: /datalake
    method: GET
    data_selector: records
    params: {}
- name: cw_dq
  endpoint:
    path: /cw_dq
    method: GET
    data_selector: records
    params: {}
- name: my_datasource_name
  endpoint:
    path: /
    method: CONNECT
    data_selector: data
    params: {}
- name: test_login
  endpoint:
    path: /test-login
    method: GET
    data_selector: ''
    params: {}
- name: checks
  endpoint:
    path: /checks
    method: GET
    params:
      size: 10
      page: 0
- name: checks
  endpoint:
    path: /api/v1/checks
    method: GET
    data_selector: content
    params: {}
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: records
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: records
- name: datasets
  endpoint:
    path: /api/v1/datasets
    method: GET
    data_selector: datasets
    params: {}
- name: update_dataset_responsibilities
  endpoint:
    path: /api/v1/datasets/{datasetId}/responsibilities
    method: POST
    data_selector: responsibilities
    params: {}
- name: update_dataset_responsibilities
  endpoint:
    path: /api/v1/datasets/{datasetId}/responsibilities
    method: POST
    data_selector: responsibilities
    params:
      datasetId: string
- name: update_incident
  endpoint:
    path: /incidents/{incidentId}
    method: POST
    data_selector: Response
    params: {}
- name: scans
  endpoint:
    path: /api/v1/scans
    method: POST
    data_selector: scan_results
    params: {}
- name: scan_status
  endpoint:
    path: /api/v1/scans/{scan_id}
    method: GET
    data_selector: status
    params: {}
- name: scan_logs
  endpoint:
    path: /api/v1/scans/{scan_id}/logs
    method: GET
    data_selector: logs
    params: {}
- name: scan
  endpoint:
    path: /api/v1/scans
    method: POST
    data_selector: null
    params: {}
- name: scan_logs
  endpoint:
    path: /api/v1/scans/{scanId}/logs
    method: GET
    data_selector: content
    params:
      page: 0
      size: 1000
- name: scan_logs
  endpoint:
    path: /api/v1/scans/{scanId}/logs
    method: GET
    data_selector: content
    params:
      page: 0
      size: 1000
- name: user_group
  endpoint:
    path: /api/v1/usergroups
    method: GET
    data_selector: records
- name: users
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: records
- name: user_groups
  endpoint:
    path: /api/v1/userGroups
    method: GET
    data_selector: content
    params:
      size: '1000'
      page: '0'
- name: users
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: content
    params:
      size: '1000'
      page: '0'
- name: users
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: content
    params:
      size: 1000
      page: 0
- name: test_connection
  endpoint:
    path: /api/v1/test-login
    method: GET
    data_selector: organisationName
    params: {}
- name: datasets
  endpoint:
    path: /api/v1/datasets
    method: GET
    data_selector: content
- name: checks
  endpoint:
    path: /api/v1/checks
    method: GET
    data_selector: content
- name: checks
  endpoint:
    path: /checks
    method: GET
    data_selector: records
    params: {}
- name: datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: records
    params: {}
- name: ping
  endpoint:
    path: /v1//ping
    method: GET
    data_selector: No content
    params: {}
- name: last_etl_refresh
  endpoint:
    path: /v1//last_etl_refresh
    method: GET
    data_selector: LastETLRefreshResult
    params: {}
- name: get_token
  endpoint:
    path: /v1//get_token
    method: GET
    data_selector: GetTokenResult
- name: audit_trail
  endpoint:
    path: /v1/audit_trail
    method: GET
    data_selector: data
    params:
      from_datetime: '2023-10-09T09:18:16.610035'
      to_datetime: '2023-11-09T09:18:16.610057'
- name: checks
  endpoint:
    path: /checks
    method: POST
    data_selector: records
- name: datasets
  endpoint:
    path: /datasets
    method: POST
    data_selector: records
- name: checks
  endpoint:
    path: /v1//coverage/checks
    method: POST
- name: dataset_coverage
  endpoint:
    path: /v1//coverage/dataset_coverage
    method: POST
- name: dataset_coverage
  endpoint:
    path: /coverage/dataset_coverage
    method: POST
    data_selector: relative_coverage_score
    params: {}
- name: check_results
  endpoint:
    path: /v1//quality/check_results
    method: POST
    data_selector: data
    params:
      page: 1
      size: 400
- name: dataset_health
  endpoint:
    path: /v1//quality/dataset_health
    method: POST
    data_selector: data
    params:
      page: 1
      size: 400
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: data
    params: {}
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: data
    params: {}
- name: incidents
  endpoint:
    path: /v1//impact/incidents
    method: POST
    data_selector: data
    params:
      default:
        page: 1
        size: 400
        status: unresolved
        from_datetime: '2023-10-09T09:18:16.255021'
        to_datetime: '2023-11-09T09:18:16.255029'
- name: datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: data
    params: {}
- name: checks
  endpoint:
    path: /checks
    method: GET
    data_selector: data
    params: {}
- name: check_results
  endpoint:
    path: /reporting-api-v1/quality/check_results
    method: POST
    data_selector: results
    params:
      dataset_ids: ''
      from_datetime: ''
- name: dataset_health
  endpoint:
    path: /reporting-api-v1/quality/dataset_health
    method: POST
    data_selector: health_data
    params:
      dataset_ids: ''
      from_datetime: ''
- name: dim_customer
  endpoint:
    path: /data/dim_customer
    method: GET
    data_selector: records
    params: {}
- name: dim_customer
  endpoint:
    path: /data/dim_customer
    method: GET
    data_selector: records
    params: {}
- name: dim_customer
  endpoint:
    params: {}
- name: dim_product
  endpoint:
    params: {}
- name: contract_verification
  endpoint:
    path: /data-contracts/verify
    method: POST
    data_selector: results
    params: {}
- name: users
  endpoint:
    path: /users
    method: GET
    data_selector: data
- name: datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: data
- name: checks
  endpoint:
    path: /api/v1/checks
    method: GET
    data_selector: records
    params: {}
- name: datasets
  endpoint:
    path: /api/v1/datasets
    method: GET
    data_selector: records
    params: {}
- name: dim_reseller
  endpoint:
    path: /checks/dim_reseller
    method: POST
    data_selector: checks
    params: {}
- name: dim_customer
  endpoint:
    path: /checks/dim_customer
    method: POST
    data_selector: checks
    params: {}
- name: dim_department_group
  endpoint:
    path: /checks/dim_department_group
    method: POST
    data_selector: checks
    params: {}
- name: fact_internet_sales
  endpoint:
    path: /checks/fact_internet_sales
    method: POST
    data_selector: checks
    params: {}
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: records
    params: {}
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: records
    params: {}
- name: datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: datasets
- name: roles
  endpoint:
    path: /roles
    method: GET
    data_selector: roles
- name: user_groups
  endpoint:
    path: /user-groups
    method: GET
    data_selector: user_groups
- name: datasets
  endpoint:
    path: /v1/datasets
    method: GET
    data_selector: data
    params: {}
- name: checks
  endpoint:
    path: /v1/checks
    method: GET
    data_selector: data
    params: {}
- name: validity_metrics
  endpoint:
    path: /api/v1/validity_metrics
    method: GET
    data_selector: records
    params: {}
- name: dim_customer
  endpoint:
    path: checks for dim_customer
    data_selector: invalid_percent(email_address)
    params:
      valid format: email
      threshold: 0
- name: dim_employee
  endpoint:
    path: checks for dim_employee
    data_selector: invalid_count(gender)
    params:
      valid values:
      - M
      - Q
      samples columns:
      - employee_key
      - first_name
- name: schema_checks
  endpoint:
    path: /soda-cloud-api-v1/schema-checks
    method: POST
    data_selector: checks
- name: schema_evolution_checks
  endpoint:
    path: /soda-cloud-api-v1/schema-evolution-checks
    method: POST
    data_selector: checks
- name: dim_product
  endpoint:
    path: /check/dim_product
    method: POST
    data_selector: checks
    params: {}
- name: MAD Configuration
  endpoint:
    path: /mad/configuration
    method: GET
    data_selector: configuration
    params: {}
- name: metrics_observability
  endpoint:
    path: /api/v4/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: data_testing
  endpoint:
    path: /data/testing
    method: GET
    data_selector: datasets
- name: data_contracts
  endpoint:
    path: /data/contracts
    method: POST
    data_selector: contracts
- name: requests
  endpoint:
    path: /requests
    method: GET
    data_selector: requests
    params: {}
- name: organization_dashboard
  endpoint:
    path: /v1/organization/dashboard
    method: GET
    data_selector: data
    params: {}
- name: activity_section
  endpoint:
    path: /v1/activity
    method: GET
    data_selector: data
    params: {}
- name: checks
  endpoint:
    path: /checks
    method: GET
    data_selector: checks
    params: {}
- name: incidents
  endpoint:
    path: /rest-api/incidents
    method: GET
    data_selector: incidents
    params: {}
- name: dataset_attributes
  endpoint:
    path: /api/v1/datasets/{datasetId}/attributes
    method: POST
- name: dataset_responsibilities
  endpoint:
    path: /api/v1/datasets/{datasetId}/responsibilities
    method: POST
- name: Soda Core
  endpoint:
    path: /soda-v4
    method: GET
    data_selector: metadata
    params: {}
- name: Soda-hosted Agent
  endpoint:
    path: /soda-v4/deployment-options/deploy-soda-agent
    method: GET
    data_selector: metadata
    params: {}
- name: Self-hosted Agent
  endpoint:
    path: /soda-v4/deployment-options/deploy-soda-agent
    method: GET
    data_selector: metadata
    params: {}
- name: global_roles
  endpoint:
    path: /global-roles
    method: GET
    data_selector: roles
- name: dataset_roles
  endpoint:
    path: /dataset-roles
    method: GET
    data_selector: roles
- name: Dataset Roles
  endpoint:
    path: /settings/dataset-roles
    method: GET
    data_selector: roles
    params: {}
- name: Incident
  endpoint:
    path: /api/v1/incidents
    method: POST
    data_selector: incident
    params: {}
- name: Scripted REST API
  endpoint:
    method: POST
- name: notifications
  endpoint:
    path: /manage-issues/notifications
    method: POST
    data_selector: alert
- name: incidents
  endpoint:
    path: /manage-issues/incidents
    method: POST
    data_selector: incident
- name: Webhook Integration
  endpoint:
    path: /integrations/webhook
    method: POST
- name: Verify Contracts on Pull Request
  endpoint:
    path: /verify-contracts
    method: POST
    data_selector: results
- name: fetch_proposal
  endpoint:
    path: /request/fetch
    method: POST
    data_selector: proposal
    params:
      request: -r
      proposal: -p
      soda_cloud: --soda-cloud
      file: --f
- name: push_proposal
  endpoint:
    path: /request/push
    method: POST
    data_selector: proposal
    params:
      soda_cloud: -sc
      file: -f
      request: -r
      message: -m
- name: transition_request
  endpoint:
    path: /request/transition
    method: POST
    data_selector: request
    params:
      soda_cloud: -sc
      request: -r
      status: -s
- name: status
  endpoint:
    path: datasource/db/schema/dataset
    method: GET
    data_selector: columns
    params: {}
- name: contract_verification_session_result
  endpoint:
    path: /contract_verification/session/result
    method: GET
    data_selector: contract_verification_results
    params: {}
- name: contract_verification_result
  endpoint:
    path: /contract_verification/result
    method: GET
    data_selector: contract_verification_result
    params: {}
- name: check_result
  endpoint:
    path: /check/result
    method: GET
    data_selector: check_results
    params: {}
- name: event_scopes
  endpoint:
    path: /webhook/event_scopes
    method: GET
    data_selector: scopes
- name: check_evaluation
  endpoint:
    path: /checkEvaluation
    method: POST
    data_selector: checkResults
    params: {}
- name: incident_created
  endpoint:
    path: /incidentCreated
    method: POST
    data_selector: incident
    params: {}
- name: incident_updated
  endpoint:
    path: /incidentUpdated
    method: POST
    data_selector: incident
    params: {}
- name: contract_published
  endpoint:
    path: /contractPublished
    method: POST
    data_selector: contract
    params: {}
notes:
- Soda Cloud account is free for a 45-day trial.
- Ensure values are formatted as email addresses checks that all entries in the email_address
  column are formatted as name@domain.extension.
- Ensure there are no null values in the Last Name column automatically checks for
  NULL values in the last_name column.
- No duplicate phone numbers validates that each value in the phone column is unique.
- Columns have not been added, removed, or changed compares the schema of the dataset
  to the last scan result to determine if any columns were added, deleted, changed
  data type, or changed index.
- Data in this dataset is less than 7 days old confirms that the data in the dataset
  is less than seven days old.
- Soda hosts agents in a secure environment in Amazon AWS.
- Soda is SOC 2 Type 2 certified.
- Soda Library must communicate with a Soda Cloud account via API keys.
- Use cloud.us.soda.io for US region
- Soda-hosted agent is available for all newly-created accounts.
- The keys a Soda Agent uses are different from the API keys Soda Library uses to
  connect to Soda Cloud.
- Soda supports Kubernetes cluster version 1.21 or greater.
- MS SQL Server/MS Fabric with Windows Authentication does not work with Soda Agent
  out-of-the-box.
- Use https://cloud.us.soda.io for US region; use https://cloud.soda.io for EU region
- Soda uses Kubernetes Secrets as part of the Soda Agent deployment.
- 'Specify the value for soda.cloud.endpoint according to your local region: https://cloud.us.soda.io
  for the United States, or https://cloud.soda.io for all else.'
- Soda Agent communicates with Soda Cloud account using API public and private keys.
- Kubernetes cluster version 1.21 or greater is required.
- Soda uses Kubernetes Secrets for sensitive values
- Soda uses Kubernetes Secrets for sensitive values.
- Specify the value for endpoint according to your local region.
- The Soda Agent communicates with your Soda Cloud account using API public and private
  keys.
- Uses Kubernetes Secrets as part of the Soda Agent deployment.
- The value you specify for the soda-cloud-enpoint must correspond with the region
  you selected when you signed up for a Soda Cloud account
- Soda Agent does not require setting any inbound rules as it only polls Soda Cloud
  looking for instruction, which requires only outbound communication
- Soda Agent communicates with your Soda Cloud account using API public and private
  keys.
- Soda Agent does not require setting any inbound rules as it only polls Soda Cloud
  looking for instruction, which requires only outbound communication.
- Uses API public and private keys for authentication.
- Kubernetes cluster must be version 1.21 or greater.
- Container images required for the self-hosted Soda agent will be distributed using
  private registries, hosted by Soda.
- Existing or new Soda cloud API keys can be used to authenticate to the Soda-hosted
  registries.
- Starting from version 1.2.0, the soda-agent Helm chart offers support for working
  with Soda-hosted image registries.
- As of July 2025, the container images required for the self-hosted Soda agent will
  be distributed using private registries, hosted by Soda.
- By default, Soda uses Kubernetes Secrets as part of the Soda Agent deployment.
- Uses Kubernetes Secrets for sensitive values
- Refresh interval set to 1m for reconciling remoteRef values.
- The hard query cursor limit setting controls how many rows Soda Library can store
  in memory during a scan.
- If you need to work with larger sets of sample data or failed rows, you can raise
  the `query_cursor_hard_limit`.
- Use cloud.soda.io for EU region; use cloud.us.soda.io for US region.
- Soda Library pushes scan results to Soda Cloud.
- Changing the scan definition name affects how results are recorded in Soda Cloud.
- 'Soda-hosted agents are only compatible with the following data sources: BigQuery,
  Databricks SQL, MS SQL Server, MySQL, PostgreSQL, Redshift, Snowflake.'
- When you migrate to a Soda-hosted agent, Soda Cloud resets all the connection configuration
  details for your data source.
- No downtime associated with the exercise of upgrading a self-hosted Soda Agent.
- Best practice dictates that you set your cluster to have at least 2 CPU and 2GB
  of RAM.
- Soda API requires OAuth2 authentication.
- Uses API keys for authentication.
- Ensure to create a Soda Cloud account to access scan results.
- Check warns when any NULL values exist in the column
- Soda Cloud account gives access to visualized scan results, tracks trends in data
  quality over time, enables you to set alert notifications.
- The value for `scan-name` must be unique to every programmatic scan you define.
- Soda officially supports Python versions 3.8, 3.9, and 3.10.
- Sensitive data source login credentials and Soda Cloud API key values are fetched
  from an Azure Key Vault.
- Data Engineers run Soda data quality checks before copying the data to a Redshift
  data source.
- Soda checks that fail route failed row samples back to their own S3 bucket.
- Use cloud.us.soda.io for US regions; use cloud.soda.io for European region.
- Use cloud.soda.io for EU region
- Use Soda to run reconciliation checks on both the source and target data sources
  to validate that the data has been transformed and loaded as expected.
- Requires setup of Soda Cloud account and API keys.
- Sensitive credentials should be configured using GitHub secrets.
- Soda sends alert notifications via email by default.
- This setup provides a secure, out-of-the-box Soda-hosted Agent to manage access
  to data sources from within your Soda Cloud account.
- A self-hosted Soda Agent is a tool that empowers Soda Cloud users to securely access
  data sources to scan for data quality.
- Follow the Sigma documentation to Connect to Snowflake.
- Follow Sigma documentation to access the metadata you stored in Snowflake, either
  by Modeling data from database tables, or Creating a dataset by writing custom SQL.
- Create a new workbook in Sigma where you can create your visualizations.
- Requires access to a Grafana account
- Requires access to a PostgreSQL data source
- Requires a Soda Cloud account
- If you prefer to avoid building psycopg2 from source, please install the PyPI 'psycopg2-binary'
  package instead.
- Requires setup of Soda Cloud account and appropriate permissions
- Python libraries requests and psycopg2 are needed to connect
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Free for a 45-day trial
- Use Terraform to set up and configure a local Kubernetes cluster
- Deploy External Secrets Operator to integrate with Hashicorp Vault
- Uses external secrets manager configuration to fetch username and password
- Soda does not collect sample rows of data by default.
- Dataset profiling can be resource-heavy, so carefully consider the datasets for
  which you truly need column profile information.
- By default, Soda Library implicitly pushes samples of any failed rows to Soda Cloud
  for missing, validity, duplicate, and reference checks.
- Uses API key for authentication.
- Double-onboarding a data source renders Soda Library API keys invalid. After double-onboarding
  a data source, if you run a programmatic or CLI scan of that data source using Soda
  Library, an error appears to indicate that the API keys are invalid. As a workaround,
  generate new API keys in Soda Cloud, then, in your configuration YAML, replace the
  old API key values with the newly-generated ones.
- Requires Soda Core Scientific
- Requires Soda Core
- Requires Soda Library + Soda Cloud
- Requires Soda Agent + Soda Cloud
- A schema check requires a minimum of two measurements before it yields a useful
  check result.
- Some checks require Soda Core Scientific
- Some checks supported in Soda Core
- Supported in Soda Library + Soda Cloud
- Requires Soda Core Scientific (included in a Soda Agent)
- Some available as no-code checks
- Soda Checks Language (SodaCL) is a YAML-based, domain-specific language for data
  reliability.
- 'Get your logic straight: your check defines a passing state, what you expect to
  see in your dataset. Do not define a failed state.'
- Take careful note of the data type of the column against which you run a check.
  For example, if numeric values are stored in a column as data type TEXT, a numeric
  check such as min or avg is incalculable.
- A check that uses alert configurations only ever returns one check result. See Expect
  one check result.
- The invalid format configuration key only works with data type TEXT. See Specify
  valid format.
- Not all checks support in-check filters.
- Soda Checks Language (SodaCL) is a YAML-based language for data reliability.
- 'Get your logic straight: your check defines a *passing* state, what you expect
  to see in your dataset. Do not define a failed state.'
- Take careful note of the data type of the column against which you run a check.
  For example, if numeric values are stored in a column as data type TEXT, a numeric
  check such as `min` or `avg` is incalculable.
- A check that uses [alert configurations](/sodacl-reference/optional-config#add-alert-configurations)
  only ever returns *one* check result. See [Expect one check result](/sodacl-reference/optional-config#expect-one-check-result).
- The `invalid format` configuration key only works with data type TEXT. See [Specify
  valid format](/sodacl-reference/validity-metrics#specify-valid-format).
- Not all checks support in-check filters. See [List of compatible metrics and checks](/sodacl-reference/filters#list-of-compatible-metrics-and-checks).
- To avoid typos or spelling errors, best practice dictates that you copy + paste
  any dataset or column names into your checks.
- It is good practice to add a [custom name](/sodacl-reference/numeric-metrics) to
  your check. Establish a naming convention – word order, underscores, identifiers
  – and apply easily-digestible check names for any colleagues with whom you collaborate.
- Be sure to add a colon to the end of a check whenever you add a second line to a
  check such as for a missing or invalid configuration key, or if you add a [custom
  name](/sodacl-reference/numeric-metrics) for your check.
- Indentations in the SodaCL syntax are critical. If you encounter an error, check
  your indentation first.
- 'Spaces in the SodaCL syntax are critical. For example, be sure to add a space before
  and after your threshold symbol ( `=`, `>`, `>=` ); do *not* add a space between
  a metric and the column to which it applies, such as `duplicate_count(column1)`. '
- All comma-separated values in lists in SodaCL use a comma + space syntax, such as
  `duplicate_count(column1, column2)`; do not forget to add the space.
- Note that multi-word checks such as `missing_count` use underscores, but configuration
  keys, such as `missing regex`, do not. See [List of missing metrics](/soda-cl-overview/quick-start-sodacl#missing-and-invalid-checks)
  and [List of validity metrics](/soda-cl-overview/quick-start-sodacl).
- If you use `missing values` or `invalid values` configuration keys, note that values
  in a comma-separated list must be enclosed in square brackets. For example, `[US,
  BE, CN]`.
- Column names that contain colons or periods can interfere with SodaCL’s YAML-based
  syntax. For any column names that contain these punctuation marks, [apply quotes](/sodacl-reference/optional-config#use-quotes-in-a-check)
  to the column name in the check to prevent issues.
- Check that there are no null values in a column
- If the type of data a dataset contains is TEXT (string, character varying, etc.),
  you can use an invalid metric to surface any rows that contain ill-formatted data.
- If you want to surface more than just null values as missing, you can specify a
  list of values that qualify as missing.
- Soda Checks Language (SodaCL) is used for data reliability.
- You can run a scan to execute your checks.
- Check an email column that all values are in email format
- Check that fewer than 5% of values in column contain missing values
- Check that values in a column exist in another column in a different dataset
- Check that values in two columns exist in two other columns in a different dataset
- Check for any schema changes to dataset
- Check for absent or forbidden columns in dataset
- Spaces in the SodaCL syntax are critical. For example, be sure to add a space before
  and after your threshold symbol ( `=`, `>`, `>=` ); do *not* add a space between
  a metric and the column to which it applies, such as `duplicate_count(column1)`.
- Supported in Soda Core, Soda Library + Soda Cloud, and Soda Cloud + Soda Agent
- Compatible with BigQuery, PostgreSQL, Snowflake data sources
- Add quotes to dataset names that Soda acts upon automatically.
- Supported in Soda Cloud + self-hosted Soda Agent connected to any Soda-supported
  data source, except Spark, and Dask and Pandas
- Supported in Soda Cloud + Soda-hosted Agent connected to a BigQuery, Databricks
  SQL, MS SQL Server, MySQL, PostgreSQL, Redshift, or Snowflake data source
- Soda can only profile columns that contain NUMBERS or TEXT type data; it cannot
  profile columns that contain TIME or DATE data except to create a freshness check
  for the anomaly dashboard.
- Column profiling can be resource-heavy, so carefully consider the datasets for which
  you truly need column profile information.
- Supported in Soda Core
- Supported in Soda Cloud Agreements + Soda Agent
- User-defined checks and failed rows checks enable you to define your own metrics.
- You can use a user-defined metric to compare date values in the same dataset.
- Verify that, if there is an update date, it is greater than the creation date.
- Soda uses the input in the checks and data source connection configurations to prepare
  a scan that it runs against the data in a dataset.
- By default, Soda collects up to 100 failed row samples for any check that implicitly
  or explicitly.
- By default, Soda collects up to 100 failed row samples for any check that implicitly
  or explicitly and displays them in a check’s Check History page in Soda Cloud.
- Disabling failed row samples for datasets and columns via data source exclude_columns
  configuration prevents Soda from displaying any failed row samples for checks that
  implicitly collect samples.
- Disabling sampling for checks via sampling parameters specifies sampling instructions,
  or prevents/allows sampling for individual checks.
- When excluding columns from failed rows sampling, those columns are not included
  in queries.
- Soda does not collect samples for excluded columns.
- By default, Soda sets the scan timeout to two hours.
- Soda compute occurs almost solely in the data source and it runs queries to gather
  metrics in the data source, and then only evaluates the outcome of the metrics in
  Python.
- For user-defined failed rows queries, Soda executes the query as provided, so if
  user includes select * … , then Soda loads data in Python.
- For record-level reconciliation checks, Soda loads all data into memory, but only
  one row at a time (or a defined batch of rows based on configuration).
- Soda caches the response from the Slack API, refreshing it hourly
- Supported in Soda Cloud + self-hosted Soda Agent connected to BigQuery, Databricks
  SQL, MS SQL Server, MySQL, PostgreSQL, Redshift, or Snowflake data source
- Soda anomaly dashboard does not profile columns that contain timestamps or dates.
- To define new check attributes, you must have the permission to do so on your Soda
  Cloud account.
- You cannot use variables in checks you write in an agreement in Soda Cloud as it
  is impossible to provide the variable values at scan time.
- Soda Cloud establishes two notification rules by default.
- Admins can access an Audit Trail of user actions.
- Soda Cloud uses roles and access permissions to manage user actions.
- Soda Cloud uses roles, groups, and access permissions for dataset-level permissions.
- By default, the user who added the data source becomes the Data Source Owner and
  Dataset Owner of all datasets in that data source.
- Soda Cloud automatically assigns the role of Manager to the new Dataset Owner.
- Integrates with various tools for data quality checks.
- Supports SSO for secure access.
- You have completed at least one Soda scan to validate that the data source’s datasets
  appear in Soda Cloud as expected.
- You have an Atlan account with the privileges necessary to allow you to set up a
  Connection in your Atlan workspace.
- The Action does not yet support sending notifications via Slack, only email.
- Windows runners are not supported, including the use of official Windows-based images
  such as windows-latest
- MacOS runners require Docker installation because the macos-latest does not come
  with Docker pre-installed.
- Webhook integration to connect Soda with Jira is required.
- 'Authorization header value must be formatted like: Basic <base64_encoded_credentials>'
- Use the Alert Notification scope to enable Soda Cloud to send alert notifications
  to an MS Teams channel.
- Use the Incident scope to notify your team when a new incident has been created
  in Soda Cloud.
- Use the Discussions scope to post to a channel when a user creates or modifies a
  Soda Cloud discussion.
- Soda caches the response from the Slack API, refreshing it hourly.
- Soda Cloud supports both Identity Provider Initiated (IdP-initiated), and Service
  Provider Initiated (SP-initiated) single sign-on integrations.
- Soda Cloud supports both Identity Provider Initiated (IdP-initiated), and Service
  Provider Initiated (SP-initiated) single sign-on integrations; be sure to indicate
  which type of SSO your organization uses.
- Can return an HTTP status code between 200 and 400
- Can reply to a request within 10 seconds (otherwise the request from Soda Cloud
  times out)
- Provides an SSL-secured endpoint (https://) of TLS 1.2 or greater
- Avoid applying the same customized check names in multiple agreements.
- Check identity acts as a correlation key so Soda Cloud can associate results with
  the correct check even if the check definition changes.
- Anomaly detection checks are deprecated.
- Requires Soda Core Scientific (included in a Soda Agent).
- Anomaly detection check requires at least four measurements before it can start
  detecting anomalies.
- Measurements don't count if the frequency of occurrence is too random.
- This check is being deprecated.
- Soda Scientific is included in Soda Agent deployment.
- 'Anomaly Detection Frequency Warning: Coerced into daily dataset with last daily
  time point kept'
- Data frame must have at least 4 measurements
- Set the value of the profile parameter to coverage to tolerate small noises in the
  data.
- Use the dynamic tuning configuration only if necessary.
- Requires Soda Core Scientific for anomaly check (included in a Soda Agent)
- Supported in Soda Library 1.1.27 or greater + Soda Cloud
- Supported in Soda Cloud Agreements + Soda Agent 0.8.57 or greater
- Anomaly score checks are deprecated. Soda recommends using the new Metric Monitoring
  features.
- Requires Soda Library
- Not yet supported in Soda Cloud
- 'Known issue: Dataset filters are not compatible with user-defined metrics in check
  templates.'
- The data sources do not need to be the same type; you can compare a dataset in a
  PostgreSQL data source to a dataset in a BigQuery data source.
- Some objects like Contact may return nulls in deeply nested fields
- Best practice recommends installing Soda Library and Soda Scientific in a virtual
  environment to avoid library conflicts.
- Before executing the command, examine the volume of data the column(s) contains
  and ensure that your system can accommodate storing it in local memory.
- Distribution checks will no longer be supported in Soda v4; they will deprecated
  and replaced by MAD.
- For continuous columns, Soda stores up to one million records in local memory.
- If there are more than one million distinct categories, Soda skips the distribution
  check and issues a warning.
- Available as a no-code check with a self-hosted Soda Agent connected to any Soda-supported
  data source, except Spark, and Dask and Pandas OR with a Soda-hosted Agent connected
  to a BigQuery, Databricks SQL, MS SQL Server, MySQL, PostgreSQL, Redshift, or Snowflake
  data source
- 'Known issue: Dataset filters are not compatible with failed rows checks which use
  a SQL query. With such a check, Soda does not apply the dataset filter at scan time.'
- Some objects may return nulls in deeply nested fields
- Soda dataset names matching is case insensitive.
- For each is not compatible with dataset filters.
- Only works with columns that contain data types TIMESTAMP or DATE
- Use quotes when identifying dataset or column names; the type of quotes you use
  must match that which your data source uses.
- The only comparison symbol that you can use with freshness checks that employ an
  alert configuration is '>'.
- Uses a group by configuration to customize the group of data quality check results
  by category.
- Be aware that a check that contains one or more alert configurations only ever yields
  a single check result; one check yields one check result.
- This feature is not supported in Soda Core OSS.
- Soda Library 1.0.x
- Soda Core 3.0.x
- Requires setup of connected app in Soda
- SodaCL considers NULL as the default value for 'missing'.
- You can set the `samples limit` to `0` to prevent Soda from collecting and sending
  failed rows samples for an individual check.
- Some available as a no-code check with a self-hosted Soda Agent connected to any
  Soda-supported data source, except Spark, and Dask and Pandas
- Uses numeric metrics in checks with alert configurations to establish warn and fail
  zones
- Requires Soda Library 1.2.0 or greater
- Best practice dictates that you first configure and run metric reconciliation checks,
  then use the output to write refined record reconciliation checks to fine-tune the
  comparison.
- Available as a no-code check
- To efficiently use resources at scan time, best practice dictates that you first
  configure and run metric reconciliation checks, then use the output to write refined
  record reconciliation checks to fine-tune the comparison.
- If primary keys exist in your dataset, best practice recommends that you use a simple
  strategy for executing a record-by-record comparison.
- The default number of failed row samples that Soda collects and displays is 100.
- Best practice dictates that you add filters when using record reconciliation checks
  to mitigate heavy memory usage and long scan times when performing record-to-record
  comparisons of data.
- Some API responses may vary depending on the endpoint
- If the data type of the column you are checking is TEXT (such as character, character
  varying, or string) then you can use the `valid format` configuration key.
- For security, you can add a configuration to your data source connection details
  to prevent Soda from collecting failed rows samples from specific columns that contain
  sensitive data.
- If your check involves a threshold that compares relative values, Soda needs a value
  for a previous measurement before it can make a comparison.
- Column names that contain colons or periods can interfere with SodaCL’s YAML-based
  syntax.
- When using single-quoted strings, any single quote inside its contents must be doubled
  to escape it.
- In Databricks, when dealing with column names that start with numbers or contain
  special characters such as spaces, you typically need to use backticks to enclose
  the column identifier.
- Uses OAuth2 with refresh token — requires setup of connected app in Soda
- Some responses may contain nulls in certain fields
- The table in BigQuery is configured to require partitioning.
- Soda offers indirect support for ClickHouse data sources using the soda-mysql package.
- You do not need to configure a connection to a data source; you must still configure
  a connection to Soda Cloud using API Keys.
- Prior to `soda-pandas-dask` version 1.6.4, Soda only supported `dask-sql` versions
  up to `2023.10`.
- Upgrade soda-pandas-dask to version 1.6.4 or greater and use the optional use_dask_count_star_as_count_one=True
  parameter when calling scan.add_dask_dataframe() or scan.add_pandas_dataframe()
  to persist old dask-sql behavior.
- 'Upgrade soda-pandas-dask to version 1.6.4 or greater and use the dask.config.set({"dataframe.convert-string":
  False}) parameter set to False to avoid KeyError: string[pyarrow] errors.'
- Some users have reported issues using the database key, but have been successful
  using path instead.
- Consider using system variables to retrieve username and password securely.
- Default port is 1523 if not specified.
- Use one authentication method per connection.
- While role is technically optional, providing it avoids confusing access errors.
- You can pass any other Snowflake parameters you wish by adding the key:value pairs
  to your Snowflake connection configuration.
- When testing the connection to your Snowflake data source, Snowflake returns an
  error message about using the `use database` command.
- 'When Soda attempts to connect to your Snowflake data source, it produces a connectivity
  error that includes something like `RunteimError: Command failed with exit code
  2: ..... ocsp_response_validation_cash.lock`.'
- Be aware that, by default, Snowflake returns columns names in uppercase.
- When connecting to a Snowflake data source by proxy, be sure to set the new proxy
  environment variables from the command-line using export statements.
- Unlike other data sources, Soda Library for SparkDF does not require a configuration
  YAML file to run scans against Spark DataFrames.
- Use pip install pip-system-certs to potentially resolve SSL certificate errors on
  Windows machines.
- Be sure to upgrade your version of PySpark to 3.4.0 or greater for compatibility
  with Soda packages.
- Soda requires pydantic v2 to work.
- This API fully replaces the existing responsibilities. The entire list of responsibilities
  must be provided in the request, including both new and existing entries, as any
  omissions will result in their removal.
- 'User authentication required: true'
- Any Soda Cloud user in your organization may execute this query.
- 'Rate limiting: 60 requests/60 seconds'
- Supported in Soda Cloud + Soda Library
- Supported in Soda Cloud + Soda Agent
- ApiKey authentication is strongly recommended.
- HTTPBasic and TokenAuth are up for deprecation.
- Requires setup of OAuth2 for secure access
- Ensure to set up the OAuth2 credentials properly.
- API requires OAuth2 authentication.
- Most endpoints use pagination.
- The reporting API refreshes the data for all endpoints once per day between 10:00pm
  and 11:00pm EST.
- Soda data contracts checks do not use SodaCL.
- Data contracts enforce data quality standards in a data pipeline.
- Experimentally supported in Soda Core 3.3.3 or greater for PostgreSQL, Snowflake,
  and Spark
- Every check must have a unique identity
- Duplicate check identity indicates two checks exist with the same type and name,
  or same type and no name
- Changing or adding a name to a data contract check is considered a new check and
  discards previous check result's history
- Ensure to set up OAuth2 credentials in the Soda dashboard.
- Uses OAuth2 with refresh token
- Soda utilizes user-defined input to prepare SQL queries to find bad data, visualize
  results, set up alerts, and track dataset health over time.
- Soda Library does not send data to Soda Cloud; it only ever pushes metadata to the
  cloud.
- Soda Library is a Python library and command-line tool that serves as the backbone
  of Soda technology.
- Kubernetes is a system for orchestrating containerized applications.
- Soda Library only pushes metadata to Soda Cloud.
- Soda encrypts the secrets you provide via Soda Cloud both in transit and at rest.
- If your data source accepts allowlisted IP addresses, add the Soda Cloud IP address
  to the allowlist to access your data sources via the Soda-hosted Agent.
- Soda Library collects usage statistics by default.
- 'You can opt-out from sending Soda Library usage statistics at any time by adding
  the following to your ~/.soda/config.yml or .soda/config.yml file: send_anonymous_usage_stats:
  false'
- Soda offers free support to the Soda community of users in Slack.
- This code of conduct applies to our community’s spaces.
- Soda Core is an open-source library and CLI tool that enables you to use the Soda
  Checks Language to turn user-defined input into SQL queries.
- Requires OAuth2 authentication with refresh token.
- Released in Preview state, you can now deploy a Soda Agent in an Azure Kubernetes
  Service cluster.
- Deploy a Soda Agent in an Azure Kubernetes Service cluster.
- Deploy a Soda Agent in a Google Kubernetes Engine cluster.
- Introducing the general availability of the new self-serve features in Soda Cloud.
- Soda v3
- Fix using known partition column for v4 datasets
- 'Soda Library supports SodaCL’s newest checks: Group By and Group Evolution.'
- Soda Library supports Check Suggestions, a helpful CLI tool that assists you in
  generating basic data quality checks.
- Upgrade to 1.0.0
- Soda Agent 1.0.0 removes idle mode in the scan-launcher.
- Soda Agent 1.0.0 no longer uses logging sidecars.
- Soda Agent 1.0.0 changes the default value for soda.polling.interval to 5 seconds.
- Soda Agent 1.0.0 does not use Kubernetes Cron jobs for executing scans.
- Soda Agent 1.0.0 favors managed or self-managed node groups over AWS Fargate, AKS
  Virtual Nodes, or GKE Autopilot profiles.
- Uses OAuth2 with refresh token — requires setup of connected app in Soda Cloud
- Soda Cloud enables visualization of data quality test results and setting alerts.
- Soda Cloud now supports no-code check creation.
- Soda Cloud users may set up multiple Soda Cloud organizations for use with different
  environments in a network infrastructure, such as staging and production.
- Uses OAuth2 with refresh token — requires setup of connected app in Soda.
- Soda Core API v1 includes Authentication, Examples, Checks, Datasets, Incidents,
  Scans, Users, Utility, Models.
- Soda Cloud Reporting API v1 includes Authentication, Examples, Status, Auditing,
  Coverage, Quality, Platform Impact, Models.
- Refer to the Soda Core Release Notes for details.
- Soda’s anomaly detection engine was built in-house (no third-party libraries) and
  optimized for high precision.
- Designed to minimize false positives and missed detections, it shows a 70% improvement
  in detecting anomalous data quality metrics compared to Facebook Prophet across
  hundreds of diverse, internally curated datasets containing known data quality issues.
- SodaCL considers NULL as the default value for 'missing'
- You can set the samples limit to 0 to prevent Soda from collecting failed rows samples
  for an individual check.
- Available as a no-code check with a self-hosted Soda Agent connected to any Soda-supported
  data source, except Spark, and Dask and Pandas
- Apply the check to any dataset that begins with retail.
- Use quotes when identifying dataset or column names.
- Observability is a fast and efficient way to get initial coverage.
- Requires minimal configuration to get started.
- Metric Monitoring is developed to be a hassle-free feature.
- Unlock organization‐wide observability through Soda Cloud’s no-code dataset onboarding.
- Soda establishes a statistical baseline for each metric and continually compares
  new scan results against that baseline.
- Soda's Observability tools work out of the box with predefined baselines.
- Exclusions are not supported for schema-drift monitors.
- MAD is enabled during data source onboarding.
- Monitor schedule runs daily, executing once per day.
- Enable or disable backfilling to generate Multivariate Drift Scores for historical
  data.
- OAuth2 authentication is required for API access.
- Non-UTC timestamps are not recommended when connecting Soda to Oracle data sources.
- Soda uses timezone data when available, but assumes UTC when the timezone is not
  provided by the data source.
- Data contracts can be authored in both UI and YAML.
- Git-managed contracts offer a code-first way to define and enforce data quality
  expectations.
- Faster time to value – no setup required
- Accessible to everyone – empower domain experts, not just engineers
- Built for collaboration – share, comment, and propose changes in a shared UI
- Easily operationalized – schedule tests and trigger verifications programmatically
- Each request in Soda has a status to reflect its lifecycle.
- Participants are automatically notified by email when a new proposal is created
  or an iteration is made.
- Reconciliation checks are a validation step used to ensure that data remains consistent
  and accurate.
- Row-level reconciliation is critical in scenarios where accuracy at the record level
  is non-negotiable.
- Row-level reconciliation is inherently heavier than metric-level reconciliation,
  as it requires comparing records across potentially large datasets.
- A paginated approach is used to maintain scalability; this ensures that memory usage
  remains stable, but execution time will increase as the dataset size and column
  count grow.
- Soda uses the official Python packages for each supported data source.
- Secrets are encrypted and securely stored in Soda Cloud.
- Selecting the right scan time is essential for accurate data monitoring and reliable
  metric collection.
- 'Consistency is key: Running scans at the same time every day allows to build up
  a reliable baseline of expected behavior.'
- Custom dashboards can be built using the Soda REST API.
- Only users with the Manage Attributes permission can create or edit attributes.
- Notification rules define when and to whom a notification is sent.
- Every dataset has a default Dataset Owner role, which cannot be removed.
- Soda Core is required for in-memory sources like Spark and Pandas DataFrames.
- Soda-hosted Agent requires no setup or management.
- Soda-hosted agents are included in all Free, Team, and Enterprise plans at no additional
  cost.
- Self-hosted agents require an Enterprise plan.
- EU cloud customers will use the EU registry located at registry.cloud.soda.io.
- US cloud customers will use the US registry located at registry.us.soda.io.
- If you are a customer using the US instance of Soda Cloud, you'll have to configure
  your Agent setup accordingly.
- Only users with the Manage Organization Settings global role can access and modify
  these settings.
- Invited users will receive an email with a link to set their password and join your
  organization in Soda Cloud.
- Deactivating a user blocks their access to Soda Cloud and disables any existing
  API keys associated with their account.
- By default, there is an Everyone group which is not editable and contains all the
  users from the organization
- Soda Cloud blocks all non-SSO login attempts and password changes once SSO is enabled.
- Soda Cloud uses Global Roles and Dataset Roles to manage access and permissions.
- By default, all dataset owners have the 'Manager' role.
- The 'Everyone' group is assigned as a 'Viewer' for all new datasets.
- You have verified some contracts and published the results to Soda Cloud.
- You have a Purview account with the privileges necessary to collect the information
  Soda needs to complete the integration.
- Webhook integration to connect Soda Cloud with Jira for incident tracking.
- Ensure that contract changes in PRs are valid and do not break expectations before
  merging.
- 'Make sure these are set in your repository’s GitHub Secrets: DATASOURCE_USERNAME,
  DATASOURCE_PASSWORD'
- These secrets can be customized depending on the data source type and your needs.
- Currently, contract creation only works with --use-agent.
- A Soda data contract is a YAML document that contains data quality checks.
- If contract verification fails, notifications can be sent out or the new data can
  be stopped from being released to users.
- Use one of the following threshold configuration keys to specify an open range.
- The metric value must be equal to the configured value.
- Each configuration must include `type`, `name`, and a `connection` block.
- Use the exact structure required by the underlying Python driver.
- Be first to try Soda's new AI-powered metrics observability, and collaborative data
  contracts.
- Use API keys to authenticate access if your organization employs Single Sign On
  (SSO) to access Soda Cloud.
- Enable or disable verbose logging
- Uses Contract Verification to aggregate results for one or more contracts.
- Webhook integrations allow your system to receive real-time notifications from Soda
  Cloud when certain events occur, such as check evaluations, incident changes, or
  contract updates.
- Introduced automatic partition column detection.
- Introduced support for metric monitoring (both dataset- and column-level monitors).
- Introduced sampling strategy for dataset profiling.
- Introduced support for primary key detection.
- Introduced support for Fabric data source.
- Implemented new dataset onboarding process to easily onboard datasets with metric
  monitoring
- Introduced Data Contracts to formalize requirements and expectations of user datasets
errors:
- 'Connection failed: Check your API keys or data source configuration.'
- No duplicate phone numbers [FAILED]
- Data is fresh [FAILED]
- 'UnauthorizedOperation: You are not authorized to perform this operation.'
- '401 Unauthorized: Recheck API key values'
- '0: all checks passed'
- '1: Soda issues a warning on a check(s)'
- '2: Soda issues a failure on a check(s)'
- '3: Soda encountered a runtime issue, and was able to submit scan results to Soda
  Cloud'
- '4: Soda encountered a runtime issue, but was unable to submit any results to Soda
  Cloud'
- '401 Unauthorized: Check your API key and secret.'
- '400 Bad Request: Check your API key or request format.'
- '401 Unauthorized: Recheck your API key and permissions.'
- '401 Unauthorized: Recheck API key or token expiration'
- '401 Unauthorized: Check API keys or permissions.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- Be sure to add a colon to the end of a check whenever you add a second line to a
  check such as for a missing or invalid configuration key, or if you add a custom
  name for your check.
- Indentations in the SodaCL syntax are critical. If you encounter an error, check
  your indentation first.
- Spaces in the SodaCL syntax are critical. For example, be sure to add a space before
  and after your threshold symbol (=, >, >=); do not add a space between a metric
  and the column to which it applies.
- All comma-separated values in lists in SodaCL use a comma + space syntax.
- If you use missing values or invalid values configuration keys, note that values
  in a comma-separated list must be enclosed in square brackets.
- 'Known issue: Currently, SodaCL does not support column exclusion for the column
  profiling and dataset discovery configurations when connecting to a Spark DataFrame
  data source.'
- 'Known issue: SodaCL does not support using variables in column profiling and dataset
  discovery configurations.'
- Ephemeral scan results do not persist historical measurements
- Ephemeral scan results cannot send notifications according to Notification Rules
- 'NOT EVALUATED: Not enough historical data to detect anomalies.'
- Sample size limit of one million rows is based on simulations that used the Kolmogorov-Smirnov
  test.
- Invalid staleness threshold "when < 3256d"
- 'Invalid check "freshness(start_date) > 1d": no viable alternative at input '' >'''
- 'Invalid check "freshness(end_date) ${NOW} < 1d": mismatched input ''${NOW}'' expecting
  {''between'', ''not'', ''!='', ''<>'', ''<='', ''>='', ''='', ''<'', ''>''}'
- 'Evaluation of check group by failed: Total number of groups 11 exceeds configured
  group limit: 2'
- '404 Not Found: Verify the endpoint and parameters'
- 400 Cannot query over table 'event_logs' without a filter over column(s) 'serverTimestamp'
  that can be used for partition elimination.
- 'Failed to connect to DB: mydb.eu-west-1.snowflakecomputing.com:443. Incoming request
  with IP/Token xx.xxx.xx.xxx is not allowed to access Snowflake.'
- 'Could not connect to data source "name_db": 250001 (08001): Failed to connect to
  DB: mydb.eu-west-1.snowflakecomputing.com:443. Incoming request with IP/Token xx.xxx.xx.xxx
  is not allowed to access Snowflake.'
- 'ImportError: cannot import name ''field_validator'' from ''pydantic''...'
- Pre-scan validation failed, see logs for details.
- '401 Unauthorized: Recheck credentials'
- 400 Bad request
- 401 Unauthorized
- 403 Forbidden
- 404 Not found
- 429 Too many requests
- 500 Internal server error
- '400: Bad request'
- '401: Unauthorized'
- '403: Forbidden'
- '404: Not found'
- '429: Too many requests'
- '500: Internal server error'
- '401 Unauthorized: Please check your API keys and/or permissions in Soda.'
- '403 Forbidden: Please check your API keys and/or permissions in Soda.'
- '429 API Rate Limit reached: Throttle API calls or reduce frequency'
- '404 Not Found: Verify endpoint path'
- '401 Unauthorized: Recheck username and password'
- '200: Successful Response'
- '422: Validation Error'
- '400 Bad Request: Check the request payload for required fields.'
- '401 Unauthorized: Verify OAuth token validity.'
- '429 Too Many Requests: Implement retry logic.'
- '401 Unauthorized: Check client credentials or token validity.'
- '404 Not Found: Verify endpoint paths.'
- '401 Unauthorized: Check your OAuth token.'
- '404 Not Found: Verify the endpoint path.'
- '429 Too Many Requests: Throttle API calls or reduce frequency'
- Ensure egress IP addresses are allowed in your firewall settings to avoid disruptions
  in receiving events from Soda Cloud.
- '401 Unauthorized: Check your client credentials and token.'
- '429 Too Many Requests: Implement exponential backoff'
- '401 Unauthorized: Check your API token or credentials.'
- '404 Not Found: The requested resource could not be found.'
- '401 Unauthorized: Check OAuth credentials or token expiration.'
auth_info:
  mentioned_objects:
  - API keys
  - mssparkutils
  - api_key_id
  - api_key_secret
  - SodaCore
  - SodaAgent
  - SODA_CLOUD_API_KEY
  - SODA_CLOUD_API_SECRET
  - SNOWFLAKE_USERNAME
  - SNOWFLAKE_PASSWORD
  - OauthToken
  - AuthProvider
  - ApiKey
  - HTTPBasic
  - TokenAuth
  - NamedCredential
client:
  base_url: https://cloud.soda.io
  auth:
    type: apikey
    location: header
    header_name: Authorization
  headers:
    Accept: application/json
source_metadata: null
