resources:
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: topics
- name: topic_metadata
  endpoint:
    path: /topics/{topic_name}
    method: GET
    data_selector: name
- name: produce_messages
  endpoint:
    path: /topics/{topic_name}
    method: POST
    data_selector: offsets
- name: partitions
  endpoint:
    path: /topics/{topic_name}/partitions
    method: GET
    data_selector: partitions
    params: {}
- name: partition_metadata
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: partition
    params: {}
- name: partition_offsets
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}/offsets
    method: GET
    data_selector: offsets
    params: {}
- name: produce_messages
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}
    method: POST
    data_selector: offsets
    params: {}
- name: consumers
  endpoint:
    path: /consumers/
    method: POST
    data_selector: instance_id
- name: consumer_positions
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions
    method: POST
- name: consumer_positions_beginning
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions/beginning
    method: POST
- name: consumer_positions_end
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions/end
    method: POST
- name: consumer_records
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/records
    method: GET
- name: brokers
  endpoint:
    path: /brokers
    method: GET
    data_selector: brokers
- name: cells
  endpoint:
    path: /clusters/{cluster_id}/cells
    method: GET
    data_selector: cells
    params:
      cluster_id: string
- name: clusters
  endpoint:
    path: /clusters
    method: GET
    data_selector: data
    params: {}
- name: cluster_by_id
  endpoint:
    path: /clusters/{cluster_id}
    method: GET
    data_selector: metadata
    params: {}
- name: broker_configs
  endpoint:
    path: /clusters/{cluster_id}/broker-configs
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: broker_configs_alter
  endpoint:
    path: /clusters/{cluster_id}/broker-configs:alter
    method: POST
    data_selector: data
    params:
      cluster_id: string
- name: broker_config
  endpoint:
    path: /clusters/{cluster_id}/broker-configs/{name}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      name: string
- name: update_broker_config
  endpoint:
    path: /clusters/{cluster_id}/broker-configs/{name}
    method: PUT
    data_selector: data
    params:
      cluster_id: string
      name: string
- name: broker_configs
  endpoint:
    path: /clusters/{cluster_id}/broker-configs/{name}
    method: DELETE
    data_selector: metadata
    params: {}
- name: dynamic_broker_configs
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/configs
    method: GET
    data_selector: data
    params: {}
- name: broker_specific_configs
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs
    method: GET
    data_selector: data
    params: {}
- name: Reset Broker Config
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs/{name}
    method: DELETE
    data_selector: null
    params: {}
- name: List Topic Configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs
    method: GET
    data_selector: null
    params: {}
- name: Batch Alter Topic Configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs:alter
    method: POST
    data_selector: null
    params: {}
- name: list_topic_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/-/configs
    method: GET
    data_selector: data
    params: {}
- name: list_new_topic_default_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
    data_selector: data
    params: {}
- name: brokers
  endpoint:
    path: /clusters/{cluster_id}/brokers
    method: GET
    data_selector: data
    params: {}
- name: broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
    data_selector: data
    params: {}
- name: broker_partition_replicas
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/partition-replicas
    method: GET
    data_selector: data
    params: {}
- name: partition_replicas
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/partition-replicas
    method: GET
    data_selector: data
    params: {}
- name: delete_brokers
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: POST
    data_selector: data
    params: {}
- name: replicas
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replicas
    method: GET
    data_selector: data
    params: {}
- name: replica
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replicas/{broker_id}
    method: GET
    data_selector: data
    params: {}
- name: batch_create_acls
  endpoint:
    path: /clusters/{cluster_id}/acls:batch
    method: POST
    data_selector: data
    params: {}
- name: list_acls
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: GET
    data_selector: data
    params: {}
- name: create_acl
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: POST
    data_selector: resource_type
    params: {}
- name: delete_acls
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: DELETE
    data_selector: ''
    params: {}
- name: consumer_groups
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups
    method: GET
    data_selector: data
    params: {}
- name: consumer_group
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}
    method: GET
    data_selector: data
    params: {}
- name: consumers
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers
    method: GET
    data_selector: data
    params: {}
- name: consumer_group_lag_summary
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lag-summary
    method: GET
    data_selector: data
    params: {}
- name: consumer_lags
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lags
    method: GET
    data_selector: data
    params: {}
- name: consumer_lag
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lags/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: data
    params: {}
- name: consumer
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}
    method: GET
    data_selector: data
    params: {}
- name: consumer_assignments
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}/assignments
    method: GET
    data_selector: data
    params: {}
- name: consumer_assignment
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}/assignments/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: data
    params: {}
- name: partitions_list
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions
    method: GET
- name: partition_details
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}
    method: GET
- name: replica_reassignments
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/reassignment
    method: GET
- name: replica_reassignment
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/reassignment
    method: GET
    data_selector: data
    params: {}
- name: get_replica_reassignment
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/reassignment
    method: GET
    data_selector: data
    params: {}
- name: list_topics
  endpoint:
    path: /clusters/{cluster_id}/topics
    method: GET
    data_selector: data
    params: {}
- name: create_topic
  endpoint:
    path: /clusters/{cluster_id}/topics
    method: POST
    data_selector: ''
    params: {}
- name: get_topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: GET
    data_selector: ''
    params: {}
- name: topics
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: PATCH
    data_selector: metadata
    params: {}
- name: delete_topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: DELETE
    data_selector: ''
    params: {}
- name: produce_records
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/records
    method: POST
    data_selector: records
    params: {}
- name: cluster_links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
    data_selector: data
    params: {}
- name: create_cluster_link
  endpoint:
    path: /clusters/{cluster_id}/links
    method: POST
    data_selector: data
    params: {}
- name: describe_cluster_link
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: GET
    data_selector: data
    params: {}
- name: link_configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs
    method: GET
    data_selector: data
    params: {}
- name: link_config_description
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: GET
    data_selector: data
    params: {}
- name: delete_link
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: DELETE
    data_selector: null
    params: {}
- name: alter_cluster_link_config
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: PUT
    data_selector: data
    params: {}
- name: reset_cluster_link_config
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: DELETE
    data_selector: data
    params: {}
- name: batch_alter_cluster_link_configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs:alter
    method: PUT
    data_selector: data
    params: {}
- name: create_mirror_topic
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors
    method: POST
    data_selector: data
    params: {}
- name: list_mirror_topics
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors
    method: GET
    data_selector: data
    params: {}
- name: list_all_mirror_topics
  endpoint:
    path: /clusters/{cluster_id}/links/-/mirrors
    method: GET
    data_selector: data
    params: {}
- name: describe_mirror_topic
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors/{mirror_topic_name}
    method: GET
    data_selector: data
    params: {}
- name: promote_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:promote
    method: POST
    data_selector: data
    params: {}
- name: failover_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:failover
    method: POST
    data_selector: data
    params: {}
- name: pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
    data_selector: data
    params: {}
- name: pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
    data_selector: mirror_topic_names
- name: resume_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:resume
    method: POST
    data_selector: mirror_topic_names
- name: reverse_and_start_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:reverse-and-start-mirror
    method: POST
    data_selector: mirror_topic_names
- name: reverse_and_pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:reverse-and-pause-mirror
    method: POST
    data_selector: null
    params: {}
- name: truncate_and_restore
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:truncate-and-restore
    method: POST
    data_selector: null
    params: {}
- name: share_group
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups
    method: GET
    data_selector: data
    params: {}
- name: share_group_details
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}
    method: GET
    data_selector: kind
    params: {}
- name: share_group_consumers
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers
    method: GET
    data_selector: data
    params: {}
- name: share_group_consumer
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers/{consumer_id}
    method: GET
    data_selector: data
- name: share_group_consumer_assignments
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers/{consumer_id}/assignments
    method: GET
    data_selector: data
- name: balancer_status
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
    data_selector: status
    params:
      cluster_id: string
- name: any_uneven_load_status
  endpoint:
    path: /clusters/{cluster_id}/balancer/any-uneven-load
    method: GET
    data_selector: status
    params:
      cluster_id: string
- name: broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
- name: broker_tasks_specific
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
      broker_id: broker_id
- name: broker_tasks_by_type
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks/{task_type}
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
      task_type: task_type
- name: broker_task
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks/{task_type}
    method: GET
    data_selector: data
    params: {}
- name: broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: broker_replica_exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions/{broker_id}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      broker_id: integer
- name: create_broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:create
    method: POST
    data_selector: data
    params:
      cluster_id: string
- name: delete_broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:delete
    method: POST
    data_selector: data
    params:
      cluster_id: string
- name: remove_broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: GET
    data_selector: data
    params: {}
- name: get_remove_broker_task
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks/{broker_id}
    method: GET
    data_selector: data
    params: {}
- name: unregister_broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: POST
    data_selector: data
    params: {}
- name: replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: partition_replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: string
      topic_name: string
- name: specific_partition_replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: string
      topic_name: string
      partition_id: integer
- name: clusters
  endpoint:
    path: /kafka/v3/clusters
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/topics
    method: GET
    data_selector: data
    params: {}
- name: cluster_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
- name: broker_delete
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: DELETE
- name: broker_unregister
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: DELETE
- name: broker_get
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
- name: cluster_links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
- name: cluster_balancer
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
- name: broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
- name: broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
- name: remove_broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: DELETE
- name: replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
- name: kafka_topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: Production Environments
  endpoint:
    path: installing_cp/zip-tar.html#prod-kafka-cli-install
    method: GET
- name: Development Environments
  endpoint:
    path: ../get-started/platform-quickstart.html#quickstart
    method: GET
- name: Confluent Signing Keys
  endpoint:
    path: https://www.confluent.io/blog/confluent-update-regarding-codecov-incident/?session_ref=direct&url_ref=https%3A%2F%2Fdocs.confluent.io%2Fplatform%2Fcurrent%2Finstallation%2Foverview.html
    method: GET
- name: kafka_topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: ksql_info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Compatibility
  endpoint:
    path: /platform/current/installation/versions-interoperability.html
    method: GET
- name: Patch Versions
  endpoint:
    path: /platform/current/installation/versions-interoperability.html
    method: GET
- name: Operating Systems
  endpoint:
    path: /platform/current/installation/versions-interoperability.html
    method: GET
- name: Control Center
  endpoint:
    path: /control-center/version
    method: GET
    data_selector: records
    params: {}
- name: Unified Stream Manager Agent
  endpoint:
    path: /usm-agent/version
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka
  endpoint:
    path: /etc/kafka/
    method: GET
    data_selector: files
    params: {}
- name: download_zip
  endpoint:
    path: /archive/8.1/confluent-8.1.0.zip
    method: GET
- name: download_tar
  endpoint:
    path: /archive/8.1/confluent-8.1.0.tar.gz
    method: GET
- name: download_community_zip
  endpoint:
    path: /archive/8.1/confluent-community-8.1.0.zip
    method: GET
- name: download_community_tar
  endpoint:
    path: /archive/8.1/confluent-community-8.1.0.tar.gz
    method: GET
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: confluent_platform_zip
  endpoint:
    path: /confluent-8.1.0.zip
    method: GET
- name: confluent_platform_tar
  endpoint:
    path: /confluent-8.1.0.tar.gz
    method: GET
- name: confluent_community_zip
  endpoint:
    path: /confluent-community-8.1.0.zip
    method: GET
- name: confluent_community_tar
  endpoint:
    path: /confluent-community-8.1.0.tar.gz
    method: GET
- name: confluent_zip
  endpoint:
    path: /confluent-8.1.0.zip
    method: GET
- name: confluent_tar
  endpoint:
    path: /confluent-8.1.0.tar.gz
    method: GET
- name: confluent_community_zip
  endpoint:
    path: /confluent-community-8.1.0.zip
    method: GET
- name: confluent_community_tar
  endpoint:
    path: /confluent-community-8.1.0.tar.gz
    method: GET
- name: kafka_broker
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
    data_selector: properties
    params: {}
- name: kafka_controller
  endpoint:
    path: /etc/kafka/controller.properties
    method: GET
    data_selector: properties
    params: {}
- name: kafka_server
  endpoint:
    path: /etc/kafka/server.properties
    method: GET
    data_selector: properties
    params: {}
- name: streaming_app
  endpoint:
    path: /api/v1/streaming
    method: GET
    data_selector: data
- name: ksql_query
  endpoint:
    path: /api/v1/ksql
    method: POST
    data_selector: query_results
- name: kafka
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
    data_selector: process.roles, node.id, controller.quorum.voters
    params: {}
- name: Confluent
  endpoint:
    path: /rpm/8.1
    method: GET
    data_selector: packages
    params: {}
- name: Confluent-Clients
  endpoint:
    path: /clients/rpm/centos/$releasever/$basearch
    method: GET
    data_selector: packages
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: queryResult
    params: {}
- name: cluster_status
  endpoint:
    path: /ksqldb/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqldb/info
    method: GET
    data_selector: info
    params: {}
- name: Docker Image
  endpoint:
    path: /docker/image
    method: GET
    data_selector: images
    params: {}
- name: kafka
  endpoint:
    path: /cp-kafka
    method: GET
    data_selector: records
- name: schema_registry
  endpoint:
    path: /cp-schema-registry
    method: GET
    data_selector: records
- name: kafka_connect
  endpoint:
    path: /cp-kafka-connect
    method: GET
    data_selector: records
- name: ksqldb_server
  endpoint:
    path: /cp-ksqldb-server
    method: GET
    data_selector: records
- name: ksql_server
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: kafka
  endpoint:
    path: /cp-kafka
    method: GET
    data_selector: kafka_configurations
- name: schema_registry
  endpoint:
    path: /cp-schema-registry
    method: GET
    data_selector: schema_registry_configurations
- name: kafka_connect
  endpoint:
    path: /cp-kafka-connect
    method: GET
    data_selector: kafka_connect_configurations
- name: ksqldb_server
  endpoint:
    path: /cp-ksqldb-server
    method: GET
    data_selector: ksqldb_configurations
- name: ksql_server
  endpoint:
    path: /ksql
    method: POST
    data_selector: ksql
    params: {}
- name: bootstrap_servers
  endpoint:
    path: /
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: schema_registry_url
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /ksql
    method: POST
    data_selector: queryResults
    params: {}
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: config
    params: {}
- name: schema_registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: config
    params: {}
- name: kafka_connect
  endpoint:
    path: /kafka-connect
    method: GET
    data_selector: config
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksqlDB
    method: GET
    data_selector: config
    params: {}
- name: ksql_service
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: Replicator
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: Control Center
  endpoint:
    path: /control-center
    method: GET
    data_selector: status
    params: {}
- name: cp-base-java
  endpoint:
    path: https://github.com/confluentinc/common-docker/tree/master/base-java
    method: GET
- name: cp-base-lite
  endpoint:
    path: https://github.com/confluentinc/common-docker/tree/master/base-lite
    method: GET
- name: cp-base-new
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-base-new
    method: GET
- name: cp-kafka
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka
    method: GET
- name: confluent-local
  endpoint:
    path: https://hub.docker.com/r/confluentinc/confluent-local
    method: GET
- name: cp-server
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-server
    method: GET
- name: cp-schema-registry
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-schema-registry
    method: GET
- name: cp-kafka-connect
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka-connect
    method: GET
- name: cp-ksqldb-server
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-ksqldb-server
    method: GET
- name: cp-kafka-rest
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka-rest
    method: GET
- name: confluent-cli
  endpoint:
    path: https://hub.docker.com/r/confluentinc/confluent-cli
    method: GET
- name: cp-enterprise-control-center-next-gen
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-enterprise-control-center-next-gen
    method: GET
- name: cpc-gateway
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cpc-gateway
    method: GET
- name: cp-kcat
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kcat
    method: GET
- name: cp-kafka-mqtt
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka-mqtt
    method: GET
- name: cp-enterprise-replicator
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-enterprise-replicator
    method: GET
- name: cp-enterprise-replicator-executable
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-enterprise-replicator-executable
    method: GET
- name: confluent-operator
  endpoint:
    path: https://hub.docker.com/r/confluentinc/confluent-operator
    method: GET
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: Kafka Connect Base Image
  endpoint:
    path: /kafka-connect
    method: GET
- name: Confluent Hub Client
  endpoint:
    path: /confluent-hub
    method: GET
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
    params: {}
- name: docker_images
  endpoint:
    path: /docker/images
    method: GET
    data_selector: images
    params: {}
- name: stream
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: ksql
  endpoint:
    path: /api/v1/ksql
    method: POST
    data_selector: results
    params: {}
- name: kafka-connect-mongodb
  endpoint:
    path: /kafka-connect-mongodb
    method: GET
    data_selector: records
    params: {}
- name: kafka-connect-iothub
  endpoint:
    path: /kafka-connect-iothub
    method: GET
    data_selector: records
    params: {}
- name: kafka-connect-bigquery
  endpoint:
    path: /kafka-connect-bigquery
    method: GET
    data_selector: records
    params: {}
- name: kafka
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
- name: kafka-connect
  endpoint:
    path: /etc/kafka-connect/config.properties
    method: GET
- name: kafka-rest
  endpoint:
    path: /etc/kafka-rest/config.properties
    method: GET
- name: ksql
  endpoint:
    path: /etc/ksql/config.properties
    method: GET
- name: schema-registry
  endpoint:
    path: /etc/schema-registry/config.properties
    method: GET
- name: Control Center
  endpoint:
    path: /control-center/current/installation/license.html
    method: GET
    data_selector: license
    params: {}
- name: Kafka Connect
  endpoint:
    path: /connect/license.html
    method: GET
    data_selector: license
    params: {}
- name: Schema Registry
  endpoint:
    path: /schema-registry/index.html
    method: GET
    data_selector: license
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksqldb/overview.html
    method: GET
    data_selector: license
    params: {}
- name: REST Proxy
  endpoint:
    path: /kafka-rest/index.html
    method: GET
    data_selector: license
    params: {}
- name: topic
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: data
    params: {}
- name: consumer_group
  endpoint:
    path: /v1/consumer-groups
    method: GET
    data_selector: data
    params: {}
- name: confluent-kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: components
    params: {}
- name: confluent-control-center
  endpoint:
    path: /control-center
    method: GET
    data_selector: components
    params: {}
- name: confluent-schema-registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: components
    params: {}
- name: librdkafka-dev
  endpoint:
    path: /librdkafka-dev
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka1-dbg
  endpoint:
    path: /librdkafka1-dbg
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka1
  endpoint:
    path: /librdkafka1
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka-devel
  endpoint:
    path: /librdkafka-devel
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka1-debuginfo
  endpoint:
    path: /librdkafka1-debuginfo
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka
  endpoint:
    path: /librdkafka
    method: GET
    data_selector: package_description
    params: {}
- name: librdkafka
  endpoint:
    path: /librdkafka/source
    method: GET
    data_selector: source_package
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: librdkafka
  endpoint:
    path: /platform/8.1/clients/api-docs/librdkafka.html#librdkafka-api
    method: GET
- name: confluent-kafka-go
  endpoint:
    path: /platform/8.1/clients/api-docs/confluent-kafka-go.html#go-api
    method: GET
- name: confluent-kafka-dotnet
  endpoint:
    path: /platform/8.1/clients/api-docs/confluent-kafka-dotnet.html#dotnet-api
    method: GET
- name: confluent-kafka-python
  endpoint:
    path: /platform/8.1/clients/api-docs/confluent-kafka-python.html#python-api
    method: GET
- name: confluent-kafka-javascript
  endpoint:
    path: /platform/8.1/clients/api-docs/confluent-kafka-javascript.html#javascript-api
    method: GET
- name: Admin API
  endpoint:
    path: /admin
    method: GET
- name: Control Center metrics integration
  endpoint:
    path: /control-center
    method: GET
- name: Kafka Streams
  endpoint:
    path: /streams
    method: GET
- name: Schema Registry
  endpoint:
    path: /schemas
    method: GET
- name: Topic Metadata API
  endpoint:
    path: /topics
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /platform/current/clients/consumer.html
    method: GET
- name: clusters
  endpoint:
    path: /api/clusters
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /api/topics
    method: GET
    data_selector: data
    params: {}
- name: Kafka Producer Configuration
  endpoint:
    path: /platform/8.1/installation/configuration/producer-configs.html
    method: GET
    data_selector: ''
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
    params: {}
- name: topics
  endpoint:
    path: /topics/{topicName}
    method: POST
    data_selector: null
    params: {}
- name: topics_partitions
  endpoint:
    path: /topics/{topicName}/partitions/{partition}
    method: POST
    data_selector: null
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: producer
  endpoint:
    path: /producer
    method: POST
- name: consumer
  endpoint:
    path: /consumer
    method: GET
- name: cluster_status
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: producer
  endpoint:
    path: /clients/cloud/groovy/ProducerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: consumer
  endpoint:
    path: /clients/cloud/groovy/ConsumerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: streams
  endpoint:
    path: /clients/cloud/groovy/StreamsExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: cluster_status
  endpoint:
    path: /v1/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
    params: {}
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: test1
  endpoint:
    path: /kafka-topics
    method: POST
    data_selector: topics
    params:
      topic: test1
      replication_factor: 3
      partitions: 6
- name: test2
  endpoint:
    path: /topic/test2
    method: POST
    data_selector: records
- name: subjects
  endpoint:
    path: /subjects
    method: GET
- name: subject_version
  endpoint:
    path: /subjects/test2-value/versions/1
    method: GET
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: producer
  endpoint:
    path: /clients/cloud/kcat/
    method: POST
    data_selector: messages
    params: {}
- name: test
  endpoint:
    path: /topics/test
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /clients/cloud/kotlin/ProducerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: consumer
  endpoint:
    path: /clients/cloud/kotlin/ConsumerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: streams
  endpoint:
    path: /clients/cloud/kotlin/StreamsExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: producer
  endpoint:
    path: /v1/producer
    method: POST
    data_selector: records
- name: consumer
  endpoint:
    path: /v1/consumer
    method: GET
    data_selector: records
- name: consume_records
  endpoint:
    path: /consume_records
    method: GET
    data_selector: records
- name: producer
  endpoint:
    path: /clients/cloud/rust/producer
    method: POST
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /clients/cloud/rust/consumer
    method: POST
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /streams
    method: GET
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /clients/cloud/scala/Producer
    method: POST
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /clients/cloud/scala/Consumer
    method: POST
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /clients/cloud/scala/Streams
    method: POST
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: topics
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: records
    params: {}
- name: create_topics
  endpoint:
    path: create_topics
    method: POST
    data_selector: futures
- name: delete_topics
  endpoint:
    path: delete_topics
    method: POST
    data_selector: futures
- name: list_topics
  endpoint:
    path: list_topics
    method: GET
    data_selector: metadata
- name: list_groups
  endpoint:
    path: list_groups
    method: GET
    data_selector: groups
- name: create_partitions
  endpoint:
    path: create_partitions
    method: POST
    data_selector: futures
- name: describe_configs
  endpoint:
    path: describe_configs
    method: POST
    data_selector: futures
- name: delete_acls
  endpoint:
    path: delete_acls
    method: POST
    data_selector: futures
- name: NewTopic
  endpoint:
    path: /admin/create_topics
    method: POST
    data_selector: topic
    params: {}
- name: NewPartitions
  endpoint:
    path: /admin/create_partitions
    method: POST
    data_selector: topic
    params: {}
- name: ConfigSource
  endpoint:
    path: /admin/config_sources
    method: GET
    data_selector: config_sources
    params: {}
- name: ConfigEntry
  endpoint:
    path: /admin/config_entries
    method: GET
    data_selector: config_entries
    params: {}
- name: ConfigResource
  endpoint:
    path: /admin/config_resources
    method: GET
    data_selector: config_resources
    params: {}
- name: AclBinding
  endpoint:
    path: /admin/acls
    method: GET
    data_selector: acl_bindings
    params: {}
- name: DeletedRecords
  endpoint:
    path: /confluent_kafka/admin/DeletedRecords
    method: GET
    data_selector: low_watermark
    params: {}
- name: MemberAssignment
  endpoint:
    path: /confluent_kafka/admin/MemberAssignment
    method: GET
    data_selector: topic_partitions
    params: {}
- name: MemberDescription
  endpoint:
    path: /confluent_kafka/admin/MemberDescription
    method: GET
    data_selector: member_id, client_id, host, assignment, group_instance_id, target_assignment
    params: {}
- name: consumer
  endpoint:
    path: /consumer
    method: GET
    data_selector: records
    params: {}
- name: DeserializingConsumer
  endpoint:
    path: /modules/confluent_kafka/deserializing_consumer.html
    method: GET
- name: Producer
  endpoint:
    path: /modules/confluent_kafka/producer.html
    method: GET
- name: SerializingProducer
  endpoint:
    path: /serializing_producer
    method: POST
    data_selector: producer
    params: {}
- name: Deserializer
  endpoint:
    path: /deserializer
    method: POST
    data_selector: deserializer
    params: {}
- name: DoubleDeserializer
  endpoint:
    path: /double_deserializer
    method: POST
    data_selector: double_deserializer
    params: {}
- name: IntegerDeserializer
  endpoint:
    path: /integer_deserializer
    method: POST
    data_selector: integer_deserializer
    params: {}
- name: StringDeserializer
  endpoint:
    path: /string_deserializer
    method: POST
    data_selector: string_deserializer
    params: {}
- name: DoubleSerializer
  endpoint:
    path: /double_serializer
    method: POST
    data_selector: double_serializer
    params: {}
- name: IntegerSerializer
  endpoint:
    path: /integer_serializer
    method: POST
    data_selector: integer_serializer
    params: {}
- name: StringSerializer
  endpoint:
    path: /string_serializer
    method: POST
    data_selector: string_serializer
    params: {}
- name: delete_records
  endpoint:
    path: /AdminClient/delete_records
    method: POST
    data_selector: future
    params:
      request_timeout: socket.timeout.ms/1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: elect_leaders
  endpoint:
    path: /AdminClient/elect_leaders
    method: POST
    data_selector: future
    params:
      request_timeout: socket.timeout.ms*1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: NewTopic
  endpoint:
    path: /AdminClient/create_topics
    method: POST
    data_selector: NewTopic
    params: {}
- name: NewPartitions
  endpoint:
    path: /AdminClient/create_partitions
    method: POST
    data_selector: NewPartitions
    params: {}
- name: ListConsumerGroupsResult
  endpoint:
    path: /admin/list_consumer_groups
    method: GET
    data_selector: valid
    params: {}
- name: ConsumerGroupDescription
  endpoint:
    path: /admin/describe_consumer_groups
    method: GET
    data_selector: members
    params: {}
- name: DeletedRecords
  endpoint:
    path: /admin/deleted_records
    method: GET
    data_selector: low_watermark
    params: {}
- name: MemberAssignment
  endpoint:
    path: /admin/member_assignment
    method: GET
    data_selector: topic_partitions
    params: {}
- name: MemberDescription
  endpoint:
    path: /admin/member_description
    method: GET
    data_selector: assignment
    params: {}
- name: AdminClient
  endpoint:
    path: /admin
    method: GET
    data_selector: operations
    params: {}
- name: classic_protocol
  endpoint:
    path: /protocols/classic
    method: GET
    data_selector: records
    params:
      group.protocol: classic
      partition.assignment.strategy: <range,roundrobin,sticky
      session.timeout.ms: 45000
      heartbeat.interval.ms: 15000
- name: next_gen_protocol
  endpoint:
    path: /protocols/consumer
    method: GET
    data_selector: records
    params:
      group.protocol: consumer
- name: ElectLeadersRequest
  endpoint:
    path: /ElectLeadersRequest
    method: POST
    data_selector: result
    params: {}
- name: ElectLeadersResult
  endpoint:
    path: /ElectLeadersResult
    method: GET
    data_selector: result
    params: {}
- name: Error
  endpoint:
    path: /Error
    method: GET
    data_selector: result
    params: {}
- name: client_deprecation
  endpoint:
    path: /clients/deprecate-how-to
    method: GET
- name: cluster_status
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: temperature
  endpoint:
    path: /temperature
    method: POST
    data_selector: records
    params: {}
- name: brightness
  endpoint:
    path: /brightness
    method: POST
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksqlDB/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqlDB/info
    method: GET
    data_selector: info
    params: {}
- name: stream
  endpoint:
    path: /mqtt/stream
    method: POST
    data_selector: records
    params: {}
- name: confluent.license
  endpoint:
    path: /confluent/license
    method: GET
    data_selector: license_key
- name: confluent.topic.bootstrap.servers
  endpoint:
    path: /confluent/topic/bootstrap/servers
    method: GET
    data_selector: servers
- name: confluent.topic
  endpoint:
    path: /confluent/topic
    method: GET
    data_selector: topic_name
- name: confluent.topic.replication.factor
  endpoint:
    path: /confluent/topic/replication/factor
    method: GET
    data_selector: replication_factor
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: input_topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: output_topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: cluster_status
  endpoint:
    path: /kafka/v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /kafka/v1/info
    method: GET
    data_selector: info
    params: {}
- name: Event-Driven Microservice
  endpoint:
    path: /orders
    method: POST
    data_selector: orders
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka-streams
  endpoint:
    path: /kafka-streams
    method: GET
    data_selector: records
    params: {}
- name: kafka-clients
  endpoint:
    path: /kafka-clients
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksqldb/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqldb/info
    method: GET
    data_selector: info
    params: {}
- name: streams_config
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: state_store
  endpoint:
    path: /state/store
    method: GET
- name: cluster_status
  endpoint:
    path: /ksqldb/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqldb/info
    method: GET
    data_selector: info
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
- name: KStream
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#stream(java.lang.String)
    method: GET
- name: KTable
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#table-java.lang.String(java.lang.String)
    method: GET
- name: GlobalKTable
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#globalTable-java.lang.String(java.lang.String)
    method: GET
- name: transformations
  endpoint:
    path: /streams/transformations
    method: GET
    data_selector: transformations
    params: {}
- name: aggregate
  endpoint:
    path: /streams/aggregate
    method: POST
    data_selector: records
- name: count
  endpoint:
    path: /streams/count
    method: POST
    data_selector: records
- name: reduce
  endpoint:
    path: /streams/reduce
    method: POST
    data_selector: records
- name: categorize_logs_by_severity
  endpoint:
    path: /categorize/logs/severity
    method: GET
    data_selector: records
- name: cumulative_discounts_loyalty_program
  endpoint:
    path: /cumulative/discounts/loyalty/program
    method: GET
    data_selector: records
- name: replace_slang_in_text_messages
  endpoint:
    path: /replace/slang/text/messages
    method: GET
    data_selector: records
- name: traffic_radar_monitoring
  endpoint:
    path: /traffic/radar/monitoring
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: input
  endpoint:
    path: input
    method: GET
    data_selector: records
- name: output
  endpoint:
    path: output
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: stream_processor
  endpoint:
    path: /api/v1/stream_processor
    method: POST
    data_selector: processor
    params: {}
- name: versioned_key_value_state_stores
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/state/VersionedBytesStoreSupplier.html
    method: GET
    data_selector: records
    params: {}
- name: fault_tolerant_state_stores
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/state/Stores.html#persistentKeyValueStore(java.lang.String).html
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Implementing custom Serdes
  endpoint:
    path: /implementing-custom-serdes
    method: GET
    data_selector: source_code_references
- name: Kafka Streams DSL for Scala implicit serdes
  endpoint:
    path: /kafka-streams-dsl-for-scala-implicit-serdes
    method: GET
    data_selector: default_impl
- name: kafka_topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: local_state
  endpoint:
    path: /local/state
    method: GET
- name: remote_state
  endpoint:
    path: /remote/state
    method: GET
- name: word_count
  endpoint:
    path: /word-count
    method: GET
    data_selector: result
    params: {}
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: stream_query
  endpoint:
    path: /v1/streams/query
    method: POST
    data_selector: queryResults
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: application_reset_tool
  endpoint:
    path: /application/reset
    method: POST
    data_selector: actions
    params: {}
- name: streams
  endpoint:
    path: /streams
    method: GET
    data_selector: records
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
- name: cluster_status
  endpoint:
    path: /kafka/v1/clusters
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /kafka/v1/info
    method: GET
    data_selector: info
    params: {}
- name: upgrade
  endpoint:
    path: /streams/upgrade-guide
    method: GET
    data_selector: upgrade_details
- name: deserialization_exception_handler
  endpoint:
    path: /streams/config
    method: PUT
    data_selector: streamsSettings
    params:
      DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG: LogAndFailExceptionHandler.class.getName()
- name: cp-ksqldb-server
  endpoint:
    path: /ksqldb-server
    method: GET
- name: cp-ksqldb-cli
  endpoint:
    path: /ksqldb-cli
    method: GET
- name: kafka_topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
- name: kafka_consumer
  endpoint:
    path: /api/v1/consumers
    method: GET
    data_selector: data
- name: ksql_server
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: cluster_status
  endpoint:
    path: /status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: kafka_topic
  endpoint:
    path: /topics
    method: GET
    data_selector: topics
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
    params: {}
- name: topic_info
  endpoint:
    path: /topics/{topic_name}
    method: GET
    data_selector: records
    params: {}
- name: partitions_info
  endpoint:
    path: /topics/{topic_name}/partitions
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: streams
  endpoint:
    path: /streams
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: brokers.list
  endpoint:
    path: /brokers
    method: GET
- name: consumer.commit
  endpoint:
    path: /consumers/{group}/instances/{instance}/offsets
    method: POST
- name: consumer.create
  endpoint:
    path: /consumers/{group}
    method: POST
- name: consumer.delete
  endpoint:
    path: /consumers/{group}/instances/{instance}
    method: DELETE
- name: consumer.topic.read-avro
  endpoint:
    path: /consumers/{group}/instances/{instance}/topics/{topic}
    method: GET
    headers:
      Accept: application/vnd.kafka.avro.v1+json
- name: consumer.topic.read-binary
  endpoint:
    path: /consumers/{group}/instances/{instance}/topics/{topic}
    method: GET
    headers:
      Accept: application/vnd.kafka.binary.v1+json
- name: partition.get
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: GET
- name: partition.produce-avro
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.avro.v1+json
- name: partition.produce-json
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.json.v1+json
- name: partition.produce-binary
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.binary.v1+json
- name: partitions.list
  endpoint:
    path: /topics/{topic}/partitions
    method: GET
- name: topic.get
  endpoint:
    path: /topics/{topic}
    method: GET
- name: topic.produce-avro
  endpoint:
    path: /topics/{topic}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.avro.v1+json
- name: topic.produce-binary
  endpoint:
    path: /topics/{topic}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.binary.v1+json
- name: topics.list
  endpoint:
    path: /topics
    method: GET
- name: streaming_application
  endpoint:
    path: /api/v1/streaming
    method: GET
    data_selector: data
    params: {}
notes:
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Uses OAuth2 with refresh token for authentication.
- If you are using a MacBook with an M1 or later chip, you can use the Confluent CLI
  and the ARM64 version of the Confluent Platform Docker images for local install.
  The `confluent local` subcommands are currently not supported on the ARM64 environment.
- 'APT shows error if the public key is not available: NO_PUBKEY KEY_ID'
- YUM/DNF may show a warning if the key is not available
- Some objects like Contact may return nulls in deeply nested fields
- Confluent Platform supports both ARM64 and X86 hardware architecture.
- If deploying Confluent Platform on AWS VMs and running Replicator as a connector,
  be aware that VMs with burstable CPU types will not support high throughput streaming
  workloads.
- Enterprise support is not provided unless the `confluent-security` package is installed
  on all nodes and a valid Enterprise license key is configured.
- You should enable compression to help mitigate the performance impacts while maintaining
  business continuity.
- You should disable vMotion and disk snapshotting for Confluent Platform as the features
  could cause a full cluster outage when used with Kafka.
- You must minimally configure the following components.
- Ensure the ksqlDB server is running before making API calls
- Refer to the REST API documentation for detailed usage
- You must complete these steps for each node in your cluster.
- For Kafka in KRaft mode, you must configure a node to be a broker or a controller.
- Minimum of three brokers and three controllers recommended for production.
- You must minimally configure the process.roles, node.id and controller.quorum.voters
  for each node.
- Requires setup of OAuth2 credentials in Confluent Cloud
- Check API rate limits to avoid throttling
- This configuration is for a three node multi-node cluster.
- RHEL 7 support is deprecated in Confluent Platform 7.x and will be removed in Confluent
  Platform 8.x.
- Starting with Confluent Platform 8.0, the librdkafka, Avro, and libserdes C/C++
  client packages will only be available from the clients location.
- Starting with Confluent Platform 8.0, the librdkafka, Avro, and libserdes C/C++
  client packages will only be available from the https://packages.confluent.io/clients
  location.
- Uses OAuth2 with refresh token  requires setup of connected app in Confluent
- Some operations may have limitations on performance
- 'Multi-node Environment: For more information, see Configure a Multi-Node Confluent
  Platform Environment with Docker'
- 'Persistent Data (Mounted Volumes): When deploying Kafka images, you should always
  use Mount Docker External Volumes in Confluent Platform for the file systems those
  images use for their persistent data.'
- 'Bridge Networking vs. Host Networking: Bridge networking is currently only supported
  on a single host.'
- 'Adding Connectors to the Kafka Connect Image: Build a new Docker image that has
  the connector installed or add the connector JARs via volumes.'
- 'Supported Java: The Confluent Docker images are tested and shipped with Temurin
  JDK.'
- 'Untested Features: The images are not currently tested on Docker Swarm.'
- Requires setup of connected app in Confluent API
- Multi-node Environment requires configuration.
- Persistent Data should use mounted volumes for Kafka images.
- Bridge networking is only supported on a single host.
- KRaft mode is required for metadata management.
- Enterprise features require a valid license key.
- Combined mode is for local experimentation only and is not supported for production.
- Connect to a secure Kafka cluster using SASL_SSL.
- KRaft combined mode is for local experimentation only.
- Ensure that the configuration file exists.
- Docker containers require specific configuration for optimal performance.
- Confluent Platform supports cluster encryption and authentication, including a mix
  of authenticated and unauthenticated, and encrypted and non-encrypted clients.
- Using security with your Kafka clusters is optional, but recommended.
- Starting with Confluent Platform version 6.0 release, many connectors previously
  bundled with Confluent Platform are now available for download from Confluent Hub.
- Requires setup of connected app in API
- API may have limitations on request frequency
- Docker version 1.11 or later is required.
- Docker memory is allocated minimally at 8 GB.
- Older versions of the Oracle JDK are not recommended for production use as they
  no longer receive security patches.
- Ensure proper OAuth2 setup for access
- Rate limits may apply to API calls
- Make sure the log directory is world-writable
- Docker memory is allocated minimally at 8 GB
- Some objects may return nulls in deeply nested fields
- The systemd service unit files are optimized for staging and production deployments
  on Linux-based systems.
- Each component runs under its own user and a common `confluent` group.
- Developer licenses never expire.
- Uses OAuth2 with refresh token  requires setup of Confluent Cloud API credentials
- Confluent Platform 8.1 does not support ZooKeeper for metadata management.
- While enabling Telemetry and Health+ is highly encouraged and beneficial to minimize
  downtime, these features are not mandatory.
- You must migrate to KRaft mode before upgrading to Confluent Platform 8.1.
- Ensure all your self-managed Confluent Platform components are connected with Confluent
  Cloud.
- 'Command to start a broker: kafka-server-start -daemon /etc/kafka/broker.properties'
- 'Command to start a controller: kafka-server-start -daemon /etc/kafka/controller.properties'
- Confluent Platform and Kafka compatibility is provided.
- Requires setup of connected app in Confluent
- Some endpoints may return nulls in nested fields
- Confluent Server requires that the confluent.license property be set to a valid
  license string in each broker properties file.
- You should not move from Kafka to Confluent Platform while also doing a migration
  from ZooKeeper to KRaft mode.
- Confluent Server requires that the confluent.license property be set to a valid
  license string in each broker properties file
- The consumer must set 'bootstrap.servers' property to find the Kafka cluster.
- It is recommended to set 'client.id' to correlate requests on the broker with the
  client instance.
- It is advisable to configure a 'group.id' for consumer groups.
- All members in the group must advertise a compatible, ordered list of assignors.
- Do not switch to listing only CooperativeStickyAssignor until all members run client
  versions that support it (2.4.0 or later); otherwise the group fails to find a common
  assignor.
- Uses OAuth2 with refresh token  requires setup of a Confluent Cloud account
- REST Proxy relies on content type information to properly convert data to Avro 
  specify the Content-Type header.
- Requires setup of connected app in Confluent.
- Some API calls may be rate limited.
- The easiest way to follow this tutorial is with Confluent Cloud because you dont
  have to run a local Kafka cluster.
- If you dont want to use Confluent Cloud, you can also use this tutorial with a
  Kafka cluster running on your local host or any other remote server.
- Uses OAuth2 with refresh token  requires setup of connected app in ksqlDB
- Some queries may return nulls in results
- 'Best practice for higher availability in Apache Kafka clients prior to 3.0: session.timeout.ms=45000'
- 'Best practice for Kafka producer to prevent data loss: acks=all'
- Uses SASL_SSL security protocol with Plain Login Module for authentication
- Best practice for Kafka producer to prevent data loss
- Required for correctness in Apache Kafka clients prior to 2.6
- Use the kafka-avro-console-consumer to consume records from topic test2.
- Required connection configs for Kafka producer, consumer, and admin include bootstrap.servers,
  security.protocol, and sasl.jaas.config.
- Requires customization of connection configuration with broker endpoint and API
  key/secret.
- You can also use a Kafka cluster running on your local host or any other remote
  server.
- Rate limits apply for API calls
- Uses Confluent Cloud for easier setup without local Kafka cluster
- Some API calls may have rate limits
- Requires broker version v0.11.0.0 or later.
- group.id must be set and bootstrap.servers should be set
- This class is experimental and likely to be removed, or subject to incompatible
  API changes in future versions of the library.
- The new consumer group protocol defined in KIP-848 is not enabled by default.
- 'On close() or unsubscribe with auto-commit enabled: Member retries committing offsets
  until a timeout expires.'
- Currently uses the default remote session timeout.
- Future KIP-1092 will allow custom commit timeouts.
- 'Kafka admin client: create, view, alter, and delete topics and resources.'
- 'The overall request timeout in seconds, including broker lookup, request transmission,
  operation time on broker, and response. Default: socket.timeout.ms/1000.0.'
- message and offsets are mutually exclusive. The stored offsets will be committed
  according to auto.commit.interval.ms or manual offset-less commit()
- enable.auto.offset.store must be set to False when using this API.
- Ensure only one active instance per group.instance.id.
- Session & heartbeat now controlled by broker
- 'Default: socket.timeout.ms/1000.0'
- message and offsets are mutually exclusive. The stored offsets will be committed
  according to auto.commit.interval.ms or manual offset-less commit(). Note that
  enable.auto.offset.store must be set to False when using this API.
- The next generation consumer group rebalance protocol defined in KIP-848 is production-ready.
- The new consumer group protocol is not enabled by default.
- KIP-848 fences new member on duplicate group.instance.id
- KIP-848 does not return error on subscription if topic missing
- Upgrade to confluent-kafka-python  2.12.0 (GA release)
- Run against Kafka brokers  4.0.0
- Pre-built binary support will be dropped after the EOL of the node version or the
  OS.
- Topic creation is non-atomic and may succeed for some topics but fail for others,
  make sure to check the result for topic-specific errors.
- Requires broker version >= 0.10.1.0
- Requires broker version >= 0.11.0.0
- Requires broker version >= 2.3.0
- Default operation timeout is 0 (return immediately).
- Default isolation level is ReadUncommitted.
- Build version go1.22.2.
- The 3.7 release marked these older, client protocol API versions deprecated with
  a notice that they will be completely dropped by 4.0.
- Any environment using versions other than established minimums can encounter one
  or more version-related errors.
- MQTT Proxy is deprecated in Confluent Platform version 7.9 and will be removed in
  a future version.
- Uses OAuth2 with refresh token  requires setup of connected app in API
- 'Kafka: 6.0.0-ccs'
- MQTT Clients supporting the MQTT 3.1.1 protocol
- The MQTT Proxy is deprecated in Confluent Platform version 7.9 and will be removed
  in a future version.
- Kafka Streams is a client library for building applications and microservices.
- The project needs a few parameters to connect with your Kafka cluster.
- Confluent for VS Code enables creating Kafka topics easily within VS Code.
- sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required
  username='' password='';
- The Streams API is not compatible with Kafka clusters running older Kafka versions
  (0.7, 0.8, 0.9).
- Kafka Streams is tightly integrated with Apache Kafka
- Ingestion-time may approximate event-time reasonably well if we assume that the
  time difference between creation of the record and its ingestion into Kafka is sufficiently
  small.
- Out-of-order records can only be considered for event-time semantics, where timestamps
  are set by producers specifically to indicate event-time.
- Records that arrive after a window is closed are dropped and not processed.
- Kafka Streams partitions data for processing it.
- Kafka Streams is not a resource manager, but a library that 'runs' anywhere its
  stream processing application runs.
- Kafka Streams does not use a backpressure mechanism because it does not need one.
- Kafka Streams regulates the progress of streams by the timestamps of data records.
- This website includes content developed at the Apache Software Foundation under
  the terms of the Apache License v2.
- The SHUTDOWN_APPLICATION option is best-effort only and doesnt guarantee that all
  application instances will be stopped.
- The maximum acceptable lag is 10,000.
- Application server must be different for each instance.
- Kafka Streams metrics can be pushed to the brokers, similar to client metrics.
- Enables enforcement of explicit naming for all internal resources of the topology,
  including internal topics.
- The group protocol used by the Kafka Streams client for coordination.
- Controls the output interval for summary information.
- Controls how long Kafka Streams waits to fetch data to ensure in-order processing
  semantics.
- The maximum number of warmup replicas.
- The period of time in milliseconds after which a refresh of metadata is forced.
- The number of samples maintained to compute metrics.
- The highest recording level for metrics.
- The window of time a metrics sample is computed over.
- The minimum number of in-sync replicas available for replication if the producer
  is configured with acks='all'.
- The number of standby replicas.
- This specifies the number of stream threads in an instance of the Kafka Streams
  application.
- The amount of time in milliseconds to block waiting for input.
- The maximum time to wait before triggering a rebalance to probe for warmup replicas.
- The processing exception handler enables managing exceptions triggered during the
  processing of a record.
- The processing guarantee that should be used.
- The production exception handler enables you to manage exceptions triggered when
  trying to interact with a broker.
- This configuration sets the cost of moving a task from the original assignment computed
  either by StickyTaskAssignor or HighAvailabilityTaskAssignor.
- This configuration sets the strategy Kafka Streams uses for rack-aware task assignment.
- Kafka Streams uses different default values for some of the underlying client configurations.
- Ensure proper token management for OAuth2
- API may have rate limiting
- Kafka Streams makes your stream processing applications elastic and scalable.
- You can add and remove processing capacity dynamically during application runtime
  without any downtime or data loss.
- The parallelism of a Kafka Streams application is primarily determined by how many
  partitions the input topics have.
- To achieve balanced workload processing across application instances and to prevent
  processing hotpots, you should distribute data and processing workloads.
- Requires OAuth2 authentication setup
- Always close the test driver at the end to make sure all resources are released
  properly.
- Ensure to configure the necessary security settings for authentication.
- State stores are fault-tolerant.
- The initializer and aggregator must be stateless.
- Input records with null keys are ignored.
- The windowed aggregate behaves per window.
- Typically, you should only disable record caches for testing or debugging purposes.
  Under normal circumstances, its better to leave record caches enabled.
- 'Data must be co-partitioned: The input data for both sides must be co-partitioned.'
- Causes data re-partitioning of a stream if and only if the stream was marked for
  re-partitioning (if both are marked, both are re-partitioned).
- The join is key-based, that is, with the join predicate leftRecord.key == rightRecord.key.
- Input records with a null key or a null value are ignored and do not trigger the
  join.
- Uses Kafka Streams for processing
- Suppression is just like any other Kafka Streams operator, so you can build a topology
  with two branches emerging from the count, one suppressed, and one not.
- If you dont want to reset your internal topics after you change window duration,
  you can erase RocksDB and let it rebuild from the changelog.
- Several deprecated methods in the Kafka Streams API have been removed in version
  4.0.
- Uses Kafka Streams DSL for Scala
- All Serdes specified in the config will be ignored by the Scala APIs
- Names of state stores and changelog/repartition topics are stateful while processor
  names are stateless.
- Giving user-defined names to state stores prevents name shifting due to topology
  changes.
- It is a good practice to name your processing nodes when using the DSL.
- If you have an existing topology and you haven't named your state stores and changelog
  topics, you should do so as a best practice.
- If you have a new topology, make sure you name the persistent parts of your topology.
- Optimizations are disabled by default.
- A rolling upgrade isn't possible after enabling optimizations.
- Combines the DSL with the Processor API for enhanced functionality.
- Stateful processors require state stores.
- Versioned stores dont support caching or interactive queries.
- You cant version window stores or global tables.
- Uses Schema Registry-compatible Serdes for serialization and deserialization.
- Requires configuration of schema.registry.url for Avro and Protobuf Serdes.
- Uses OAuth2 with refresh token  requires setup of a connected app in Confluent
- Some requests may require specific permissions
- Requires setup of connected app in api
- Querying state stores is always read-only to guarantee that the underlying state
  stores will never be mutated out-of-band.
- You must add a Remote Procedure Call (RPC) layer to your application to allow application
  instances to communicate over the network.
- The application server property must be configured for remote state store discovery.
- Some API endpoints may have specific rate limits.
- Performance Guidelines
- Schema Inference With ID
- You can specify the total memory (RAM) size used for internal caching and compacting
  of records.
- The record caches are implemented slightly different in the DSL and Processor API.
- With default settings, caching is enabled in Kafka Streams and RocksDB.
- The default RocksDB block-cache size is 50 MB per store, but the default size of
  the Kafka Streams record cache is 10 MB for caching for the entire instance.
- 'Iterators should be closed explicitly to release resources: Store iterators (e.g.,
  KeyValueIterator and WindowStoreIterator) must be closed explicitly upon completeness
  to release resources such as open file handlers and in-memory read buffers, or use
  try-with-resources statement (available since JDK7) for this Closeable class.'
- Otherwise, stream applications memory usage keeps increasing when running until
  it hits an OOM.
- User topics must be created and manually managed ahead of time (e.g., via the topic
  tools).
- If security is enabled on the Kafka brokers, you must grant the underlying clients
  admin permissions so that they can create internal topics.
- Client authentication and TLS/SSL encryption enabled for data-in-transit
- Monitor your Kafka Streams application log files for such error messages to spot
  any misconfigured applications quickly.
- OAuth2 authentication is required for accessing the API.
- Ensure the correct permissions are set for the client.
- Use this tool with care and double-check its parameters
- The application reset tool does not reset output topics of an application
- State stores arent cleaned up automatically when records in a topic expire.
- Kafka Streams uses Kafkas producer and consumer APIs.
- Load balancing is automatic.
- 'The store/RocksDB performance appears low: The workload might be IO bound.'
- 'RocksDBs file sizes appear larger than expected: RocksDB tends to allocate sparse
  files.'
- 'The apps memory utilization seems high: If you have many stores in your topology,
  there is a fixed per-store memory cost.'
- Kafka Streams can report metrics through JMX.
- The metrics recording level can be configured to info, debug, or trace.
- Dont confuse the runtime state of a KafkaStreams instance with state stores.
- The restoration process could take a non-negligible amount of time.
- All of the running instances of a Kafka Streams application appear as a single consumer
  group.
- Restore consumers of an application are displayed separately.
- Some objects may return nulls in deeply nested fields.
- Upgrading from any earlier Kafka Streams version to Confluent Platform 8.1.0 is
  supported.
- If you skip or delay the second rolling bounce, your deployment continues using
  the previous eager rebalancing protocol.
- All deprecated methods, classes, APIs, and config parameters up to and including
  Confluent Platform 7.6.x (Kafka Streams 3.6) have been removed.
- Kafka Streams now enables you to customize the logging interval of stream-thread
  runtime summary, by using the newly added configuration, log.summary.interval.ms.
- Downgrading from Confluent Platform 7.5.x (Kafka Streams 3.5.x) or later to Confluent
  Platform 7.4.x or earlier requires special attention.
- Starting with Confluent Platform 7.3.0, source and sink node metrics for consumed
  and produced throughput are available in Kafka Streams.
- Paused topologies skip processing, punctuation, and standby tasks.
- Kafka Streams applications are normal Java applications that use the Kafka Streams
  library.
- Kafka Streams has no external dependencies on systems other than Kafka.
- 'Be careful when returning null: null record values have special semantics for tables,
  where they are interpreted as tombstones that will cause the deletion of record
  keys from the table.'
- If you get an exception similar to the one shown below, there are multiple possible
  causes.
- This error means that the timestamp extractor of your Kafka Streams application
  failed to extract a valid timestamp from a record.
- If you are using the default `FailOnInvalidTimestamp` timestamp extractor, it is
  most likely that your records do not carry an embedded timestamp.
- You can consider using an alternative timestamp extractor like `UsePartitionTimeOnInvalidTimestamp`
  or `LogAndSkipOnInvalidTimestamp`.
- In this situation, you should debug and fix the erroneous extractor.
- There is a known issue with RocksDB when running in environments with just one CPU
  core.
- In some scenarios, the symptom is that the applications performance might be very
  slow or unresponsive.
- The workaround is to set a particular RocksDB configuration using the RocksDB config
  setter.
- Fully-managed data streaming platform with a cloud-native Kafka engine
- Some data fields may be nested or complex
- Secure REST Proxy
- Queries in ksqlDB, including non-persistent queries such as `SELECT * FROM myTable
  EMIT CHANGES`, are continuous streaming queries.
- ksqlDB doesnt clean up its internal topics? Make sure that your Apache Kafka cluster
  is configured with `delete.topic.enable=true`.
- The data that are produced are transient and are intended to be temporary.
- ksqlDB doesnt support structured keys.
- If the 'merge repartition topics' optimization is enabled in your configuration,
  which is the default, upgrading to ksqlDB 8.0.0 may be unsafe, due to a known bug
  in KStream.processValues().
- If you are migrating to Confluent Platform 8.0.x from earlier versions, you may
  want to use existing Log4j configs, but this is not recommended.
- Queries written for ksqlDB 6.1 continue to run without changes under ksqlDB 6.2.
  New queries issued on 6.2 may have different behavior, even if the text of the query
  statement is the same.
- 'If you try to downgrade ksqlDB to an earlier version, you may see the following
  internal RocksDB error: Caused by: org.rocksdb.RocksDBException: Unknown Footer
  version. Maybe this file was created with newer version of RocksDB?'
- As a solution, ensure that the RocksDB version is consistent across your environment.
- You may need to update the existing data to ensure compatibility with the current
  RocksDB version. This may involve steps like exporting the data before the version
  change and importing it after the change.
- You may be able to delete the state store so ksqlDB recreates the state store from
  the changelog topic.
- Fully-managed data streaming platform with a cloud-native Kafka engine (KORA) for
  elastic scaling, with enterprise security, stream processing, governance.
- Fully-managed data streaming platform with a cloud-native Kafka engine.
- Some features may be limited based on the selected Confluent product
- A fully-managed data streaming platform with a cloud-native Apache Kafka engine
  for elastic scaling, enterprise-grade security, stream processing, and governance.
- Some objects may have specific access controls
- Get started with the fully-managed Confluent Cloud Quick Start
- Both ksqlDB and Control Center must be running on either HTTP or HTTPS.
- When ksqlDB and Control Center communicate over a VPN, the advertised URL isnt
  localhost.
- ksqlDB allows you to query, read, write, and process data in Apache Kafka in real-time.
- ksqlDB is licensed under the Confluent Community License.
- Uses OAuth2 with refresh token  requires setup of connected app in Confluent Cloud
- ksqlDB doesnt support structured keys, so you cant create a stream from a windowed
  aggregate.
- Make sure that your Kafka cluster is configured with delete.topic.enable=true.
- If the interbroker listener of the broker that the REST Proxy is running on has
  security enabled and there is an authorizer.class.name configured, you must manually
  configure the Java clients in the REST Proxy so that they can securely communicate
  with Kafka.
- If confluent.metadata.server.listeners is used instead of confluent.http.server.listeners,
  then the Metadata Service will also be enabled on the same listener.
- If the interbroker listener of the broker that the REST Proxy is running on has
  security enabled, you must manually configure the Java clients in the REST Proxy.
- Without principal propagation, authentication terminates at the REST Proxy.
- HTTP Basic Authentication requires a username and password pair.
- HTTPS is recommended, but not required.
- RBAC-enabled Kafka and Schema Registry clusters are required.
- The REST Proxy does not store any state on disk.
- A fast and reliable network will likely have the biggest impact on the REST Proxys
  performance.
- Requires setup of Confluent Cloud for OAuth2 authentication
- The configuration property `auto.register.schemas` is not supported for Kafka REST
  Proxy.
- License configurations are only required if you are using principal propagation.
- Without the license key, you can use Confluent security plugins for a 30-day trial
  period.
- The REST Proxy reports a variety of metrics through JMX.
- Metrics for all requests are aggregated into a global instance for each one.
- No authorization actually occurs on REST Proxy (unlike in Schema Registry) so you
  must set broker ACLs to enforce any restrictions.
- If you have high latency from Schema Registry to MDS for requests, increase the
  timeout value of the optional setting confluent.metadata.http.request.timeout.ms
  to account for the extra latency.
- You may want to increase the value specified for the optional setting confluent.metadata.request.timeout.ms
  proportionally so that any retries have a sufficient buffer in time.
- Uses OAuth2 with refresh token.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Kafka Authentication Error'
- '403 Forbidden: Kafka Authorization Error'
- '404 Not Found: Topic not found'
- '422 Unprocessable Entity: The request payload is either improperly formatted or
  contains semantic errors'
- '500 Internal Server Error: Kafka error'
- '400 Bad Request: Serialization error'
- '422 Unprocessable Entity: Request includes invalid schema'
- '408 Request Timeout: Schema registration or lookup failed'
- '404 Not Found: Partition not found'
- '422 Unprocessable Entity: Request includes keys and uses a format that requires
  schemas'
- '409 Conflict: Consumer instance with the specified name already exists.'
- '422 Unprocessable Entity: Invalid consumer configuration.'
- '404 Not Found: Consumer instance not found'
- '406 Not Acceptable: Consumer format does not match the embedded format requested
  by the Accept header'
- '400 Bad Request: Indicates a bad request error. It could be caused by an unexpected
  request body format or other forms of request validation failure.'
- '401 Unauthorized: Indicates a client authentication error.'
- '403 Forbidden: Indicates a client authorization error.'
- '429 Too Many Requests: Indicates that a rate limit threshold has been reached,
  and the client should retry again later.'
- '500 Internal Server Error: A server-side problem that might not be addressable
  from the client side.'
- '400 Bad Request: Indicates a bad request error.'
- '429 Too Many Requests: Indicates that a rate limit threshold has been reached.'
- '5XX: A server-side problem that might not be addressable from the client side.'
- '401 Unauthorized: Indicates a client authentication error. Kafka authentication
  failures will contain error code 40101 in the response body.'
- '403 Forbidden: Indicates a client authorization error. Kafka authorization failures
  will contain error code 40301 in the response body.'
- '404 Not Found: Indicates attempted access to an unreachable or non-existing resource.'
- '204 No Content: No Content'
- '404 Not Found: Indicates attempted access to an unreachable or non-existing resource
  like e.g. an unknown topic or partition.'
- '404 Not Found: Broker not found.'
- '400 Bad Request: Indicates a bad request error'
- '401 Unauthorized: Authentication failed'
- '403 Forbidden: Request is not authorized'
- '429 Too Many Requests: Rate limit threshold has been reached'
- '404 Not Found: Broker not found'
- '500 Internal Server Error: Generic internal server error'
- '500 Internal Server Error: Indicates a server-side problem.'
- '401 Unauthorized: Authentication failed.'
- '403 Forbidden: Request is not authorized.'
- '429 Too Many Requests: A rate limit threshold has been reached.'
- '500 Internal Server Error: Internal Server Error'
- '5XX: A server-side problem that might not be addressable from the client side.
  Retriable Kafka errors will contain error code 50003 in the response body.'
- '500 Internal Server Error: A server-side problem.'
- '413 Request Entity Too Large: This implies the client is sending a request payload
  that is larger than the maximum message size the server can accept.'
- '415 Unsupported Media Type: This implies the client is sending the request payload
  format in an unsupported format.'
- '422 Unprocessable Entity: Indicates a bad request error. It could be caused by
  an unexpected request body format or other forms of request validation failure.'
- '400: Bad Request'
- '401: Unauthorized'
- '429: Too Many Requests'
- '500: Internal Server Error'
- '500 Internal Server Error: Indicates a generic internal server error.'
- '404 Not Found: Indicates that the requested resource could not be found.'
- '404 Not Found: The given broker ID was not registered in the cluster.'
- '401 Unauthorized: Check OAuth scopes or token expiration'
- '400 Bad Request: Check the request format and parameters'
- '404 Not Found: Verify the endpoint path'
- '500 Internal Server Error: Check the server logs for more details'
- '403 Forbidden: Verify permissions on the API key'
- '404 Not Found: Check the endpoint path'
- '500 Internal Server Error: Retry after some time'
- '401 Unauthorized: Check license key and permissions.'
- '500 Internal Server Error: Check server logs for details.'
- '401 Unauthorized: Recheck license key for enterprise features.'
- '400 Bad Request: Check your configuration settings.'
- '401 Unauthorized: Recheck security configurations.'
- '401 Unauthorized: Recheck your authentication and permissions.'
- '404 Not Found: Check the endpoint path.'
- '401 Unauthorized: Check your API key and secret'
- '403 Forbidden: Insufficient permissions for this request'
- '404 Not Found: The specified endpoint does not exist'
- '403 Forbidden: Check permissions for accessing the resource'
- 'unmet dependencies: occurs when trying to install a specific older Confluent package
  when a newer patch version is also available.'
- '401 Unauthorized: Check client credentials or token expiration'
- 'KafkaException: Operation failed locally or on broker.'
- 'TypeException: Invalid input.'
- 'ValueException: Invalid input.'
- 'TypeError: Invalid input type.'
- 'ValueError: Invalid input value.'
- '_BAD_MSG: Local: Bad message format'
- '_BAD_COMPRESSION: Local: Invalid compressed data'
- '_DESTROY: Local: Broker handle destroyed for termination'
- '_FAIL: Local: Communication failure with broker'
- '_TRANSPORT: Local: Broker transport failure'
- '_CRIT_SYS_RESOURCE: Local: Critical system resource failure'
- '_RESOLVE: Local: Host resolution failure'
- '_MSG_TIMED_OUT: Local: Message timed out'
- '_PARTITION_EOF: Broker: No more messages'
- '_UNKNOWN_PARTITION: Local: Unknown partition'
- '_FS: Local: File or filesystem error'
- '_UNKNOWN_TOPIC: Local: Unknown topic'
- '_ALL_BROKERS_DOWN: Local: All broker connections are down'
- '_INVALID_ARG: Local: Invalid argument or configuration'
- '_TIMED_OUT: Local: Timed out'
- '_QUEUE_FULL: Local: Queue full'
- '_ISR_INSUFF: Local: ISR count insufficient'
- '_NODE_UPDATE: Local: Broker node update'
- '_SSL: Local: SSL error'
- '_WAIT_COORD: Local: Waiting for coordinator'
- '_UNKNOWN_GROUP: Local: Unknown group'
- '_IN_PROGRESS: Local: Operation in progress'
- '_PREV_IN_PROGRESS: Local: Previous operation in progress'
- '_EXISTING_SUBSCRIPTION: Local: Existing subscription'
- '_ASSIGN_PARTITIONS: Local: Assign partitions'
- '_REVOKE_PARTITIONS: Local: Revoke partitions'
- '_CONFLICT: Local: Conflicting use'
- '_STATE: Local: Erroneous state'
- '_UNKNOWN_PROTOCOL: Local: Unknown protocol'
- '_NOT_IMPLEMENTED: Local: Not implemented'
- '_AUTHENTICATION: Local: Authentication failure'
- '_NO_OFFSET: Local: No offset stored'
- '_OUTDATED: Local: Outdated'
- '_TIMED_OUT_QUEUE: Local: Timed out in queue'
- '_UNSUPPORTED_FEATURE: Local: Required feature not supported by broker'
- '_WAIT_CACHE: Local: Awaiting cache update'
- '_INTR: Local: Operation interrupted'
- '_KEY_SERIALIZATION: Local: Key serialization error'
- '_VALUE_SERIALIZATION: Local: Value serialization error'
- '_KEY_DESERIALIZATION: Local: Key deserialization error'
- '_VALUE_DESERIALIZATION: Local: Value deserialization error'
- '_PARTIAL: Local: Partial response'
- '_READ_ONLY: Local: Read-only object'
- '_NOENT: Local: No such entry'
- '_UNDERFLOW: Local: Read underflow'
- '_INVALID_TYPE: Local: Invalid type'
- '_RETRY: Local: Retry operation'
- '_PURGE_QUEUE: Local: Purged in queue'
- '_PURGE_INFLIGHT: Local: Purged in flight'
- '_FATAL: Local: Fatal error'
- '_INCONSISTENT: Local: Inconsistent state'
- '_GAPLESS_GUARANTEE: Local: Gap-less ordering would not be guaranteed if proceeding'
- '_MAX_POLL_EXCEEDED: Local: Maximum application poll interval (max.poll.interval.ms)
  exceeded'
- '_UNKNOWN_BROKER: Local: Unknown broker'
- '_NOT_CONFIGURED: Local: Functionality not configured'
- '_FENCED: Local: This instance has been fenced by a newer instance'
- '_APPLICATION: Local: Application generated error'
- '_ASSIGNMENT_LOST: Local: Group partition assignment lost'
- '_NOOP: Local: No operation performed'
- '_AUTO_OFFSET_RESET: Local: No offset to automatically reset to'
- '_LOG_TRUNCATION: Local: Partition log truncation detected'
- '_INVALID_DIFFERENT_RECORD: Local: an invalid record in the same batch caused the
  failure of this message too'
- '_DESTROY_BROKER: Local: Broker handle destroyed without termination'
- 'UNKNOWN: Unknown broker error'
- 'NO_ERROR: Success'
- 'OFFSET_OUT_OF_RANGE: Broker: Offset out of range'
- 'INVALID_MSG: Broker: Invalid message'
- 'UNKNOWN_TOPIC_OR_PART: Broker: Unknown topic or partition'
- 'INVALID_MSG_SIZE: Broker: Invalid message size'
- 'LEADER_NOT_AVAILABLE: Broker: Leader not available'
- 'NOT_LEADER_FOR_PARTITION: Broker: Not leader for partition'
- 'REQUEST_TIMED_OUT: Broker: Request timed out'
- 'BROKER_NOT_AVAILABLE: Broker: Broker not available'
- 'REPLICA_NOT_AVAILABLE: Broker: Replica not available'
- 'MSG_SIZE_TOO_LARGE: Broker: Message size too large'
- 'STALE_CTRL_EPOCH: Broker: StaleControllerEpochCode'
- 'OFFSET_METADATA_TOO_LARGE: Broker: Offset metadata string too large'
- 'NETWORK_EXCEPTION: Broker: Broker disconnected before response received'
- 'COORDINATOR_LOAD_IN_PROGRESS: Broker: Coordinator load in progress'
- 'COORDINATOR_NOT_AVAILABLE: Broker: Coordinator not available'
- 'NOT_COORDINATOR: Broker: Not coordinator'
- 'TOPIC_EXCEPTION: Broker: Invalid topic'
- 'RECORD_LIST_TOO_LARGE: Broker: Message batch larger than configured server segment
  size'
- 'NOT_ENOUGH_REPLICAS: Broker: Not enough in-sync replicas'
- 'NOT_ENOUGH_REPLICAS_AFTER_APPEND: Broker: Message(s) written to insufficient number
  of in-sync replicas'
- 'INVALID_REQUIRED_ACKS: Broker: Invalid required acks value'
- 'ILLEGAL_GENERATION: Broker: Specified group generation id is not valid'
- 'INCONSISTENT_GROUP_PROTOCOL: Broker: Inconsistent group protocol'
- 'INVALID_GROUP_ID: Broker: Invalid group.id'
- 'UNKNOWN_MEMBER_ID: Broker: Unknown member'
- 'INVALID_SESSION_TIMEOUT: Broker: Invalid session timeout'
- 'REBALANCE_IN_PROGRESS: Broker: Group rebalance in progress'
- 'INVALID_COMMIT_OFFSET_SIZE: Broker: Commit offset data size is not valid'
- 'TOPIC_AUTHORIZATION_FAILED: Broker: Topic authorization failed'
- 'GROUP_AUTHORIZATION_FAILED: Broker: Group authorization failed'
- 'CLUSTER_AUTHORIZATION_FAILED: Broker: Cluster authorization failed'
- 'INVALID_TIMESTAMP: Broker: Invalid timestamp'
- 'UNSUPPORTED_SASL_MECHANISM: Broker: Unsupported SASL mechanism'
- 'ILLEGAL_SASL_STATE: Broker: Request not valid in current SASL state'
- 'UNSUPPORTED_VERSION: Broker: API version not supported'
- 'TOPIC_ALREADY_EXISTS: Broker: Topic already exists'
- 'INVALID_PARTITIONS: Broker: Invalid number of partitions'
- 'INVALID_REPLICATION_FACTOR: Broker: Invalid replication factor'
- 'INVALID_REPLICA_ASSIGNMENT: Broker: Invalid replica assignment'
- 'INVALID_CONFIG: Broker: Configuration is invalid'
- 'NOT_CONTROLLER: Broker: Not controller for cluster'
- 'INVALID_REQUEST: Broker: Invalid request'
- 'UNSUPPORTED_FOR_MESSAGE_FORMAT: Broker: Message format on broker does not support
  request'
- 'POLICY_VIOLATION: Broker: Policy violation'
- 'OUT_OF_ORDER_SEQUENCE_NUMBER: Broker: Broker received an out of order sequence
  number'
- 'DUPLICATE_SEQUENCE_NUMBER: Broker: Broker received a duplicate sequence number'
- 'INVALID_PRODUCER_EPOCH: Broker: Producer attempted to operate with an old epoch'
- 'INVALID_TXN_STATE: Broker: Producer attempted a transactional operation in an invalid
  state'
- 'INVALID_PRODUCER_ID_MAPPING: Broker: Producer attempted to use a producer id which
  is not currently assigned to its transactional'
- 'INVALID_TRANSACTION_TIMEOUT: Broker: Transaction timeout is larger than the maximum
  value allowed by the brokers max.transaction'
- 'CONCURRENT_TRANSACTIONS: Broker: Producer attempted to update a transaction while
  another concurrent operation on the same transaction is in progress'
- 'TRANSACTION_COORDINATOR_FENCED: Broker: Indicates that the transaction coordinator
  sending a WriteTxnMarker is no longer the current coordinator'
- 'TRANSACTIONAL_ID_AUTHORIZATION_FAILED: Broker: Transactional Id authorization failed'
- 'SECURITY_DISABLED: Broker: Security features are disabled'
- 'OPERATION_NOT_ATTEMPTED: Broker: Operation not attempted'
- 'KAFKA_STORAGE_ERROR: Broker: Disk error when trying to access log file on disk'
- 'LOG_DIR_NOT_FOUND: Broker: The user-specified log directory is not found in the
  broker config'
- 'SASL_AUTHENTICATION_FAILED: Broker: SASL Authentication failed'
- 'UNKNOWN_PRODUCER_ID: Broker: Unknown Producer Id'
- 'REASSIGNMENT_IN_PROGRESS: Broker: Partition reassignment is in progress'
- 'DELEGATION_TOKEN_AUTH_DISABLED: Broker: Delegation Token feature is not enabled'
- 'DELEGATION_TOKEN_NOT_FOUND: Broker: Delegation Token is not found on server'
- 'DELEGATION_TOKEN_OWNER_MISMATCH: Broker: Specified Principal is not valid Owner/Renewer'
- 'DELEGATION_TOKEN_REQUEST_NOT_ALLOWED: Broker: Delegation Token requests are not
  allowed on this connection'
- 'DELEGATION_TOKEN_AUTHORIZATION_FAILED: Broker: Delegation Token authorization failed'
- 'DELEGATION_TOKEN_EXPIRED: Broker: Delegation Token is expired'
- 'INVALID_PRINCIPAL_TYPE: Broker: Supplied principalType is not supported'
- 'NON_EMPTY_GROUP: Broker: The group is not empty'
- 'GROUP_ID_NOT_FOUND: Broker: The group id does not exist'
- 'FETCH_SESSION_ID_NOT_FOUND: Broker: The fetch session ID was not found'
- 'INVALID_FETCH_SESSION_EPOCH: Broker: The fetch session epoch is invalid'
- 'LISTENER_NOT_FOUND: Broker: No matching listener'
- 'TOPIC_DELETION_DISABLED: Broker: Topic deletion is disabled'
- 'FENCED_LEADER_EPOCH: Broker: Leader epoch is older than broker epoch'
- 'UNKNOWN_LEADER_EPOCH: Broker: Leader epoch is newer than broker epoch'
- 'UNSUPPORTED_COMPRESSION_TYPE: Broker: Unsupported compression type'
- 'STALE_BROKER_EPOCH: Broker: Broker epoch has changed'
- 'OFFSET_NOT_AVAILABLE: Broker: Leader high watermark is not caught up'
- 'MEMBER_ID_REQUIRED: Broker: Group member needs a valid member ID'
- 'PREFERRED_LEADER_NOT_AVAILABLE: Broker: Preferred leader was not available'
- 'GROUP_MAX_SIZE_REACHED: Broker: Consumer group has reached maximum size'
- 'FENCED_INSTANCE_ID: Broker: Static consumer fenced by other consumer with same
  group.instance.id'
- 'ELIGIBLE_LEADERS_NOT_AVAILABLE: Broker: Eligible partition leaders are not available'
- 'ELECTION_NOT_NEEDED: Broker: Leader election not needed for topic partition'
- 'NO_REASSIGNMENT_IN_PROGRESS: Broker: No partition reassignment is in progress'
- 'GROUP_SUBSCRIBED_TO_TOPIC: Broker: Deleting offsets of a topic while the consumer
  group is subscribed to it'
- 'INVALID_RECORD: Broker: Broker failed to validate record'
- 'UNSTABLE_OFFSET_COMMIT: Broker: There are unstable offsets that need to be cleared'
- 'THROTTLING_QUOTA_EXCEEDED: Broker: Throttling quota has been exceeded'
- 'PRODUCER_FENCED: Broker: There is a newer producer with the same transactionalId
  which fences the current one'
- 'RESOURCE_NOT_FOUND: Broker: Request illegally referred to resource that does not
  exist'
- 'DUPLICATE_RESOURCE: Broker: Request illegally referred to the same resource twice'
- 'UNACCEPTABLE_CREDENTIAL: Broker: Requested credential would not meet criteria for
  acceptability'
- 'INCONSISTENT_VOTER_SET: Broker: Indicates that either the sender or recipient of
  a voter-only request is not one of the configured voters'
- 'INVALID_UPDATE_VERSION: Broker: Invalid update version'
- 'FEATURE_UPDATE_FAILED: Broker: Unable to update finalized features due to server
  error'
- 'PRINCIPAL_DESERIALIZATION_FAILURE: Broker: Request principal deserialization failed
  during forwarding'
- 'UNKNOWN_TOPIC_ID: Broker: Unknown topic id'
- 'FENCED_MEMBER_EPOCH: Broker: The member epoch is fenced by the group coordinator'
- 'UNRELEASED_INSTANCE_ID: Broker: The instance ID is still used by another member
  in the consumer group'
- 'UNSUPPORTED_ASSIGNOR: Broker: The assignor or its version range is not supported
  by the consumer group'
- 'STALE_MEMBER_EPOCH: Broker: The member epoch is stale'
- 'UNKNOWN_SUBSCRIPTION_ID: Broker: Client sent a push telemetry request with an invalid
  or outdated subscription ID'
- 'TELEMETRY_TOO_LARGE: Broker: Client sent a push telemetry request larger than the
  maximum size the broker will accept'
- 'REBOOTSTRAP_REQUIRED: Broker: Client metadata is stale, client should rebootstrap
  to obtain new metadata'
- 'UNKNOWN_TOPIC_OR_PART: No longer returned if a topic is missing in the local cache
  when subscribing; the subscription proceeds.'
- 'TOPIC_AUTHORIZATION_FAILED: Reported once per heartbeat or subscription change,
  even if only one topic is unauthorized.'
- RuntimeError if called on a closed consumer
- 'UNKNOWN_TOPIC_OR_PART: No longer returned if a topic is missing in the local cache
  when subscribing.'
- 'TOPIC_AUTHORIZATION_FAILED: Reported once per heartbeat or subscription change.'
- KafkaException
- 1) the token data is invalid (meaning an expiration time in the past or either a
  token value or an extension key or value that does not meet the regular expression
  requirements as per https://tools.ietf.org/html/rfc7628#section-3.1);
- 2) SASL/OAUTHBEARER is not supported by the underlying librdkafka build;
- 3) SASL/OAUTHBEARER is supported but is not configured as the client's authentication
  mechanism.
- 'ERROR_CODE: Check documentation for specific error handling'
- 'ErrBadMsg: Bad message format'
- 'ErrBadCompression: Invalid compressed data'
- 'ErrDestroy: Broker handle destroyed for termination'
- 'ErrFail: Communication failure with broker'
- 'ErrTransport: Broker transport failure'
- 'ErrCritSysResource: Critical system resource failure'
- 'ErrResolve: Host resolution failure'
- 'ErrMsgTimedOut: Message timed out'
- 'ErrPartitionEOF: No more messages'
- 'ErrUnknownPartition: Unknown partition'
- 'ErrFs: File or filesystem error'
- 'ErrUnknownTopic: Unknown topic'
- 'ErrAllBrokersDown: All broker connections are down'
- 'ErrInvalidArg: Invalid argument or configuration'
- 'ErrTimedOut: Timed out'
- 'ErrQueueFull: Queue full'
- 'ErrIsrInsuff: ISR count insufficient'
- 'ErrNodeUpdate: Broker node update'
- 'ErrSsl: SSL error'
- 'ErrWaitCoord: Waiting for coordinator'
- 'ErrUnknownGroup: Unknown group'
- 'ErrInProgress: Operation in progress'
- 'ErrPrevInProgress: Previous operation in progress'
- 'ErrExistingSubscription: Existing subscription'
- 'ErrAssignPartitions: Assign partitions'
- 'ErrRevokePartitions: Revoke partitions'
- 'ErrConflict: Conflicting use'
- 'ErrState: Erroneous state'
- 'ErrUnknownProtocol: Unknown protocol'
- 'ErrNotImplemented: Not implemented'
- 'ErrAuthentication: Authentication failure'
- 'ErrNoOffset: No offset stored'
- 'ErrOutdated: Outdated'
- 'ErrTimedOutQueue: Timed out in queue'
- 'ErrUnsupportedFeature: Required feature not supported by broker'
- 'ErrWaitCache: Awaiting cache update'
- 'ErrIntr: Operation interrupted'
- 'ErrKeySerialization: Key serialization error'
- 'ErrValueSerialization: Value serialization error'
- 'ErrKeyDeserialization: Key deserialization error'
- 'ErrValueDeserialization: Value deserialization error'
- 'ErrPartial: Partial response'
- 'ErrReadOnly: Read-only object'
- 'ErrNoent: No such entry'
- 'ErrUnderflow: Read underflow'
- 'ErrInvalidType: Invalid type'
- 'ErrRetry: Retry operation'
- 'ErrPurgeQueue: Purged in queue'
- 'ErrPurgeInflight: Purged in flight'
- 'ErrFatal: Fatal error'
- 'ErrInconsistent: Inconsistent state'
- 'ErrGaplessGuarantee: Gap-less ordering would not be guaranteed if proceeding'
- 'ErrMaxPollExceeded: Maximum application poll interval (max.poll.interval.ms) exceeded'
- 'ErrUnknownBroker: Unknown broker'
- 'ErrNotConfigured: Functionality not configured'
- 'ErrFenced: This instance has been fenced by a newer instance'
- 'ErrApplication: Application generated error'
- 'ErrAssignmentLost: Group partition assignment lost'
- 'ErrNoop: No operation performed'
- 'ErrAutoOffsetReset: No offset to automatically reset to'
- 'ErrLogTruncation: Partition log truncation detected'
- 'ErrInvalidDifferentRecord: an invalid record in the same batch caused the failure
  of this message too'
- 'ErrDestroyBroker: Broker handle destroyed without termination'
- 'ErrUnknown: Unknown broker error'
- 'ErrNoError: Success'
- 'ErrOffsetOutOfRange: Offset out of range'
- 'ErrInvalidMsg: Invalid message'
- 'ErrUnknownTopicOrPart: Unknown topic or partition'
- 'ErrInvalidMsgSize: Invalid message size'
- 'ErrLeaderNotAvailable: Leader not available'
- 'ErrNotLeaderForPartition: Not leader for partition'
- 'ErrRequestTimedOut: Request timed out'
- 'ErrBrokerNotAvailable: Broker not available'
- 'ErrReplicaNotAvailable: Replica not available'
- 'ErrMsgSizeTooLarge: Message size too large'
- 'ErrStaleCtrlEpoch: StaleControllerEpochCode'
- 'ErrOffsetMetadataTooLarge: Offset metadata string too large'
- 'ErrNetworkException: Broker disconnected before response received'
- 'ErrCoordinatorLoadInProgress: Coordinator load in progress'
- 'ErrCoordinatorNotAvailable: Coordinator not available'
- 'ErrNotCoordinator: Not coordinator'
- 'ErrTopicException: Invalid topic'
- 'ErrRecordListTooLarge: Message batch larger than configured server segment size'
- 'ErrNotEnoughReplicas: Not enough in-sync replicas'
- 'ErrNotEnoughReplicasAfterAppend: Message(s) written to insufficient number of in-sync
  replicas'
- 'ErrInvalidRequiredAcks: Invalid required acks value'
- 'ErrIllegalGeneration: Specified group generation id is not valid'
- 'ErrInconsistentGroupProtocol: Inconsistent group protocol'
- 'ErrInvalidGroupID: Invalid group.id'
- 'ErrUnknownMemberID: Unknown member'
- 'ErrInvalidSessionTimeout: Invalid session timeout'
- 'ErrRebalanceInProgress: Group rebalance in progress'
- 'ErrInvalidCommitOffsetSize: Commit offset data size is not valid'
- 'ErrTopicAuthorizationFailed: Topic authorization failed'
- 'ErrGroupAuthorizationFailed: Group authorization failed'
- 'ErrClusterAuthorizationFailed: Cluster authorization failed'
- 'ErrInvalidTimestamp: Invalid timestamp'
- 'ErrUnsupportedSaslMechanism: Unsupported SASL mechanism'
- 'ErrIllegalSaslState: Request not valid in current SASL state'
- 'ErrUnsupportedVersion: API version not supported'
- 'ErrTopicAlreadyExists: Topic already exists'
- 'ErrInvalidPartitions: Invalid number of partitions'
- 'ErrInvalidReplicationFactor: Invalid replication factor'
- 'ErrInvalidReplicaAssignment: Invalid replica assignment'
- 'ErrInvalidConfig: Configuration is invalid'
- 'ErrNotController: Not controller for cluster'
- 'ErrInvalidRequest: Invalid request'
- 'ErrUnsupportedForMessageFormat: Message format on broker does not support request'
- 'ErrPolicyViolation: Policy violation'
- 'ErrOutOfOrderSequenceNumber: Broker received an out of order sequence number'
- 'ErrDuplicateSequenceNumber: Broker received a duplicate sequence number'
- 'ErrInvalidProducerEpoch: Producer attempted an operation with an old epoch'
- 'ErrInvalidTxnState: Producer attempted a transactional operation in an invalid
  state'
- 'ErrInvalidProducerIDMapping: Producer attempted to use a producer id which is not
  currently assigned to its transactional id'
- 'ErrInvalidTransactionTimeout: Transaction timeout is larger than the maximum value
  allowed by the broker''s max.transaction.timeout.ms'
- 'ErrConcurrentTransactions: Producer attempted to update a transaction while another
  concurrent operation on the same transaction was ongoing'
- 'ErrTransactionCoordinatorFenced: Indicates that the transaction coordinator sending
  a WriteTxnMarker is no longer the current coordinator for a given producer'
- 'ErrTransactionalIDAuthorizationFailed: Transactional Id authorization failed'
- 'ErrSecurityDisabled: Security features are disabled'
- 'ErrOperationNotAttempted: Operation not attempted'
- 'ErrKafkaStorageError: Disk error when trying to access log file on disk'
- 'ErrLogDirNotFound: The user-specified log directory is not found in the broker
  config'
- 'ErrSaslAuthenticationFailed: SASL Authentication failed'
- 'ErrUnknownProducerID: Unknown Producer Id'
- 'ErrReassignmentInProgress: Partition reassignment is in progress'
- 'ErrDelegationTokenAuthDisabled: Delegation Token feature is not enabled'
- 'ErrDelegationTokenNotFound: Delegation Token is not found on server'
- 'ErrDelegationTokenOwnerMismatch: Specified Principal is not valid Owner/Renewer'
- 'ErrDelegationTokenRequestNotAllowed: Delegation Token requests are not allowed
  on this connection'
- 'ErrDelegationTokenAuthorizationFailed: Delegation Token authorization failed'
- 'ErrDelegationTokenExpired: Delegation Token is expired'
- C.RD_KAFKA_RESP_ERR_INVALID_PRINCIPAL_TYPE
- C.RD_KAFKA_RESP_ERR_NON_EMPTY_GROUP
- C.RD_KAFKA_RESP_ERR_GROUP_ID_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_FETCH_SESSION_ID_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_INVALID_FETCH_SESSION_EPOCH
- C.RD_KAFKA_RESP_ERR_LISTENER_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_TOPIC_DELETION_DISABLED
- C.RD_KAFKA_RESP_ERR_FENCED_LEADER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNKNOWN_LEADER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNSUPPORTED_COMPRESSION_TYPE
- C.RD_KAFKA_RESP_ERR_STALE_BROKER_EPOCH
- C.RD_KAFKA_RESP_ERR_OFFSET_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_MEMBER_ID_REQUIRED
- C.RD_KAFKA_RESP_ERR_PREFERRED_LEADER_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_GROUP_MAX_SIZE_REACHED
- C.RD_KAFKA_RESP_ERR_FENCED_INSTANCE_ID
- C.RD_KAFKA_RESP_ERR_ELIGIBLE_LEADERS_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_ELECTION_NOT_NEEDED
- C.RD_KAFKA_RESP_ERR_NO_REASSIGNMENT_IN_PROGRESS
- C.RD_KAFKA_RESP_ERR_GROUP_SUBSCRIBED_TO_TOPIC
- C.RD_KAFKA_RESP_ERR_INVALID_RECORD
- C.RD_KAFKA_RESP_ERR_UNSTABLE_OFFSET_COMMIT
- C.RD_KAFKA_RESP_ERR_THROTTLING_QUOTA_EXCEEDED
- C.RD_KAFKA_RESP_ERR_PRODUCER_FENCED
- C.RD_KAFKA_RESP_ERR_RESOURCE_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_DUPLICATE_RESOURCE
- C.RD_KAFKA_RESP_ERR_UNACCEPTABLE_CREDENTIAL
- C.RD_KAFKA_RESP_ERR_INCONSISTENT_VOTER_SET
- C.RD_KAFKA_RESP_ERR_INVALID_UPDATE_VERSION
- C.RD_KAFKA_RESP_ERR_FEATURE_UPDATE_FAILED
- C.RD_KAFKA_RESP_ERR_PRINCIPAL_DESERIALIZATION_FAILURE
- C.RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_ID
- C.RD_KAFKA_RESP_ERR_FENCED_MEMBER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNRELEASED_INSTANCE_ID
- C.RD_KAFKA_RESP_ERR_UNSUPPORTED_ASSIGNOR
- C.RD_KAFKA_RESP_ERR_STALE_MEMBER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNKNOWN_SUBSCRIPTION_ID
- C.RD_KAFKA_RESP_ERR_TELEMETRY_TOO_LARGE
- C.RD_KAFKA_RESP_ERR_REBOOTSTRAP_REQUIRED
- 'UNSUPPORTED_VERSION: Kafka brokers that return an UNSUPPORTED_VERSION error when
  they receive a request with any of the removed API versions.'
- 'UnsupportedVersionException: Java clients that throw UnsupportedVersionException
  when interacting with brokers that do not support the minimum protocol API versions.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- '400 Bad Request: Check the request parameters'
- '500 Internal Server Error: Try the request again later'
- '401 Unauthorized: Check your client credentials'
- '403 Forbidden: Insufficient permissions for the requested resource'
- '401 Unauthorized: Check client credentials and token validity'
- '400 Bad Request: Check the request parameters.'
- '404 Not Found: Verify endpoint path.'
- '500 Internal Server Error: Retry the request.'
- 'FORBIDDEN: Check user permissions or scopes'
- 'Failed to construct kafka producer: Keystore was tampered with, or password was
  incorrect'
- Password verification failed
- '401 Unauthorized: Check client ID and secret.'
- '403 Forbidden: Ensure appropriate permissions are granted.'
- 'Invalid application ID: Check for typos'
- 'Consumer group still active: Ensure all instances are stopped'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- 'IllegalStateException: This should not happen as topic() should only be called
  while a record is processed.'
- 'REQUEST_LIMIT_EXCEEDED: Reduce API call frequency'
- '401 Unauthorized: Check OAuth credentials or token expiration'
- 'Connection reset: Check the server port and ensure it is running.'
- Mismatch between the ksqlDB CLI version and the ksqlDB server version.
- 'SocketException: Connection reset'
- 'java.lang.NullPointerException: at index 2'
- 'java.lang.NoClassDefFoundError: Could not initialize class org.rocksdb.Options'
- '401 Unauthorized: Check your credentials and OAuth scopes'
- 'consumer.instance.timeout.ms: Default is equivalent to 5 minutes (300,000ms).'
- '403 Forbidden: User does not have the requisite role or ACL permission for the
  requested resource.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - OAuthToken
client:
  base_url: https://www.confluent.io
  headers:
    Accept: application/json
source_metadata: null
