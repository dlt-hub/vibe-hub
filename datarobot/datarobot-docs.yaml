resources:
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: records
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
- name: registered_model
  endpoint:
    path: /registeredModels/
    method: GET
    data_selector: id
- name: model_version
  endpoint:
    path: /registeredModels/${existing_model_id}/versions/
    method: POST
    data_selector: id
- name: registered_models
  endpoint:
    path: /registeredModels/
    method: GET
    data_selector: id
- name: model_versions
  endpoint:
    path: /registeredModels/${existing_model_id}/versions/
    method: POST
    data_selector: id
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
    params: {}
- name: prediction_environment
  endpoint:
    path: /deployments/{deployment_id}/
    method: GET
    data_selector: predictionUrl
- name: registered_model
  endpoint:
    path: /models/
    method: POST
    data_selector: modelId
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: records
- name: projects
  endpoint:
    path: /projects
    method: GET
- name: predictionEnvironments
  endpoint:
    path: /predictionEnvironments
    method: GET
- name: registeredModels
  endpoint:
    path: /registeredModels
    method: GET
- name: deployments
  endpoint:
    path: /deployments/fromRegisteredModelVersion
    method: POST
- name: prediction
  endpoint:
    path: /deployments/${deployment_id}/
    method: GET
    data_selector: predictionUrl
- name: registered_model
  endpoint:
    path: /models/
    method: POST
    data_selector: registeredModel
- name: registered_model_version
  endpoint:
    path: /registeredModels/${registered_model_id}/versions/
    method: POST
    data_selector: modelVersion
- name: model_status
  endpoint:
    path: /registeredModels/${registered_model_version_id}/
    method: GET
    data_selector: buildStatus
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
- name: predictionEnvironments
  endpoint:
    path: /predictionEnvironments
    method: GET
- name: registeredModels
  endpoint:
    path: /registeredModels
    method: GET
- name: deployments
  endpoint:
    path: /deployments/fromRegisteredModelVersion/
    method: POST
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: assets
  endpoint:
    path: /api/v2/assets
    method: GET
    data_selector: records
- name: import_methods
  endpoint:
    path: /import/methods
    method: GET
    data_selector: methods
    params: {}
- name: Users
  endpoint:
    path: /path/to/users
    method: GET
    data_selector: records
- name: Orders
  endpoint:
    path: /path/to/orders
    method: GET
    data_selector: records
- name: Transactions
  endpoint:
    path: /path/to/transactions
    method: GET
    data_selector: records
- name: primary_dataset
  endpoint:
    path: /primary/dataset
    method: POST
    data_selector: datasets
    params: {}
- name: secondary_dataset
  endpoint:
    path: /secondary/dataset
    method: POST
    data_selector: datasets
    params: {}
- name: Users
  endpoint:
    path: /ai_catalog/users
    method: GET
    data_selector: records
- name: Orders
  endpoint:
    path: /ai_catalog/orders
    method: GET
    data_selector: records
- name: Transactions
  endpoint:
    path: /ai_catalog/transactions
    method: GET
    data_selector: records
- name: real_time_scoring_methods
  endpoint:
    path: /predictions/realtime/index.html
    method: GET
- name: batch_prediction_methods
  endpoint:
    path: /predictions/batch/index.html
    method: GET
- name: portable_prediction_methods
  endpoint:
    path: /predictions/port-pred/index.html
    method: GET
- name: predictions_testing
  endpoint:
    path: /predictions/pred-test.html
    method: GET
- name: predictions_reference
  endpoint:
    path: /predictions/pred-file-limits.html
    method: GET
- name: primary_dataset
  endpoint:
    path: /primary/dataset
    method: GET
    data_selector: records
- name: secondary_dataset
  endpoint:
    path: /secondary/dataset
    method: GET
    data_selector: records
- name: data_quality_checks
  endpoint:
    path: /data/quality/checks
    method: GET
    data_selector: checks
    params: {}
- name: feature_engineering_controls
  endpoint:
    path: /feature_engineering_controls
    method: GET
    data_selector: transformations
    params: {}
- name: feature_reduction
  endpoint:
    path: /feature_reduction
    method: GET
    data_selector: redundant_features
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: Feature Associations Matrix
  endpoint:
    path: /feature_associations
    method: GET
    data_selector: matrix
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: raw_dataset
  endpoint:
    path: /
    method: GET
    data_selector: records
- name: transformed_dataset
  endpoint:
    path: /
    method: POST
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: raw_dataset
  endpoint:
    path: /
    method: GET
    data_selector: records
- name: transformed_dataset
  endpoint:
    path: /
    method: POST
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
    params: {}
- name: leaderboard
  endpoint:
    path: /leaderboard
    method: GET
    data_selector: models
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: feature_list
  endpoint:
    path: /feature-lists
    method: GET
    data_selector: feature_lists
- name: Download Scoring Code from the Leaderboard
  endpoint:
    path: /predictions/port-pred/scoring-code/sc-download-leaderboard.html
    method: GET
    data_selector: records
    params: {}
- name: Download Scoring Code from a deployment
  endpoint:
    path: /predictions/port-pred/scoring-code/sc-download-deployment.html
    method: GET
    data_selector: records
    params: {}
- name: Download Scoring Code from the Leaderboard (Legacy)
  endpoint:
    path: /predictions/port-pred/scoring-code/sc-download-legacy.html
    method: GET
    data_selector: records
    params: {}
- name: Scoring Code for time series projects
  endpoint:
    path: /predictions/port-pred/scoring-code/sc-time-series.html
    method: GET
    data_selector: records
    params: {}
- name: Scoring at the command line
  endpoint:
    path: /predictions/port-pred/scoring-code/scoring-cli.html
    method: GET
    data_selector: records
    params: {}
- name: external_model
  endpoint:
    path: /external/model
    method: POST
    data_selector: model
    params: {}
- name: external_model
  endpoint:
    path: /external/models
    method: POST
    data_selector: model
    params: {}
- name: Train-time image augmentation
  endpoint:
    path: /special-workflows/visual-ai/tti-augment
    method: GET
- name: predictions
  endpoint:
    path: /predictions
    method: GET
    data_selector: records
- name: accuracy_over_time
  endpoint:
    path: /accuracy_over_time
    method: GET
    data_selector: data
    params: {}
- name: batch_predictions
  endpoint:
    path: /predictions/batch
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: period_definition_file
  endpoint:
    path: /path/to/period_definition_file
    method: POST
    data_selector: results
    params: {}
- name: compute_period_accuracy
  endpoint:
    path: /path/to/compute_period_accuracy
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: confusion_matrix
  endpoint:
    path: /confusion-matrix
    method: GET
    data_selector: matrix
    params: {}
- name: Series Insights
  endpoint:
    path: /series-insights
    method: GET
- name: series_insights
  endpoint:
    path: /series/insights
    method: GET
    data_selector: series_data
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: series_insights
  endpoint:
    path: /series/insights
    method: GET
    data_selector: series
    params: {}
- name: model_selection
  endpoint:
    path: /model/selection
    method: GET
    data_selector: models
    params: {}
- name: loss_graph
  endpoint:
    path: /loss/graph
    method: GET
    data_selector: loss_data
    params: {}
- name: accuracy_graph
  endpoint:
    path: /accuracy/graph
    method: GET
    data_selector: accuracy_data
    params: {}
- name: learning_rate
  endpoint:
    path: /learning/rate
    method: GET
    data_selector: learning_rate_data
    params: {}
- name: momentum
  endpoint:
    path: /momentum
    method: GET
    data_selector: momentum_data
    params: {}
- name: hyperparameter_comparison
  endpoint:
    path: /hyperparameter/comparison
    method: GET
    data_selector: hyperparameter_data
    params: {}
- name: prediction_explanations
  endpoint:
    path: /xemp/prediction/explanations
    method: GET
    data_selector: explanations
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Prediction Explanations
  endpoint:
    path: /prediction_explanations
    method: GET
    data_selector: predictions
    params: {}
- name: prediction_explanations
  endpoint:
    path: /prediction_explanations/clusters
    method: GET
- name: prediction_explanations
  endpoint:
    path: /predictions/explanations
    method: GET
    data_selector: explanations
- name: Prediction Explanations
  endpoint:
    path: /prediction_explanations
    method: GET
    data_selector: explanations
    params: {}
- name: Upload Dataset
  endpoint:
    path: /upload_dataset
    method: POST
    data_selector: upload_status
    params: {}
- name: cluster_insights
  endpoint:
    path: /prediction_explanations/cluster_insights
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: prediction_explanations
  endpoint:
    path: /get-prediction-explanations
    method: GET
    data_selector: explanations
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: coefficient_chart
  endpoint:
    path: /coefficients
    method: GET
- name: Coefficients
  endpoint:
    path: /coefficients
    method: GET
    data_selector: records
- name: coefficients
  endpoint:
    path: /coefficients
    method: GET
    data_selector: coefficients
    params: {}
- name: export
  endpoint:
    path: /export
    method: GET
    data_selector: exports
    params: {}
- name: coefficients
  endpoint:
    path: /api/v1/coefficients
    method: GET
    data_selector: parameters
    params: {}
- name: Eureqa Model
  endpoint:
    path: /eureqa/models
    method: GET
    data_selector: models
- name: Eureqa Model
  endpoint:
    path: /eureqa/models
    method: GET
    data_selector: models
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: rating_table_validation
  endpoint:
    path: /rating-table-validation
    method: GET
    data_selector: records
    params: {}
- name: GA2M output
  endpoint:
    path: /ga2m/output
    method: GET
    data_selector: output
- name: rating_table
  endpoint:
    path: /rating-table
    method: GET
    data_selector: records
- name: GA2M output
  endpoint:
    path: /ga2m/output
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: compliance_template
  endpoint:
    path: /template/compliance
    method: GET
    data_selector: templates
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: predictions
  endpoint:
    path: /predictions
    method: POST
    data_selector: results
- name: Create and edit templates
- name: Customize templates with key values
- name: Share a template
- name: activation_maps
  endpoint:
    path: /activation/maps
    method: GET
    data_selector: records
- name: anomaly_detection
  endpoint:
    path: /anomaly/detection
    method: GET
    data_selector: records
- name: category_cloud
  endpoint:
    path: /category/cloud
    method: GET
    data_selector: records
- name: hotspots
  endpoint:
    path: /hotspots
    method: GET
    data_selector: records
- name: image_embeddings
  endpoint:
    path: /image/embeddings
    method: GET
    data_selector: records
- name: text_mining
  endpoint:
    path: /text/mining
    method: GET
    data_selector: records
- name: word_cloud
  endpoint:
    path: /word/cloud
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Cross-Class Data Disparity
  endpoint:
    path: /cross-class-data-disparity
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: activation_maps
  endpoint:
    path: /activation/maps
    method: GET
    data_selector: visualizations
    params: {}
- name: anomaly_detection
  endpoint:
    path: /anomaly/detection
    method: GET
    data_selector: results
    params: {}
- name: category_cloud
  endpoint:
    path: /category/cloud
    method: GET
    data_selector: features
    params: {}
- name: hotspots
  endpoint:
    path: /hotspots
    method: GET
    data_selector: rules
    params: {}
- name: image_embeddings
  endpoint:
    path: /image/embeddings
    method: GET
    data_selector: images
    params: {}
- name: text_mining
  endpoint:
    path: /text/mining
    method: GET
    data_selector: words
    params: {}
- name: word_cloud
  endpoint:
    path: /word/cloud
    method: GET
    data_selector: keywords
    params: {}
- name: blueprint
  endpoint:
    path: /blueprint
    method: GET
    data_selector: data
- name: custom_task
  endpoint:
    path: /cml/custom_tasks
    method: POST
    data_selector: task
    params: {}
- name: DRUM Installation
  endpoint:
    path: /installation
    method: GET
    data_selector: installation_steps
    params: {}
- name: DRUM Usage
  endpoint:
    path: /usage
    method: GET
    data_selector: usage_steps
    params: {}
- name: 10K Lending Club Loans
  endpoint:
    path: /10K_Lending_Club_Loans.csv
    method: GET
    data_selector: data
    params: {}
- name: custom_task
  endpoint:
    path: /custom/task
    method: POST
    data_selector: task
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: installation_workflow
  endpoint:
    path: /install
    method: GET
    data_selector: installation
- name: performance_testing
  endpoint:
    path: /performance
    method: GET
    data_selector: performance
- name: augmented_models
  endpoint:
    path: /api/train-time-image-augmentation
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: image_embeddings
  endpoint:
    path: /understand/image_embeddings
    method: GET
    data_selector: images
    params: {}
- name: activation_maps
  endpoint:
    path: /understand/activation_maps
    method: GET
    data_selector: maps
    params: {}
- name: neural_network_visualizer
  endpoint:
    path: /describe/neural_network_visualizer
    method: GET
    data_selector: layers
    params: {}
- name: anomaly_detection
  endpoint:
    path: /anomaly_detection
    method: POST
    data_selector: results
    params: {}
- name: Visual AI overview
  endpoint:
    path: /vai-overview.html
    method: GET
- name: Build Visual AI models
  endpoint:
    path: /vai-model.html
    method: GET
- name: Train-time image augmentation
  endpoint:
    path: /tti-augment/index.html
    method: GET
- name: Model insights
  endpoint:
    path: /vai-insights.html
    method: GET
- name: Tune models
  endpoint:
    path: /vai-tuning.html
    method: GET
- name: Visual AI predictions
  endpoint:
    path: /vai-predictions.html
    method: GET
- name: multilabel_modeling
  endpoint:
    path: /multilabel/modeling
    method: GET
    data_selector: data
- name: Isolation Forest
  endpoint:
    details: Up to 2 million rows, Dataset < 500MB, Number of numerical + categorical
      + text columns > 2, Up to 26 text columns
- name: Double Mean Absolute Deviation (MAD)
  endpoint:
    details: Any number of rows, Datasets of all sizes, Up to 26 text columns
- name: One Class Support Vector Machine (SVM)
  endpoint:
    details: Up to 10,000 rows, Dataset < 500MB, Number of numerical + categorical
      + text columns < 500
- name: Local outlier factor
  endpoint:
    details: Up to 500,001 rows, Dataset < 500MB, Up to 26 text columns
- name: Mahalanobis Distance
  endpoint:
    details: Any number of rows, Datasets of all sizes, Up to 26 text columns, At
      least one numerical or categorical column
- name: backtest_partitions
  endpoint:
    path: /set-backtest-partitions-globally
    method: GET
- name: time_aware_models
  endpoint:
    path: /build-time-aware-models
    method: GET
- name: augmented_models
  endpoint:
    path: /train-time-image-augmentation/augmented_models
    method: GET
    data_selector: records
- name: Image Embeddings
  endpoint:
    path: /analyze-models/understand/image-embeddings
    method: GET
    data_selector: images
    params: {}
- name: Activation Maps
  endpoint:
    path: /analyze-models/understand/activation-maps
    method: GET
    data_selector: maps
    params: {}
- name: Combined Model
  endpoint:
    path: /models/combined
    method: POST
    data_selector: model
- name: backtest_partitions
  endpoint:
    path: /set-backtest-partitions
    method: GET
- name: time_aware_models
  endpoint:
    path: /build-time-aware-models
    method: GET
- name: date/time partitioning
  endpoint:
    path: /services/data/vXX.X/sobjects/Date/TimePartitioning
    method: GET
    data_selector: records
- name: Create the modeling dataset
  endpoint:
    path: /ts-create-data
    method: GET
- name: Data prep for time series
  endpoint:
    path: /ts-data-prep
    method: GET
- name: Restore features removed by reduction
  endpoint:
    path: /restore-features
    method: GET
- name: modeling_dataset
  endpoint:
    path: /api/modeling_dataset
    method: GET
- name: model_package_download
  endpoint:
    path: /Predict/Portable Predictions
    method: GET
- name: deployment_model_package_download
  endpoint:
    path: /Predictions/Portable Predictions
    method: GET
- name: hyperparameters
  endpoint:
    path: /model-metadata.yaml
    method: GET
    data_selector: hyperparameters
    params: {}
- name: Combined Model
  endpoint:
    path: /api/v1/combined_model
    method: POST
    data_selector: model
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: predictions
  endpoint:
    path: /api/reference/batch-prediction-api/index.html
    method: POST
    data_selector: predictions
    params: {}
- name: prediction_source
  endpoint:
    path: /path/to/prediction/source
    method: POST
    data_selector: records
    params: {}
- name: prediction_destination
  endpoint:
    path: /path/to/prediction/destination
    method: POST
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: monitoring_jobs
  endpoint:
    path: /monitoring/jobs
    method: POST
    data_selector: job_definition
- name: batchMonitoringJobDefinitions
  endpoint:
    path: /api/v2/batchMonitoringJobDefinitions/
    method: POST
- name: batchJobs
  endpoint:
    path: /api/v2/batchJobs/
    method: GET
- name: hyperparameters
  endpoint:
    path: /model-metadata.yaml
    method: GET
    data_selector: hyperparameters
    params: {}
- name: Real-time predictions
  endpoint:
    path: /realtime/index.html
    method: GET
- name: Batch predictions
  endpoint:
    path: /batch/index.html
    method: GET
- name: Portable predictions
  endpoint:
    path: /port-pred/index.html
    method: GET
- name: Monitor external predictions
  endpoint:
    path: /batch/pred-monitoring-jobs/index.html
    method: GET
- name: Batch Prediction API
  endpoint:
    path: /batch/batch-dep/index.html
    method: GET
- name: Scoring Code
  endpoint:
    path: /port-pred/scoring-code/index.html
    method: GET
- name: Portable Prediction Server
  endpoint:
    path: /port-pred/pps/index.html
    method: GET
- name: scoring_code
  endpoint:
    path: /mlops/deployment/download_scoring_code
    method: GET
    data_selector: scoring_code
    params: {}
- name: scoring_code
  endpoint:
    path: /leaderboard/scoring-code
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
- name: prediction_source
  endpoint:
    path: /prediction/source
    method: POST
    data_selector: data
    params: {}
- name: prediction_destination
  endpoint:
    path: /prediction/destination
    method: POST
    data_selector: data
    params: {}
- name: prediction_monitoring_jobs
  endpoint:
    path: /api/prediction_monitoring_jobs
    method: GET
    data_selector: jobs
    params: {}
- name: batchMonitoringJobDefinitions
  endpoint:
    path: /batchMonitoringJobDefinitions
    method: GET
- name: batchJobs
  endpoint:
    path: /batchJobs
    method: GET
- name: batchMonitoringJobDefinitions
  endpoint:
    path: /batchMonitoringJobDefinitions/
    method: POST
    data_selector: ''
    params: {}
- name: batchJobs
  endpoint:
    path: /batchJobs/
    method: GET
    data_selector: ''
    params: {}
- name: predictions
  endpoint:
    path: /predictions
    method: POST
    data_selector: data
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: ''
- name: ping
  endpoint:
    path: /ping
    method: GET
    data_selector: ''
- name: deployments_predictions
  endpoint:
    path: /deployments/:id/predictions
    method: POST
    data_selector: data
- name: deployments_info
  endpoint:
    path: /deployments/:id/info
    method: GET
    data_selector: ''
- name: deployments_upload
  endpoint:
    path: /deployments/:id
    method: POST
    data_selector: ''
- name: deployments_delete
  endpoint:
    path: /deployments/:id
    method: DELETE
    data_selector: ''
- name: deployments_list
  endpoint:
    path: /deployments
    method: GET
    data_selector: ''
- name: time_series_parameters
  endpoint:
    path: /time_series/parameters
    method: GET
    data_selector: parameters
    params: {}
- name: job_definition_jdbc
  endpoint:
    path: /docker/container/filesystem/path/job_definition_jdbc.json
    method: POST
    data_selector: prediction_endpoint
    params: {}
- name: job_definition_filesystem
  endpoint:
    path: /docker/container/filesystem/path/job_definition.json
    method: POST
    data_selector: prediction_endpoint
    params: {}
- name: job_definition_jdbc
  endpoint:
    path: /tmp/job_definition_jdbc.json
    method: POST
    data_selector: records
- name: job_definition_filesystem
  endpoint:
    path: /tmp/job_definition.json
    method: POST
    data_selector: records
- name: intake_settings
  endpoint:
    path: /tmp/portable_batch_predictions/job_definition.json
    method: POST
    data_selector: intake_settings
    params: {}
- name: output_settings
  endpoint:
    path: /tmp/portable_batch_predictions/job_definition.json
    method: POST
    data_selector: output_settings
    params: {}
- name: intake_settings
  endpoint:
    path: https://batchpredictionsdev.blob.core.windows.net/datasets/euro_date.csv
    method: GET
    data_selector: data
    params: {}
- name: output_settings
  endpoint:
    path: https://batchpredictionsdev.blob.core.windows.net/results/output_ts.csv
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: predictions
  endpoint:
    path: /predictions
    method: POST
- name: info
  endpoint:
    path: /info
    method: GET
- name: ping
  endpoint:
    path: /ping
    method: GET
- name: drum_environment_variables
  endpoint:
    path: /drum/env
    method: GET
    data_selector: variables
    params:
      MODEL_ID: Autogenerated value
      DEPLOYMENT_ID: Undefined; must be provided.
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: batch_prediction
  endpoint:
    path: /batch/prediction
    method: POST
    data_selector: predictions
    params: {}
- name: monitoring_jobs
  endpoint:
    path: /monitoring/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: predictions
  endpoint:
    path: /predictions
    method: POST
    data_selector: data
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: ''
- name: ping
  endpoint:
    path: /ping
    method: GET
    data_selector: ''
- name: deployments_predictions
  endpoint:
    path: /deployments/:id/predictions
    method: POST
    data_selector: data
- name: deployments_info
  endpoint:
    path: /deployments/:id/info
    method: GET
    data_selector: ''
- name: deployments_upload
  endpoint:
    path: /deployments/:id
    method: POST
    data_selector: ''
- name: deployments_delete
  endpoint:
    path: /deployments/:id
    method: DELETE
    data_selector: ''
- name: deployments_list
  endpoint:
    path: /deployments
    method: GET
    data_selector: ''
- name: job_definition
  endpoint:
    path: /tmp/job_definition.json
    method: POST
    data_selector: prediction_endpoint
    params: {}
- name: jdbc_scoring
  endpoint:
    path: /jdbc/settings
    method: POST
    data_selector: jdbc_settings
- name: filesystem_scoring
  endpoint:
    path: /filesystem/settings
    method: POST
    data_selector: filesystem_settings
- name: s3_scoring
  endpoint:
    path: /s3/settings
    method: POST
    data_selector: s3_settings
- name: intake_settings
  endpoint:
    path: /tmp/portable_batch_predictions/job_definition.json
    method: POST
    data_selector: intake_settings
    params: {}
- name: output_settings
  endpoint:
    path: /tmp/portable_batch_predictions/job_definition.json
    method: POST
    data_selector: output_settings
    params: {}
- name: intake_settings
  endpoint:
    path: /datasets/euro_date.csv
    method: GET
    data_selector: data
    params: {}
- name: output_settings
  endpoint:
    path: /results/output_ts.csv
    method: GET
    data_selector: data
    params: {}
- name: Model Registry
  endpoint:
    path: /reg-create.html
    method: GET
- name: Register DataRobot models
  endpoint:
    path: /dr-model-reg.html
    method: GET
- name: Register custom models
  endpoint:
    path: /reg-custom-models.html
    method: GET
- name: Register external models
  endpoint:
    path: /reg-external-models.html
    method: GET
- name: Deploy registered models
  endpoint:
    path: /reg-deploy.html
    method: GET
- name: Manage model packages
  endpoint:
    path: /reg-action.html
    method: GET
- name: Generate model compliance documentation
  endpoint:
    path: /reg-compliance.html
    method: GET
- name: Customize compliance documentation with key values
  endpoint:
    path: /reg-key-values.html
    method: GET
- name: Custom jobs
  endpoint:
    path: /reg-custom-jobs.html
    method: GET
- name: Custom Model Workshop
  endpoint:
    path: /../custom-models/custom-model-workshop/index.html
    method: GET
- name: Import .mlpkg files exported from DataRobot AutoML
  endpoint:
    path: /reg-transfer.html
    method: GET
- name: Model logs for model packages (legacy)
  endpoint:
    path: /reg-model-pkg-logs.html
    method: GET
- name: registered_models
  endpoint:
    path: /RegisteredModels
    method: GET
    data_selector: models
    params: {}
- name: predictions
  endpoint:
    path: /predictions
    method: POST
    data_selector: data
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
- name: ping
  endpoint:
    path: /ping
    method: GET
    data_selector: ping
- name: registered_models
  endpoint:
    path: /model_registry/registered_models
    method: GET
    data_selector: models
    params: {}
- name: key_values
  endpoint:
    path: /model_registry/key_values
    method: GET
    data_selector: key_values
    params: {}
- name: custom_job
  endpoint:
    path: /model_registry/custom_jobs
    method: POST
    data_selector: job_info
- name: custom_inference_model
  endpoint:
    path: /custom-models
    method: POST
    data_selector: model
    params: {}
- name: register_models
  endpoint:
    path: /reg-create.html
    method: GET
- name: register_data_robot_models
  endpoint:
    path: /dr-model-reg.html
    method: GET
- name: register_custom_models
  endpoint:
    path: /reg-custom-models.html
    method: GET
- name: register_external_models
  endpoint:
    path: /reg-external-models.html
    method: GET
- name: deploy_registered_models
  endpoint:
    path: /reg-deploy.html
    method: GET
- name: manage_model_packages
  endpoint:
    path: /reg-action.html
    method: GET
- name: generate_model_compliance_documentation
  endpoint:
    path: /reg-compliance.html
    method: GET
- name: customize_compliance_documentation
  endpoint:
    path: /reg-key-values.html
    method: GET
- name: custom_jobs
  endpoint:
    path: /reg-custom-jobs.html
    method: GET
- name: custom_model_workshop
  endpoint:
    path: /../custom-models/custom-model-workshop/index.html
    method: GET
- name: import_mlpkg_files
  endpoint:
    path: /reg-transfer.html
    method: GET
- name: model_logs
  endpoint:
    path: /reg-model-pkg-logs.html
    method: GET
- name: custom_model
  endpoint:
    path: /CustomModelWorkshop
    method: POST
    data_selector: model
    params: {}
- name: custom_models
  endpoint:
    path: /custom-models
    method: POST
    data_selector: model
    params: {}
- name: registered_models
  endpoint:
    path: /models
    method: GET
    data_selector: models
- name: registered_models
  endpoint:
    path: /registered_models
    method: GET
    data_selector: models
    params: {}
- name: drop_in_environments
  endpoint:
    path: /public_dropin_environments
    method: GET
    data_selector: environments
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: custom_job
  endpoint:
    path: /model_registry/custom_jobs
    method: POST
    data_selector: job_info
    params: {}
- name: prediction_environment
  endpoint:
    path: /predictions/environments
    method: POST
    data_selector: environment
    params: {}
- name: prediction_environment
  endpoint:
    path: /predictions/environments
    method: GET
    data_selector: environments
    params: {}
- name: model_deployment
  endpoint:
    path: /deployments
    method: POST
    data_selector: deployments
    params: {}
- name: prediction_environment
  endpoint:
    path: /deployments/prediction_environments
    method: POST
    data_selector: environment
    params: {}
- name: Batch prediction UI
  endpoint:
    path: /predictions/batch/batch-dep
    method: GET
- name: Real-time scoring methods
  endpoint:
    path: /predictions/realtime
    method: GET
- name: Prediction monitoring jobs
  endpoint:
    path: /predictions/batch/pred-monitoring-jobs
    method: GET
- name: bitbucket_server
  endpoint:
    path: /add/bitbucket_server
    method: POST
- name: github
  endpoint:
    path: /add/github
    method: POST
- name: github_enterprise
  endpoint:
    path: /add/github_enterprise
    method: POST
- name: s3
  endpoint:
    path: /add/s3
    method: POST
- name: gitlab
  endpoint:
    path: /add/gitlab
    method: POST
- name: gitlab_enterprise
  endpoint:
    path: /add/gitlab_enterprise
    method: POST
- name: model
  endpoint:
    path: /model
    method: POST
    data_selector: model_data
- name: custom_model
  endpoint:
    path: /custom-models
    method: POST
    data_selector: model
    params: {}
- name: deployment
  endpoint:
    path: /deployments
    method: POST
    data_selector: deployment
    params: {}
- name: real_time_scoring_methods
  endpoint:
    path: /predictions/realtime/index.html
    method: GET
- name: batch_prediction_methods
  endpoint:
    path: /predictions/batch/index.html
    method: GET
- name: portable_prediction_methods
  endpoint:
    path: /predictions/port-pred/index.html
    method: GET
- name: Python 3.X
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/tree/master/public_dropin_environments/python311
    method: GET
- name: Python 3.X GenAI Agents
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/tree/master/public_dropin_environments/python311_genai_agents
    method: GET
- name: Python 3.X ONNX Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_onnx
    method: GET
- name: Python 3.X PMML Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_pmml
    method: GET
- name: Python 3.X PyTorch Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_pytorch
    method: GET
- name: Python 3.X Scikit-Learn Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_sklearn
    method: GET
- name: Python 3.X XGBoost Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_xgboost
    method: GET
- name: Python 3.X Keras Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/python3_keras
    method: GET
- name: Java Drop-In
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/java_codegen
    method: GET
- name: R Drop-in Environment
  endpoint:
    path: https://github.com/datarobot/datarobot-user-models/blob/master/public_dropin_environments/r_lang
    method: GET
- name: agentMaxAggregatedRecords
  endpoint:
    path: /agent/config
    method: GET
    data_selector: records
    params: {}
- name: prediction_environment
  endpoint:
    path: /PredictionEnvironments
    method: GET
    data_selector: environments
    params: {}
- name: deployment
  endpoint:
    path: /Deployments
    method: GET
    data_selector: deployments
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: prediction_environment
  endpoint:
    path: /predictions/environments
    method: POST
    data_selector: environment
    params: {}
- name: model_deployment
  endpoint:
    path: /models/deployments
    method: POST
    data_selector: deployment
    params: {}
- name: model
  endpoint:
    path: /models
    method: POST
    data_selector: model
    params: {}
- name: registered_model
  endpoint:
    path: /registeredModels
    method: GET
    data_selector: registeredModels
    params: {}
- name: model_registry
  endpoint:
    path: /model_registry
    method: GET
- name: predictions
  endpoint:
    path: /predictions
    method: POST
- name: agentMaxAggregatedRecords
  endpoint:
    path: /agent/config
    method: GET
    data_selector: records
    params:
      agentMaxAggregatedRecords: '10'
- name: Batch prediction methods
  endpoint:
    path: /predictions/batch/index.html
    method: GET
- name: Portable prediction methods
  endpoint:
    path: /predictions/port-pred/index.html
    method: GET
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- The API requires a registered DataRobot account.
- Uses API key for authentication — requires setup of API key in DataRobot UI.
- Some objects like Contact may return nulls in deeply nested fields
- Uses API key for authentication — requires setup in DataRobot UI
- Uses API token for authentication
- Be cautious to never commit your credentials to Git.
- DataRobot's recommended authentication method is to use a drconfig.yaml file.
- Model build status can be 'READY', 'complete', or 'FAILED'.
- Requires a valid API token for authentication.
- Use the API Token for authentication by setting it in the Authorization header.
- Uses Bearer token for authentication
- Microsoft Azure support for OAuth
- Some objects may return nulls in deeply nested fields
- Never commit your credentials to Git.
- Ingestion of XLSX files often does not work as well as using the corresponding CSV
  format.
- When using the prediction API, there is a maximum 50MB body size limitation for
  real-time deployment prediction requests.
- Exportable Java scoring code uses extra RAM during model building and therefore,
  dataset size should be less than 8GB.
- Some endpoints may have rate limits.
- Check the API documentation for specific usage guidelines.
- In most cases, only snapshotted datasets can be downloaded.
- There is a 10GB file size limit; attempting to download a dataset larger than 10GB
  will fail.
- The AI Catalog must be enabled in order to schedule snapshot refreshes.
- For Self-Managed AI Platform installations, the Model Management Service must also
  be installed.
- Consider Fast EDA for large sets up to 10GB; use scalable ingest for sets up to
  100GB.
- Fast EDA is calculated on the first X rows of the dataset, not a random sample.
- When working with large datasets, there are some differences in behaviors that you
  should note.
- Static assets can only be versioned by uploads of the same type; datasets created
  by local files are versioned from local file uploads, and datasets created from
  a data stage are versioned from data stage uploads.
- Requires setup of connected app in DataRobot
- JDBC drivers must be compatible with Java 1.8 and later.
- For secondary datasets, only uploaded files and JDBC sources registered in the AI
  Catalog are supported.
- 'Maximum supported values: 30 datasets per project—DataRobot counts each feature
  derivation window and secondary dataset as a ''dataset.'''
- The combined size of a project's primary and secondary datasets cannot exceed 100GB.
  Individual dataset size limits are based on AI Catalog limits.
- If the primary dataset is larger than 40MB, CV partitioning is disabled by default.
- Column names in Feature Discovery datasets cannot contain a trailing or leading
  single quote or a trailing or leading space.
- When there is an error during project start, you cannot return to defining relationships.
  You must restart the configuration.
- There can be issues with the colors used in the visualization of linkages in the
  Feature Engineering relationship editor.
- You must allow the IP addresses listed on the Allowed source IP addresses page to
  connect to the DataRobot JDBC connector.
- Feature discovery predictions will be slower than other DataRobot models because
  feature engineering is applied.
- A particular upload method may be disabled on your cluster, in which case a button
  for that method does not appear.
- Some import methods may need to be configured by an admin before use.
- Feature Discovery automates joining and aggregating datasets.
- A valid Feature Discovery project requires at least one secondary dataset
- Time-aware feature engineering requires a date feature as a prediction point
- Datasets with blue text indicate the dataset is in use and part of the project.
- Datasets with white text indicate the dataset is loaded but not part of the relationship
  definition.
- The number of datasets linked with this dataset is indicated.
- The number of secondary datasets and how many have relationships defined is provided.
- A Snowflake data connection is set up.
- All secondary datasets are stored in Snowflake.
- All Snowflake sources are stored in the same warehouse.
- All datasets are configured as dynamic datasets in the AI Catalog.
- You have write permissions to one of the schemas in use or one PUBLIC schema of
  the database in use.
- You can influence how DataRobot conducts feature engineering by setting feature
  engineering controls.
- Set the feature engineering options in the relationship editor prior to EDA2.
- During Feature Discovery, DataRobot generates new features then removes the features
  that have low impact or are redundant.
- 'The following features are not supported in Feature Discovery projects: Scoring
  Code, Time series, Challenger models, V1.0 prediction API, Portable Prediction Server
  (PPS), Automated Retraining, Sliced insights, Clustering.'
- 'Maximum supported values: 30 datasets per project—DataRobot counts each feature
  derivation window and secondary dataset as a ''dataset.'' The combined size of a
  project''s primary and secondary datasets cannot exceed 100GB. Individual dataset
  size limits are based on AI Catalog limits.'
- Column names in Feature Discovery datasets cannot contain trailing or leading single
  quotes or spaces.
- Uses OAuth2 with refresh token — requires setup of connected app in DataRobot
- DataRobot automates joining and aggregating datasets.
- Ensure secondary datasets are up-to-date for scoring models.
- Feature Discovery only explores Categorical Statistics for categorical columns that
  have at most 50 unique values.
- Only DataRobot models are supported, no external or custom model support.
- Governance workflow and Feature Discovery model package export is not supported
  for Feature Discovery models.
- You cannot replace a Feature Discovery model with a non-Feature Discovery model
  or vice versa.
- You cannot change the configuration once a deployment is created. To use a different
  configuration, you must create a new deployment.
- When a Feature Discovery model is replaced with another Feature Discovery model,
  the configuration used by the new model becomes the default configuration.
- Feature Discovery predictions will be slower than other DataRobot models because
  feature engineering is also applied.
- Use the AI Catalog to load additional datasets into the project
- Transformed features (including numeric features created as user-defined functions)
  cannot be used for special variables, such as Weight, Offset, Exposure, and Count
  of Events.
- When using a model that contains transformed features for predictions, DataRobot
  automatically includes the new feature in any uploaded dataset.
- DataRobot supports transforming up to 500 features at a time.
- The dataset is in use and part of the project.
- The dataset is loaded but not part of the relationship definition.
- The assessment automatically detects and surfaces common data quality issues.
- It includes a warning level to help determine issue severity.
- The values and displays for a feature may differ between EDA1 and EDA2.
- You can instead include all features when building models by disabling feature reduction.
- When converting from numeric variable types to categorical, be aware that DataRobot
  drops any values after the decimal point.
- If DataRobot does not let you transform the features, you should correct the list
  of features and try the transformation again.
- The matrix becomes available once EDA2 completes.
- Default display is by Mutual Information values.
- You cannot add or remove features from a feature list. Instead, create a new feature
  list with all desired features.
- If your dataset has text, Comprehensive mode runs TinyBERT models, which can be
  10x to 100x slower than other models with the same data.
- You cannot run Comprehensive Autopilot for time series or anomaly detection projects.
- You cannot use Add models from selected to change settings on blended models.
- Uses frozen run feature to optimize model performance
- Recommended to use integer row counts for sample sizes in large datasets
- DataRobot caps that amount of data at either 64% or 500MB, whichever is smaller.
- DataRobot provides special handling of larger datasets to make viewing and model
  building work more efficiently.
- Early target selection allows you to set build parameters and set the project to
  start automatically when ingestion completes.
- Set additional parameters and modify values that can effect model builds.
- Set conditions that help calculate fairness, as well as identify and attempt to
  mitigate bias in a model's predictive behavior.
- Set the number of clusters that DataRobot discovers in a time series clustering
  project.
- Bring external model(s) into the DataRobot AutoML environment, view them on the
  Leaderboard, and run a subset of DataRobot's evaluative insights for comparison
  against DataRobot models.
- Set monotonic constraints to control the influence between variables and target.
- Set how data is partitioned for training/validation/holdout and the validation type.
- Set how data is partitioned for OTV or time series projects.
- Downsample the majority class for faster model build time.
- Set a variety or time series-specific advanced options.
- Create new training images to increase the amount of training data.
- Enable GPU support to improve runtime for deep learning models.
- Changing the metric is an advanced functionality and recommended only for those
  who understand the metrics and the algorithms behind them.
- Bias mitigation applies to a single, categorical protected feature.
- For the ROBC mitigation technique, the mitigation feature must have at least two
  classes that each have at least 100 rows in the training data.
- External model prediction values must be numeric. For binary classification projects,
  the prediction values must be between `[0.0, 1.0]`. For regression projects, the
  prediction values must be between `(-inf, inf)`.
- After importing data and selecting a target variable, the Data page appears.
- Feature Constraints
- When more than 1,000 classes are detected, the selection is on and cannot be changed.
- When more than 1,000 labels are detected, the selection is on and cannot be changed.
- If you selected to set up time-aware modeling on the Start screen, all partitioning
  methods except Date/Time are disabled.
- Not all partition types support smart downsampling.
- If the selected target has only two unique values, DataRobot assumes that it is
  as classification task and recommends a classification metric.
- If the selected target represents a regression task, the most popular metrics for
  regression are RMSE (Root Mean Square Error) and MAE (Mean Absolute Error).
- If you are using smart downsampling to downsize your dataset or you selected a weight
  column as your target, only weighted metrics are available.
- For Self-Managed AI Platform users running v11.1, see the on-premise platform documentation.
- Bias and Fairness testing provides methods to calculate fairness for a binary classification
  model and attempt to identify any biases in the model's predictive behavior.
- Uses advanced options settings that can impact DataRobot's feature engineering and
  modeling.
- Enabling cross-series feature generation may result in errors at prediction time
  if not all series are present.
- Calendar events must be daily; if more granular time steps are needed, use known
  in advance features.
- DataRobot automatically detects whether or not a project's target value is stationary.
- Enabled by default, supervised feature reduction discards low-impact features prior
  to modeling.
- External model prediction values must be numeric.
- For binary classification projects, the prediction values must be between [0.0,
  1.0].
- For regression projects, the prediction values must be between (-inf, inf).
- DataRobot API requires OAuth2 authentication
- When this option is selected, DataRobot applies no monotonic constraints during
  training.
- This option is only available when all features in the Raw Features list are of
  the type numeric, percentage, length, and/or currency.
- This option is only available when all features in the Informative Features list
  are of the type numeric, percentage, length, and/or currency.
- Provides key plots and statistics needed to judge and interpret a model’s effectiveness.
- Accuracy Over Time training computation is disabled if the dataset exceeds the configured
  threshold after creation of the modeling dataset. The default threshold is 5 million
  rows.
- Using the advanced options settings can impact DataRobot's feature engineering and
  how it models data.
- For multiseries modeling, the series identifier can be set or changed before modeling.
- When Allow partial history is checked, it instructs Autopilot to run blueprints
  optimized for cold start and partial history modeling.
- You cannot use Advanced Tuning with blended models.
- Baseline models do not offer any tunable parameters.
- Availability of unlimited classes in multiclass projects is dependent on your DataRobot
  package.
- If unlimited multiclass is enabled, all projects can be converted.
- Without unlimited multiclass, you can convert from numeric to multiclass when there
  are up to 100 unique numeric values.
- For Self-Managed AI Platform users running v11.1, see the on-premise platform documentation
- The Forecast vs Actual chart is not available for OTV or unsupervised projects.
- Because multiseries projects can have up to 1 million series and up to 1000 forecast
  distances, calculating accuracy charts for all series data can be extremely compute-intensive
  and often unnecessary.
- The Lift Chart shows how well a model segments the target population.
- A good model will be accurate across each level, or bin, of risk.
- Only the first 1000 series are computed.
- Maximum period definition file size is 5MB. An unlimited number of period files
  are allowed.
- Insight export is not supported.
- The Residuals tab is only accessible for models created with version 5.2 and later
  (or after 7/1/2019 for managed AI Platform users).
- The Residual vs Predicted plot and the Residuals histogram are only available for
  version 5.3 and later (or after 11/13/2019 for managed AI Platform users).
- The Residuals tab is not available for frozen run models if there are no out-of-sample
  predictions.
- This tab is only accessible for models created with version 5.2 and later (or after
  7/1/2019 for managed AI Platform users).
- The standard deviation calculation for these scatter plots is only displayed for
  Self-Managed AI Platform users with version 5.3 or later.
- DataRobot displays the ROC Curve tab only for models created for a binary classification
  target.
- The ROC Curve tab provides tools for exploring classification, performance, and
  statistics related to a selected model at any point on the probability scale.
- DataRobot displays the ROC Curve tab only for models created for a binary classification
  target (a target with two unique values).
- The Display Threshold defaults to maximize F1.
- If you switch to a different model, the Display Threshold updates to maximize F1
  for the new model.
- Accuracy yields misleading results if the dataset is unbalanced.
- When smart downsampling is enabled, the confusion matrix totals may differ slightly
  from the size of the data partitions.
- OAuth2 with refresh token required.
- The Prediction Distribution graph visually expresses model performance for the selected
  data source.
- The confusion matrix facilitates more detailed analysis than relying on accuracy
  alone.
- AUC does not display automatically in the Metrics pane. Click Select metrics and
  select Area Under the Curve (AUC) to display it.
- The KS metric does not display automatically in the Metrics pane. Click Select metrics
  and select Kolmogorov-Smirnov Score to display it.
- The Prediction Distribution graph illustrates the distribution of actual values
  in relation to the display threshold.
- Using the profit curve is not recommended for baseline (majority class classifier)
  models.
- Cumulative charts help assess model performance by exploring cumulative characteristics.
- Uses OAuth2 with refresh token
- The metrics values on the ROC curve display might not always match those shown on
  the Leaderboard.
- Minute details might be lost when selecting metrics.
- Cumulative charts help assess model performance.
- Cumulative Gain and Cumulative Lift charts determine model success.
- You can select up to six metrics to display.
- For large datasets, DataRobot computes scores and values after downsampling.
- Initial calculations are for the first 1000 series sorted by ID.
- Uses data visualizations that are initially computed for the first 1000 series.
- Accuracy calculations must be complete before full functionality is available.
- The Training Dashboard tab is currently available for Keras-based models only.
- The Training Dashboard tab is currently available for Keras-based (deep learning)
  models only.
- Running a large grid-search to find the best performing model without first performing
  a deeper assessment of the model is likely to result in a suboptimal model.
- Applying both training and test (validation) data for the entire training procedure
  helps to easily assess whether each candidate model is overfitting, is underfitting,
  or has a good fit.
- The maximum number of features computed for Cluster Insights is 100.
- When interpreting the results of a Feature Effects chart within a time series project,
  the derived Datetime (Day of Week) (actual) feature correlates a day to a numeric.
- Calculations for partial dependence are based on a sample of 1000 records.
- DataSelection dropdown works differently for time-aware projects.
- To retrieve the SHAP-based Feature Impact visualization, you must enable the Include
  only models with SHAP value support advanced option prior to model building.
- Feature Impact shows, at a high level, which features are driving model decisions
  the most.
- Feature Impact differs from the feature importance measure shown in the Data page.
- Feature Impact is calculated using a sample of the model's training data.
- Prediction Explanations are only generated on datasets that are 1GB or less.
- Predictions requested with Prediction Explanations will typically take longer to
  generate than predictions without explanations, although actual speed is model-dependent.
- Uses SHAP-based explanations to understand model predictions.
- In XEMP-based projects, one significant difference between methodologies is the
  ability to additionally generate Prediction Explanations for multiclass projects.
- DataRobot applies the default or user-specified baseline thresholds to all datasets
  using the same model.
- Text is shown in its original format, without modification by a tokenizer.
- Use the Prediction API for time-aware projects.
- Uses default settings for number of explanations and thresholds for computation.
- DataRobot only computes Prediction Explanations for the validation partition of
  backtest one in the training data.
- The Coefficients tab is only available for a limited number of models.
- Data Quality Handling Report
- Data Quality Handling Report can be found in a model's Describe division.
- Use Search to find a specific feature.
- Filter by column header.
- The Data Quality Handling Report can be found in a model's Describe division.
- Eureqa models require specific configuration for access.
- Eureqa models are run in full Autopilot, not Quick.
- Eureqa blueprints don't support stacked predictions.
- If you receive text-based insight model errors, see this note for a description
  of how DataRobot handles single-character 'words.'
- For time series projects, the output also includes backtesting information, including
  execution time for each backtest.
- Rating table modification does not support changing the header row of the dataset
  or data type of the columns.
- Rating tables are not created for models with Japanese text columns (they do not
  support the MeCab tokenizer).
- You cannot upload a new rating table to the child model. You can only upload rating
  tables to the parent model.
- Insights on validation and holdout sets are only available if the model was not
  trained into those sets (if data is out-of-sample).
- If *any* partitions remain out-of-sample for the model, insights are provided for
  that partition.
- If any partition is wholly or partially in the training period for the model, insights
  are *not* provided for that partition.
- Pairwise interactions found by the GA2M model have specific characteristics.
- Feature Strength describes the strength of each feature and pairwise interaction.
- Specified pairwise interactions are not guaranteed to appear in a model's output.
- CSV encoding must be UTF-8.
- Pairwise interactions found by the GA2M model have additional table headings labeled
  (Var1 & Var2).
- Deploy tab behaves differently in environments without a dedicated prediction server.
- The Downloads tab allows you to download model artifacts—chart/graph PNGs and model
  data—in a single ZIP file.
- Charts and graphs are exported in PNG format; model data is exported in CSV format.
- A particular upload method may be disabled on your cluster.
- There are slight differences in the Make Predictions tab depending on your project
  type.
- Availability of compliance documentation is dependent on your configuration. Contact
  your DataRobot representative for more information.
- Data sources used for insights vary across use cases.
- The Downloads tab previously contained Scoring Code for downloading. Scoring Code
  is now available from the Leaderboard or a deployment.
- If Feature Effects is computed, you can export the chart image for individual features.
- Bias and fairness testing is only available for binary classification projects.
- Protected features must be categorical features in the dataset.
- Getting additional data can be expensive, but may be worthwhile if it increases
  model accuracy.
- Not all models show three sample sizes in the Learning Curves graph.
- You must unlock holdout to display Validation scores.
- Model performance can decrease with increasing sample size.
- High-bias models may do better at small sample sizes, while high-variance models
  often perform better at large sample sizes.
- The Cross-Class Accuracy tab calculates evaluation metrics and ROC curve-related
  scores segmented by class.
- Model Comparison tab is available for all project types except multiclass, multilabel,
  unsupervised clustering, unsupervised anomaly detection for time-aware projects,
  and parent projects in segmented modeling.
- Requires setup of connected app in api
- The particular insights that display are dependent on the model type, which is in
  turn dependent on the project type.
- Small datasets also impact the number of stages run and shown.
- A blueprint represents the high-level end-to-end procedure for fitting the model,
  including any preprocessing steps, modeling, and post-processing steps.
- DataRobot validates modifications to ensure that changes are intentional, not to
  enforce requirements.
- Input data is limited to a single post-EDA2 dataset. No joins can be defined inside
  a blueprint.
- The Model Comparison tab is available for all project types except multiclass, multilabel,
  unsupervised clustering, and unsupervised anomaly detection for time-aware projects.
- Users only need to build their own environment when they need to install Linux packages.
- DRUM is not regularly tested on Windows or Mac.
- Enable network access for custom tasks
- In order to have network access from within a custom task, you need to specifically
  enable it in the Custom Task Version using the outgoingNetworkPolicy field.
- This example replaces a built-in Missing Values Imputed task with a custom imputation
  task.
- Once trained, the model appears in the project Leaderboard where you can compare
  its accuracy with other custom and DataRobot models.
- The Tesseract OCR engine may not recognize documents with very small text.
- Blueprints with validation warnings are saved and can be trained, despite the warnings.
- Location AI enhances the standard AutoML workflow to capture a broad range of geospatial
  problems.
- Users need to build their own environment when they need to install Linux packages.
- Validating functionality in advance can save much time and debugging in the future.
- Because you will use the command line to run tests, open a terminal window.
- A custom environment is only required when a custom task requires additional Linux
  packages, requires a different operating system, or uses a language other than Python,
  R, or Java.
- If you delete an environment, you are removing the environment for everyone that
  it may have been shared with.
- DRUM is not regularly tested on Windows or Mac. These steps may differ depending
  on the configuration of your machine.
- Uses unsupervised learning for anomaly detection.
- Anomaly scores are calibrated to be interpreted as probabilities.
- Network access for custom tasks requires usage of DataRobot's early access Python
  client.
- In the case of numeric missing values, DataRobot supplies the imputed median (which,
  by definition, is non-anomalous).
- The higher the number of features in a dataset, the longer it takes DataRobot to
  detect anomalies and the more difficult it is to interpret results.
- If you train an anomaly detection model on greater than 1000 features, Insights
  in the Understand tab are not available.
- Because anomaly scores are normalized, DataRobot labels some rows as anomalies even
  if they’re not too far away from normal.
- Synthetic AUC is an approximation based on creating synthetic anomalies and inliers
  from the training data.
- Synthetic AUC scores are not available for blenders that contain image features.
- Feature Impact for anomaly detection models trained from DataRobot blueprints is
  always computed using SHAP.
- Because time series anomaly detection is not yet optimized for pure text data anomalies,
  data must contain some numerical or categorical columns.
- Train-time image augmentation is a processing step that randomly transforms existing
  images, augmenting the training data. You can configure augmentation both before
  and after model building.
- Visual AI class limit is the same as non-Visual AI (1000 classes).
- File and folder names cannot contain whitespaces.
- Use / (not \) for file paths.
- Read an overview of augmention.
- Store all the parameter settings for a given augmentation strategy.
- See examples of leveraging domain knowledge to craft a beneficial augmentation strategy.
- If you add a secondary dataset with images to a primary tabular dataset, the augmentation
  options described above are not available.
- When image augmentation improves the LogLoss of a model, it improves it on average
  by approximately 10%, with a very large variance model-to-model and dataset-to-dataset.
- Image Embeddings and Activation Maps are available from the Insights tab.
- The canvas allows zoom controls and comparison of predictions.
- Anomaly detection can be used in cases where there are thousands of normal transactions
  with a low percentage of abnormalities.
- The higher the number of features in a dataset, the longer it takes DataRobot to
  detect anomalies and the more difficult it is to interpret results. If you have
  more than 1000 features, be aware that the anomaly score becomes difficult to interpret,
  making it potentially difficult to identify the root cause of anomalies.
- Time-aware (time series and OTV) modeling is not supported.
- DataRobot supports the creation of projects with any number of unique labels, using
  2-1,000 labels in each multicategorical feature. Multilabel insights reflect only
  the 100 (after trimming settings are applied) most frequent labels.
- Multicategorical features are only supported as the target feature. To use a multicategorical
  as a non-target modeling feature, convert the values to summarized categorical before
  uploading the dataset.
- Because the size of predictions is proportional to the number of labels, the number
  of rows that can be used for real-time predictions decreases with the number of
  labels.
- Target drift and accuracy tracking is not supported for multicategorical targets.
- OTV is date/time partitioning with additional components such as sophisticated preprocessing.
- To activate time-aware modeling, your dataset must contain a column with a variable
  type 'Date'.
- Backtests require at least 20 rows in each validation and holdout fold and at least
  100 rows in each training fold.
- Train-time image augmentation is a processing step that randomly transforms existing
  images, augmenting the training data.
- Model parameter selection has not been customized for date/time-partitioned projects.
- Time-series modeling is a recommended practice for data science problems where conditions
  may change over time.
- Time-aware modeling can make predictions on a single row, or, with its core time
  series functionality, can extract patterns from recent history and forecast multiple
  events into the future.
- Be careful retraining on all your data. In Time Series it is very common for historical
  data to have a negative impact on current predictions.
- Compute prediction intervals option is off by default because the computation and
  inclusion of prediction intervals can significantly increase the amount of time
  required to generate a model package.
- The ability to download a model package from the Portable Predictions tab depends
  on the MLOps configuration for your organization.
- The ability to download a model package from the Deploy tab requires the Enable
  MMM model package export preview feature flag.
- When you deploy a model package with prediction intervals, the Predictions > Prediction
  Intervals tab is available in the deployment.
- Select Understand > Image Embeddings to view up to 100 images from the validation
  set projected onto a two-dimensional plane.
- Select Understand > Activation Maps for a model to preview up to 100 sample images
  from the project's validation set.
- DataRobot automatically detects when multiseries is required and provides a multiseries
  modeling workflow.
- With regression projects, you can aggregate the target value across all series in
  the multiseries project.
- To ensure that the entire global history is used for feature transformations and
  modeling, be certain to have at least one series that contains dates across the
  full date range of the training dataset.
- The entry in the ID (third) column must match the dataset's series identifier column.
- Use the same dataset format as the one used to create the project (upload a ZIP
  archive with one or more images).
- For the API Client and HTTP Interface options, use the same dataset format as the
  original dataset used to create the project.
- Encode a binary image file (not decoded pixel contents) to base64.
- Multilabel modeling is dependent on your DataRobot package.
- Multicategorical features are only supported when selected as the target.
- Time series segmented modeling deployments do not support data drift monitoring.
- Automatic retraining for segmented deployments that use clustering models is disabled;
  retraining must be done manually.
- OTV is date/time partitioning, with additional components such as sophisticated
  preprocessing and insights from the Accuracy over Time graph.
- Models are trained on data between two ISO dates.
- The prediction file used for comparison must not have more than 20% missing values
  in any backtest.
- Column names must match exactly and the date and series ID columns must match the
  original data.
- Having raw dates in modeling can be risky (overfitting, for example, or tree-based
  models that do not extrapolate well).
- DataRobot generally excludes raw dates from the Informative Features list if date
  transformation features were derived.
- The automation process creates new duration features calculated from the difference
  between date features and the primary date.
- Duration features are named `duration(<from date>, <to date>)`, where the `<from
  date>` is the primary date.
- DataRobot computes durations in seconds and displays the information potentially
  as huge integers.
- Date/time partitioning does not support dates before 1900.
- Most users, the defaults that DataRobot selects provide optimized modeling.
- DataRobot's default partitioning settings are optimized for the specific dataset
  and target feature selected.
- When you specify a date, DataRobot includes start dates and excludes end dates.
- If you build a project with a single backtest, the Leaderboard does not display
  a backtest column.
- When changing partition year/month/day settings, the month and year values rebalance
  to fit the larger class.
- DataRobot provides default window settings and partition sizes for time series projects.
- If you choose to modify the default configurations, keep in mind that setting up
  a project requires matching your actual prediction requests with your work environment.
- DataRobot uses rows outside of the training partitions to calculate features as
  part of the time series feature derivation process.
- If there is seasonality, replace FDW + 1 with FDW + Seasonal period.
- DataRobot handles missing value imputation differently with time series projects.
- DataRobot validates a time series prediction dataset once it is uploaded, checking
  whether there are sufficient historical rows to produce the engineered features
  required by the project.
- This feature is not enabled for all organizations and is subject to change.
- Preview functionality should not be used in production.
- If DataRobot does not detect multiple series in your data, you can manually set
  a series ID.
- You must enable the Quantile Metric feature flag before dataset ingestion to the
  AI Catalog.
- When using quantile loss, some insights may look unusual or need to be interpreted
  differently.
- Hyperparameters for custom tasks is off by default. Contact your DataRobot representative
  or administrator for information on enabling the feature.
- DataRobot derives features from the target by default, enabling automatic time-based
  feature engineering.
- Batch predictions for TTS and LSTM models
- Projects can be active or inactive based on worker assignment.
- Deleted projects cannot be recovered on the managed AI Platform.
- Using advanced options settings can impact DataRobot's feature engineering and how
  it models data.
- For multiseries modeling, you can change the series identifier before modeling.
- Allowing partial history supports prediction datasets with series history that is
  only partially known.
- Cross-series feature generation allows for features that consider historical observations
  across series.
- Known in advance features can significantly improve forecast accuracy.
- Excluding features from derivation can prevent automated time-based feature engineering.
- Calendars provide a way to specify dates or events in a dataset that require additional
  attention.
- Customizing model splits can optimize the number of models trained based on available
  workers.
- If the target is *not* stationary, DataRobot attempts to make it stationary by applying
  a differencing strategy prior to modeling.
- If you set a number of backtests that results in any of the partitions not meeting
  the criteria, DataRobot only runs the number of backtests that do meet the minimums.
- DataRobot cannot account for leap years or days in a month as it relates to your
  data.
- There is no way to remove the EDITED label from a backtest, even if you manually
  reset the durations back to the original settings.
- DataRobot provides default window settings for time series projects.
- Modifying project settings out of context to increase accuracy independent of your
  use case often results in disappointing outcomes.
- DataRobot auto-suggests values for the windows based on the dataset's time unit.
- DataRobot creates a maximum of five lags, regardless of the Feature Derivation Window
  size.
- If you don't care about tomorrow's prediction because it is too soon to take action
  on, change the Forecast Window to the point from which you want predictions forward.
- Predictions are available for download for 48 hours.
- Scheduled jobs do not require you to provide connection, authentication, and prediction
  options for each request.
- DataRobot validates that a data source is applicable with the deployed model when
  possible but not in all cases.
- As a deployment Owner, you can edit any other information freely
- In DataRobot, you cannot share connection credentials
- Using JDBC can be costly in terms of IOPS and expense for data warehouses.
- The Snowflake adapter reduces load on database engines during prediction scoring.
- Monitoring jobs don't support monitoring predictions made by time series models.
- DataRobot validates that a data source is compatible with the model when possible.
- Although these features have been tested within the engineering and quality environments,
  they should not be used in production at this time.
- Preview functionality is subject to change and that any Support SLA agreements are
  not applicable.
- You must enable the Quantile Metric feature flag before dataset ingestion to the
  AI Catalog. That is, you cannot use a dataset that was loaded into the AI Catalog
  prior to enabling the flag.
- As a deployment Owner, you can edit any other information freely, and if the Prediction
  source dataset is from the AI Catalog and it is shared with you, you can edit the
  existing connection directly.
- DataRobot uses different types of workers for different types of jobs.
- By default, DataRobot splits your data into a 20% holdout and an 80% cross-validation
  partition.
- Jobs that do not use streaming intake
- Jobs that have finished running
- Any job—completed successfully, aborted, or in progress
- Any job
- Any job—except Challenger jobs
- Using Scoring Code in production requires additional development efforts to implement
  model management and model monitoring, which the DataRobot API provides out of the
  box.
- Exportable Java Scoring Code requires extra RAM during model building. As a result,
  to use this feature, you should keep your training dataset under 8GB.
- Availability information depends on the MLOps configuration for your organization.
- Downloading Scoring Code makes the deployment permanent, meaning that it cannot
  be deleted.
- Legacy users will see the option described below.
- MLOps users can download Scoring Code from the Leaderboard and directly from a deployment.
- Exportable Java Scoring Code requires extra RAM during model building. As a result,
  to use this feature, you should keep your training dataset under 8GB. Projects larger
  than 8GB may fail due to memory issues. If you get an out-of-memory error, decrease
  the sample size and try again. The memory requirement does not apply during model
  scoring. During scoring, the only limitation on the dataset is the RAM of the machine
  on which the Scoring Code is run.
- As a deployment Owner, you can edit any other information freely.
- If the Prediction source dataset is from the AI Catalog and it is shared with you,
  you can edit the existing connection directly.
- For more information, see Scoring Code usage examples.
- Using JDBC to transfer data can be costly in terms of IOPS and expense for data
  warehouses.
- The Snowflake adapter reduces the load on database engines during prediction scoring
  by using cloud storage and bulk insert.
- The MLOps monitoring library requires Java 11 or higher.
- Without monitoring, a model's Scoring Code JAR file requires Java 8 or higher; however,
  when using the MLOps library to instrument monitoring, a model's Scoring Code JAR
  file requires Java 11 or higher.
- For Self-managed AI platform installations, the Java 11 requirement applies to DataRobot
  v11.0 and higher.
- DataRobot provides 'source' .jar files for downloading to simplify the process of
  model inspection.
- By using the 'source' download option, you get only the code that directly implements
  the model.
- The Compatible Model API always supports the newest API and is backward-compatible
  with all versions of DataRobot.
- Supported Android versions are 8.0 (API 26) or later.
- The Portable Prediction Server is a premium feature exclusive to DatRobot MLOps.
- DataRobot strongly recommends using an Intel CPU to run the Portable Prediction
  Server.
- Use SM mode when only a single model package has been mounted into the Docker container
  inside the /opt/ml/model directory.
- For users on pricing plans from before March 2020, downloading Scoring Code makes
  the deployment permanent, meaning that it cannot be deleted.
- Prediction routes only support CSV and JSON records scoring datasets. The maximum
  payload size is 50MB.
- The forecastPoint parameter cannot be used if predictionsStartDate and predictionsEndDate
  are passed.
- Default value for relaxKnownInAdvanceFeaturesCheck is false.
- Uses Docker for running predictions.
- Supports various input/output types like filesystem, JDBC, S3, and Snowflake.
- 'Portable batch predictions can use the following adapters to score datasets: Filesystem,
  JDBC, AWS S3, Azure Blob, GCS, Snowflake, Synapse.'
- Azure Blob connection string is required to access external stage
- DataRobot supports Linux-based prediction environments for PPS.
- It is possible to use other Unix-based prediction environments, but only Linux-based
  systems are validated and officially supported.
- Prediction routes only support comma-separated value (CSV) scoring datasets.
- The maximum payload size is 50MB.
- Your DataRobot API key must be provided.
- The ability to download a model package from the Leaderboard depends on the MLOps
  configuration for your organization.
- Support for external test sets is available for all project types except supervised
  time series.
- Insights are not computed if an external dataset has fewer than 10 rows; however,
  metric scores are computed and displayed on the Leaderboard.
- The ROC Curve insight is disabled if the external dataset only contains single class
  actuals.
- 'If you configure your DataRobot cluster with ENABLE_FIPS_140_2_MODE: true, that
  cluster builds MLKPG files that require you to launch the PPS with ENABLE_FIPS_140_2_MODE:
  true.'
- Prediction file size limits vary for Self-Managed AI Platform installations and
  limits are configurable.
- When performing predictions, the positive class has multiple representations that
  DataRobot can choose from.
- 'Self-Managed AI Platform only: By default, the 25 most important features and the
  target are tracked for data drift.'
- For real-time, deployment predictions, the maximum payload size is 50MB for both
  Dedicated and Serverless prediction environments.
- DataRobot deployments only track predictions made against dedicated prediction servers
  by deployment_id.
- The first 1,000,000 predictions per deployment per hour are tracked for data drift
  analysis and computed for accuracy.
- If you score larger datasets (up to 5GB), there will be a longer wait time for the
  predictions to become available.
- DataRobot recommends that you do not submit multiple prediction rows that use the
  same association ID.
- Missing values for known-in-advance features are allowed in the forecast window
  at prediction time when relaxKnownInAdvanceFeaturesCheck is true.
- Disable SSL certificate verification by setting ALLOW_SELF_SIGNED_CERTS to True
  if using self-signed certificates
- API keys are the preferred method for authenticating requests to the DataRobot API.
- Portable batch predictions require setup of Portable Prediction Server
- Specify environment variables for credentials
- Uses Azure Blob for data intake and output
- Each registered model must have a unique name.
- Global models are a premium feature.
- DataRobot recommends that you confirm the model passes testing before proceeding.
- Untested custom models prompt a dialog box warning that the custom model is not
  tested.
- The ability to create new DataRobot Prime models has been removed from the application.
- To register an external model monitored by the monitoring agent, add an external
  model as a registered model or version.
- Provide holdout data when registering an external model package and specify the
  column containing predictions.
- While the Batch Prediction API isn't limited to a specific file size, prediction
  monitoring is still subject to an hourly rate limit.
- Registered models can contain DataRobot, custom, and external models.
- Only the execution environment and an entry point file (typically run.sh) are required.
- The first 1,000,000 predictions per deployment per hour are tracked for data drift
  analysis and computed for accuracy. Further predictions within an hour where this
  limit has been reached are not processed for either metric.
- If you score larger datasets (up to 5GB), there will be a longer wait time for the
  predictions to become available, as multiple prediction jobs must be run.
- After making prediction requests, it can take 30 seconds or so for data drift and
  accuracy metrics to update.
- If you want to write back your predictions to a cloud location or database, you
  must use the Prediction API.
- Custom inference models are not custom DataRobot models. They are user-defined models
  created outside of DataRobot and assembled in the Custom Model Workshop for deployment,
  monitoring, and governance.
- Inference models receive raw CSV data and must handle all preprocessing themselves.
- A model's existing training data can only be changed if the model is not actively
  deployed.
- The target name can only be changed if a model has no training data and has not
  been deployed.
- There is a per-user limit on the number of custom model deployments (30), custom
  environments (30), and custom environment versions (30) you can have.
- Custom inference model server start-up is limited to 3 minutes.
- The file size for training data is limited to 1.5GB.
- Dependency management only works with packages in a proper index. Packages from
  URLs cannot be installed.
- Unpinned python dependencies are not updated once the dependency image has been
  built. To update to a newer version, you will need to create a new requirements
  file with version constraints. DataRobot recommends always pinning versions.
- 'SaaS AI Platform only: Custom inference models have no access to the internet and
  outside networks.'
- Custom inference models are user-defined models created outside of DataRobot and
  assembled in the Custom Model Workshop for access to deployment, monitoring, and
  governance.
- DataRobot recommends you reference the custom model assembly guidelines.
- You must also upload the model requirements and a `start_server.sh` file to your
  model's folder unless you are pairing the model with a drop-in environment.
- Custom model dependencies aren't applied when testing a model locally with DRUM.
- Uses OAuth2 with refresh token — requires setup of connected app in prediction API
- Users can determine the maximum memory allocated for a model, but only organization
  admins can configure additional resource settings.
- DataRobot recommends configuring resource settings only when necessary.
- Every new custom model you create has public network access by default; however,
  when you create new versions of any custom model created before October 2023, those
  new versions remain isolated from public networks until you enable public access
  for a new version.
- API keys are the preferred method for authenticating requests to the DataRobot API;
  they replace the legacy API token method.
- MLOps-exclusive model package options available
- Models can be registered from the Leaderboard
- Git Large File Storage (LFS) is supported by default for GitHub integrations.
- The Deploy tab behaves differently in environments without a dedicated prediction
  server.
- If you set the prediction threshold before the deployment preparation process, the
  value does not persist.
- Although a model package can be created without testing the custom model, DataRobot
  recommends that you confirm the model passes testing before proceeding.
- Create a custom model as a proxy for an external model using the Custom Model Workshop.
- Creating two commits in quick succession can result in a ResourceNotFoundError.
- DataRobot can build the environment for you, allowing you to reuse the environment
  for as many models as you want.
- Ensure OAuth2 setup is completed before making requests.
- You can search and filter registered models by various criteria.
- Most general purpose DataRobot custom model drop-in environments are security-hardened
  container images.
- Only shell code following the POSIX-shell standard is supported in security-hardened
  environments.
- DataRobot recommends using an environment template and not building your own environment
  except for specific use cases.
- Your environment must include a Dockerfile that installs any requirements you may
  want.
- Custom models require a simple webserver to make predictions.
- An executable start_server.sh file is required to start the model server.
- If you upload both a context and an image file, the priority is given to the image
  file.
- Do not log sensitive information in your custom job code. The logs for a custom
  job run contain any information logged in that job's code.
- You can only select one of DataRobot or DataRobot Scoring Code.
- Uses OAuth2 authentication.
- DataRobot recommends using an administrative service account as the account holder
  (an account that has access to each deployment that uses the configured prediction
  environment).
- Model logs for model packages in the Model Registry are off by default, requiring
  the deprecated legacy Model Packages tab.
- Model logs only report the operations of the underlying model, not the model package
  operations.
- Unpinned python dependencies are not updated once the dependency image has been
  built. To update to a newer version, you will need to create a new requirements
  file with version constraints.
- No specific authentication details provided.
- Maximum payload size for real-time deployment predictions is 50MB.
- Each change to a runtime parameter creates a new minor version of the custom model.
- DataRobot warns you that a new environment must be built to account for the model
  dependencies.
- If a model allocates more than the configured maximum memory value, it is evicted
  by the system.
- Challenger models and model replacement are not supported (challenger prediction
  servers can't be set to an external or serverless prediction environment).
- Only CSV files are supported for predictions. XLSX files are not supported by the
  code snippet.
- On the Service health tab, information such as latency, throughput, and error rate
  is unavailable for external, agent-monitored deployments.
- Requires setup of personal access tokens for repositories
- Automated deployment and replacement of Scoring Code in AzureML is a premium feature.
- DataRobot management of Scoring Code in AzureML requires existing Azure Service
  Principal Credentials.
- Sharing capability allows appropriate user roles to grant permissions on a custom
  model or environment.
- If you have the appropriate permissions, you can delete a custom model or environment
  from the Model Registry.
- DataRobot management of Scoring Code in Sagemaker requires an existing data connection
  to Sagemaker with stored credentials.
- The custom model you create as a proxy for an external model should contain custom
  code in the custom.py file to connect the proxy model with the externally-hosted
  model.
- DataRobot recommends testing that your model can make predictions before deploying.
- GitHub Actions for custom models is a premium feature.
- To send predictions, first configure the monitoring agent.
- Uploading the historical predictions directly to the deployment inventory enables
  you to analyze data drift and accuracy statistics in the past.
- Drop-in environments contain the model requirements and the start_server.sh file
  for the custom model.
- Custom environments do not contain the model requirements or the start_server.sh
  file for the custom model.
- If you are part of an organization with deployment limits, the Deployment billing
  section notifies you of the number of deployments your organization is using against
  the deployment limit and the deployment cost if your organization has exceeded the
  limit.
- Data drift tracking is only available for deployments using deployment-aware prediction
  API routes (i.e., https://example.datarobot.com/predApi/v1.0/deployments/<deploymentId>/predictions).
- If the Deploy model button is inactive, be sure to either specify an association
  ID or toggle off Require association ID in prediction requests.
- Users with the Owner role can add historical prediction data to external model deployments
  if data drift is enabled.
- 'These datasets must meet the following requirements: Historical prediction data
  must have the same features as the original prediction dataset.'
- Starting with the March 2025 Managed AI Platform release, most general purpose DataRobot
  custom model drop-in environments are security-hardened container images.
- Starting with the 11.0 Self-Managed AI Platform release, most general purpose DataRobot
  custom model drop-in environments are security-hardened container images.
- The MLOps agents run on Linux.
- The MLOps agents don't support Windows environments.
- The MLOps agents' releases are backward compatible with the last two versions of
  DataRobot.
- Any code and start_server.sh should be copied to /opt/code/ by your Dockerfile.
- Java requirement applies to DataRobot v11.0 and higher.
- The filesystem spooler directory must be an absolute path to the '/tmp/ta' directory.
- Deployment to production may require a strict approval workflow
- You can only select one of DataRobot or DataRobot Scoring Code
- Holdout data is required for setting an accuracy baseline.
- If you configure these settings programmatically in your code and by defining environment
  variables, the environment variables take precedence.
- Some responses may contain null values in certain fields
- By default, DataRobot tracks up to 25 features.
- Self-managed installations can increase the limit to 200 features using the PREDICTION_API_MONITOR_RAW_MAX_FEATURE
  setting.
- The absolute maximum number of features DataRobot can receive is 300.
- Organizations created after November 2024 have access to a pre-provisioned DataRobot
  Serverless prediction environment.
- Trial accounts have access to a pre-provisioned DataRobot Serverless prediction
  environment.
- New Self-Managed organizations running DataRobot 10.2+ installations have access
  to a pre-provisioned DataRobot Serverless prediction environment.
- Maximum payload size for real-time deployment predictions on Serverless prediction
  environments is 50MB.
- Some environment variable values contain double quotes (`"). Take care when setting
  environment variables that include this special character (or others).
- Requires Azure Service Principal credentials for management of Scoring Code.
- Requires setup of API key for authentication.
- Uses OAuth2 — requires setup of connected app in DataRobot
- If the Deploy model button is inactive, be sure to either specify an association
  ID (required for enabling accuracy monitoring) or toggle off Require association
  ID in prediction requests.
- You cannot remove data from a deployment later.
- The filesystem spooler directory must be an absolute path to the "/tmp/ta" directory.
- You must install the Python version of the MLOps library.
- While the mlops-cli tool is suitable for simple use cases, you should use the agent
  for production scenarios.
- Self-managed installations can increase the limit to 200 features using the PREDICTION_API_MONITOR_RAW_MAX_FEATURE
  setting in the DataRobot configuration.
- The maximum number of features that DataRobot can receive is set using PREDICTION_API_POST_MAX_FEATURES
  and the absolute maximum number of features DataRobot can receive is 300.
- For agent-monitored deployments, the 300 feature limit applies, even if you configure
  the agent to send more than 300 features using MLOPS_MAX_FEATURES_TO_MONITOR.
- MLOps library requires Java 11 or higher.
errors:
- '401 Unauthorized: Recheck API key validity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Recheck API key validity.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck API token'
- '401 Unauthorized: Recheck API token or token expiration'
- '401 Unauthorized: Recheck API token or permissions'
- '401 Unauthorized: Check the API token.'
- '401 Unauthorized: Recheck API token validity.'
- '401 Unauthorized: Recheck API token or its permissions.'
- '401 Unauthorized: Check your API token and permissions.'
- '429 Too Many Requests: Rate limit exceeded.'
- The total number of refresh jobs a user can have across all AI Catalog datasets
  is limited to 100.
- The total number of refresh jobs that can exist for a specific dataset for all users
  is limited to 5.
- The total number of stored snapshots that can exist for a specific dataset until
  the dataset refresh job is automatically disabled is limited to 100.
- No public API is available yet.
- DataRobot supports transforming up to 500 features at a time.
- If you see a message indicating more than 500 features are selected for transformation,
  you need to deselect features.
- No column or value limits.
- Prediction Explanations are not returned for features that have extremely low, or
  no, importance.
- 'Invalid request: Ensure the request parameters are correct.'
- 'Timeout: The request took too long and was aborted.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- Projects with weights or offsets, including smart downsampling not supported
- Scoring Code not supported
- Anomaly detection does not consider geospatial data
- 'Scoring Code: Not supported'
- 'Challenger models: Not supported'
- 'Image augmentation: Not supported'
- 'Agents: Not supported'
- 'Prediction Explanations: Not supported'
- 'Stratified partitioning: Not supported'
- 'Monotonic constraints: Not supported'
- 'Offsets: Not supported'
- 'Currency data types: Not supported'
- 'Export of ROC charts: Not supported'
- 'External holdout: Not supported'
- 'Compliance documentation generation: Not supported'
- Prediction intervals in DataRobot serverless prediction environments require pre-computed
  prediction intervals when registering the model package.
- If you find that your dataset and project settings lead to excessive sampling levels,
  try reconfiguring the project or modeling approach.
- If you set a number of backtests that results in any of the partitions not meeting
  that criteria, DataRobot only runs the number of backtests that do meet the minimums.
- If you set a number of backtests that results in any of the partitions not meeting
  that criteria, the project could fail.
- Ensure the frequency with which you retrain your models matches the validation length.
- Select the number of backtests to balance your confidence in the model against the
  time to train and availability of training data.
- Validation errors may occur when saving monitoring job definitions.
- 'OutOfMemoryError: Java heap space error'
- Validation errors prevent saving monitoring job definition.
- 'Out of memory error: decrease the sample size and try again.'
- '401 Unauthorized: Check API token and permissions.'
- '404 Not Found: Verify the prediction endpoint path.'
- '500 Internal Server Error: Check the server logs for details.'
- '401 Unauthorized: Recheck API token or authentication method.'
- 'HTTP 404 Not found: Previously deprecated endpoints using project_id and model_id
  instead of deployment_id.'
- '401 Unauthorized: Recheck API host or token'
- '401 Unauthorized: Check API token'
- '400 Bad Request: Ensure job definition is correctly formatted'
- 'HTTP 404 Not found: returned by deprecated endpoints using project_id and model_id
  instead of deployment_id.'
- 'ResourceNotFoundError: Wait for an action''s execution to complete before pushing
  new commits.'
- '401 Unauthorized: Check your OAuth token and permissions.'
- '500 Internal Server Error: Check server status or logs.'
- '403 Forbidden: Verify user permissions for the action.'
- '401 Unauthorized: Check API key validity.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Microsoft Entra ID app
  - Azure Service Principal
client:
  base_url: https://www.datarobot.com
  auth:
    type: oauth2
    flow: refresh_token
source_metadata: null
