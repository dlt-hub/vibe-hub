resources:
- name: model
  endpoint:
    path: /v2/models/{model_name}/infer
    method: POST
    data_selector: results
- name: infer_batch
  endpoint:
    path: /infer/batch
    method: POST
    data_selector: results
    params: {}
- name: infer_sample
  endpoint:
    path: /infer/sample
    method: POST
    data_selector: results
    params: {}
- name: model_config
  endpoint:
    path: /model/config
    method: GET
    data_selector: config
    params: {}
- name: wait_for_model
  endpoint:
    path: /model/wait
    method: GET
    data_selector: ready
    params: {}
- name: model
  endpoint:
    path: /v2/models/{model_name}
    method: GET
    data_selector: ''
    params: {}
- name: models
  endpoint:
    path: /v2/models/MyModel
    method: POST
    data_selector: results
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: outputs
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: outputs
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
- name: model
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: ''
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: POST
    data_selector: outputs
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
- name: models
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: model_data
    params: {}
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: outputs
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: POST
    data_selector: outputs
    params: {}
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: outputs
    params: {}
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: GET
    data_selector: records
- name: MyModel
  endpoint:
    path: /v2/models/MyModel/infer
    method: POST
    data_selector: outputs
    params: {}
- name: model_inference
  endpoint:
    path: /v2/models/MyModel/infer
    method: POST
- name: MyModel
  endpoint:
    path: /v2/models/MyModel
    method: POST
    data_selector: outputs
- name: MyModel
  endpoint:
    path: /v2/models/MyModel/infer
    method: POST
    data_selector: outputs
    params: {}
- name: MyModel
  endpoint:
    path: /v2/models/MyModel/infer
    method: POST
- name: Linear
  endpoint:
    path: /v2/models/Linear/infer
    method: POST
    data_selector: inputs
- name: infer
  endpoint:
    path: /v2/models/<model name>/infer
    method: POST
- name: model_binding
  endpoint:
    path: /bind
    method: POST
    data_selector: model_name
    params: {}
- name: ModelConfig
  endpoint:
    path: /binding_configuration
    method: POST
    data_selector: model_config
    params: {}
- name: ModelConfig
  endpoint:
    path: /model_config
    method: GET
    data_selector: records
- name: DynamicBatcher
  endpoint:
    path: /dynamic_batcher
    method: GET
    data_selector: records
- name: MyModel
  endpoint:
    path: /models/MyModel
    method: POST
    data_selector: outputs
- name: AddSub
  endpoint:
    path: /
    method: POST
    data_selector: bind
    params:
      model_name: AddSub
      max_batch_size: 8
      response_cache: true
- name: batch
  endpoint:
    path: /decorators/batch
    method: GET
    data_selector: examples
- name: sample
  endpoint:
    path: /decorators/sample
    method: GET
    data_selector: examples
- name: group_by_keys
  endpoint:
    path: /decorators/group_by_keys
    method: GET
    data_selector: examples
- name: group_by_values
  endpoint:
    path: /decorators/group_by_values
    method: GET
    data_selector: examples
- name: fill_optionals
  endpoint:
    path: /decorators/fill_optionals
    method: GET
    data_selector: examples
- name: pad_batch
  endpoint:
    path: /decorators/pad_batch
    method: GET
    data_selector: examples
- name: first_value
  endpoint:
    path: /decorators/first_value
    method: GET
    data_selector: examples
- name: triton_context
  endpoint:
    path: /decorators/triton_context
    method: GET
    data_selector: examples
- name: stacking_multiple_decorators
  endpoint:
    path: /decorators/stacking_multiple_decorators
    method: GET
    data_selector: examples
- name: ModelClient
  endpoint:
    path: /predict
    method: GET
    data_selector: response
- name: predict
  endpoint:
    path: /predict
    method: GET
    data_selector: response
    params: {}
- name: shared-memory
  endpoint:
    path: /shared-memory
    method: GET
- name: model-repository
  endpoint:
    path: /model-repository
    method: GET
- name: statistics
  endpoint:
    path: /statistics
    method: GET
- name: trace
  endpoint:
    path: /trace
    method: GET
- name: logging
  endpoint:
    path: /logging
    method: GET
- name: shared-memory
  endpoint:
    path: /v2/models/stats
    method: GET
- name: model-repository
  endpoint:
    path: /v2/models
    method: GET
- name: statistics
  endpoint:
    path: /v2/models/stats
    method: GET
- name: trace
  endpoint:
    path: /v2/trace
    method: GET
- name: logging
  endpoint:
    path: /v2/logging
    method: GET
- name: passthrough
  endpoint:
    path: /v2/models/passthrough/generate
    method: POST
    data_selector: responses
- name: passthrough
  endpoint:
    path: /v2/models/passthrough/generate
    method: POST
    data_selector: response
    params: {}
- name: model_infer_function
  endpoint:
    path: /model_infer_function
    method: POST
    data_selector: outputs
    params: {}
- name: ModelConfig
  endpoint:
    path: /pytriton/model_config/ModelConfig
    method: GET
- name: Tensor
  endpoint:
    path: /pytriton/model_config/Tensor
    method: GET
- name: DeviceKind
  endpoint:
    path: /pytriton/model_config/DeviceKind
    method: GET
- name: DynamicBatcher
  endpoint:
    path: /pytriton/model_config/DynamicBatcher
    method: GET
- name: QueuePolicy
  endpoint:
    path: /pytriton/model_config/QueuePolicy
    method: GET
- name: TimeoutAction
  endpoint:
    path: /pytriton/model_config/TimeoutAction
    method: GET
- name: ConstantPadder
  endpoint:
    path: /decorators/ConstantPadder
    method: GET
- name: ModelConfigDict
  endpoint:
    path: /decorators/ModelConfigDict
    method: GET
- name: TritonContext
  endpoint:
    path: /decorators/TritonContext
    method: GET
- name: infer_batch
  endpoint:
    path: /infer_batch
    method: POST
    data_selector: results
    params: {}
- name: infer_sample
  endpoint:
    path: /infer_sample
    method: POST
    data_selector: results
    params: {}
- name: AsyncioDecoupledModelClient
  endpoint:
    path: /create_client
    method: POST
    data_selector: result
    params: {}
- name: infer_batch
  endpoint:
    path: /infer_batch
    method: POST
    data_selector: results
    params: {}
- name: infer_sample
  endpoint:
    path: /infer_sample
    method: POST
    data_selector: results
    params: {}
- name: wait_for_model
  endpoint:
    path: /wait_for_model
    method: GET
    data_selector: status
    params: {}
- name: model_config
  endpoint:
    path: /model_config
    method: GET
    data_selector: Future
- name: load_model
  endpoint:
    path: /load_model
    method: POST
    data_selector: response
    params: {}
- name: unload_model
  endpoint:
    path: /unload_model
    method: DELETE
    data_selector: response
    params: {}
- name: wait_for_model
  endpoint:
    path: /wait_for_model
    method: GET
    data_selector: Future
    params:
      timeout_s: float
- name: wait_for_server
  endpoint:
    path: /wait_for_server
    method: GET
    data_selector: response
    params:
      timeout_s: float
- name: model
  endpoint:
    path: /models
    method: GET
    data_selector: models
- name: model
  endpoint:
    path: /model
    method: GET
    data_selector: model_config
- name: wait_for_model
  endpoint:
    path: /wait_for_model
    method: GET
    data_selector: Future
    params:
      timeout_s: 300
- name: model_config
  endpoint:
    path: /model_config
    method: GET
    data_selector: Future
    params: {}
- name: Linear
  endpoint:
    path: /v2/models/Linear/infer
    method: POST
    data_selector: inputs
- name: model
  endpoint:
    path: /bind
    method: POST
    data_selector: model
    params: {}
- name: ModelConfig
  endpoint:
    path: /model/config
    method: POST
    data_selector: model_config
    params: {}
- name: DynamicBatcher
  endpoint:
    path: /dynamic/batcher
    method: POST
    data_selector: dynamic_batcher
    params: {}
- name: inference_callable
  endpoint:
    path: /inference/callable
    method: POST
    data_selector: requests
- name: ImgModel
  endpoint:
    method: POST
    data_selector: requests
- name: ImageNet
  endpoint:
    path: /predict
    method: GET
    data_selector: result
notes:
- Decorator prepares groups of requests with the same set of keys and calls wrapped
  function for each group separately.
- Uses Triton Inference Server for model deployment
- If model_version is None inference on latest model will be performed.
- If no scheme is provided http scheme will be used as default.
- Asynchronous inference can be performed using infer_batch and infer_sample methods.
- Model configuration can be obtained using model_config method.
- Use Triton Inference Server for serving models via HTTP/gRPC.
- Inference queries can be sent to localhost:8000/v2/models/MyModel.
- Uses blocking mode for serving inference callback function
- The Triton server can be stopped at any time using the stop method
- Uses Triton Inference Server for handling HTTP/gRPC requests
- In background mode, model can be deployed on Triton Inference Server for handling
  requests
- Uses Triton Inference Server to handle HTTP/gRPC requests
- Supports multiple machine learning frameworks
- Uses blocking mode for serving models
- Supports HTTP/gRPC requests
- The Triton server can be stopped at any time using the stop method.
- Uses a blocking mode where the application is a long-lived process deployed in your
  cluster to serve the requests from clients.
- Supports model serving in both blocking and background modes
- Uses Triton Inference Server to serve models
- Background mode available for Jupyter Notebook
- Uses Triton Inference Server for model serving
- Uses Triton Inference Server for handling HTTP/gRPC requests.
- Triton Inference Server is installed as part of PyTriton
- The server can be accessed via HTTP/gRPC at localhost:8000
- The inference queries can be sent to localhost:8000/v2/models/MyModel
- Uses Triton Inference Server to handle HTTP/gRPC requests.
- Token-based access restriction for endpoints
- By default, shared-memory, model-repository, statistics, trace, and logging endpoints
  now require access token
- Decoupled models support
- AsyncioDecoupledModelClient, which works in async frameworks and decoupled Triton
  models like some Large Language Models.
- An operating system with glibc >= 2.35
- Python version >= 3.8
- pip >= 20.3
- Triton Inference Server binaries are automatically installed as part of the PyTriton
  package.
- 'The ''bind'' method''s mandatory arguments are: model_name, infer_func, inputs,
  outputs, config, strict.'
- Batching is enabled by default for the model.
- Dynamic batching collects samples from multiple clients into a single batch processed
  by the model.
- Batching is enabled by default.
- Dynamic batching improves throughput and latency.
- Dynamic batching is supported.
- Custom parameters and headers can only be accessed in undecorated inference function.
- You need Linear model described in quick_start. You should run it so client can
  connect to it.
- This example requires torch module
- 'You can test your server using curl: curl -X ''GET'' ''http://127.0.0.1:8015/predict''
  -H ''accept: application/json'''
- Token-based access restriction is implemented to secure model endpoints.
- gRPC requires the triton-grpc-protocol- prefix before the header name
- Always include headers in both metadata requests and inference requests
- Uses --grpc-restricted-protocol for gRPC endpoint protection
- Uses --http-restricted-api for HTTP endpoint protection
- 'Follows Triton Server''s standard token format: <endpoints>:triton-access-token=<token>'
- Compatible with all Triton Server versions that support these features
- Protect sensitive endpoints with authentication tokens
- Client will use the token for all endpoint access
- Check endpoint names against the supported list
- Use hyphens, not underscores (e.g., `model-config` not `model_config`)
- Verify the server was started with token restrictions enabled
- Check server logs for authentication errors
- Ensure you're using the correct token from `get_access_token()`
- Include headers in both metadata and inference requests
- Verify you're connecting to the gRPC port (usually 8001) not HTTP port (8000)
- The Triton Inference Server will send trace spans to the OpenTelemetry collector.
- Uses a chunking strategy to handle inputs larger than the maximum batch size.
- Utilizes custom parameters or headers to extract a URL and download data from an
  external source, such as an S3 bucket.
- If lazy_init argument is False, model configuration will be read from inference
  server during initialization.
- Default timeout for inference process is 60 seconds.
- If `lazy_init` argument is False, model configuration will be read from inference
  server during initialization.
- Uses Triton Inference Server
- Client will wait init_timeout_s for the server to get into readiness state before
  obtaining the model configuration.
- DecoupledModelClient is only supported for grpc protocol
- This method waits for the server to get into readiness state before obtaining the
  model configuration.
- If the server and model are not in readiness state before the given timeout, a timeout
  error will be raised.
- Asyncio client for model deployed on the Triton Inference Server.
- Uses asynchronous inference methods.
- Inference inputs can be provided either as positional or keyword arguments
- Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError
- Model initialization is lazy by default.
- Ensure model is ready before inference.
- Uses ModelClient to interact with Triton Inference Server
- Model configuration will be read from inference server during initialization.
- Uses Docker for optional installation.
- Ensure glibc version is >= 2.35.
- 'The library can be installed in any of the following ways: system environment,
  virtualenv, Docker image.'
- Using Docker is optional but not required.
- The serve method is blocking and waits for incoming HTTP/gRPC requests.
- Dynamic batching collects samples from multiple clients into a single batch.
- The default timeout is set to 5 minutes (300 seconds).
- The default inference timeout is 60 seconds.
errors:
- 'PyTritonClientModelUnavailableError: If model with given name (and version) is
  unavailable.'
- 'PyTritonClientTimeoutError: if `lazy_init` argument is False and wait time for
  server and model being ready exceeds `init_timeout_s`.'
- 'PyTritonClientUrlParseError: In case of problems with parsing url.'
- 'PyTritonClientValueError: if mixing of positional and named arguments passing detected.'
- 'PyTritonClientModelDoesntSupportBatchingError: if model doesn''t support batching.'
- 'PyTritonClientInferenceServerError: If error occurred on inference callable or
  Triton Inference Server side.'
- 'PyTritonClientClosedError: If the FuturesModelClient is closed.'
- 'PyTritonClientTimeoutError: If server and model are not in readiness state before
  given timeout.'
- 'PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Unauthorized: Access token is required for protected endpoints.'
- 'StatusCode.UNAVAILABLE: This protocol is restricted, expecting header ''triton-grpc-protocol-triton-access-token'''
- 'StatusCode.UNAUTHENTICATED: Invalid access token'
- 'StatusCode.PERMISSION_DENIED: Access to this endpoint is restricted'
- '401 Unauthorized: Verify the token is correctly included in requests'
- '403 Forbidden: Check that the endpoint is actually protected'
- 'PyTritonClientInferenceServerError: Error occurred during inference request. Message:
  [request id: 0] inference request batch-size must be <= 2 for ''Batch2'''
- 'PyTritonClientTimeoutError: if lazy_init argument is False and wait time for server
  and model being ready exceeds init_timeout_s.'
- 'PyTritonClientTimeoutError: If the wait time for the server and model being ready
  exceeds `init_timeout_s`.'
- 'PyTritonClientValueError: If mixing of positional and named arguments passing detected.'
- 'PyTritonClientTimeoutError: If the wait time for the server and model being ready
  exceeds `init_timeout_s` or inference request time exceeds `inference_timeout_s`.'
- 'PyTritonClientModelUnavailableError: If the model with the given name (and version)
  is unavailable.'
- 'PyTritonClientInferenceServerError: If an error occurred on the inference callable
  or Triton Inference Server side.'
- 'PyTritonClientValueError: if mixing of positional and named arguments passing detected'
- 'PyTritonClientTimeoutError: If server and model are not in readiness state before
  given timeout'
- 'PyTritonClientModelUnavailableError: If model with given name (and version) is
  unavailable'
- 'PyTritonClientInferenceServerError: If error occurred on inference callable or
  Triton Inference Server side'
- 'PyTritonClientTimeoutError: If the wait time for the server and model being ready
  exceeds init_timeout_s or inference request time exceeds inference_timeout_s.'
- 'PyTritonClientTimeoutError: If the server and model are not in readiness state
  before the given timeout.'
- 'KeyboardInterrupt: If the hosting process receives SIGINT.'
- 'PyTritonClientClosedError: If the ModelClient is closed.'
- 'PyTritonClientTimeoutError: If `lazy_init` argument is False and wait time for
  server and model being ready exceeds `init_timeout_s`.'
- 'PyTritonClientClosedError: If the FuturesModelClient is closed'
- 'PyTritonClientTimeoutError: If server is not in readiness state before given timeout.'
- 'PyTritonClientTimeoutError: If wait time for server and model being ready exceeds
  `init_timeout_s`.'
- PyTritonClientTimeutError
auth_info:
  mentioned_objects: []
client:
  base_url: grpc://localhost:8001
  auth:
    type: none
  headers: {}
  paginator: {}
source_metadata: null
