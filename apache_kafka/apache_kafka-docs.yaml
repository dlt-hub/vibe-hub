resources:
- name: MirrorMaker
  endpoint:
    path: /connect-mirror-maker.properties
    method: GET
    data_selector: configuration
    params: {}
- name: west-1 to west-2
  endpoint:
    path: west-1->west-2.enabled
    method: SET
    data_selector: ''
    params: {}
- name: west-2 to west-1
  endpoint:
    path: west-2->west-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: east-1 to east-2
  endpoint:
    path: east-1->east-2.enabled
    method: SET
    data_selector: ''
    params: {}
- name: east-2 to east-1
  endpoint:
    path: east-2->east-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: north-1 to north-2
  endpoint:
    path: north-1->north-2.enabled
    method: SET
    data_selector: ''
    params: {}
- name: north-2 to north-1
  endpoint:
    path: north-2->north-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: west-1 to east-1
  endpoint:
    path: west-1->east-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: west-1 to north-1
  endpoint:
    path: west-1->north-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: east-1 to west-1
  endpoint:
    path: east-1->west-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: east-1 to north-1
  endpoint:
    path: east-1->north-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: north-1 to west-1
  endpoint:
    path: north-1->west-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: north-1 to east-1
  endpoint:
    path: north-1->east-1.enabled
    method: SET
    data_selector: ''
    params: {}
- name: topics
  endpoint:
    path: /bin/kafka-topics.sh
    method: GET
    data_selector: ''
    params: {}
- name: consumer_groups
  endpoint:
    path: /bin/kafka-consumer-groups.sh
    method: GET
    data_selector: ''
    params: {}
- name: configs
  endpoint:
    path: /bin/kafka-configs.sh
    method: GET
    data_selector: ''
    params: {}
- name: west-1 to west-2
  endpoint:
    path: west-1->west-2.enabled
    method: ''
    data_selector: ''
    params: {}
- name: west-2 to west-1
  endpoint:
    path: west-2->west-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: east-1 to east-2
  endpoint:
    path: east-1->east-2.enabled
    method: ''
    data_selector: ''
    params: {}
- name: east-2 to east-1
  endpoint:
    path: east-2->east-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: north-1 to north-2
  endpoint:
    path: north-1->north-2.enabled
    method: ''
    data_selector: ''
    params: {}
- name: north-2 to north-1
  endpoint:
    path: north-2->north-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: west-1 to east-1
  endpoint:
    path: west-1->east-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: west-1 to north-1
  endpoint:
    path: west-1->north-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: east-1 to west-1
  endpoint:
    path: east-1->west-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: east-1 to north-1
  endpoint:
    path: east-1->north-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: north-1 to west-1
  endpoint:
    path: north-1->west-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: north-1 to east-1
  endpoint:
    path: north-1->east-1.enabled
    method: ''
    data_selector: ''
    params: {}
- name: Kafka Connect
  endpoint:
    path: /connect
    method: GET
    data_selector: records
    params: {}
- name: Kafka Cluster
  endpoint:
    path: /clusters
    method: GET
    data_selector: clusters
    params: {}
- name: connect-test
  endpoint:
    path: /connect-test
    method: GET
    data_selector: records
    params: {}
- name: streams-file-input
  endpoint:
    path: /streams-file-input
    method: POST
    data_selector: records
    params: {}
- name: streams-wordcount-output
  endpoint:
    path: /streams-wordcount-output
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
- name: consumer_groups
  endpoint:
    path: /consumer-groups
    method: GET
    data_selector: records
- name: MirrorMaker
  endpoint:
    path: /connect-mirror-maker
    method: POST
    data_selector: status
    params: {}
- name: zookeeper
  endpoint:
    path: zookeeper.connect
    method: GET
    data_selector: servers
- name: kafka
  endpoint:
    path: num.partitions
    method: GET
    data_selector: partitions
- name: replication
  endpoint:
    path: default.replication.factor
    method: GET
    data_selector: replication_factor
- name: west-1 to west-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: west-2 to west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: east-1 to east-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: east-2 to east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: north-1 to north-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: north-2 to north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: west-1 to east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: west-1 to north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: east-1 to west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: east-1 to north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: north-1 to west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: north-1 to east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      enabled: 'true'
- name: log_configuration
  endpoint:
    path: log.dir
    method: GET
    data_selector: records
    params:
      num.partitions: 8
      default.replication.factor: 3
- name: replication_flows
  endpoint:
    path: clusters
    method: GET
    data_selector: ''
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
- name: consumer_groups
  endpoint:
    path: /consumer-groups
    method: GET
    data_selector: records
- name: zookeeper
  endpoint:
    path: /zookeeper/connect
    method: GET
    data_selector: connect
    params: {}
- name: log_configuration
  endpoint:
    path: /log/configuration
    method: GET
    data_selector: configuration
    params: {}
- name: west-1 to west-2 replication
  endpoint:
    path: west-1->west-2.enabled
    method: POST
    data_selector: ''
    params: {}
- name: east-1 to east-2 replication
  endpoint:
    path: east-1->east-2.enabled
    method: POST
    data_selector: ''
    params: {}
- name: north-1 to north-2 replication
  endpoint:
    path: north-1->north-2.enabled
    method: POST
    data_selector: ''
    params: {}
- name: west-1 to east-1 replication
  endpoint:
    path: west-1->east-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: west-1 to north-1 replication
  endpoint:
    path: west-1->north-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: east-1 to west-1 replication
  endpoint:
    path: east-1->west-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: east-1 to north-1 replication
  endpoint:
    path: east-1->north-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: north-1 to west-1 replication
  endpoint:
    path: north-1->west-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: north-1 to east-1 replication
  endpoint:
    path: north-1->east-1.enabled
    method: POST
    data_selector: ''
    params: {}
- name: west-1
  endpoint:
    path: connect-mirror-maker.properties --clusters west-1 west-2
    method: START
    data_selector: enabled
    params: {}
- name: west-2
  endpoint:
    path: connect-mirror-maker.properties --clusters west-1 west-2
    method: START
    data_selector: enabled
    params: {}
- name: east-1
  endpoint:
    path: connect-mirror-maker.properties --clusters east-1 east-2
    method: START
    data_selector: enabled
    params: {}
- name: east-2
  endpoint:
    path: connect-mirror-maker.properties --clusters east-1 east-2
    method: START
    data_selector: enabled
    params: {}
- name: north-1
  endpoint:
    path: connect-mirror-maker.properties --clusters north-1 north-2
    method: START
    data_selector: enabled
    params: {}
- name: north-2
  endpoint:
    path: connect-mirror-maker.properties --clusters north-1 north-2
    method: START
    data_selector: enabled
    params: {}
- name: MirrorMaker
  endpoint:
    path: /connect-mirror-maker
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
    params: {}
- name: consumer_groups
  endpoint:
    path: /consumer-groups
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
- name: consumer_groups
  endpoint:
    path: /consumer_groups
    method: GET
- name: west-1->west-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: west-2->west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: east-1->east-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: east-2->east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: north-1->north-2
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: north-2->north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: west-1->east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: west-1->north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: east-1->west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: east-1->north-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: north-1->west-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: north-1->east-1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: topics
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: records
- name: consumer_groups
  endpoint:
    path: /api/v1/consumer_groups
    method: GET
    data_selector: records
- name: primary
  endpoint:
    path: primary.bootstrap.servers
    method: GET
- name: secondary
  endpoint:
    path: secondary.bootstrap.servers
    method: GET
- name: primary_to_secondary
  endpoint:
    path: primary->secondary.topics
    method: GET
    data_selector: topics
    params:
      enabled: 'true'
- name: secondary_to_primary
  endpoint:
    path: secondary->primary.topics
    method: GET
    data_selector: topics
    params:
      enabled: 'false'
- name: bidirectional_flow
  endpoint:
    path: us-west->us-east
    method: GET
    data_selector: clusters
    params:
      enabled: 'true'
- name: tieredTopic
  endpoint:
    path: /topics/tieredTopic
    method: POST
    data_selector: config
    params:
      remote.storage.enable: 'true'
      local.retention.ms: '1000'
      retention.ms: '3600000'
      segment.bytes: '1048576'
      file.delete.delay.ms: '10000'
notes:
- Kafka administrators can define data flows that cross the boundaries of individual
  Kafka clusters.
- It is generally not advisable to run a single Kafka cluster that spans multiple
  datacenters over a high-latency link.
- Multi-tenancy requires proper control and management to ensure coexistence of different
  needs.
- 'Example topic naming structure: <organization>.<team>.<dataset>.<event-name>'
- 'Example topic naming structure: <project>.<product>.<event-name>'
- It may be useful to disable the Kafka feature to auto-create topics on demand by
  setting auto.create.topics.enable=false in the broker configuration.
- Kafka supports a variety of use cases including messaging, website activity tracking,
  metrics, log aggregation, stream processing, event sourcing, and commit log.
- Kafka should run well on any unix system and has been tested on Linux and Solaris.
- It is unlikely to require much OS-level tuning.
- Kafka always immediately writes all data to the filesystem and supports the ability
  to configure the flush policy.
- In general you don't need to do any low-level tuning of the filesystem.
- We recommend monitoring GC time and other stats and various server stats such as
  CPU utilization, I/O service time, etc.
- On the client side, we recommend monitoring the message/byte rate (global and per
  topic), request rate/size/time, and on the consumer side, max lag in messages among
  all partitions and min fetch request rate.
- For a consumer to keep up, max lag needs to be less than a threshold and min fetch
  rate needs to be larger than 0.
- Kafka supports replication across multiple data centers using MirrorMaker.
- It's recommended to run MirrorMaker processes close to their target clusters to
  minimize latency.
- It is unlikely to require much OS-level tuning, but there are three potentially
  important OS-level configurations.
- Kafka always immediately writes all data to the filesystem and supports the ability
  to configure the flush policy that controls when data is forced out of the OS cache
  and onto disk using the flush.
- Kafka supports data ingestion from multiple sources and provides a wide range of
  functionalities for messaging, tracking, and processing.
- Kafka must eventually call fsync to know that data was flushed.
- Kafka uses Yammer Metrics for metrics reporting in the server.
- Apache Kafka disables remote JMX by default.
- Kafka uses ZooKeeper for managing cluster metadata.
- Kafka Connect is used to import/export data to/from Kafka.
- Topics cannot be longer than 249 characters.
- Kafka does not currently support reducing the number of partitions for a topic.
- Geo-replication with MirrorMaker replicates data across Kafka clusters.
- This deployment pattern allows datacenters to act as independent entities and allows
  us to manage and tune inter-datacenter replication centrally.
- Kafka naturally batches data in both the producer and consumer so it can achieve
  high-throughput even over a high-latency connection.
- Multi-tenancy is a many-sided subject, including but not limited to creating user
  spaces for tenants, configuring topics with data retention policies, securing topics
  and clusters with encryption, authentication, and authorization, isolating tenants
  with quotas and rate limits, monitoring and metering, inter-cluster data sharing.
- Kafka supports high-throughput event streaming.
- Geo-replication can be configured for inter-cluster data flows.
- The partition reassignment tool does not have the ability to automatically generate
  a reassignment plan for decommissioning brokers yet.
- 'Note on preventing replication ''loops'': As long as you define the flows in the
  same MirrorMaker configuration file, you do not need to explicitly add topics.exclude
  settings to prevent replication loops between the two clusters.'
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Multi-tenancy is a many-sided subject, including but not limited to creating user
  spaces for tenants, configuring topics with data retention policies, securing topics
  and clusters, isolating tenants with quotas and rate limits, monitoring and metering,
  inter-cluster data sharing.
- Multi-tenancy is a many-sided subject, including creating user spaces for tenants,
  configuring topics with data retention policies, securing topics and clusters with
  encryption, authentication, and authorization.
- Kafka server's process.role should be set to either broker or controller but not
  both.
- Combined mode can be used in development environment but it should be avoided in
  critical deployment environments.
- For redundancy, a Kafka cluster should use 3 controllers. More than 3 servers is
  not recommended in critical environments.
- The Kafka controllers store all of the metadata for the cluster in memory and on
  disk. We believe that for a typical Kafka cluster 5GB of main memory and 5GB of
  disk space on the metadata log director is sufficient.
- 'The partition reassignment tool can run in 3 mutually exclusive modes: --generate,
  --execute, --verify'
- Increasing the replication factor of an existing partition is easy.
- Multi-tenancy is a many-sided subject, including but not limited to creating user
  spaces for tenants, configuring topics with data retention policies, securing topics
  and clusters with encryption, authentication, and authorization, isolating tenants
  with quotas and rate limits, monitoring and metering, inter-cluster data sharing
  (geo-replication).
- It is recommended to define logical spaces based on a hierarchical topic naming
  structure.
- Certain information should not be put in a topic name, such as information likely
  to change over time or technical details available elsewhere.
- To enforce a topic naming structure, use prefix ACLs, define a custom CreateTopicPolicy,
  disable topic creation for normal users, and disable Kafka's auto-create topics
  feature.
- We recommend using the default flush settings which disable application fsync entirely.
- Both JMX and the 4 letter words (4lw) commands are very useful for monitoring.
- Kafka is regularly updated to include the latest release in the 3.5 series.
- Kafka supports both ZooKeeper and KRaft modes.
- In KRaft mode, a majority of controllers must be alive to maintain availability.
- It is recommended to enable TRACE level logging for the migration components while
  the migration is active.
- Kafka servers' process.role should be set to either broker or controller but not
  both.
- Controlled shutdown will only succeed if all the partitions hosted on the broker
  have replicas.
- Kafka supports high-throughput and low-latency message processing.
- Kafka requires a unique broker id for each server.
- It is recommended to set tasks.max to at least 2 for MirrorMaker.
- Kafka is ideal for high-throughput messaging and stream processing.
- Ensure proper topic naming structure for multi-tenancy.
- Java 11 and later versions perform significantly better if TLS is enabled.
- Kafka should have its own dedicated disk(s) or SSD(s).
- Kafka can serve as a kind of external commit-log for a distributed system.
- It is recommended to monitor MirrorMaker processes to ensure all defined replication
  flows are up and running correctly.
- Kafka supports a wide range of metrics, such as the rate of failed authentication
  attempts, request latency, consumer lag, total number of consumer groups.
- We recommend at least 100000 allowed file descriptors for the broker processes as
  a starting point.
- Keep an eye at this OS-level property when considering the maximum number of partitions
  a broker may have.
- The XFS filesystem has a significant amount of auto-tuning in place, so it does
  not require any change in the default settings.
- 'Disabling journaling is a tradeoff: it makes reboots faster after server crashes
  but it introduces a great deal of additional locking.'
- To prevent replication loops, define flows in the same MirrorMaker configuration
  file.
- Kafka server's process.role should be set to either broker or controller but not
  both. Combined mode can be used in development environments, but it should be avoided
  in critical deployment environments.
- Multi-tenancy involves creating user spaces, securing topics with encryption, authentication,
  and authorization.
- Kafka supports multiple data retention policies.
- Geo-replication can be used for disaster recovery.
- You must enable security when enabling remote JMX in production scenarios.
- Not advisable to run a single Kafka cluster that spans multiple datacenters over
  a high-latency link.
- MirrorMaker replicates data across Kafka clusters, which is different from intra-cluster
  replication.
- MirrorMaker is based on the Kafka Connect framework. Any Kafka Connect, source connector,
  and sink connector settings can be used directly in the MirrorMaker configuration.
- Combined mode can be used in development environments, but it should be avoided
  in critical deployment environments.
- For redundancy, a Kafka cluster should use 3 controllers. More than 3 controllers
  is not recommended in critical environments.
- In the rare case of a partial network failure it is possible for the cluster metadata
  quorum to become unavailable. This limitation will be addressed in a future release
  of Kafka.
- The Kafka controllers store all the metadata for the cluster in memory and on disk.
  We believe that for a typical Kafka cluster 5GB of main memory and 5GB of disk space
  on the metadata log director is sufficient.
- ZooKeeper to KRaft migration is considered an Early Access feature and is not recommended
  for production clusters.
- ZooKeeper is now marked deprecated
- Migration to KRaft is currently in Preview
- Kafka supports high throughput and low latency for real-time data processing.
- It is recommended to set 'tasks.max' to at least 2 for MirrorMaker.
- Java 11 performs significantly better if TLS is enabled
- From a security perspective, we recommend the latest released patch version as older
  freely available versions have disclosed security vulnerabilities.
- By default, on a number of Linux systems, the value of vm.max_map_count is somewhere
  around 65535.
- 'For any filesystem used for data directories, on Linux systems, the following options
  are recommended to be used at mount time: noatime.'
- Kafka supports geo-replication using MirrorMaker.
- Use caution when modifying topic configurations.
- Kafka is used as a highly scalable event streaming platform for multi-tenant environments.
- Kafka works well as a replacement for a more traditional message broker.
- Kafka is often used for operational monitoring data.
- 'Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale
  proof-of-concept on a single machine; and you only need to run additional instances
  of your application on multiple machines to scale up to high-volume production workloads.'
- Kafka Streams transparently handles the load balancing of multiple instances of
  the same application by leveraging Kafka's parallelism model.
- You should keep an eye at this OS-level property when considering the maximum number
  of partitions a broker may have.
- Kafka always immediately writes all data to the filesystem and supports the ability
  to configure the flush policy that controls when data is forced out of the OS cache
  and onto disk.
- ZooKeeper is now marked deprecated.
- Removal of ZooKeeper is planned in the next major release of Apache Kafka (version
  4.0).
- Kafka supports OAuth2 authentication.
- Ensure proper configuration for secure connections.
- ZooKeeper is deprecated in Kafka 3.5 and will be removed in version 4.0.
- For new MirrorMaker clusters, set the exactly.once.source.support property to enabled
  for all targeted Kafka clusters that should be written to with exactly-once semantics.
- It is also necessary to enable intra-cluster communication between the MirrorMaker
  nodes.
- MirrorMaker processes can be configured to replicate data to nearby clusters only
  using the --clusters parameter.
- Multi-tenancy is a many-sided subject, including but not limited to creating user
  spaces for tenants, configuring topics with data retention policies, securing topics
  and clusters with encryption, authentication, and authorization, isolating tenants
  with quotas and rate limits, monitoring and metering, and inter-cluster data sharing.
- ZooKeeper is now marked deprecated. Removal of ZooKeeper is planned in the next
  major release of Apache Kafka (version 4.0).
- Recommended to begin planning for migration to KRaft.
- Tiered storage is considered as an early access feature, and is not recommended
  for use in production environments
- Deleting tiered storage enabled topics is required before disabling tiered storage
  at the broker level
- Kafka supports various use cases such as messaging, website activity tracking, metrics
  collection, log aggregation, and stream processing.
- Kafka Streams is available starting from version 0.10.0.0 for data processing.
- Durability in Kafka does not require syncing data to disk.
- Removal of ZooKeeper is planned in the next major release of Apache Kafka (version
  4.0), which is scheduled to happen no sooner than April 2024.
- ZooKeeper is deprecated and will be removed in the next major release.
- ZooKeeper is deprecated; migration to KRaft is recommended.
errors:
- 'OutOfMemoryError: Map failed due to insufficient vm.max_map_count.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- OutOfMemoryError (Map failed) on a system with default vm.max_map_count.
- Throttle API calls or reduce frequency
- Break down filters or add selectivity
- Recheck OAuth scopes or token expiration
- Throttled traffic can cause replication to not make progress if set too low.
- Log directory ... is already formatted
- 'max(BytesInPerSec) > throttle: If the throttle is set too low, replication may
  not make progress.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- ZooKeeper to KRaft migration is not recommended for production clusters.
- 'The following features are not yet supported for ZK to KRaft migrations: Downgrading
  to ZooKeeper mode during or after the migration.'
- '401 Unauthorized: Check your credentials and permissions.'
- '404 Not Found: Verify the endpoint URL.'
- '429 Too Many Requests: Rate limit exceeded, reduce the request frequency.'
- 'Configuration Conflict: Ensure consistent configuration across replication flows
  to the same target cluster.'
- No support for clusters with multiple log directories (i.e. JBOD feature)
- No support for compacted topics
- Cannot disable tiered storage at the topic level
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  base_url: https://your-kafka-instance.api-name.com
  auth:
    type: oauth2
    flow: refresh_token
    token_url: https://login.kafka.com/services/oauth2/token
    client_id: '{{ dlt.secrets[''kafka_client_id''] }}'
    client_secret: '{{ dlt.secrets[''kafka_client_secret''] }}'
    refresh_token: '{{ dlt.secrets[''kafka_refresh_token''] }}'
    location: header
    header_name: Authorization
  headers:
    Accept: application/json
    log.dir: '[List of directories. Kafka should have its own dedicated disk(s) or
      SSD(s).]'
    num.partitions: '8'
    default.replication.factor: '3'
    Xmx: 6g
    Xms: 6g
    XX:MetaspaceSize: 96m
    XX:+UseG1GC: ''
    XX:MaxGCPauseMillis: '20'
    XX:InitiatingHeapOccupancyPercent: '35'
    XX:G1HeapRegionSize: 16M
    XX:MinMetaspaceFreeRatio: '50'
    XX:MaxMetaspaceFreeRatio: '80'
    XX:+ExplicitGCInvokesConcurrent: ''
  paginator: {}
source_metadata: null
