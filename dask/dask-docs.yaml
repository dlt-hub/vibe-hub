resources:
- name: DatasetSpec
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: ColumnSpec
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: RangeIndexSpec
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DatetimeIndexSpec
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: LocalCluster
  endpoint:
    path: /localcluster
    method: GET
    data_selector: cluster_info
- name: dask-jobqueue
  endpoint:
    path: /dask-jobqueue
    method: GET
    data_selector: records
    params: {}
- name: dask-mpi
  endpoint:
    path: /dask-mpi
    method: GET
    data_selector: records
    params: {}
- name: dask_cluster
  endpoint:
    path: /dask-cluster
    method: POST
    data_selector: cluster
    params: {}
- name: dask_gateway
  endpoint:
    path: /dask-gateway
    method: POST
    data_selector: gateway
    params: {}
- name: scheduler
  endpoint:
    path: /dask/scheduler
    method: GET
- name: worker
  endpoint:
    path: /dask/worker
    method: GET
- name: dask
  endpoint:
    path: ghcr.io/dask/dask
    method: GET
- name: dask-notebook
  endpoint:
    path: ghcr.io/dask/dask-notebook
    method: GET
- name: Scheduler
  endpoint:
    path: /distributed/Scheduler
    method: GET
    data_selector: ''
    params: {}
- name: Worker
  endpoint:
    path: /distributed/Worker
    method: GET
    data_selector: ''
    params: {}
- name: Client
  endpoint:
    path: /distributed/Client
    method: GET
    data_selector: ''
    params: {}
- name: Nanny
  endpoint:
    path: /distributed/Nanny
    method: GET
    data_selector: ''
    params: {}
- name: workers
  endpoint:
    path: /workers
    method: GET
    data_selector: workers
    params: {}
- name: workers_to_close
  endpoint:
    path: /workers_to_close
    method: GET
- name: metrics
  endpoint:
    path: /metrics
    method: GET
- name: scheduler
  endpoint:
    path: /distributed/scheduler
    method: GET
- name: worker
  endpoint:
    path: /distributed/worker
    method: GET
- name: nanny
  endpoint:
    path: /distributed/nanny
    method: GET
- name: client
  endpoint:
    path: /distributed/client
    method: GET
- name: map_overlap
  endpoint:
    path: /generated/dask.array.map_overlap.html
    method: GET
    data_selector: records
    params: {}
- name: Array
  endpoint:
    path: /array
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /dataframe
    method: GET
    data_selector: records
- name: Bag
  endpoint:
    path: /bag
    method: GET
    data_selector: records
- name: Delayed
  endpoint:
    path: /delayed
    method: GET
    data_selector: records
- name: Futures
  endpoint:
    path: /futures
    method: GET
    data_selector: records
- name: stack
  endpoint:
    path: /stack
    method: GET
    data_selector: records
- name: concatenate
  endpoint:
    path: /concatenate
    method: GET
    data_selector: records
- name: block
  endpoint:
    path: /block
    method: GET
    data_selector: records
- name: absolute
  endpoint:
    path: numpy.absolute
    method: GET
    data_selector: null
    params: {}
- name: add
  endpoint:
    path: numpy.add
    method: GET
    data_selector: null
    params: {}
- name: all
  endpoint:
    path: numpy.all
    method: GET
    data_selector: null
    params: {}
- name: allclose
  endpoint:
    path: numpy.allclose
    method: GET
    data_selector: null
    params: {}
- name: amax
  endpoint:
    path: numpy.amax
    method: GET
    data_selector: null
    params: {}
- name: amin
  endpoint:
    path: numpy.amin
    method: GET
    data_selector: null
    params: {}
- name: nansum
  endpoint:
    path: /numpy/nansum
    method: GET
- name: nanvar
  endpoint:
    path: /numpy/nanvar
    method: GET
- name: negative
  endpoint:
    path: /numpy/negative
    method: GET
- name: nextafter
  endpoint:
    path: /numpy/nextafter
    method: GET
- name: nonzero
  endpoint:
    path: /numpy/nonzero
    method: GET
- name: not_equal
  endpoint:
    path: /numpy/not_equal
    method: GET
- name: ones
  endpoint:
    path: /numpy/ones
    method: GET
- name: ones_like
  endpoint:
    path: /numpy/ones_like
    method: GET
- name: outer
  endpoint:
    path: /numpy/outer
    method: GET
- name: pad
  endpoint:
    path: /numpy/pad
    method: GET
- name: percentile
  endpoint:
    path: /numpy/percentile
    method: GET
- name: piecewise
  endpoint:
    path: /numpy/piecewise
    method: GET
- name: positive
  endpoint:
    path: /numpy/positive
    method: GET
- name: power
  endpoint:
    path: /numpy/power
    method: GET
- name: prod
  endpoint:
    path: /numpy/prod
    method: GET
- name: ptp
  endpoint:
    path: /numpy/ptp
    method: GET
- name: quantile
  endpoint:
    path: /numpy/quantile
    method: GET
- name: radians
  endpoint:
    path: /numpy/radians
    method: GET
- name: ravel
  endpoint:
    path: /numpy/ravel
    method: GET
- name: real
  endpoint:
    path: /numpy/real
    method: GET
- name: reciprocal
  endpoint:
    path: /numpy/reciprocal
    method: GET
- name: remainder
  endpoint:
    path: /numpy/remainder
    method: GET
- name: repeat
  endpoint:
    path: /numpy/repeat
    method: GET
- name: reshape
  endpoint:
    path: /numpy/reshape
    method: GET
- name: sqrt
  endpoint:
    path: /numpy/sqrt
    method: GET
- name: square
  endpoint:
    path: /numpy/square
    method: GET
- name: squeeze
  endpoint:
    path: /numpy/squeeze
    method: GET
- name: stack
  endpoint:
    path: /numpy/stack
    method: GET
- name: std
  endpoint:
    path: /numpy/std
    method: GET
- name: Dask Bags
  endpoint:
    path: /create-dask-bags
    method: GET
    data_selector: records
- name: Bag
  endpoint:
    path: /dask/bag
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /dask/dataframe
    method: GET
    data_selector: records
- name: read_csv
  endpoint:
    path: /generated/dask.dataframe.read_csv
    method: GET
    data_selector: records
- name: read_parquet
  endpoint:
    path: /generated/dask.dataframe.read_parquet
    method: GET
    data_selector: records
- name: write_to_parquet
  endpoint:
    path: /generated/dask.dataframe.to_parquet
    method: POST
    data_selector: records
- name: Dask DataFrame Metadata
  endpoint:
    path: /dataframe/metadata
    method: GET
    data_selector: _meta
- name: Dask DataFrame Partitions
  endpoint:
    path: /dataframe/partitions
    method: GET
    data_selector: divisions
- name: Dask DataFrame Groupby
  endpoint:
    path: /dataframe/groupby
    method: GET
    data_selector: groupby_results
- name: Index
  endpoint:
    path: /dask/dataframe/Index
    method: GET
    data_selector: Index-like Expr Collection
- name: parquet_files
  endpoint:
    path: /path/to/my/parquet/
    method: GET
    data_selector: records
    params: {}
- name: read_parquet
  endpoint:
    path: /generated/dask.dataframe.read_parquet.html
    method: GET
    data_selector: records
- name: to_parquet
  endpoint:
    path: /generated/dask.dataframe.to_parquet.html
    method: GET
    data_selector: records
- name: dataframe
  endpoint:
    path: /dataframe/indexing
    method: GET
    data_selector: records
- name: categoricals
  endpoint:
    path: /categoricals
    method: GET
    data_selector: records
- name: hive_partitioning
  endpoint:
    path: /hive/partitioning
    method: GET
    data_selector: records
- name: delayed_example
  endpoint:
    path: /delayed
    method: GET
    data_selector: result
    params: {}
- name: hyperparameter_optimization
  endpoint:
    path: /hyperparameter/optimization
    method: GET
    data_selector: records
    params: {}
- name: gradient_boosted_trees
  endpoint:
    path: /gradient/boosted/trees
    method: GET
    data_selector: records
    params: {}
- name: batch_prediction
  endpoint:
    path: /batch/prediction
    method: GET
    data_selector: records
    params: {}
- name: Local Threads
  endpoint:
    path: /local/threads
    method: POST
    data_selector: scheduling
    params: {}
- name: Local Processes
  endpoint:
    path: /local/processes
    method: POST
    data_selector: scheduling
    params: {}
- name: Single Thread
  endpoint:
    path: /single/thread
    method: POST
    data_selector: scheduling
    params: {}
- name: Dask Distributed (local)
  endpoint:
    path: /distributed/local
    method: POST
    data_selector: scheduling
    params: {}
- name: Dask Distributed (Cluster)
  endpoint:
    path: /distributed/cluster
    method: POST
    data_selector: scheduling
    params: {}
- name: task_graph
  endpoint:
    path: /task_graph
    method: GET
    data_selector: graphs
- name: HighLevelGraph
  endpoint:
    path: /dask/highlevelgraph
    method: GET
    data_selector: layers
    params: {}
- name: ProgressBar
  endpoint:
    path: /dask/diagnostics/ProgressBar
    method: GET
- name: Profiler
  endpoint:
    path: /dask/diagnostics/Profiler
    method: GET
- name: ResourceProfiler
  endpoint:
    path: /dask/diagnostics/ResourceProfiler
    method: GET
- name: CacheProfiler
  endpoint:
    path: /dask/diagnostics/CacheProfiler
    method: GET
- name: Callback
  endpoint:
    path: /dask/diagnostics/Callback
    method: GET
- name: visualize
  endpoint:
    path: /dask/diagnostics/visualize
    method: GET
- name: Dask Arrays
  endpoint:
    path: /array
    method: GET
    data_selector: records
- name: Dask Bags
  endpoint:
    path: /bag
    method: GET
    data_selector: records
- name: Dask DataFrames
  endpoint:
    path: /dataframe
    method: GET
    data_selector: records
- name: Dask Machine Learning
  endpoint:
    path: /ml
    method: GET
    data_selector: records
- name: delayed
  endpoint:
    path: /dask/delayed
    method: GET
    data_selector: Delayed
    params: {}
- name: config
  endpoint:
    path: /config
    method: GET
    data_selector: records
- name: distributed.nanny
  endpoint:
    path: /distributed/nanny
    method: GET
    data_selector: records
    params: {}
- name: distributed.admin
  endpoint:
    path: /distributed/admin
    method: GET
    data_selector: records
    params: {}
- name: distributed.rmm
  endpoint:
    path: /distributed/rmm
    method: GET
    data_selector: records
    params: {}
- name: Queue
  endpoint:
    path: /distributed.Queue
    method: GET
    data_selector: Queue
    params: {}
- name: Variable
  endpoint:
    path: /distributed.Variable
    method: GET
    data_selector: Variable
    params: {}
- name: Lock
  endpoint:
    path: /distributed.Lock
    method: GET
    data_selector: Lock
    params: {}
- name: Event
  endpoint:
    path: /distributed.Event
    method: GET
    data_selector: Event
    params: {}
- name: Semaphore
  endpoint:
    path: /distributed.Semaphore
    method: GET
    data_selector: Semaphore
    params: {}
- name: nthreads
  endpoint:
    path: /nthreads
    method: GET
    data_selector: nthreads
- name: persist
  endpoint:
    path: /persist
    method: POST
    data_selector: persist
- name: processing
  endpoint:
    path: /processing
    method: GET
    data_selector: processing
- name: profile
  endpoint:
    path: /profile
    method: GET
    data_selector: profile
- name: publish_dataset
  endpoint:
    path: /publish_dataset
    method: POST
    data_selector: publish_dataset
- name: rebalance
  endpoint:
    path: /rebalance
    method: POST
    data_selector: rebalance
- name: replicate
  endpoint:
    path: /replicate
    method: POST
    data_selector: replicate
- name: restart
  endpoint:
    path: /restart
    method: POST
    data_selector: restart
- name: restart_workers
  endpoint:
    path: /distributed/client/restart_workers
    method: POST
- name: retire_workers
  endpoint:
    path: /distributed/client/retire_workers
    method: POST
- name: retry
  endpoint:
    path: /distributed/client/retry
    method: POST
- name: run
  endpoint:
    path: /distributed/client/run
    method: POST
- name: run_on_scheduler
  endpoint:
    path: /distributed/client/run_on_scheduler
    method: POST
- name: scatter
  endpoint:
    path: /distributed/client/scatter
    method: POST
- name: scheduler_info
  endpoint:
    path: /distributed/client/scheduler_info
    method: GET
- name: set_metadata
  endpoint:
    path: /distributed/client/set_metadata
    method: POST
- name: shutdown
  endpoint:
    path: /distributed/client/shutdown
    method: POST
- name: start
  endpoint:
    path: /distributed/client/start
    method: POST
- name: story
  endpoint:
    path: /distributed/client/story
    method: GET
- name: submit
  endpoint:
    path: /distributed/client/submit
    method: POST
- name: subscribe_topic
  endpoint:
    path: /distributed/client/subscribe_topic
    method: POST
- name: unpublish_dataset
  endpoint:
    path: /unpublish_dataset
    method: POST
    data_selector: result
    params: {}
- name: unregister_scheduler_plugin
  endpoint:
    path: /unregister_scheduler_plugin
    method: POST
    data_selector: result
    params: {}
- name: unregister_worker_plugin
  endpoint:
    path: /unregister_worker_plugin
    method: POST
    data_selector: result
    params: {}
- name: unsubscribe_topic
  endpoint:
    path: /unsubscribe_topic
    method: POST
    data_selector: result
    params: {}
- name: upload_file
  endpoint:
    path: /upload_file
    method: POST
    data_selector: result
    params: {}
- name: wait_for_workers
  endpoint:
    path: /wait_for_workers
    method: POST
    data_selector: result
    params: {}
- name: who_has
  endpoint:
    path: /who_has
    method: POST
    data_selector: result
    params: {}
- name: write_scheduler_file
  endpoint:
    path: /write_scheduler_file
    method: POST
    data_selector: result
    params: {}
notes:
- This API is still experimental and will likely change in the future.
- Join the monthly community meetings on the first Thursday of the month at 10:00
  US Central Time.
- Meeting notes are available in this Google doc.
- Dask is a Python library for parallel and distributed computing.
- Dask can set itself up easily in your Python session if you create a LocalCluster
  object.
- Dask can be installed via conda or pip.
- Dask collections match existing numpy and pandas methods.
- Methods can be chained together just like in pandas and NumPy.
- Coiled is recommended for deploying Dask on the cloud.
- Many of the cluster managers in Dask Cloud Provider work by launching VMs with a
  startup script that pulls down the Dask Docker image.
- Some clusters benefit from a shared File System (NFS, GPFS, Lustre or alike)
- HPC job schedulers are optimized for large monolithic jobs with many nodes
- Dask Gateway provides a secure, multi-tenant server for managing Dask clusters.
- The scheduler and workers both need to accept TCP connections on an open port.
- By default, the scheduler binds to port 8786 and the worker binds to a random open
  port.
- The dashboard is served on port 8787 by default.
- Each image installs the full Dask conda environment (including the distributed scheduler),
  Numpy, and Pandas on top of a Miniconda installation on top of a Debian image.
- These images are large, around 1GB for dask and about 2GB for dask-notebook.
- Both dask-scheduler and dask-worker support a --preload option
- 'Deprecated since version 2023.9.2: Use Client.register_plugin() instead.'
- Getting Dask running is often not the last step, but the first step.
- Setting up and managing a mature Dask deployment can involve a fair amount of complexity
  outside of Dask itself.
- Embedding large objects like pandas DataFrames or Arrays into the computation is
  a frequent pain point for Dask users.
- Using Dask to load these objects instead avoids these issues and improves the performance
  of a computation significantly.
- Dask is used within the world’s largest banks, national labs, retailers, technology
  companies, and government agencies.
- Dask can run on existing clusters without significant change.
- Dask is designed to operate with user-level permissions.
- Dask adds another layer of complexity which may get in the way.
- Optimal sizes and shapes are highly problem specific.
- Chunk sizes between 10MB-1GB are common, depending on the availability of RAM and
  the duration of computations.
- 'Chunks includes three special values: -1: no chunking along this dimension, None:
  no change to the chunking along this dimension, ''auto'': allow the chunking in
  this dimension to accommodate ideal chunk sizes.'
- Automatic chunking expands or contracts all dimensions marked with 'auto' to try
  to reach chunk sizes with a number of bytes equal to the config value array.chunk-size,
  which is set to 128MiB by default.
- It may perform poorly on Windows if the file is not in the file system cache. On
  Linux it performs well under most circumstances.
- The main API for these computations is the map_overlap method.
- Dask Array supports most of the NumPy slicing syntax.
- This experimental generalized ufunc integration is not complete
- where parameter not supported.
- initial parameter not supported.
- Input must be a dask array.
- order parameter not supported.
- Sort operations are notoriously difficult to do in parallel. Parallel-friendly alternatives
  sort the k largest elements.
- out parameter not supported.
- Use of numpy.matrix is discouraged in NumPy and thus there is no need to add it.
- mode parameter not supported.
- keepdims parameter not supported.
- fweights, aweights, dtype parameters not supported.
- like parameter not supported. Callable functions not supported.
- Not implemented with more than one output.
- edge_order parameter not supported.
- Chunking of the input data (sample) is only allowed along the 0th (row) axis.
- Only implemented for monotonic obj arguments.
- overwrite_input parameter not supported.
- copy parameter not supported.
- Bags provide very general computation (any Python function).
- By default, they rely on the multiprocessing scheduler, which has its own set of
  known limitations.
- Bags are immutable and so you can not change individual elements.
- Bag operations tend to be slower than array/DataFrame computations in the same way
  that standard Python containers tend to be slower than NumPy arrays and Pandas DataFrames.
- Bag’s `groupby` is slow. You should try to use Bag’s `foldby` if possible.
- Dask can load data directly from text files.
- Dask Bag can read binary files in the Avro format if fastavro is installed.
- Dask DataFrame helps you process large tabular data by parallelizing pandas.
- Dask DataFrame supports Query Planning since version 2024.03.0
- Usual pandas performance tips like avoiding apply, using vectorized operations,
  using categoricals, etc., all apply equally to Dask DataFrame.
- Apache Parquet is recommended for storage of large volumes of tabular data.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Parquet I/O requires pyarrow to be installed.
- For remote filesystems you may need to configure credentials.
- When loading parquet data, specify a subset of columns via the columns keyword argument
  to reduce IO costs and memory usage.
- Dask DataFrame supports some of Pandas’ indexing behavior.
- Dask does not support arbitrary text queries, only whole tables and SQLAlchemy sql
  expressions.
- The chunksize argument is not used, since the partitioning must be via an index
  column.
- Dask will use on-disk shuffling by default, but will switch to a distributed shuffling
  algorithm if the default scheduler is set to use a dask.distributed.Client.
- The `aggregate_files` argument is currently listed as experimental.
- Dask delayed operates on functions like dask.delayed(f)(x, y), not on their results
  like dask.delayed(f(x, y)).
- Don’t Modify Data In-Place
- Avoid Holding the GIL
- Users working with custom graphs or computations may find that applying optimization
  methods results in substantial speedups.
- These optimizations are already performed automatically in the Dask collections.
- There are some situations where computations with Dask collections will result in
  suboptimal memory usage.
- Both visualization engines require optional dependencies to be installed.
- Graphviz takes a while on graphs larger than about 100 nodes.
- Dashboard is built with Bokeh and starts automatically with the scheduler.
- Profiling parallel code can be challenging.
- Multiple profilers can be used in a with block.
- Bokeh must be installed in your scheduler’s environment to run the dashboard.
- Depending on your configuration, you might also need to install jupyter-server-proxy
  to access the dashboard.
- Operations on Dask collections build task graphs.
- Reduce graph size by increasing chunk size if possible.
- This is an advanced topic that most users won’t need to worry about.
- Dask's ordering does well most of the time, but there might be cases where you observe
  unexpectedly high memory usage or communication.
- Opportunistic caching is not available when using the distributed scheduler.
- Restricting your cache to a fixed size like 2GB requires Dask to accurately count
  the size of each of our objects in memory.
- The shared memory scheduler works on a single machine
- The threaded scheduler is limited by the GIL on Python code, so if your operations
  are pure python functions, you should not expect a multi-core speedup
- The multiprocessing scheduler must serialize functions between workers, which can
  fail
- The multiprocessing scheduler must serialize data between workers and the central
  process, which can be expensive
- The multiprocessing scheduler cannot transfer data directly between worker processes;
  all data routes through the main process.
- The default shared memory scheduler used by most dask collections lives in dask/local.py.
- This scheduler dynamically schedules tasks to new workers as they become available.
- It operates in a shared memory environment without consideration to data locality,
  all workers have access to all data equally.
- We find that our workloads are best served by trying to minimize the memory footprint.
- Our policies to accomplish this in our scheduling budget of one millisecond per
  task, irrespective of the number of tasks.
- This release contains several graph optimization fixes for issues introduced in
  the 2025.4.0 release.
- Some objects like Contact may return nulls in deeply nested fields
- Legacy Dask DataFrame implementation is deprecated.
- Dask Array added new quantile and nanquantile methods.
- The current Dask DataFrame implementation is deprecated.
- The new implementation is already available and can be enabled by installing the
  `dask-expr` library.
- This release contains several improvements to Dask’s object tokenization logic.
- The distributed.PipInstall plugin now has more robust restart logic and also supports
  environment variables.
- Released on December 10, 2021
- This is the first release with support for Python 3.9 and the last release with
  support for Python 3.6
- Compatibility for NumPy dtype deprecation
- Taking full advantage of Dask sometimes requires user configuration.
- Configuration values can be specified in YAML files, environment variables, or directly
  within Python.
- Dask's configuration system merges configurations from various sources, with user-specified
  values taking precedence.
- It is possible to configure Dask inline with dot notation, with YAML or via environment
  variables.
- Default values provided for certain configurations.
- Dask works with GPUs in a few ways.
- GPU computing is a quickly moving field today and as a result the information in
  this page is likely to go out of date quickly.
- Starts a diagnostic dashboard at http://localhost:8787
- Dask provides distributed versions of coordination primitives like locks, events,
  queues, and global variables that, where appropriate, match their in-memory counterparts.
- These features are rarely necessary for common use of Dask. We recommend that beginning
  users stick with using the simpler futures found above.
- Actors allow workers to manage rapidly changing state without coordinating with
  the central scheduler.
- These benefits come at a cost. The scheduler is unaware of actors and so they don’t
  benefit from diagnostics, load balancing, or resilience.
- Actors avoid the central scheduler they can be high-performing, but not resilient.
- Uses Dask's futures for task scheduling and execution
errors: []
auth_info:
  mentioned_objects: []
client:
  base_url: http://localhost:8787
  headers:
    Accept: application/json
  paginator:
    type: cursor
source_metadata: null
