resources:
- name: pretrained_ycb_v10_tgz
  endpoint:
    path: https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.monty/pretrained_ycb_v10.tgz
    method: GET
- name: pretrained_ycb_v10_zip
  endpoint:
    path: https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.monty/pretrained_ycb_v10.zip
    method: GET
- name: experiment_results
  endpoint:
    path: /experiments/results
    method: GET
    data_selector: results
- name: Running Your First Experiment
  endpoint:
    path: /docs/running-your-first-experiment
    method: GET
- name: Pretraining a Model
  endpoint:
    path: /docs/pretraining-a-model
    method: GET
- name: Running Inference with a Pretrained Model
  endpoint:
    path: /docs/running-inference-with-a-pretrained-model
    method: GET
- name: Unsupervised Continual Learning
  endpoint:
    path: /docs/unsupervised-continual-learning
    method: GET
- name: Multiple Learning Modules
  endpoint:
    path: /docs/multiple-learning-modules
    method: GET
- name: first_experiment
  endpoint:
    path: /benchmarks/configs/my_experiments.py
    method: POST
    data_selector: experiments
    params: {}
- name: environment_interface
  endpoint:
    path: /environment/interface
    method: GET
- name: MontySupervisedObjectPretrainingExperiment
  endpoint:
    path: /benchmarks/configs/my_experiments.py
    method: POST
- name: surf_agent_2obj_train
  endpoint:
    path: /path/to/surf_agent_2obj_train
    method: POST
    data_selector: results
- name: experiment
  endpoint:
    path: /benchmarks/experiments
    method: POST
    data_selector: results
    params: {}
- name: unsupervised_learning_experiment
  endpoint:
    path: /experiments/unsupervised
    method: POST
    data_selector: results
    params:
      experiment_class: MontyObjectRecognitionExperiment
      n_train_epochs: 3
      max_train_steps: 2000
      max_total_steps: 5000
- name: surf_agent_2obj_unsupervised
  endpoint:
    path: /benchmarks/configs/names.py
    method: POST
    data_selector: experiments
    params: {}
- name: experiment
  endpoint:
    path: /benchmarks/configs/my_experiments.py
    method: POST
    data_selector: experiments
    params: {}
- name: omniglot_training
  endpoint:
    path: /omniglot/training
    method: POST
    data_selector: experiment_config
    params: {}
- name: omniglot_inference
  endpoint:
    path: /omniglot/inference
    method: POST
    data_selector: experiment_config
    params: {}
- name: env_interface_config
  endpoint:
    path: /configs/WorldImageEnvironmentInterfaceConfig
    method: GET
    data_selector: env_init_args
    params: {}
- name: eval_env_interface_args
  endpoint:
    path: /configs/WorldImageEnvironmentInterfaceArgs
    method: GET
    data_selector: scenes
    params: {}
- name: Jump
  endpoint:
    path: /actuate/jump
    method: POST
    data_selector: action
    params: {}
- name: experiment
  endpoint:
    path: /frameworks/run
    method: POST
    data_selector: experiments
    params: {}
- name: MontyExperiment
  endpoint:
    path: /docs/experiment
    method: GET
    data_selector: ''
    params: {}
- name: EnvironmentInterface
  endpoint:
    path: /environment/interface
    method: GET
- name: EnvironmentInterfacePerObject
  endpoint:
    path: /environment/interface/perobject
    method: GET
- name: InformedEnvironmentInterface
  endpoint:
    path: /environment/interface/informed
    method: GET
- name: OmniglotEnvironmentInterface
  endpoint:
    path: /environment/interface/omniglot
    method: GET
- name: EmbodiedEnvironment
  endpoint:
    path: /environment/embodied
    method: GET
- name: HabitatEnvironment
  endpoint:
    path: /environment/habitat
    method: GET
- name: RealRobotsEnvironment
  endpoint:
    path: /environment/realrobots
    method: GET
- name: OmniglotEnvironment
  endpoint:
    path: /environment/omniglot
    method: GET
- name: SensorModule
  endpoint:
    path: /docs/sensor-modules
    method: GET
    data_selector: features at pose
    params: {}
- name: HabitatSM
  endpoint:
    path: /docs/sensor-modules/habitat-sm
    method: GET
    data_selector: features at pose
    params: {}
- name: Probe
  endpoint:
    path: /docs/sensor-modules/probe
    method: GET
    data_selector: raw observations
    params: {}
- name: learning_module_memory
  endpoint:
    path: /docs/learning-module
    method: GET
    data_selector: records
- name: graph_building
  endpoint:
    path: /docs/graph-building
    method: GET
    data_selector: records
- name: GetGoodView
  endpoint:
    path: /GetGoodView
    method: POST
    data_selector: results
    params:
      good_view_percentage: desired_value
      desired_object_distance: desired_value
- name: touch_object
  endpoint:
    path: /touch_object
    method: POST
    data_selector: results
    params:
      desired_object_distance: desired_value
- name: future_work_widget_metadata
  endpoint:
    path: /docs/future-work-widget-metadata
    method: GET
    data_selector: fields
- name: issues
  endpoint:
    path: /github.com/thousandbrainsproject/tbp.monty/issues
    method: GET
    data_selector: issues
    params:
      is: issue
      is_open: true
      not_triaged: true
- name: pull_requests
  endpoint:
    path: /github.com/thousandbrainsproject/tbp.monty/pulls
    method: GET
    data_selector: pull_requests
    params:
      is: pull request
      is_open: true
      not_draft: true
      not_triaged: true
- name: Extract better features
  endpoint:
    path: /docs/extract-better-features
    method: GET
- name: Detect local and global flow
  endpoint:
    path: /docs/detect-local-and-global-flow
    method: GET
- name: Change detecting SM
  endpoint:
    path: /docs/change-detecting-sm
    method: GET
- name: object_behaviors
  endpoint:
    path: /docs/implement-test-gnns-to-model-object-behaviors-states
    method: GET
- name: Interpret goal states in motor system & switch policies
  endpoint:
    path: /docs/interpret-goal-states-in-motor-system-switch-policies
    method: GET
- name: Implement switching between learning and inference-focused policies
  endpoint:
    path: /docs/implement-switching-between-learning-and-inference-focused-policies
    method: GET
- name: Bottom-up exploration policy for surface agent
  endpoint:
    path: /docs/bottom-up-exploration-policy-for-surface-agent
    method: GET
- name: Model-based exploration policy
  endpoint:
    path: /docs/model-based-exploration-policy
    method: GET
- name: Implement efficient saccades driven by model-free and model-based signals
  endpoint:
    path: /docs/implement-efficient-saccades-driven-by-model-free-and-model-based-signals
    method: GET
- name: Learn policy using RL and simplified action space
  endpoint:
    path: /docs/learn-policy-using-rl
    method: GET
- name: Decompose goals into subgoals & communicate
  endpoint:
    path: /docs/decompose-goals-into-subgoals-communicate
    method: GET
- name: Reuse hypothesis testing policy target points
  endpoint:
    path: /docs/reuse-hypothesis-testing-policy-target-points
    method: GET
- name: Implement a simple cross-modal policy
  endpoint:
    path: /docs/implement-a-simple-cross-modal-policy-for-sensory-guidance
    method: GET
- name: Model-based policy to recognize an object before moving onto a new object
  endpoint:
    path: /docs/model-based-policy-to-recognize-an-object-before-moving-onto-a-new-object
    method: GET
- name: Policy to quickly move to a new object
  endpoint:
    path: /docs/policy-to-quickly-move-to-a-new-object
    method: GET
- name: use_pose_for_voting
  endpoint:
    path: /docs/use-pose-for-voting
    method: GET
- name: outline_routing_protocol_attention
  endpoint:
    path: /docs/outline-routing-protocol-attention
    method: GET
- name: generalize_voting_to_associative_connections
  endpoint:
    path: /docs/generalize-voting-to-associative-connections
    method: GET
- name: change_cmp_to_use_displacements
  endpoint:
    path: /docs/can-we-change-the-cmp-to-use-displacements-instead-of-locations
    method: GET
- name: vote_on_state
  endpoint:
    path: /docs/vote-on-state
    method: GET
- name: performance_measures
  endpoint:
    path: /docs/figure-out-performance-measure-and-supervision-in-heterarchy
    method: GET
- name: top_down_connections
  endpoint:
    path: /docs/add-top-down-connections
    method: GET
- name: heterarchy_experiments
  endpoint:
    path: /docs/run-analyze-experiments-with-2lms-in-heterarchy-testbed
    method: GET
- name: multi_object_experiments
  endpoint:
    path: /docs/run-analyze-experiments-in-multiobject-environment-looking-at-scene-graphs
    method: GET
- name: learning_speeds
  endpoint:
    path: /docs/test-learning-at-different-speeds-depending-on-level-in-hierarchy
    method: GET
- name: similarity_encoding
  endpoint:
    path: /docs/send-similarity-encoding-object-id-to-next-level-test
    method: GET
- name: global_interval_timer
  endpoint:
    path: /docs/global-interval-timer
    method: GET
- name: state_in_cmp
  endpoint:
    path: /docs/include-state-in-CMP
    method: GET
- name: Global Interval Timer
  endpoint:
    path: /global-interval-timer
    method: GET
    data_selector: records
- name: compositional_objects
  endpoint:
    path: /docs/make-dataset-to-test-compositional-objects
    method: GET
- name: object_behavior_test_bed
  endpoint:
    path: /docs/object-behavior-test-bed
    method: GET
- name: object_manipulation_environment
  endpoint:
    path: /docs/set-up-environment-that-allows-for-object-manipulation
    method: GET
- name: object_manipulation_benchmark_tasks
  endpoint:
    path: /docs/set-up-object-manipulation-benchmark-tasks-and-evaluation-measures
    method: GET
- name: categories_generalization_metrics
  endpoint:
    path: /docs/create-dataset-and-metrics-to-evaluate-categories-and-generalization
    method: GET
- name: feature_morphology_pairs_metrics
  endpoint:
    path: /docs/create-dataset-and-metrics-to-test-new-feature-morphology-pairs
    method: GET
- name: experiment_setup
  endpoint:
    path: /object_behavior_test_bed/experiment_setup
    method: GET
- name: podcast_series
  endpoint:
    path: /docs/organize-start-podcast-series
    method: GET
- name: condensed_videos
  endpoint:
    path: oss-communication-improvements/make-more-condensed-videos-about-the-project-monty
    method: GET
notes:
- This guide will not work on Windows or non-x86_64/amd64 Linux.
- While the repository contains a uv.lock file, this is currently experimental and
  not supported.
- Unpack the archive in the ~/tbp/results/monty/pretrained_models/ folder.
- To unpack via the command line, copy the archive into the ~/tbp/results/monty/pretrained_models/
  folder.
- The Thousand Brains Project is an open-source framework for sensorimotor learning
  systems.
- Our goal in the near term is to continue building a sensorimotor learning framework
  based on the principles listed above with a general set of abilities for modeling
  and interacting with the world.
- We will provide access to the simple SDK and examples to get started.
- Monty is designed for sensorimotor applications.
- It is not designed to learn from static datasets like many current AI systems are.
- The parameters used here aim to strike a good balance between speed and accuracy
  to allow our researchers to iterate quickly and evaluate algorithm changes regularly.
- Monty is a sensorimotor system made to efficiently learn and infer by interacting
  with an environment. It is not designed for large, static datasets.
- These benchmark experiments are not common benchmarks from the AI field.
- The parameters used here aim to strike a good balance between speed and accuracy.
- 'These benchmark experiments track the progress on RFC 9: Hypotheses resampling.'
- Note that rotation errors are meaningless since no ground truth rotation is provided
- In addition to ensuring that the 'smoothing' option is toggled on (currently off
  by default), lie the iPad on its side, ensuring that the volume bottom is at the
  top, so that the orientation of images are consistent across the data-sets.
- While the repository contains a `uv.lock` file, this is currently experimental and
  not supported.
- Don’t install Linux GPU drivers in WSL, you don’t need them, NVIDIA even warns against
  installing them.
- If you did not save the data (e.g., YCB objects) in the ~/tbp/data folder, you will
  need to set the MONTY_DATA environment variable.
- JSON Logs can get Large Fast
- Detailed JSON stats are not logged by default since they can get large quickly.
- It is also recommended to not log too many episodes with the detailed logger so
  to keep the file size small.
- In this particular experiment, n_train_epochs was set to 1, and max_train_steps
  was set to 1.
- Specify monty experiment and its args.
- This will take a few minutes to complete and then you can inspect and visualize
  the learned models.
- Uses a policy whereby learning-modules generate actions to test hypotheses by producing
  'goal-states' for the low-level motor system.
- Uses unsupervised learning for object recognition
- Uses default sensor modules and motor system from SurfaceAndViewMontyConfig
- Randomly rotates an object at the beginning of each episode using RandomRotationObjectInitializer
- Monty only supports distant agents with multi-LM models.
- Monty is a sensorimotor modeling system. It is NOT made for learning from static
  datasets.
- Monty Currently Expects Movement to be in 3D Euclidean Space.
- Be aware that in Numpy, and in the saved CSV result files, quaternions follow the
  wxyz format, where 'w' is the real component.
- In contrast however, `Scipy.Rotation` expects them to be in xyzw format.
- When operating with quaternions, it is therefore important to be aware of what format
  you should be using for the particular setting.
- Note that in Habitat (and therefore the Monty code-base), the 'z' direction is positive
  in the direction coming out of the screen, while 'y' is the 'up' direction.
- '''x'' is positive pointing to the right, again if you are facing the screen.'
- Note that the rotation that learning modules store in their Most-Likely Hypothesis
  (MLH) is the rotation required to transform a feature (such as a surface normal)
  to match the feature on the object in the environment.
- As such, it is the *inverse* of the actual orientation of the object in the environment.
- Note that sensor-based actions (such as set*sensor_pose), update _all* the sensors
  associated with that agent.
- The learning module is designed to be able to learn objects unsupervised, from scratch.
- Transforms are applied to all sensors in an environment before sending observations
  to the SMs
- The universal format that all sensor modules output is features at pose in 3D space
- The State class contains required information such as location, morphological features,
  non-morphological features, confidence, use_state, sender_id, and sender_type.
- Uses a custom distance measure that takes the surface normal and curvature magnitude
  into account.
- Each episode ends when a terminal state is reached.
- Positioning procedures should only be called in `pre_episode`, i.e. before the episode
  begins.
- Positioning procedures may make use of privileged information such as the view-finder
  and semantic-sensor, which are not available to the learning module.
- The future work documents have special Frontmatter metadata that is used to power
  the future-work widget.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Protocols SHOULD be used to define behaviour that would normally be duck typed
- The `Any` type SHOULD NOT be used
- Third-party libraries with poor type hinting SHOULD be isolated as much as possible.
- May need to update static feature SM to make sure it doesn’t send output when local
  movement is detected
- May get noisy estimates of 3D movement from a 2D depth image
- Currently, Monty's internal representations of objects is only explicitly reset
  by the experimenter, for example at the start of an episode of training.
- If this fails to achieve the results we hope for, we might add a mechanism to explicitly
  reset evidence values when an LM believes it has moved on to a new object.
- This approach would also help reduce the reliance on the first observation.
- Infrastructure for testing multiple scales would be useful additions to the learning
  module.
- Currently all object poses are equally likely, because stimuli exist in a void and
  are typically rotated randomly at test time.
- As we move towards compositional and scene-like datasets where certain object poses
  are more common, we would like to account for this information in our hypothesis
  testing.
- Add a 'state' dimension to the models learned in Monty that conditions which features
  to expect at what locations.
- Both behavior & morphology models can have different states and sequences and both
  can be driven by time or other factors.
- This might be a noisy process and require voting to work well.
- Currently, a Monty system cannot flexibly switch between a learning-focused policy
  and an inference-focused policy.
- Model-based exploration policies will be particularly important in an unsupervised
  setting.
- Such policies are particularly important in an unsupervised setting, where we will
  want to more efficiently explore objects in order to rapidly determine their identity,
  given we have no supervised signal to tell us whether this is a familiar object,
  or an entirely new one.
- When exploring an environment with multiple objects, it is beneficial to quickly
  move to a new object when the current one has been recognized.
- Include the inferred state of an object as part of the CMP message.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
auth_info:
  mentioned_objects: []
client:
  base_url: https://github.com/thousandbrainsproject/tbp.monty
source_metadata: null
