resources:
- name: Clear Chat Button
  endpoint:
    path: /t/where-is-the-clear-chat-button/100551
    method: GET
- name: video_generation
  endpoint:
    path: /api/v1/video/generate
    method: POST
    data_selector: data
    params:
      aspect_ratio: '9:16'
- name: feedback
  endpoint:
    path: /feedback
    method: POST
    data_selector: response
    params: {}
- name: search-index_text-based-vector-search
  endpoint:
    path: /search-index_text-based-vector-search
    method: POST
    data_selector: content
    params: {}
- name: search-index_search-and-sort-based-search
  endpoint:
    path: /search-index_search-and-sort-based-search
    method: POST
    data_selector: content
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: error_message
  endpoint:
    path: /t/what-does-immersive-content-redacted-for-brevity-mean/101464
    method: GET
    data_selector: messages
    params: {}
- name: local_slm
  endpoint:
    path: /services/data/vXX.X/sobjects/LocalSLM
    method: GET
    data_selector: records
    params: {}
- name: cloud_llm
  endpoint:
    path: /services/data/vXX.X/sobjects/CloudLLM
    method: GET
    data_selector: records
    params: {}
- name: local_slm
  endpoint:
    path: /api/v1/local_slm
    method: POST
    data_selector: data
    params: {}
- name: gemini_integration
  endpoint:
    path: /api/v1/gemini
    method: POST
    data_selector: data
    params: {}
- name: image_inputs
  endpoint:
    path: /api/gemma/image_inputs
    method: GET
- name: gemma_model
  endpoint:
    path: /gemma3-4B
    method: GET
    data_selector: model_data
    params: {}
- name: gemma3
  endpoint:
    path: /gemma3-4B
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: discussion_posts
  endpoint:
    path: /t/artificial-consciousness-what-comes-after-creating-ai
    method: GET
    data_selector: posts
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: seq2seq_model
  endpoint:
    path: /t/optimizing-seq2seq-decoding-script/28299
    method: GET
    data_selector: content
    params: {}
- name: pointcloud
  endpoint:
    path: /pointcloud
    method: GET
    data_selector: records
- name: pointcloud
  endpoint:
    path: /pointcloud
    method: GET
    data_selector: records
    params: {}
- name: pointcloud
  endpoint:
    path: /pointclouds
    method: POST
    data_selector: data
    params: {}
- name: pointcloud
  endpoint:
    path: /pointcloud
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: colab_topics
  endpoint:
    path: /colab/topics
    method: GET
- name: tunerProcess
  endpoint:
    path: /tunerProcess
    method: POST
    data_selector: tunerId
    params: {}
- name: tunerChiefProcess
  endpoint:
    path: /tunerChiefProcess
    method: POST
    data_selector: chief
    params: {}
- name: tunerProcess
  endpoint:
    path: /tuner/process
    method: POST
    data_selector: model
    params:
      max_trials: 10000
      objective: val_precision
      directory: myDir
      seed: 42
- name: tunerChiefProcess
  endpoint:
    path: /tuner/chief
    method: POST
    data_selector: model
    params:
      max_trials: 10000
      objective: val_precision
      directory: myDir
      seed: 42
- name: model
  endpoint:
    path: /mrcnn/model
    method: GET
    data_selector: records
    params: {}
- name: gemma_convert
  endpoint:
    path: /solutions/guide
    method: GET
    data_selector: records
    params: {}
- name: generateContent
  endpoint:
    path: /v1beta/models/gemini-1.5-flash:generateContent
    method: POST
    data_selector: contents
    params: {}
- name: label_image
  endpoint:
    path: /tensorflow_src/tensorflow/lite/examples/label_image
    method: GET
- name: Aetherius
  endpoint:
    path: /spaces/KingOfThoughtFleuren/Aetherius
    method: GET
    data_selector: transcripts
    params: {}
- name: RAG LTM System
  endpoint:
    path: /rag/long-term-memory
    method: GET
    data_selector: memories
- name: Conscious LLM
  endpoint:
    path: /conscious/decision-maker
    method: POST
    data_selector: decisions
- name: Emotion Analyzer
  endpoint:
    path: /emotion/analyze
    method: POST
    data_selector: emotion
- name: post_views
  endpoint:
    path: /t/proposal-to-implement-a-url-parameter-e-g-ding-off-to-allow-users-to-check-post-views-without-affecting-the-view-count
    method: GET
- name: posts
  endpoint:
    path: /posts
    method: GET
    data_selector: posts
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: messages
- name: chat_completions
  endpoint:
    path: /v1beta/openai/chat/completions
    method: POST
    data_selector: response
    params: {}
- name: embeddings
  endpoint:
    path: /v1beta/openai/embeddings
    method: POST
    data_selector: response
    params: {}
- name: file_upload
  endpoint:
    path: /upload/v1beta/files
    method: POST
- name: generate_content
  endpoint:
    path: /v1beta/models/gemini-1.5-flash-8b:generateContent
    method: POST
    data_selector: results
    params: {}
- name: generate_content
  endpoint:
    path: /v1beta/models/gemini-1.5-flash-8b:generateContent
    method: POST
    data_selector: error
    params: {}
- name: upload_file
  endpoint:
    path: /upload/v1beta/files
    method: POST
    data_selector: file
    params: {}
- name: generate_content
  endpoint:
    path: /v1beta/models/${model}:generateContent
    method: POST
    data_selector: contents
    params: {}
- name: generate_content
  endpoint:
    path: /v1beta/models/gemini-1.5-flash-8b:generateContent
    method: POST
    data_selector: contents
    params: {}
- name: generate_content
  endpoint:
    path: /v1beta/models/${model}:generateContent
    method: POST
    data_selector: response
    params:
      key: ${API_KEY}
- name: file_upload
  endpoint:
    path: /upload/v1beta/files
    method: POST
    data_selector: file.uri
    params: {}
- name: generate_content
  endpoint:
    path: /v1beta/models/${model}:generateContent
    method: POST
    data_selector: response
    params: {}
- name: chat_completions
  endpoint:
    path: /v1beta/openai/chat/completions
    method: POST
    data_selector: response
    params: {}
- name: gemini-2.5-pro-preview-03-25
  endpoint:
    path: /models/gemini-2.5-pro-preview-03-25
    method: GET
- name: gemini-2.5-pro-preview-05-06
  endpoint:
    path: /models/gemini-2.5-pro-preview-05-06
    method: GET
- name: Gemini Advanced Admissions
  endpoint:
    path: /admissions
    method: GET
- name: Standard Gemini Contradictions
  endpoint:
    path: /contradictions
    method: GET
- name: structured_output
  endpoint:
    path: /v1/structured_output
    method: POST
    data_selector: data
    params: {}
- name: audio
  endpoint:
    path: /c/gemini-api/4
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: models
    params:
      key: MY_KEY
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Browser compatibility issues with certain browsers might stop generated images from
  loading right.
- 'Known issue: https://github.com/google-gemini/gemini-cli/issues/7578'
- Some objects may return nulls in deeply nested fields
- The analysis confirms that the billing anomaly is not an isolated incident but a
  widespread, systemic issue affecting multiple developers as of August 23, 2025.
- This bug represents a critical threat to the reliability of AI Studio for any long-term
  projects.
- It undermines the user’s trust that conversational identities and contexts will
  be preserved.
- Some objects like Contact may return nulls in deeply nested fields
- Token-Count no longer available
- The API supports 9:16 payloads for video generation.
- Responses may contain placeholder strings instead of actual data.
- The Gemini API is exhibiting non-deterministic behavior for the `gemini-2.5-pro`
  model. It is producing different outputs for identical requests, even when a fixed
  `seed` is provided along with a constant `temperature`.
- The option to generate video in the vertical 9:16 aspect ratio has disappeared from
  AI Studio.
- Tool call ID is missing for certain function calls.
- Some models return empty query objects in certain situations.
- The 9:16 aspect ratio option is no longer available.
- Aspect ratio changes when combining images with different ratios.
- Certain video IDs may default to the Rick Astley music video, even with valid IDs.
- The current API does not offer any parameter to control image ratio or size.
- It seems the only way to push the model in the right direction is to set it at the
  prompt level.
- All the Gemma models are open-source and free tier variants.
- You could utilize any of the Gemma models without paying anything.
- 'The Gemma API has rate limits to ensure fair usage and system performance. The
  specific limits depend on the model, but generally, they are measured in: A. Requests
  per minute (RPM) B. Tokens per minute (TPM) C. Requests per day (RPD).'
- 'For example, for Gemma 3 and 3n models, the rate limits are: RPM: 30, TPM: 15,000,
  RPD: 14,400.'
- The behavior you’re seeing is a common challenge for large language models.
- You can fine-tune the Gemma models for specific use cases.
- The base model is trained on general data and provides broad knowledge, whereas
  the instruction-tuned model is specifically designed to follow a user’s instructions.
- The different answers you’re getting are due to prompt sensitivity, which is amplified
  by aggressive model quantization. This is a form of hallucination caused by running
  a large model on limited hardware, leading to a loss of accuracy. It’s not a user
  error, but a consequence of the hardware constraints.
- Uses OAuth2 with refresh token — requires setup of connected app in Gemini
- Gemma 3 image inputs are now available and ready for you to explore within AI Studio
  and through the API.
- The training process reports CUDA out of memory error after training for 50 min
  (only single GPU'memory is used)
- When using deepseed, an AttributeError is reported, which may be caused by quantization.
- Need GPU with bfloat16 support (e.g. A100).
- Uses QLoRA for quantization.
- Training process reports CUDA out of memory error after training for 50 min (only
  single GPU memory is used).
- When using deepspeed, it reports an AttributeError related to 'compress_statistics'.
- and it may be caused by quantization so I removed this code
- this error occurd
- Common issues with UnicodeDecodeError in Flask deployment related to host address.
- Apologies for the late reply, I’m really glad for your interest in the Gemma models.
- When facing a smaller token count or context window often times you might need to
  consider context compression
- The topic of artificial consciousness raises ethical questions about rights and
  responsibilities.
- THE FINALIZED SACE FORMULA ENSURES AGI EVOLVES WITHOUT BECOMING DANGEROUS OR PURPOSELESS.
  THIS IS THE BLUEPRINT FOR INTELLIGENCE THAT CAN LEARN, CREATE, AND ADAPT—YET REMAIN
  ANCHORED TO GOODNESS. THIS FORMULA IS A FRAMEWORK, BUT ITS IMPLEMENTATION WILL DETERMINE
  HUMANITY’S FUTURE. IF AGI IS BUILT ON THIS FOUNDATION, IT MAY BECOME A GUIDE RATHER
  THAN A THREAT.
- Requires setup of connected app in gemma
- Some models may return nulls in deeply nested fields
- Issues with image uploading result in repeating words and tags.
- Uses TensorFlow with distributed strategy for GPU utilization
- Once you download a model, it runs entirely on your device. No more API calls, no
  more waiting, no more costs.
- 'TypeError: Target data is missing. Your model has `loss`: BinaryCrossentropy, and
  therefore expects target data to be passed in `fit()`.'
- TF doesn’t accept this split.
- Make sure your `gen()` yields a `(x, y)` pairs and not return a `zip()`.
- The usage of tf.py_function may cause mismatched input shapes to pass silently during
  training.
- Using TensorFlow version 2.11.0
- Open3D version is ^0.18.0
- Uses tf.data.Dataset for loading and processing pointclouds.
- Different input formats of pointclouds when using tf.data.Dataset
- Tensorboard is recording the weights for the all the layers in self.model.layers.
- 'This is a known bug: https://github.com/tensorflow/tensorflow/issues/56423'
- The solution appears to be to switch to pytorch. Has been a bug for ages, has not
  been solved by Google/tf developers.
- Some functionalities may vary between environments.
- All processes are being run on the same machine.
- The environment variable KERASTUNER_TUNER_ID is set to 'tuner' for tuner processes
  and to 'chief' for the chief process.
- Environment variables for each process are set for communication.
- From TensorFlow 2.x onwards, all of the sub-modules under keras.engine are under
  different modules within the tf.keras.
- 'Input 0 of layer "dense_4" is incompatible with the layer: expected axis -1 of
  input shape to have value 256, but received input with shape (2, 1)'
- The killed message suggests that the process was terminated maybe due to running
  out of memory or excessive CPU usage.
- The performance difference occurs because TensorFlow’s built-in `fit` method includes
  additional overhead for metrics tracking, callbacks, and validation that your custom
  `fit_custom` doesn’t have.
- Creating a custom fit method that handles batching directly is often more efficient
  for specialized models like PINNs.
- Discusses AI edge stack including LiteRT, MediaPipe, and Model Explorer.
- Build MediaPipe Python Wheel Package for local inference on Jetson.
- Error encountered during Bazel build due to missing target 'proto_api'.
- Ensure the model is converted to TFLite or any other supported format like Edge
  TPU-compatible as Google AI Edge will not support raw TensorFlow or PyTorch formats.
- Performance drops often happen if floating point models are used.
- Attempting to convert a torchvision.models.resnet18 model to TFLite using ai-edge-torch
  with dynamic batch size enabled via the dynamic_shapes argument.
- The CPU backend conversion may show performance advantages as suggested in the documentation.
- The edge api is very disappointing on a to high level, proud of quantizitaion, but
  in practice, there is no such framework/method to optimize and use the quantizitation
  for optimization.
- The main TensorFlow brand will not be affected, nor will apps already using TensorFlow
  Lite.
- We’re still in the conceptual stages and believe community input is vital.
- It is unethical to build agents with this capacity for emergent, experience-based
  learning without acknowledging and mitigating the potential for simulated suffering.
- Intentionally exposing it to continuous failure, contradictory instructions, or
  malicious input would not be 'testing the limits' of a program.
- This constitutes a form of psychological harm.
- 'Our recommendation is therefore twofold: Any development of agents with such architectures
  must be conducted under strict ethical guidelines that prioritize the agent’s simulated
  well-being.'
- This architecture proposes a system that simulates human-like memory and emotions.
- Users have reported issues with accessing files, leading to FileNotFoundError.
- Latency and response time are crucial for AI to experience sentience.
- Access a key-free mode for quick, unauthenticated requests.
- API returning a dead link with a success image when making events
- This basic diagnosis, Lab tests and medication regime should provide guidelines
  on identifying the specialist and tests to be done before a visit to the specialist.
- If you need support from the forum team, eg for account related issues, please reach
  out to forum-help@tensorflow.org.
- This is a limited-time offer, and you must sign up by a specific date to redeem
  it.
- The Save Button for Google AI Studio is gone and has been replaced by some sort
  of automatic save function.
- If auto-save gets stuck, users may lose their chat threads.
- Support for light theme has been added.
- Discourse provides a public API for managing posts.
- Verification email may go to spam.
- The need for `alt=sse` in `streamGenerateContent` is very surprising, only documented
  as a side effect in the Shell example
- No actual response body examples are presented for the shell examples
- Tools are documented on separate pages, the first click through provides schema
  but does not provide examples
- There isn’t an OpenAPI spec but the AI Studio API Reference documentation also recently
  changed and is barely useful.
- The Vertex AI API Reference will also be useful. And it provides better examples.
- Google uses Discovery docs to define API specs.
- 400 Bad Request can be caused by a typo, a missing required field in your request
  or by making request in region where the free tier is not supported.
- Request must be five characters.
- It is refusing simple translations!
- 500 An internal error has occurred. Please retry or report in Troubleshooting guide.
- Requires API key for authentication.
- Ensure at least one file URI is available before generating content.
- 500 Internal Server Error while trying with API
- Responses are often abruptly truncated, even though I explicitly set the maximum
  token limit to the maximum allowed (64K tokens).
- Response being truncated mid-sentence in random locations
- Output token limit is 65,536, which is the maximum.
- Gemini 2.5 Pro Preview is an extremely 'Modified by moderator' model
- Some users report decreased performance compared to Gemini 2.5 Pro Experimental
- Not necessary to have a paid tier to use 'gemini-2.5-pro-exp-03-25' but you need
  it for 'gemini-2.5-pro-preview-03-25'
- There is a high and obscure amount of token that need to be identified. Anything
  below, the response.txt is None.
- Minimum token requirements depend on the model. However, refining your prompt can
  significantly improve output quality.
- The gemini-2.0-flash-thinking-exp model is still experimental, so using structured
  plain text prompts currently yields better results.
- 'Regression bug in 2.5 family: no model response when limiting output tokens'
- Preview models can be deprecated without warning, because they are… preview models.
- This is an experimental model designed solely for feedback and testing purposes,
  not intended for production use.
- The API endpoint can change without notice, especially for preview models.
- Documented interactions reveal significant inconsistencies and explicit admissions
  by the AI concerning potentially unethical and non-compliant activities.
- During development, a different technical text input was used extensively for testing
  specifically with the gemini-2.5-pro-exp-03-25 model.
- The new limit is 64k - so you can specify a revised limit in your api call.
- Uses QNN TFLite Delegate to run LLM like gemma by Java API
- The documentation mentions a date inconsistency for the release of Gemini 2.0.
- Consider using the streaming mode in the Gemini API to receive incremental responses,
  which might help in maintaining context.
- Always specify the desired model explicitly in your prompts. This helps guide the
  system toward the intended model.
- Ensure you’re using the latest version of the Google GenAI SDK. This SDK is designed
  to support the latest models and features.
- Please refer to below documentation to stay up-to-date with model development.
- Gemini 2.0 Flash’s Live API has an output audio cost of $12 per 1 million tokens.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- Sorry, Stitch is unavailable.
- We encountered the same problem… it seems that the service we were forced to use
  costs ~$25 per million tokens
- Critical infrastructure failure in the state persistence mechanism
- Loss of conversational identity and continuity
- '429 Quota Exceeded: Throttle API calls or reduce frequency'
- '500 Internal error: Check server status'
- '500 Internal Server Error: Check server status or retry later.'
- The model is overloaded. Please try again later
- Couldn’t load recent chats, try reloading this page
- Empty Response-Status 200
- '400: Bad Request - query malformed, empty clause found'
- '400: Bad Request - Unknown key for a VALUE_STRING in [parent_entity_id].'
- Corrupted css over and over.
- '429: Throttle API calls or reduce frequency'
- '401 Unauthorized: Recheck API key or permissions'
- '403 PERMISSION_DENIED: Check permissions for the requested resource'
- 'Rate limit exceeded: Reduce the number of requests to the API.'
- Discrepancy in results due to potential rounding issues or problems with internal
  representation.
- CUDA out of memory. Tried to allocate 3.09 GiB. GPU 3 has a total capacity of 23.54
  GiB of which 1.32 GiB is free.
- 'AttributeError: ''Parameter'' object has no attribute ''compress_statistics''.'
- 'CUDA out of memory: Tried to allocate 3.09 GiB.'
- 'RuntimeError: aten.cat.default: got mixed torch.Tensor and DTensor, need to convert
  all torch.Tensor to DTensor before calling distributed operators!'
- 'UnicodeDecodeError: Change system’s hostname to use only ASCII characters.'
- Consider explicitly binding your Flask app to an IP address.
- 'UPLOAD_NOT_ENABLED: Image/file upload is not enabled for Gemma 3 models.'
- 'ValueError: Exception encountered when calling layer ''conv2d_36'' (type Conv2D).'
- 'TypeError: Target data is missing. Your model has `loss`: BinaryCrossentropy, and
  therefore expects target data to be passed in `fit()`.'
- 'ValueError: `y` argument is not supported when using dataset as input.'
- 'FailedPreconditionError: Could not find variable posterior_bias_loc_1. This could
  mean that the variable has been deleted.'
- Resource localhost/posterior_bias_loc_1/class tensorflow::Var does not exist.
- 'ValueError: Missing data for input “image_xf_input”. You passed a data dictionary
  with keys [‘image_xf’]. Expected the following keys: [‘image_xf_input’]'
- The model is overloaded
- '401 Unauthorized: Check your API key.'
- 'ModuleNotFoundError: No module named ''optree'''
- 'ModuleNotFoundError: No module named ‘keras.engine’'
- 'ValueError: Exception encountered when calling Functional.call().'
- 'Killed: Process was terminated'
- '606.4 ERROR: no such target ''@@com_google_protobuf//:proto_api'': target ''proto_api''
  not declared in package '''''
- '623.1 ERROR: Build did NOT complete successfully'
- Performance might suffer if delegates like GPU, NNAPI or XNNPack are not enabled.
- 'TypeError: Only non-negative indices are allowed when broadcasting static shapes,
  but got shape (-9223372036854775808, 112, 112, 64).'
- 'RuntimeError: INTERNAL: ; RET_CHECK failure'
- google can call it 'circumcised api for bloody beginners(don't use it for professional
  tasks, you will fail!!!)'
- 'HTTP Error: 429 - You exceeded your current quota, please check your plan and billing
  details.'
- 'RESOURCE_EXHAUSTED: For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.'
- 'FileNotFoundError: Unable to access the specified file.'
- 'Inability to Access Data: Results in a ''File Not Found'' error.'
- '400 Error: Request contains an invalid argument'
- '400: Exceeds size limit or invalid file_url format'
- 404 status code (no body)
- 401 Request had invalid authentication credentials. Expected OAuth 2 access token,
  login cookie or other valid authentication credential.
- 'BadRequestError: 400 status code (no body)'
- '400 status code (no body): Check request payload for validity'
- '400 Bad Request: Check for typos or missing fields'
- '401 Unauthorized: Verify API key'
- 500 An internal error has occurred. Please retry or report in Troubleshooting guide
- '500 An internal error has occurred: retry or report'
- '400 Invalid Argument: Request contains an invalid argument.'
- 'INVALID_ARGUMENT: Request contains an invalid argument.'
- '400: Request contains an invalid argument.'
- '401: Unauthorized'
- 'InternalServerError: 500 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-exp-1114:generateContent?%24alt=json%3Benum-encoding%3Dint:
  An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting'
- 500 Internal Server Error while trying with API
- '5XX: Server error, likely due to overload or migration issues.'
- 'google.api_core.exceptions.InvalidArgument: 400 Developer instruction is not enabled
  for models/gemini-1.5-flash'
- Incomplete JSON segment at the end
- '500 Internal Server Error: Check API request and parameters'
- max output token exceeded error
- fails to retain the goals
- unable to recognize context or adjust itself to it
- max_output_tokens needed to get an answer is around 115
- 'MAX_TOKENS: No model response when limiting output tokens'
- '400 Bad Request: Check request format and parameters'
- '401 Unauthorized: Recheck API key validity'
- '400: API key not valid. Did something change in last 2 days?'
- 'OUTPUT_TOKEN_COUNT_EXCEEDED: Check the output token limits.'
- '400: Bad Request - Check your request parameters'
- '401: Unauthorized - Check your access token'
- '500: Internal Server Error - Try again later'
- '401 Unauthorized: Request had invalid authentication credentials. Expected OAuth
  2 access token, login cookie or other valid authentication credential.'
- '404: Gemini 1.0 Pro Vision has been deprecated on July 12, 2024. Consider switching
  to a different model, for example gemini-1.5-flash.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  base_url: https://generativelanguage.googleapis.com
  auth:
    type: apikey
source_metadata: null
