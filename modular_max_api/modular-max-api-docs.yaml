client:
  auth: You need to provide an API key, which can be obtained from the OpenAI service,
    and it should be applied in the client initialization as `api_key="YOUR_API_KEY"`.
  most_recurring_base_url: http://0.0.0.0:8000
  paginator: Pagination information not found.
endpoints:
- 'endpoints source: https://docs.modular.com/max/api/serve/':
  - /health
  - /v1
- 'endpoints source: https://docs.modular.com/':
  - /v1
- 'endpoints source: https://docs.modular.com/max/intro':
  - /v1
- 'endpoints source: https://docs.modular.com/max/get-started':
  - http://localhost:8000/v1
- 'endpoints source: https://docs.modular.com/max/container':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/inference/text-to-text':
  - http://localhost:8000/v1/chat/completions`,
  - /v1`
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/inference/image-to-text':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/chat/completions
  - http://localhost:8000/v1/chat/completions`,
  - /v1`
- 'endpoints source: https://docs.modular.com/max/inference/embeddings':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/embeddings`,
  - http://localhost:8000/v1/embeddings
- 'endpoints source: https://docs.modular.com/max/serve/function-calling':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/serve/structured-output':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/serve/lora-adapters':
  - http://localhost:8000/v1/unload_lora_adapter
  - http://localhost:8000/v1/completions
  - http://localhost:8000/v1/load_lora_adapter
- 'endpoints source: https://docs.modular.com/max/serve/speculative-decoding':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/serve/offline-inference':
  - https://huggingface.co/settings/tokens
- 'endpoints source: https://docs.modular.com/max/deploy/benchmark': []
- 'endpoints source: https://docs.modular.com/max/get-started/':
  - http://localhost:8000/v1
- 'endpoints source: https://docs.modular.com/max/':
  - /v1
- 'endpoints source: https://docs.modular.com/max/tutorials/max-serve-local-to-cloud':
  - https://huggingface.co/settings/tokens
  - http://$PUBLIC_IP/v1/chat/completions
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/tutorials/serve-custom-model-architectures':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/api/serve/':
  - /health
  - /v1
- 'endpoints source: https://docs.modular.com/stable/max/intro':
  - /v1
- 'endpoints source: https://docs.modular.com/stable/max/get-started':
  - http://localhost:8000/v1
- 'endpoints source: https://docs.modular.com/stable/max/container':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/inference/text-to-text':
  - http://localhost:8000/v1/chat/completions`,
  - /v1`
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/inference/image-to-text':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/chat/completions
  - http://localhost:8000/v1/chat/completions`,
  - /v1`
- 'endpoints source: https://docs.modular.com/stable/max/inference/embeddings':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/embeddings`,
  - http://localhost:8000/v1/embeddings
- 'endpoints source: https://docs.modular.com/stable/max/serve/function-calling':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/serve/structured-output':
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/serve/lora-adapters':
  - http://localhost:8000/v1/unload_lora_adapter
  - http://localhost:8000/v1/completions
  - http://localhost:8000/v1/load_lora_adapter
- 'endpoints source: https://docs.modular.com/stable/max/serve/speculative-decoding':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/chat/completions
- 'endpoints source: https://docs.modular.com/stable/max/serve/offline-inference':
  - https://huggingface.co/settings/tokens
- 'endpoints source: https://docs.modular.com/stable':
  - /v1
- 'endpoints source: https://docs.modular.com/max/api/serve':
  - /health
  - /v1
- 'endpoints source: https://docs.modular.com/stable/max/api/serve':
  - /health
  - /v1
- 'endpoints source: https://docs.modular.com/stable/':
  - /v1
- 'endpoints source: https://docs.modular.com/max/tutorials/start-a-chat-endpoint':
  - http://localhost:8000/v1/chat/completions`,
  - /v1`
  - /v1
  - /v1/chat/completions
- 'endpoints source: https://docs.modular.com/max/tutorials/run-embeddings-with-max-serve':
  - http://localhost:8000/v1
  - http://localhost:8000/v1/embeddings`,
  - http://localhost:8000/v1/embeddings
