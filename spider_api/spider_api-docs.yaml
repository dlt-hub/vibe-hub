resources:
- name: crawl
  endpoint:
    path: /crawl
    method: POST
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: content
    params:
      limit: 5
      return_format: markdown
      url: https://spider.cloud
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: response
    params: {}
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: results
    params: {}
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: content
- name: links
  endpoint:
    path: /links
    method: POST
- name: links
  endpoint:
    path: /links
    method: POST
    data_selector: response
    params:
      limit: 5
      return_format: markdown
- name: screenshot
  endpoint:
    path: /screenshot
    method: POST
    data_selector: screenshots
    params: {}
- name: screenshot
  endpoint:
    path: /screenshot
    method: POST
    data_selector: content
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: content
- name: query
  endpoint:
    path: /data/query
    method: POST
    data_selector: content
- name: crawl_logs
  endpoint:
    path: /data/crawl_logs
    method: GET
    data_selector: data
- name: credits
  endpoint:
    path: /data/credits
    method: GET
    data_selector: data
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: result
- name: links
  endpoint:
    path: /links
    method: POST
    data_selector: links
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: results
- name: screenshot
  endpoint:
    path: /screenshot
    method: POST
    data_selector: image_url
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: result
- name: shutdown
  endpoint:
    path: /shutdown
    method: POST
    data_selector: message
- name: pipeline_label
  endpoint:
    path: /pipeline/label
    method: POST
    data_selector: message
- name: pipeline_extract_contacts
  endpoint:
    path: /pipeline/extract-contacts
    method: POST
    data_selector: contacts
- name: questions_and_answers
  endpoint:
    path: /pipeline/questions-and-answers
    method: POST
    data_selector: content
- name: websites_upsert
  endpoint:
    path: /data/websites
    method: POST
- name: websites_retrieve
  endpoint:
    path: /data/websites
    method: GET
    data_selector: data
- name: websites_delete
  endpoint:
    path: /data/websites
    method: DELETE
- name: pages_retrieve
  endpoint:
    path: /data/pages
    method: GET
    data_selector: data
- name: pages_delete
  endpoint:
    path: /data/pages
    method: DELETE
- name: pages_metadata_retrieve
  endpoint:
    path: /data/pages_metadata
    method: GET
    data_selector: data
- name: pages_metadata_delete
  endpoint:
    path: /data/pages_metadata
    method: DELETE
- name: contacts_retrieve
  endpoint:
    path: /data/contacts
    method: GET
    data_selector: data
- name: contacts_delete
  endpoint:
    path: /data/contacts
    method: DELETE
- name: api_keys_create
  endpoint:
    path: /data/api-keys
    method: POST
- name: api_keys_retrieve
  endpoint:
    path: /data/api-keys
    method: GET
    data_selector: data
- name: api_keys_delete
  endpoint:
    path: /data/api-keys
    method: DELETE
- name: stored_data_retrieve
  endpoint:
    path: /data/stored
    method: GET
    data_selector: data
- name: storage_retrieve
  endpoint:
    path: /data/storage
    method: GET
    data_selector: files
- name: download_file
  endpoint:
    path: /data/download
    method: GET
- name: chat_handle
  endpoint:
    path: /data/chat
    method: POST
- name: chat_completion
  endpoint:
    path: /data/chat-completion
    method: POST
- name: dataset_retrieve
  endpoint:
    path: /data/dataset
    method: GET
- name: websites
  endpoint:
    path: /websites
    method: GET
    data_selector: websites
    params: {}
- name: pages
  endpoint:
    path: /pages
    method: GET
    data_selector: pages
    params: {}
- name: pages_metadata
  endpoint:
    path: /pages_metadata
    method: GET
    data_selector: pages_metadata
    params: {}
- name: chat_queries
  endpoint:
    path: /chat_queries
    method: GET
    data_selector: chat_queries
    params: {}
- name: contacts
  endpoint:
    path: /contacts
    method: GET
    data_selector: contacts
    params: {}
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: result
    params:
      limit: 5
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: content
    params:
      request: smart
      return_format: markdown
      proxy_enabled: true
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: content
    params:
      limit: 30
      depth: 3
      request: smart
      return_format: markdown
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: content
    params:
      search_limit: 10
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: item
    params:
      url: https://www.example.com
      limit: 30
      depth: 3
      request: smart
      return_format: raw
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: json_data.other_scripts
    params:
      url: https://www.allrecipes.com/recipe/223312/nutella-hazelnut-cookies
      return_format: empty
      return_json_data: true
- name: scrape
  endpoint:
    path: /api/scrape
    method: POST
    data_selector: documents
    params: {}
- name: crawl
  endpoint:
    path: /api/crawl
    method: POST
    data_selector: documents
    params: {}
notes:
- To reduce latency, enhance performance, and save on rate limits batch multiple URLs
  into a single call.
- Lite mode reduces data transfer costs by 50%.
- Using metadata can help extract critical information to use for AI.
- Lite mode reduces data transfer costs by 50%, with trade-offs in speed, accuracy,
  geo-targeting, and reliability.
- The request type can be 'http', 'chrome', and 'smart'.
- Re-crawling can effectively use cache to keep costs low as new pages are found.
- Transform API costs 0.1 credits per transformation.
- Query API costs 0.1 credits per successful retrieval.
- Spider outputs HTML, raw, text, and various markdown formats.
- Compliance with robots.txt is default, but you can disable this if necessary.
- Complies with robots.txt by default
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- Collect data from any website and ask questions about the data.
- Every account can make up to 50,000 core API requests per second.
- Use spider-cloud-cli to authenticate with API key
- Uses API key for authentication
- Stream responses are available for faster processing
- Advanced search operators are supported.
- Spider will automatically retry failed requests.
- Default timeout for requests is set to 120 seconds.
- Create an API key, then store it as an environment variable.
- If no API key is provided it looks for SPIDER_API_KEY in the env.
- Create an API key and store it as an environment variable. If no API key is provided
  it looks for SPIDER_API_KEY in the env.
- The API has much more flexibility than the UI for performing advanced workloads
  like batching, formatting, and so on.
- The platform allows purchasing credits that gives you the ability to crawl at any
  time.
errors:
- '401 Unauthorized: Recheck API key or permissions.'
- 50,000 requests per second limit
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- '401 Unauthorized: Check API key'
- '400 Bad Request: Check parameters and request format'
- '401 Unauthorized: Invalid API key'
- '429 Too Many Requests: Rate limit exceeded'
- '401 Unauthorized: Recheck API key or token expiration'
- '404: Not Found - Resource does not exist.'
- '401: Unauthorized - Check your API key.'
- '404: Target page not found'
- '403: Forbidden'
- '401: Unauthorized'
- '400: Bad request'
- '429: Rate limit exceeded'
- '500: Server error'
- '503: Service unavailable'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
auth_info:
  mentioned_objects: []
client:
  base_url: https://api.spider.cloud
  auth:
    type: apikey
    location: header
    header_name: authorization
  headers:
    content-type: application/json
source_metadata: null
