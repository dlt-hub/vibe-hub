resources:
- name: invoices
  endpoint:
    path: /dashboard/billing
    method: GET
    data_selector: invoices
    params: {}
- name: purchase_history
  endpoint:
    path: /billing/history
    method: GET
    data_selector: purchase_history
    params: {}
- name: invoices
  endpoint:
    path: /dashboard/billing
    method: GET
    data_selector: invoices
    params: {}
- name: billing_history
  endpoint:
    path: /billing/history
    method: GET
    data_selector: billing_history
    params: {}
- name: SSH Key Management
  endpoint:
    path: /api-reference/ssh-keys
    method: GET
    data_selector: keys
    params: {}
- name: persistent_storage
  endpoint:
    path: /api/persistent_storage
    method: GET
    data_selector: volumes
    params: {}
- name: Pods characteristics
  endpoint:
    path: /compute-instances
    method: GET
    data_selector: Pods
    params: {}
- name: persistent_storage
  endpoint:
    path: /api/persistent-storage
    method: GET
    data_selector: volumes
- name: SSH Keys management
  endpoint:
    path: /api-reference/ssh-keys
    method: GET
- name: persistent_storage
  endpoint:
    path: /api/persistent_storage
    method: POST
    data_selector: volume
    params: {}
- name: Public model
  endpoint:
    path: /ai-inference-instances/inference-instance-configuration/select-your-flavor
    method: GET
    data_selector: models
    params: {}
- name: Private model
  endpoint:
    path: /ai-inference-instances/inference-instance-configuration/select-your-regions
    method: GET
    data_selector: models
    params: {}
- name: persistent_storage
  endpoint:
    path: /api/v1/persistent_storage
    method: GET
    data_selector: volumes
- name: GPU Flavors
  endpoint:
    path: /api/gpu-flavors
    method: GET
    data_selector: flavors
- name: inference_instance
  endpoint:
    path: /ai-inference-instances
    method: GET
    data_selector: instances
- name: text_generation
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: audio_transcription
  endpoint:
    path: /v1/audio/transcriptions
    method: POST
    data_selector: transcript
    params: {}
- name: inference_instance
  endpoint:
    path: /ai-inference-instances/edit-an-inference-instance
    method: GET
- name: available_offers
  endpoint:
    path: /gpu-cloud/instances/offers
    method: GET
    data_selector: object[]
- name: create_instance
  endpoint:
    path: /gpu-cloud/instances
    method: POST
    data_selector: object
- name: list_instances
  endpoint:
    path: /gpu-cloud/instances
    method: GET
    data_selector: object[]
- name: gpu_instance
  endpoint:
    path: /instances/{id}
    method: GET
    data_selector: object
    params: {}
- name: delete_gpu_instance
  endpoint:
    path: /instances/{id}
    method: DELETE
    data_selector: object
    params: {}
- name: chat_completion
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: audio_transcription
  endpoint:
    path: /v1/audio/transcriptions
    method: POST
    data_selector: transcription
    params: {}
- name: ssh_keys
  endpoint:
    path: /gpu-cloud/ssh-keys
    method: POST
    data_selector: response
    params: {}
- name: ssh_keys_list
  endpoint:
    path: /gpu-cloud/ssh-keys
    method: GET
    data_selector: response
    params: {}
- name: ssh_key_default
  endpoint:
    path: /gpu-cloud/ssh-keys/{id}/makedefault
    method: PATCH
    data_selector: response
    params: {}
- name: ssh_key_delete
  endpoint:
    path: /gpu-cloud/ssh-keys/{id}
    method: DELETE
    data_selector: response
    params: {}
- name: gpu_cloud_instances
  endpoint:
    path: /gpu-cloud/instances/offers
    method: GET
- name: offers
  endpoint:
    path: /gpu-cloud/instances/offers
    method: GET
- name: create_instance
  endpoint:
    path: /gpu-cloud/instances
    method: POST
- name: list_instances
  endpoint:
    path: /gpu-cloud/instances
    method: GET
- name: volume_offers
  endpoint:
    path: /gpu-cloud/volumes/offers
    method: GET
    data_selector: object[]
- name: create_volume
  endpoint:
    path: /gpu-cloud/volumes
    method: POST
    data_selector: object
- name: list_volumes
  endpoint:
    path: /gpu-cloud/volumes
    method: GET
    data_selector: object[]
- name: volume_details
  endpoint:
    path: /gpu-cloud/volumes/{id}
    method: GET
    data_selector: object
- name: delete_volume
  endpoint:
    path: /gpu-cloud/volumes/{id}
    method: DELETE
- name: gpu_cloud_instance
  endpoint:
    path: /gpu-cloud/instances/{id}
    method: GET
    data_selector: instance
    params: {}
- name: delete_gpu_cloud_instance
  endpoint:
    path: /gpu-cloud/instances/{id}
    method: DELETE
    data_selector: instance
    params: {}
- name: models
  endpoint:
    path: /ai-inference/models
    method: GET
    data_selector: object[]
- name: hardwares
  endpoint:
    path: /ai-inference/hardwares
    method: GET
    data_selector: object[]
- name: regions
  endpoint:
    path: /ai-inference/regions
    method: GET
    data_selector: object[]
- name: registries
  endpoint:
    path: /ai-inference/registries
    method: GET
    data_selector: object[]
- name: ssh_keys
  endpoint:
    path: /gpu-cloud/ssh-keys
    method: POST
    data_selector: created ssh key
    params: {}
- name: list_ssh_keys
  endpoint:
    path: /gpu-cloud/ssh-keys
    method: GET
    data_selector: list of user ssh keys
    params: {}
- name: mark_default_ssh_key
  endpoint:
    path: /gpu-cloud/ssh-keys/{id}/makedefault
    method: PATCH
    data_selector: SSH key marked as default
    params: {}
- name: delete_ssh_key
  endpoint:
    path: /gpu-cloud/ssh-keys/{id}
    method: DELETE
    data_selector: SSH key deleted successfully
    params: {}
- name: create_inference_instance
  endpoint:
    path: /instances
    method: POST
    data_selector: modelId
    params: {}
- name: start_inference_instance
  endpoint:
    path: /instances/{id}/start
    method: POST
    data_selector: ''
    params: {}
- name: get_inference_instances
  endpoint:
    path: /instances
    method: GET
    data_selector: ''
    params: {}
- name: get_inference_instance_details
  endpoint:
    path: /instances/{id}
    method: GET
    data_selector: ''
    params: {}
- name: update_inference_instance
  endpoint:
    path: /instances/{id}
    method: PATCH
    data_selector: ''
    params: {}
- name: stop_inference_instance
  endpoint:
    path: /instances/{id}/stop
    method: POST
    data_selector: ''
    params: {}
- name: volume_offers
  endpoint:
    path: /gpu-cloud/volumes/offers
    method: GET
    data_selector: object[]
- name: create_volume
  endpoint:
    path: /gpu-cloud/volumes
    method: POST
    data_selector: object
- name: list_volumes
  endpoint:
    path: /gpu-cloud/volumes
    method: GET
    data_selector: object[]
- name: volume_details
  endpoint:
    path: /gpu-cloud/volumes/{id}
    method: GET
    data_selector: object
- name: delete_volume
  endpoint:
    path: /gpu-cloud/volumes/{id}
    method: DELETE
    data_selector: no content
- name: large_language_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-1.-large-language-models-llms-models-training
    method: GET
    data_selector: models
    params: {}
- name: computer_vision_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-2.-computer-vision-models-training
    method: GET
    data_selector: models
    params: {}
- name: audio_speech_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-3.-audio-speech-models-training
    method: GET
    data_selector: models
    params: {}
- name: inference_models
  endpoint:
    path: /ai-inference/models
    method: GET
    data_selector: object[]
- name: inference_hardwares
  endpoint:
    path: /ai-inference/hardwares
    method: GET
    data_selector: object[]
- name: inference_regions
  endpoint:
    path: /ai-inference/regions
    method: GET
    data_selector: object[]
- name: registries
  endpoint:
    path: /ai-inference/registries
    method: GET
    data_selector: object[]
- name: model
  endpoint:
    path: /v1/models
    method: GET
    data_selector: ''
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: ''
    params: {}
- name: inference_instance
  endpoint:
    path: /instances
    method: POST
    data_selector: instance
    params: {}
- name: start_inference_instance
  endpoint:
    path: /instances/{id}/start
    method: POST
    data_selector: instance
    params: {}
- name: get_inference_instances
  endpoint:
    path: /instances
    method: GET
    data_selector: instances
    params: {}
- name: get_inference_instance_details
  endpoint:
    path: /instances/{id}
    method: GET
    data_selector: instance
    params: {}
- name: preview_pricing
  endpoint:
    path: /instances/pricing
    method: POST
    data_selector: pricing
    params: {}
- name: update_inference_instance
  endpoint:
    path: /instances/{id}
    method: PATCH
    data_selector: instance
    params: {}
- name: stop_inference_instance
  endpoint:
    path: /instances/{id}/stop
    method: POST
    data_selector: instance
    params: {}
- name: large_language_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-1.-large-language-models-llms-models-training
    method: GET
    data_selector: models
    params: {}
- name: computer_vision_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-2.-computer-vision-models-training
    method: GET
    data_selector: models
    params: {}
- name: audio_speech_models_training
  endpoint:
    path: /tutorials/which-compute-instance-for-ai-models-training-and-inference#id-3.-audio-speech-models-training
    method: GET
    data_selector: models
    params: {}
notes:
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- You can modify your account information at any time in the Settings tab on the platform.
- When you want to launch an instance or create a volume, you can select the team
  associated to this deployment in Checkout section.
- You can modify your account information at any time in the 'Settings' tab on the
  platform.
- Owners can create and share API Keys
- Your credit balance is debited in real time, every minute, according to your instances
  and volumes utilization.
- Sesterce will notify you by email if your credit balance becomes too low.
- Currently, Sesterce Cloud offers 'On-Demand' pricing.
- You will receive an email notification once your balance is credited.
- Currently, it is not possible to add a SSH Key to an instance already live.
- Make sure to add all keys you need before launching your compute instance.
- The Storage of your Pod is non persistent.
- Volumes should be hosted in the same Cloud and region as Pod selected.
- Requires setup of connected app in Sesterce Cloud
- Volume must be linked to Compute instances in the same Regions and Availability
  Zone.
- Processes data locally at the network's edge, minimizing latency and bandwidth usage
  for real-time applications.
- Directs user requests to the nearest instance of a service, optimizing performance
  and reducing response time.
- End users' queries are routed to the closest active model, ensuring low latency
  and an improved user experience.
- The cooldown period corresponds to intervals (in seconds) between trigger executions.
  You can select a value between 1 and 3600 seconds.
- You can configure up to 3 threshold limits corresponding to each of the 3 triggers
  types.
- GPU Flavor is ideal for inference of complex deep learning models, image processing,
  or multimedia content generation.
- CPU Flavor is more dedicated to light tasks as simple text processing algorithms
  or structured data processing, that do not require a very low latency time.
- Define minimum and maximum number of pods to be mobilized.
- Cooldown period corresponds to intervals (in seconds) between trigger executions,
  can select a value between 1 and 3600 seconds.
- 'Available trigger types: GPU memory utilization, Memory utilization, RAM usage,
  HTTP requests.'
- Minimum setting is 1% of the resource capacity, maximum setting is 100% of the resource
  capacity.
- Copy your API key and store it in a secured place. You'll not be able to see it
  again then, to avoid any security issue.
- The API Key secret should be sent through the header to authenticate the request.
- Deploying image generation models like Stable Diffusion for production introduces
  unique infrastructure challenges compared to traditional ML workload.
- Make sure you choose an instance with sufficient RAM to run your model.
- Wait before container is running to access the model.
errors:
- Not enough credits for deploying the resource.
- '401: Invalid API Secret'
- '404: Model not found'
- '429: Too many requests'
- '500: Server error'
- 403 API key invalid
- 404 Not found
- '403: API key invalid'
- '404: Not found'
- '404: API key not found'
- '400: You do not have enough credits to run this inference instance for at least
  one hour.'
- '404: Model image not found.'
- '404: Inference instance not found.'
- 'Instance RAM insufficient: Make sure you choose an instance with sufficient RAM
  to run your model.'
- 'Container not running yet: Wait before container is running to access the model.'
- 'Port used or unavailable: You can check port status with following command.'
auth_info:
  mentioned_objects: []
client:
  base_url: https://api.sesterce.com
  auth:
    type: oauth2
    flow: refresh_token
    token_url: https://auth.sesterce.com/oauth2/token
    location: header
    header_name: Authorization
  headers:
    Accept: application/json
source_metadata: null
