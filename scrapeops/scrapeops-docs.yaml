auth_info:
- information_source; https://scrapeops.io/docs/data-api/amazon-product-api/:
  - Authentication uses an API key passed as a query parameter. Include the `api_key`
    query parameter with your API key in every request, otherwise the API returns
    a 403 Forbidden status code. The API key is obtained by signing up for a free
    account.
- information_source; https://scrapeops.io/docs/servers-scheduling/rest-api/:
  - 'All requests to the ScrapeOps API require an API key sent via the X-API-KEY header.
    The API key is obtained by signing up for a free account and must be included
    with every request or a 403 Forbidden status will be returned.


    X-API-KEY header with your API key value, for example X-API-KEY: your_api_key_here'
- information_source; https://scrapeops.io/docs/web-scraping-proxy-api-aggregator/getting-started/api-basics/:
  - Your API key must be included with every request using the api_key query parameter.
    The endpoint is https://proxy.scrapeops.io/v1/ and requires both api_key and url
    as query parameters. If the API key is not provided, the API returns a 403 Forbidden
    Access status code.
- information_source; https://scrapeops.io/docs/web-scraping-proxy-api-aggregator/quickstart/:
  - The ScrapeOps proxy uses API key authentication. Include your API key with every
    request via the `api_key` query parameter. Requests without a valid API key will
    return a 403 Forbidden status. The API key is obtained by creating a free account
    on the ScrapeOps platform.
- information_source; https://scrapeops.io/proxy-providers/scrapfly/python-scrapfly-proxy-api-guide:
  - The Scrapfly API requires an API key for authentication. The key is passed as
    a query parameter named "key" in the payload that gets URL-encoded and appended
    to requests made to the https://api.scrapfly.io/scrape endpoint. When using the
    SDK, initialize the ScrapflyClient with the key parameter set to your API key
    string.
possible_base_urls:
- information_source; https://scrapeops.io/docs/data-api/amazon-product-api/:
  - https://proxy.scrapeops.io/v1/structured-data/amazon/product
- information_source; https://scrapeops.io/docs/servers-scheduling/rest-api/:
  - https://backend.scrapeops.io/v1/client
possible_endpoints:
- information_source; https://scrapeops.io/docs/data-api/amazon-product-api/:
  - GET https://proxy.scrapeops.io/v1/structured-data/amazon/product
- information_source; https://scrapeops.io/docs/web-scraping-proxy-api-aggregator/getting-started/api-basics/:
  - GET https://proxy.scrapeops.io/v1/
  - POST https://proxy.scrapeops.io/v1/
- information_source; https://scrapeops.io/docs/web-scraping-proxy-api-aggregator/quickstart/:
  - GET https://proxy.scrapeops.io/v1/
  - POST https://proxy.scrapeops.io/v1/
- information_source; https://scrapeops.io/proxy-providers/scrapfly/python-scrapfly-proxy-api-guide:
  - GET https://api.scrapfly.io/scrape
  - GET https://api.scrapfly.io/scrape?
- information_source; https://scrapeops.io/proxy-providers/zyte/python-zyte-proxy-api-guide:
  - GET https://proxy.scrapeops.io/v1/?
  - POST https://api.zyte.com/v1/extract
- information_source; https://scrapeops.io/python-web-scraping-playbook/python-requests-post-requests/:
  - POST https://example.com/api
