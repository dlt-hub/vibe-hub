resources:
- name: data_loading
  endpoint:
    path: /en/guides-overview-loading-data
    method: GET
- name: snowpipe
  endpoint:
    path: /en/user-guide/data-load-snowpipe-intro
    method: GET
- name: sample_product_data
  endpoint:
    path: /services/data/vXX.X/sobjects/sample_product_data
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /v1/databases
    method: GET
    data_selector: databases
    params: {}
- name: tables
  endpoint:
    path: /v1/tables
    method: GET
    data_selector: tables
    params: {}
- name: SNOWFLAKE_SAMPLE_DATA
  endpoint:
    path: /SNOWFLAKE_SAMPLE_DATA
    method: GET
- name: SNOWFLAKE_SAMPLE_DATA
  endpoint:
    path: /shared/SNOWFLAKE_SAMPLE_DATA
    method: GET
    data_selector: tables
- name: cloud_regions
  endpoint:
    path: /cloud-regions
    method: GET
    data_selector: regions
    params: {}
- name: editions
  endpoint:
    path: /en/user-guide/intro-editions
    method: GET
    data_selector: editions
    params: {}
- name: Snowflake Marketplace
  endpoint:
    path: /services/data/vXX.X/sobjects/SnowflakeMarketplace
    method: GET
    data_selector: records
    params: {}
- name: Universal Search
  endpoint:
    path: /services/data/vXX.X/sobjects/UniversalSearch
    method: GET
    data_selector: records
    params: {}
- name: Data Clean Rooms
  endpoint:
    path: /services/data/vXX.X/sobjects/DataCleanRooms
    method: GET
    data_selector: records
    params: {}
- name: Artificial Intelligence and Machine Learning
  endpoint:
    path: /services/data/vXX.X/sobjects/AIandMLFeatures
    method: GET
    data_selector: records
    params: {}
- name: Customer Support
  endpoint:
    path: /services/data/vXX.X/sobjects/CustomerSupport
    method: GET
    data_selector: records
    params: {}
- name: database
  endpoint:
    path: /sql-reference/sql/create-database
    method: POST
- name: schema
  endpoint:
    path: /sql-reference/sql/create-schema
    method: POST
- name: table
  endpoint:
    path: /sql-reference/sql/create-table
    method: POST
- name: regulatory_compliance
  endpoint:
    path: /en/user-guide/intro-compliance
    method: GET
    data_selector: certifications
    params: {}
- name: databases
  endpoint:
    path: /schemas
    method: GET
- name: schemas
  endpoint:
    path: /tables
    method: GET
- name: tables
  endpoint:
    path: /data
    method: GET
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
- name: create_user
  endpoint:
    path: /sql-reference/sql/create-user
    method: POST
    data_selector: User created
    params: {}
- name: grant_role
  endpoint:
    path: /sql-reference/sql/grant-role
    method: POST
    data_selector: Role granted
    params: {}
- name: show_users
  endpoint:
    path: /sql-reference/sql/show-users
    method: GET
    data_selector: Users list
    params: {}
- name: show_roles
  endpoint:
    path: /sql-reference/sql/show-roles
    method: GET
    data_selector: Roles list
    params: {}
- name: tasty_bytes_sample_data
  endpoint:
    path: /services/data/vXX.X/tasty_bytes_sample_data
    method: CREATE
    data_selector: records
    params: {}
- name: global_certifications
  endpoint:
    path: /certifications/global
    method: GET
    data_selector: certifications
    params: {}
- name: us_government_certifications
  endpoint:
    path: /certifications/us-government
    method: GET
    data_selector: certifications
    params: {}
- name: healthcare_certifications
  endpoint:
    path: /certifications/healthcare
    method: GET
    data_selector: certifications
    params: {}
- name: financial_services_certifications
  endpoint:
    path: /certifications/financial-services
    method: GET
    data_selector: certifications
    params: {}
- name: australia_certifications
  endpoint:
    path: /certifications/australia
    method: GET
    data_selector: certifications
    params: {}
- name: germany_certifications
  endpoint:
    path: /certifications/germany
    method: GET
    data_selector: certifications
    params: {}
- name: korea_certifications
  endpoint:
    path: /certifications/korea
    method: GET
    data_selector: certifications
    params: {}
- name: uk_certifications
  endpoint:
    path: /certifications/uk
    method: GET
    data_selector: certifications
    params: {}
- name: tasty_bytes_sample_data
  endpoint:
    path: /services/data/vXX.X/sobjects/tasty_bytes_sample_data
    method: GET
    data_selector: records
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params: {}
- name: emp_basic
  endpoint:
    path: /emp_basic
    method: GET
    data_selector: records
    params: {}
- name: cloud_data_db.s3_data.calendar
  endpoint:
    path: /services/data/vXX.X/sobjects/Calendar
    method: COPY INTO
    data_selector: records
    params: {}
- name: user
  endpoint:
    path: /sql/reference/sql/create-user
    method: POST
    data_selector: records
- name: role
  endpoint:
    path: /sql/reference/sql/grant-role
    method: POST
    data_selector: records
- name: cloud_data_db.azure_data.calendar
  endpoint:
    path: /cloud_data_db.azure_data/calendar
    method: COPY
    data_selector: rows
    params: {}
- name: tasty_bytes_sample_data
  endpoint:
    path: /services/data/vXX.X/sobjects/tasty_bytes_sample_data
    method: CREATE
    data_selector: records
    params: {}
- name: cloud_data_db
  endpoint:
    path: /services/data/vXX.X/sobjects/cloud_data_db
    method: CREATE
    data_selector: records
    params: {}
- name: menu
  endpoint:
    path: /services/data/vXX.X/sobjects/menu
    method: GET
    data_selector: records
    params: {}
- name: mycsvtable
  endpoint:
    path: /services/data/vXX.X/sobjects/mycsvtable
    method: COPY INTO
    data_selector: LOADED
    params: {}
- name: myjsontable
  endpoint:
    path: /services/data/vXX.X/sobjects/myjsontable
    method: COPY INTO
    data_selector: LOADED
    params: {}
- name: cloud_data_db.s3_data.calendar
  endpoint:
    path: /services/data/vXX.X/sobjects/calendar
    method: COPY
    data_selector: records
    params: {}
- name: save_copy_errors
  endpoint:
    path: /services/data/vXX.X/sql
    method: POST
    data_selector: RESULT
    params:
      incremental: query_id
- name: cloud_data_db
  endpoint:
    path: /services/data/load
    method: POST
    data_selector: load
- name: mycsvtable
  endpoint:
    path: /tutorials/dataloading/contacts1.csv
    method: COPY INTO
    data_selector: LOADED
    params:
      ON_ERROR: skip_file
- name: myjsontable
  endpoint:
    path: /tutorials/dataloading/contacts.json
    method: COPY INTO
    data_selector: LOADED
    params:
      ON_ERROR: skip_file
- name: gcs_data_integration
  endpoint:
    path: /services/data/vXX.X/sobjects/GCSIntegration
    method: CREATE
    data_selector: integration
    params:
      STORAGE_PROVIDER: GCS
      ENABLED: true
      STORAGE_ALLOWED_LOCATIONS:
      - gcs://tutorial24bucket/gcsdata/
- name: gcsdata_stage
  endpoint:
    path: /services/data/vXX.X/sobjects/Stage
    method: CREATE
    data_selector: stage
    params:
      STORAGE_INTEGRATION: gcs_data_integration
      URL: gcs://tutorial24bucket/gcsdata/
      FILE_FORMAT:
        TYPE: CSV
- name: raw_source
  endpoint:
    path: /services/data/vXX.X/sobjects/RAW_SOURCE
    method: COPY
    data_selector: SRC
    params: {}
- name: flattened_source
  endpoint:
    path: /services/data/vXX.X/sobjects/FLATTENED_SOURCE
    method: CREATE
    data_selector: SRC
    params: {}
- name: home_sales
  endpoint:
    path: /services/data/vXX.X/sobjects/home_sales
    method: COPY
    data_selector: records
    params: {}
- name: mycsvtable
  endpoint:
    path: /create-or-replace-temp-table
    method: POST
    data_selector: records
    params: {}
- name: myjsontable
  endpoint:
    path: /create-or-replace-temp-table
    method: POST
    data_selector: records
    params: {}
- name: cities
  endpoint:
    path: /services/data/vXX.X/sobjects/Cities
    method: COPY INTO
    data_selector: records
    params: {}
- name: lineitem
  endpoint:
    path: /tpch_sf1/lineitem
    method: SELECT
    data_selector: COUNT(*)
    params: {}
- name: TPCDS_SF10TCL
  endpoint:
    path: /snowflake_sample_data/TPCDS_SF10TCL
    method: GET
- name: TPCDS_SF100TCL
  endpoint:
    path: /snowflake_sample_data/TPCDS_SF100TCL
    method: GET
- name: save_copy_errors
  endpoint:
    path: /TABLE(VALIDATE(mycsvtable, JOB_ID=>'query_id'))
    method: CREATE
    data_selector: '*'
- name: mycsvtable
  endpoint:
    path: /mycsvtable
    method: SELECT
    data_selector: '*'
- name: myjsontable
  endpoint:
    path: /myjsontable
    method: SELECT
    data_selector: '*'
- name: lineitem
  endpoint:
    path: /snowflake_sample_data.tpch_sf1
    method: GET
    data_selector: records
    params:
      incremental: l_shipdate
- name: mycsvtable
  endpoint:
    path: /path/to/mycsvtable
    method: COPY
    data_selector: records
    params:
      ON_ERROR: skip_file
- name: myjsontable
  endpoint:
    path: /path/to/myjsontable
    method: COPY
    data_selector: records
    params:
      ON_ERROR: skip_file
- name: mycsvtable
  endpoint:
    path: /mycsvtable
    method: SELECT
    data_selector: results
- name: myjsontable
  endpoint:
    path: /myjsontable
    method: SELECT
    data_selector: results
- name: raw_source
  endpoint:
    path: /services/data/vXX.X/sobjects/raw_source
    method: COPY INTO
    data_selector: SRC
    params: {}
- name: flattened_source
  endpoint:
    path: /services/data/vXX.X/sobjects/flattened_source
    method: CREATE TABLE AS SELECT
    data_selector: SRC
    params: {}
- name: events
  endpoint:
    path: /services/data/vXX.X/sobjects/events
    method: GET
    data_selector: records
    params: {}
- name: home_sales
  endpoint:
    path: /services/data/vXX.X/sobjects/home_sales
    method: COPY
    data_selector: $1
    params:
      incremental: sale_date
- name: cities
  endpoint:
    path: /services/data/vXX.X/sobjects/Cities
    method: COPY
    data_selector: records
    params: {}
- name: users
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: users
    params: {}
- name: warehouses
  endpoint:
    path: /api/v1/warehouses
    method: GET
    data_selector: warehouses
    params: {}
- name: lineitem
  endpoint:
    path: /snowflake_sample_data/tpch_sf1/lineitem
    method: SELECT
    data_selector: records
    params: {}
- name: TPCDS_SF10TCL
  endpoint:
    path: /snowflake_sample_data/tpcds_sf10Tcl
    method: GET
- name: TPCDS_SF100TCL
  endpoint:
    path: /snowflake_sample_data/tpcds_sf100Tcl
    method: GET
- name: lineitem
  endpoint:
    path: /services/data/snowflake_sample_data/tpch_sf1/lineitem
    method: GET
    data_selector: records
    params:
      schema: snowflake_sample_data.tpch_sf1
- name: SNOWFLAKE_SAMPLE_DATA
  endpoint:
    path: /de/user-guide/sample-data-using
    method: GET
    data_selector: records
- name: TPC-DS
  endpoint:
    path: /de/user-guide/sample-data-tpcds
    method: GET
    data_selector: records
- name: TPC-H
  endpoint:
    path: /de/user-guide/sample-data-tpch
    method: GET
    data_selector: records
- name: SnowSQL
  endpoint:
    path: https://sfc-repo.azure.snowflakecomputing.com/index.html
    method: GET
- name: ODBC Driver
  endpoint:
    path: https://developers.snowflake.com/odbc/
    method: GET
- name: Snowpark API
  endpoint:
    path: https://developers.snowflake.com/snowpark/
    method: GET
- name: Drivers
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: Scala and Java connectors
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: SnowCD
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: Snowpark ML
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: worksheets
  endpoint:
    path: /api/worksheets
    method: GET
    data_selector: worksheets
- name: notebooks
  endpoint:
    path: /api/notebooks
    method: GET
    data_selector: notebooks
- name: dashboards
  endpoint:
    path: /api/dashboards
    method: GET
    data_selector: dashboards
- name: SQL Statements
  endpoint:
    path: /sql-reference/sql
    method: GET
    data_selector: statements
    params: {}
- name: data_loading
  endpoint:
    path: /api/v1/data/loading
    method: POST
    data_selector: records
    params: {}
- name: data_unloading
  endpoint:
    path: /api/v1/data/unloading
    method: POST
    data_selector: records
    params: {}
- name: virtual_warehouse
  endpoint:
    path: /user-guide/warehouses
    method: GET
    data_selector: warehouses
    params: {}
- name: databases
  endpoint:
    path: /databases
    method: GET
    data_selector: databases
- name: tables
  endpoint:
    path: /tables
    method: GET
    data_selector: tables
- name: views
  endpoint:
    path: /views
    method: GET
    data_selector: views
- name: Snowflake CLI
  endpoint:
    path: https://sfc-repo.snowflakecomputing.com/snowflake-cli/index.html
    method: GET
- name: ODBC Driver
  endpoint:
    path: https://developers.snowflake.com/odbc/
    method: GET
- name: Snowpark API
  endpoint:
    path: https://developers.snowflake.com/snowpark/
    method: GET
- name: Drivers
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: Scala and Java connectors
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: SnowCD
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: Snowpark ML
  endpoint:
    path: https://developers.snowflake.com/drivers-and-libraries/
    method: GET
- name: query_history
  endpoint:
    path: /monitoring/query_history
    method: GET
    data_selector: query_history
    params: {}
- name: SQL Statements Supported for Preparation
  endpoint:
    path: /sql-reference/sql/prepare
    method: GET
    data_selector: records
- name: SnowConvert AI
  endpoint:
    path: /en/migrations/snowconvert-docs/overview
    method: GET
    data_selector: null
    params: {}
- name: Snowpark Migration Accelerator
  endpoint:
    path: /en/migrations/sma-docs/README
    method: GET
    data_selector: null
    params: {}
- name: listings
  endpoint:
    path: /en/user-guide/collaboration/listings
    method: GET
    data_selector: records
    params: {}
- name: Iceberg Tables
  endpoint:
    path: /user-guide/tables-iceberg
    method: GET
    data_selector: tables
    params: {}
- name: External Volume
  endpoint:
    path: /user-guide/tables-iceberg/external-volume
    method: GET
    data_selector: external_volume
    params: {}
- name: Catalog Integration
  endpoint:
    path: /user-guide/tables-iceberg/catalog-integration
    method: GET
    data_selector: catalog_integration
    params: {}
- name: data_sharing
  endpoint:
    path: /user-guide/data-sharing
    method: GET
    data_selector: data
    params: {}
- name: data_clean_rooms
  endpoint:
    path: /user-guide/cleanrooms/introduction
    method: GET
    data_selector: data
    params: {}
- name: Snowflake Cortex
  endpoint:
    path: /user-guide/snowflake-cortex
    method: GET
    data_selector: features
    params: {}
- name: Snowflake ML
  endpoint:
    path: /developer-guide/snowflake-ml
    method: GET
    data_selector: features
    params: {}
- name: dynamic_tables
  endpoint:
    path: /user-guide/dynamic-tables-about
    method: GET
    data_selector: data
    params: {}
- name: organizations
  endpoint:
    path: /en/guides-overview-manage
    method: GET
- name: accounts
  endpoint:
    path: /en/user-guide/organizations
    method: GET
- name: unload_data
  endpoint:
    path: /user-guide/data-unload-overview
    method: GET
- name: replication_group
  endpoint:
    path: /account/replication
    method: POST
    data_selector: replicationGroup
    params: {}
- name: failover_group
  endpoint:
    path: /account/failover
    method: POST
    data_selector: failoverGroup
    params: {}
- name: storage_lifecycle_policy
  endpoint:
    path: /storage-lifecycle-policies
    method: GET
    data_selector: policies
    params: {}
- name: SnowConvert AI
  endpoint:
    path: /en/migrations/snowconvert-docs/overview
    method: GET
    data_selector: records
    params: {}
- name: Snowpark Migration Accelerator
  endpoint:
    path: /en/migrations/sma-docs/README
    method: GET
    data_selector: records
    params: {}
- name: Google Analytics Aggregate Data
  endpoint:
    path: /connectors/google/gaad/gaad-connector-about
    method: GET
- name: Google Analytics Raw Data
  endpoint:
    path: /connectors/google/gard/gard-connector-about
    method: GET
- name: Google Looker Studio
  endpoint:
    path: /connectors/google-looker-studio-connector
    method: GET
- name: ServiceNow
  endpoint:
    path: /connectors/servicenow/about
    method: GET
- name: MySQL
  endpoint:
    path: /connectors/mysql6/about
    method: GET
- name: PostgreSQL
  endpoint:
    path: /connectors/postgres6/about
    method: GET
- name: SharePoint
  endpoint:
    path: /connectors/unstructured-data-connectors/sharepoint/about
    method: GET
- name: data_sharing
  endpoint:
    path: /data/sharing
    method: GET
    data_selector: records
- name: Snowflake CLI
  endpoint:
    path: /developer-guide/snowflake-cli/index
    method: GET
- name: SnowSQL
  endpoint:
    path: /user-guide/snowsql
    method: GET
- name: Snowflake Extension for Visual Studio Code
  endpoint:
    path: /user-guide/vscode-ext
    method: GET
- name: Snowflake Terraform Provider
  endpoint:
    path: /user-guide/terraform
    method: GET
- name: Snowflake API Reference
  endpoint:
    path: /api-reference
    method: GET
- name: Snowflake Connectors
  endpoint:
    path: https://other-docs.snowflake.com/connectors.html
    method: GET
- name: Snowflake Ecosystem
  endpoint:
    path: /user-guide/ecosystem
    method: GET
- name: Snowflake Connector
  endpoint:
    path: /connectors.html
    method: GET
- name: organization
  endpoint:
    path: /organizations
    method: GET
    data_selector: accounts
- name: Google Analytics Aggregate Data
  endpoint:
    path: /connectors/google/gaad/gaad-connector-about
    method: GET
- name: Google Analytics Raw Data
  endpoint:
    path: /connectors/google/gard/gard-connector-about
    method: GET
- name: Google Looker Studio
  endpoint:
    path: /connectors/google-looker-studio-connector
    method: GET
- name: ServiceNow
  endpoint:
    path: /connectors/servicenow/about
    method: GET
- name: MySQL
  endpoint:
    path: /connectors/mysql6/about
    method: GET
- name: PostgreSQL
  endpoint:
    path: /connectors/postgres6/about
    method: GET
- name: SharePoint
  endpoint:
    path: /connectors/unstructured-data-connectors/sharepoint/about
    method: GET
- name: organization_account
  endpoint:
    path: /organizations
    method: GET
    data_selector: accounts
    params: {}
- name: regular_account
  endpoint:
    path: /accounts
    method: GET
    data_selector: accounts
    params: {}
- name: snowflake_open_catalog_account
  endpoint:
    path: /open_catalog/accounts
    method: GET
    data_selector: accounts
    params: {}
- name: organization_usage
  endpoint:
    path: /services/data/vXX.X/sobjects/OrganizationUsage
    method: GET
    data_selector: records
- name: premium_views
  endpoint:
    path: /services/data/vXX.X/sobjects/PremiumViews
    method: GET
    data_selector: records
- name: organization_usage
  endpoint:
    path: /services/data/organization_usage
    method: GET
    data_selector: records
- name: organization_user
  endpoint:
    path: /api/organization_users
    method: GET
    data_selector: users
- name: organization_user_group
  endpoint:
    path: /api/organization_user_groups
    method: GET
    data_selector: user_groups
- name: organization_user
  endpoint:
    path: /create-organization-user
    method: POST
    data_selector: user
    params: {}
- name: organization_user_group
  endpoint:
    path: /create-organization-user-group
    method: POST
    data_selector: user_group
    params: {}
- name: accounts
  endpoint:
    path: /organizations/accounts
    method: GET
    data_selector: accounts
    params: {}
- name: user
  endpoint:
    path: /v1/users
    method: GET
    data_selector: users
- name: user
  endpoint:
    path: /developer-guide/snowflake-python-api/reference/latest/_autosummary/snowflake.core.user.UserResource#snowflake.core.user.UserResource.fetch
    method: GET
    data_selector: records
- name: user_collection
  endpoint:
    path: /developer-guide/snowflake-python-api/reference/latest/_autosummary/snowflake.core.user.UserCollection#snowflake.core.user.UserCollection.iter
    method: GET
    data_selector: records
- name: parameters
  endpoint:
    path: /en/user-guide/admin-account-management
    method: GET
    data_selector: parameters
    params: {}
- name: user_management
  endpoint:
    path: /api/user_management
    method: GET
    data_selector: users
    params: {}
- name: users
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: data
- name: AI_COMPLETE
  endpoint:
    path: /sql-reference/functions/ai_complete
    method: GET
- name: AI_CLASSIFY
  endpoint:
    path: /sql-reference/functions/ai_classify
    method: GET
- name: AI_FILTER
  endpoint:
    path: /sql-reference/functions/ai_filter
    method: GET
- name: AI_AGG
  endpoint:
    path: /sql-reference/functions/ai_agg
    method: GET
- name: AI_EMBED
  endpoint:
    path: /sql-reference/functions/ai_embed
    method: GET
- name: AI_EXTRACT
  endpoint:
    path: /sql-reference/functions/ai_extract
    method: GET
- name: AI_SENTIMENT
  endpoint:
    path: /sql-reference/functions/ai_sentiment
    method: GET
- name: AI_SUMMARIZE_AGG
  endpoint:
    path: /sql-reference/functions/ai_summarize_agg
    method: GET
- name: AI_SIMILARITY
  endpoint:
    path: /sql-reference/functions/ai_similarity
    method: GET
- name: AI_TRANSCRIBE
  endpoint:
    path: /sql-reference/functions/ai_transcribe
    method: GET
- name: AI_PARSE_DOCUMENT
  endpoint:
    path: /sql-reference/functions/ai_parse_document
    method: GET
- name: AI_TRANSLATE
  endpoint:
    path: /sql-reference/functions/ai_translate
    method: GET
- name: SUMMARIZE
  endpoint:
    path: /sql-reference/functions/summarize-snowflake-cortex
    method: GET
- name: TO_FILE
  endpoint:
    path: /sql-reference/functions/to_file
    method: GET
- name: AI_COUNT_TOKENS
  endpoint:
    path: /sql-reference/functions/ai_count_tokens
    method: GET
- name: PROMPT
  endpoint:
    path: /sql-reference/functions/prompt
    method: GET
- name: TRY_COMPLETE
  endpoint:
    path: /sql-reference/functions/try_complete-snowflake-cortex
    method: GET
- name: staged_file_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/staged-file-costs
    method: GET
- name: database_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/database-costs
    method: GET
- name: time_travel_and_fail_safe_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/time-travel-and-fail-safe-costs
    method: GET
- name: temporary_and_transient_tables_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/temporary-and-transient-tables-costs
    method: GET
- name: hybrid_table_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/hybrid-table-costs
    method: GET
- name: cloning_tables_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/cloning-tables-schemas-and-databases-costs
    method: GET
- name: cross_cloud_auto_fulfillment_costs
  endpoint:
    path: /en/user-guide/cost-understanding-data-storage/cross-cloud-auto-fulfillment-costs
    method: GET
- name: ai_agg
  endpoint:
    path: /snowpark/functions/ai_agg
    method: GET
    data_selector: functions
    params: {}
- name: ai_classify
  endpoint:
    path: /snowpark/functions/ai_classify
    method: GET
    data_selector: functions
    params: {}
- name: ai_complete
  endpoint:
    path: /snowpark/functions/ai_complete
    method: GET
    data_selector: functions
    params: {}
- name: ai_filter
  endpoint:
    path: /snowpark/functions/ai_filter
    method: GET
    data_selector: functions
    params: {}
- name: ai_similarity
  endpoint:
    path: /snowpark/functions/ai_similarity
    method: GET
    data_selector: functions
    params: {}
- name: ai_summarize_agg
  endpoint:
    path: /snowpark/functions/ai_summarize_agg
    method: GET
    data_selector: functions
    params: {}
- name: AI_COMPLETE
  endpoint:
    path: /sql-reference/functions/ai_complete
    method: GET
- name: AI_CLASSIFY
  endpoint:
    path: /sql-reference/functions/ai_classify
    method: GET
- name: AI_FILTER
  endpoint:
    path: /sql-reference/functions/ai_filter
    method: GET
- name: AI_AGG
  endpoint:
    path: /sql-reference/functions/ai_agg
    method: GET
- name: AI_EMBED
  endpoint:
    path: /sql-reference/functions/ai_embed
    method: GET
- name: AI_EXTRACT
  endpoint:
    path: /sql-reference/functions/ai_extract
    method: GET
- name: AI_SENTIMENT
  endpoint:
    path: /sql-reference/functions/ai_sentiment
    method: GET
- name: AI_SUMMARIZE_AGG
  endpoint:
    path: /sql-reference/functions/ai_summarize_agg
    method: GET
- name: AI_SIMILARITY
  endpoint:
    path: /sql-reference/functions/ai_similarity
    method: GET
- name: AI_TRANSCRIBE
  endpoint:
    path: /sql-reference/functions/ai_transcribe
    method: GET
- name: AI_PARSE_DOCUMENT
  endpoint:
    path: /sql-reference/functions/ai_parse_document
    method: GET
- name: AI_TRANSLATE
  endpoint:
    path: /sql-reference/functions/ai_translate
    method: GET
- name: SUMMARIZE
  endpoint:
    path: /sql-reference/functions/summarize-snowflake-cortex
    method: GET
- name: TO_FILE
  endpoint:
    path: /sql-reference/functions/to_file
    method: GET
- name: AI_COUNT_TOKENS
  endpoint:
    path: /sql-reference/functions/ai_count_tokens
    method: GET
- name: PROMPT
  endpoint:
    path: /sql-reference/functions/prompt
    method: GET
- name: TRY_COMPLETE
  endpoint:
    path: /sql-reference/functions/try_complete-snowflake-cortex
    method: GET
- name: AI_COMPLETE
  endpoint:
    path: /functions/ai_complete
    method: POST
    data_selector: results
- name: AI_CLASSIFY
  endpoint:
    path: /functions/ai_classify
    method: POST
    data_selector: results
- name: AI_FILTER
  endpoint:
    path: /functions/ai_filter
    method: POST
    data_selector: results
- name: AI_AGG
  endpoint:
    path: /functions/ai_agg
    method: POST
    data_selector: results
- name: AI_EMBED
  endpoint:
    path: /functions/ai_embed
    method: POST
    data_selector: results
- name: AI_EXTRACT
  endpoint:
    path: /functions/ai_extract
    method: POST
    data_selector: results
- name: AI_SENTIMENT
  endpoint:
    path: /functions/ai_sentiment
    method: POST
    data_selector: results
- name: AI_SUMMARIZE_AGG
  endpoint:
    path: /functions/ai_summarize_agg
    method: POST
    data_selector: results
- name: AI_SIMILARITY
  endpoint:
    path: /functions/ai_similarity
    method: POST
    data_selector: results
- name: AI_TRANSCRIBE
  endpoint:
    path: /functions/ai_transcribe
    method: POST
    data_selector: results
- name: AI_PARSE_DOCUMENT
  endpoint:
    path: /functions/ai_parse_document
    method: POST
    data_selector: results
- name: AI_TRANSLATE
  endpoint:
    path: /functions/ai_translate
    method: POST
    data_selector: results
- name: embed_text_768_snowflake_arctic_embed_m_v1.5
  endpoint:
    path: /path/to/embed_text_768_snowflake_arctic_embed_m_v1.5
    method: GET
    data_selector: records
    params: {}
- name: embed_text_768_snowflake_arctic_embed_m
  endpoint:
    path: /path/to/embed_text_768_snowflake_arctic_embed_m
    method: GET
    data_selector: records
    params: {}
- name: embed_text_1024_multilingual_e5_large
  endpoint:
    path: /path/to/embed_text_1024_multilingual_e5_large
    method: GET
    data_selector: records
    params: {}
- name: ai_extract
  endpoint:
    path: /path/to/ai_extract
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /SHOW MODELS IN SNOWFLAKE.MODELS
    method: GET
    data_selector: models
    params: {}
- name: application_roles
  endpoint:
    path: /SHOW APPLICATION ROLES IN APPLICATION SNOWFLAKE
    method: GET
    data_selector: roles
    params: {}
- name: functions
  endpoint:
    path: /functions
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /SHOW_MODELS_IN_SNOWFLAKE.MODELS
    method: GET
    data_selector: models
    params: {}
- name: application_roles
  endpoint:
    path: /SHOW_APPLICATION_ROLES_IN_APPLICATION_SNOWFLAKE
    method: GET
    data_selector: application_roles
    params: {}
- name: complete
  endpoint:
    path: /cortex/complete
    method: POST
    data_selector: response
    params: {}
- name: extract_answer
  endpoint:
    path: /cortex/extract-answer
    method: POST
    data_selector: response
    params: {}
- name: sentiment
  endpoint:
    path: /cortex/sentiment
    method: POST
    data_selector: response
    params: {}
- name: summarize
  endpoint:
    path: /cortex/summarize
    method: POST
    data_selector: response
    params: {}
- name: translate
  endpoint:
    path: /cortex/translate
    method: POST
    data_selector: response
    params: {}
- name: embed_text_768_snowflake_arctic_embed_m_v1_5
  endpoint:
    path: /embed_text_768/snowflake-arctic-embed-m-v1.5
    method: GET
    data_selector: records
- name: embed_text_768_snowflake_arctic_embed_m
  endpoint:
    path: /embed_text_768/snowflake-arctic-embed-m
    method: GET
    data_selector: records
- name: embed_text_1024_multilingual_e5_large
  endpoint:
    path: /embed_text_1024/multilingual-e5-large
    method: GET
    data_selector: records
- name: ai_extract
  endpoint:
    path: /ai_extract
    method: GET
    data_selector: records
- name: billing_contact
  endpoint:
    path: /billing/contacts
    method: POST
    data_selector: contacts
    params: {}
- name: external_network_locations
  endpoint:
    path: /external_network_locations
    method: GET
- name: external_functions
  endpoint:
    path: /external_functions
    method: GET
- name: external_stages
  endpoint:
    path: /external_stages
    method: GET
- name: external_tables
  endpoint:
    path: /external_tables
    method: GET
- name: external_volumes
  endpoint:
    path: /external_volumes
    method: GET
- name: snowpipe_automation
  endpoint:
    path: /snowpipe_automation
    method: GET
- name: image_repository
  endpoint:
    path: /api/v1/image_repository
    method: POST
- name: compute_pool
  endpoint:
    path: /api/v1/compute_pool
    method: POST
- name: service
  endpoint:
    path: /api/v1/service
    method: POST
- name: external_network_locations
  endpoint:
    path: /external/network/locations
    method: GET
- name: external_functions
  endpoint:
    path: /external/functions
    method: GET
- name: external_stages
  endpoint:
    path: /external/stages
    method: GET
- name: external_tables
  endpoint:
    path: /external/tables
    method: GET
- name: external_volumes
  endpoint:
    path: /external/volumes
    method: GET
- name: snowpipe_automation
  endpoint:
    path: /snowpipe/automation
    method: GET
- name: image_repository
  endpoint:
    path: /create/image/repository
    method: POST
    data_selector: repository
    params: {}
- name: compute_pool
  endpoint:
    path: /create/compute/pool
    method: POST
    data_selector: pool
    params: {}
- name: service
  endpoint:
    path: /create/service
    method: POST
    data_selector: service
    params: {}
- name: job_service
  endpoint:
    path: /execute/job/service
    method: POST
    data_selector: job_service
    params: {}
- name: trial_account
  endpoint:
    path: /user-guide/admin-trial-account
    method: GET
- name: trial_account
  endpoint:
    path: /user-guide/admin-trial-account
    method: GET
    data_selector: trial_account_info
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO emp_basic
    data_selector: records
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: result
    params:
      file_format: (type = csv field_optionally_enclosed_by='"')
      pattern: '''.*employees0[1-5].csv.gz'''
      on_error: '''skip_file'''
- name: accounts
  endpoint:
    path: /accounts
    method: GET
    data_selector: accounts
    params: {}
- name: database
  endpoint:
    path: /en/sql-reference/sql/create-database
    method: POST
    data_selector: records
    params: {}
- name: accounts
  endpoint:
    path: /accounts
    method: GET
    data_selector: global_account_properties
    params: {}
- name: create_database
  endpoint:
    path: /en/sql-reference/sql/create-database
    method: POST
    data_selector: records
    params: {}
- name: create_table
  endpoint:
    path: /en/sql-reference/sql/create-table
    method: GET
    data_selector: data
    params: {}
- name: table_name
  endpoint:
    path: /CREATE_TABLE
    method: POST
    data_selector: table_definition
    params: {}
- name: create_table
  endpoint:
    path: /en/sql-reference/sql/create-table
    method: GET
    data_selector: records
- name: create_table_using_template
  endpoint:
    path: /create/table/using/template
    method: CREATE
    data_selector: template
    params: {}
- name: create_table_like
  endpoint:
    path: /create/table/like
    method: CREATE
    data_selector: like
    params: {}
- name: create_table_clone
  endpoint:
    path: /create/table/clone
    method: CREATE
    data_selector: clone
    params: {}
- name: create_table_from_archive_of
  endpoint:
    path: /create/table/from/archive/of
    method: CREATE
    data_selector: archive
    params: {}
- name: mytable
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable
    method: GET
    data_selector: records
    params: {}
- name: mytable_2
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable_2
    method: GET
    data_selector: records
    params: {}
- name: warehouse
  endpoint:
    path: /create-warehouse
    method: POST
    data_selector: warehouse
    params: {}
- name: create_warehouse
  endpoint:
    path: /create_warehouse
    method: POST
    data_selector: data
    params: {}
- name: mytable
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable
    method: CREATE
    data_selector: records
    params: {}
- name: mytable_2
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable_2
    method: CREATE
    data_selector: records
    params: {}
- name: demo_temporary
  endpoint:
    path: /services/data/vXX.X/sobjects/demo_temporary
    method: CREATE TEMPORARY
    data_selector: records
    params: {}
- name: demo_temp
  endpoint:
    path: /services/data/vXX.X/sobjects/demo_temp
    method: CREATE TEMP
    data_selector: records
    params: {}
- name: PUT
  endpoint:
    path: /en/sql-reference/sql/put
    method: PUT
    data_selector: files
    params: {}
- name: warehouse
  endpoint:
    path: /api/warehouse
    method: POST
    data_selector: warehouse
    params:
      incremental: created_at
- name: list_files
  endpoint:
    path: /api/v1/list
    method: GET
    data_selector: files
- name: warehouse
  endpoint:
    path: /services/data/vXX.X/sobjects/Warehouse
    method: POST
    data_selector: records
- name: COPY INTO
  endpoint:
    path: /en/sql-reference/sql/copy-into-table
    method: GET
    data_selector: COPY INTO
    params: {}
- name: PUT
  endpoint:
    path: /en/sql-reference/sql/put
    method: PUT
    data_selector: files
    params: {}
- name: LIST
  endpoint:
    path: /en/sql-reference/sql/list
    method: GET
    data_selector: records
    params: {}
- name: COPY INTO
  endpoint:
    path: /en/sql-reference/sql/copy-into-table
    method: GET
    data_selector: COPY INTO *<table>*
    params: {}
- name: CSV
  endpoint:
    params:
      COMPRESSION: AUTO
      RECORD_DELIMITER: NONE
      FIELD_DELIMITER: ''''
      MULTI_LINE: 'TRUE'
      PARSE_HEADER: 'FALSE'
      SKIP_HEADER: 0
      SKIP_BLANK_LINES: 'FALSE'
      DATE_FORMAT: AUTO
      TIME_FORMAT: AUTO
      TIMESTAMP_FORMAT: AUTO
      BINARY_FORMAT: HEX
      ESCAPE: NONE
      ESCAPE_UNENCLOSED_FIELD: \
      TRIM_SPACE: 'FALSE'
      FIELD_OPTIONALLY_ENCLOSED_BY: NONE
      NULL_IF:
      - '''\N'''
      ERROR_ON_COLUMN_COUNT_MISMATCH: 'TRUE'
      REPLACE_INVALID_CHARACTERS: 'FALSE'
      EMPTY_FIELD_AS_NULL: 'TRUE'
      SKIP_BYTE_ORDER_MARK: 'TRUE'
      ENCODING: UTF8
- name: PARQUET
  endpoint:
    path: TYPE = PARQUET
    method: GET
    data_selector: records
    params: {}
- name: XML
  endpoint:
    path: TYPE = XML
    method: GET
    data_selector: records
    params: {}
- name: COPY_INTO
  endpoint:
    params:
      RECORD_DELIMITER: NONE
      FIELD_DELIMITER: ','
      MULTI_LINE: 'TRUE'
      PARSE_HEADER: 'FALSE'
      SKIP_HEADER: 0
      SKIP_BLANK_LINES: 'FALSE'
      DATE_FORMAT: AUTO
      TIME_FORMAT: AUTO
      TIMESTAMP_FORMAT: AUTO
      BINARY_FORMAT: HEX
      ESCAPE: NONE
      ESCAPE_UNENCLOSED_FIELD: \\
      TRIM_SPACE: 'FALSE'
      FIELD_OPTIONALLY_ENCLOSED_BY: NONE
      NULL_IF:
      - \N
      ERROR_ON_COLUMN_COUNT_MISMATCH: 'TRUE'
      REPLACE_INVALID_CHARACTERS: 'FALSE'
      EMPTY_FIELD_AS_NULL: 'TRUE'
      SKIP_BYTE_ORDER_MARK: 'TRUE'
      ENCODING: UTF8
- name: parquet_format_options
  endpoint:
    data_selector: options
    params:
      compression: AUTO
      binary_as_text: true
      trim_space: false
      use_logical_type: false
      use_vectorized_scanner: false
      replace_invalid_characters: false
      null_if:
      - \N
      - 'NULL'
      - NUL
      - ''
- name: xml_format_options
  endpoint:
    data_selector: options
    params:
      compression: AUTO
      ignore_utf8_errors: false
      preserve_space: false
      strip_outer_element: false
      disable_auto_convert: false
      replace_invalid_characters: false
      skip_byte_order_mark: true
- name: customer_iceberg_ingest
  endpoint:
    path: /services/data/vXX.X/sobjects/customer_iceberg_ingest
    method: COPY INTO
    data_selector: records
    params:
      LOAD_MODE: ADD_FILES_COPY
      PURGE: true
      MATCH_BY_COLUMN_NAME: CASE_SENSITIVE
- name: customer_iceberg_ingest
  endpoint:
    path: /customer_iceberg_ingest
    method: COPY INTO
    data_selector: rows_loaded
    params:
      LOAD_MODE: ADD_FILES_COPY
      PURGE: 'TRUE'
      MATCH_BY_COLUMN_NAME: CASE_SENSITIVE
- name: formatTypeOptions
  endpoint:
    path: /create-file-format
    method: GET
- name: create_or_alter_file_format
  endpoint:
    path: /en/sql-reference/sql/create-file-format
    method: GET
    data_selector: records
    params: {}
- name: functions
  endpoint:
    path: /en/sql-reference/functions
    method: GET
    data_selector: functions
    params: {}
- name: stored_procedures
  endpoint:
    path: /en/sql-reference/stored-procedures
    method: GET
    data_selector: stored_procedures
    params: {}
- name: general_dml
  endpoint:
    path: /en/sql-reference/sql-dml
    method: GET
    data_selector: commands
- name: data_loading_unloading_dml
  endpoint:
    path: /en/sql-reference/commands-data-loading
    method: GET
    data_selector: commands
- name: file_staging_commands
  endpoint:
    path: /en/sql-reference/commands-file
    method: GET
    data_selector: commands
- name: parquet_format
  endpoint:
    path: /services/data/vXX.X/sobjects/ParquetFormat
    method: CREATE OR REPLACE
    data_selector: records
    params:
      USE_VECTORIZED_SCANNER: 'TRUE'
      USE_LOGICAL_TYPE: 'TRUE'
- name: INSERT
  endpoint:
    path: /en/sql-reference/sql/insert
    method: GET
    data_selector: records
    params: {}
- name: sf_employees
  endpoint:
    path: /services/data/vXX.X/sobjects/sf_employees
    method: INSERT
    data_selector: records
    params: {}
- name: employees
  endpoint:
    path: /services/data/vXX.X/sobjects/employees
    method: SELECT
    data_selector: records
    params: {}
- name: functions
  endpoint:
    path: /en/sql-reference/functions
    method: GET
    data_selector: functions
    params: {}
- name: stored_procedures
  endpoint:
    path: /en/sql-reference-stored-procedures
    method: GET
    data_selector: stored_procedures
    params: {}
- name: query_operators
  endpoint:
    path: /en/sql-reference/operators
    method: GET
- name: date_add_function
  endpoint:
    path: /functions/dateadd
    method: GET
    data_selector: function_details
    params: {}
- name: insert
  endpoint:
    path: /sql/insert
    method: GET
- name: merge
  endpoint:
    path: /sql/merge
    method: GET
- name: update
  endpoint:
    path: /sql/update
    method: GET
- name: delete
  endpoint:
    path: /sql/delete
    method: GET
- name: truncate_table
  endpoint:
    path: /sql/truncate-table
    method: GET
- name: copy_into_table
  endpoint:
    path: /sql/copy-into-table
    method: GET
- name: copy_into_location
  endpoint:
    path: /sql/copy-into-location
    method: GET
- name: put
  endpoint:
    path: /sql/put
    method: GET
- name: get
  endpoint:
    path: /sql/get
    method: GET
- name: list
  endpoint:
    path: /sql/list
    method: GET
- name: remove
  endpoint:
    path: /sql/remove
    method: GET
- name: employees
  endpoint:
    path: /en/sql-reference/sql/insert
    method: INSERT
    data_selector: VALUES
    params: {}
- name: contractors
  endpoint:
    path: /en/sql-reference/sql/insert
    method: INSERT
    data_selector: VALUES
    params: {}
- name: prospects
  endpoint:
    path: /en/sql-reference/sql/insert
    method: INSERT
    data_selector: SELECT PARSE_JSON(column1)
    params: {}
- name: sf_employees
  endpoint:
    path: /services/data/vXX.X/sobjects/sf_employees
    method: INSERT
    data_selector: records
    params: {}
- name: employees
  endpoint:
    path: /services/data/vXX.X/sobjects/employees
    method: SELECT
    data_selector: records
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO emp_basic
    data_selector: records
    params:
      incremental: updated_at
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: '*'
    params: {}
- name: arguments
  endpoint:
    path: /arguments
    method: GET
    data_selector: arguments
    params: {}
- name: returns
  endpoint:
    path: /returns
    method: GET
    data_selector: returns
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY
    data_selector: records
    params: {}
- name: DATEADD
  endpoint:
    path: /en/sql-reference/functions/dateadd
    method: GET
    data_selector: arguments
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
    params: {}
- name: account
  endpoint:
    path: /api/v1/account
    method: GET
    data_selector: data
    params: {}
- name: user
  endpoint:
    path: /api/v1/user
    method: GET
    data_selector: data
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
    params: {}
- name: sf_tuts
  endpoint:
    path: /services/data/vXX.X/sobjects/sf_tuts
    method: GET
    data_selector: records
    params: {}
- name: tutorials
  endpoint:
    path: /en/tutorials
    method: GET
    data_selector: tutorials
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params:
      pattern: .*employees0[1-5].csv.gz
      file_format: csv
      on_error: skip_file
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params:
      pattern: .*employees0[1-5].csv.gz
      on_error: skip_file
- name: emp_basic
  endpoint:
    path: /emp_basic
    method: SELECT
    data_selector: '*'
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY
    data_selector: records
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params:
      FILE_FORMAT: (type = csv field_optionally_enclosed_by='"')
      PATTERN: .*employees0[1-5].csv.gz
      ON_ERROR: skip_file
- name: emp_basic
  endpoint:
    path: /emp_basic
    method: SELECT
    data_selector: '*'
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: PUT
    data_selector: records
    params: {}
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params:
      FILE_FORMAT: (type = csv field_optionally_enclosed_by='"')
      PATTERN: '''.*employees0[1-5].csv.gz'''
      ON_ERROR: '''skip_file'''
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY INTO
    data_selector: records
    params:
      incremental: updated_at
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: GET
    data_selector: records
    params: {}
- name: failover_group_creation
  endpoint:
    path: /create/failover_group
    method: POST
    data_selector: result
- name: apply_global_ids
  endpoint:
    path: /apply/global_ids
    method: POST
    data_selector: result
- name: emp_basic
  endpoint:
    path: /services/data/vXX.X/sobjects/emp_basic
    method: COPY
    data_selector: data
    params:
      file_format: (type = csv field_optionally_enclosed_by='"')
      pattern: .*employees0[1-5].csv.gz
      on_error: skip_file
- name: emp_basic
  endpoint:
    path: /emp_basic
    method: SELECT
    data_selector: '*'
    params: {}
- name: failover_group
  endpoint:
    path: /ALTER/FAILOVER/GROUP/fg
    method: SET
    data_selector: OBJECT_TYPES = users, roles, warehouses, resource monitors, integrations
    params: {}
- name: network_policy
  endpoint:
    path: /ALTER/FAILOVER/GROUP/fg/REFRESH
    method: GET
    data_selector: network policies
    params: {}
- name: replication_stages
  endpoint:
    path: /replication/stages
    method: GET
    data_selector: stages
    params: {}
- name: replication_pipes
  endpoint:
    path: /replication/pipes
    method: GET
    data_selector: pipes
    params: {}
- name: pipe
  endpoint:
    path: /services/data/vXX.X/sobjects/Pipe
    method: CREATE
    data_selector: records
    params:
      incremental: updated_at
- name: failover_group
  endpoint:
    path: /services/data/vXX.X/sobjects/FailoverGroup
    method: CREATE
    data_selector: records
    params: {}
- name: Git repository objects
  endpoint:
    path: /replication/git-repositories
    method: GET
- name: failover_group
  endpoint:
    path: /user-guide/account-replication-failover-failback
    method: GET
    data_selector: group
- name: stored_procedures_and_udfs
  endpoint:
    path: /replication/stored_procedures_and_udfs
    method: GET
    data_selector: procedures
    params: {}
- name: stages
  endpoint:
    path: /replication/stages
    method: GET
    data_selector: stages
    params: {}
- name: streams
  endpoint:
    path: /replication/streams
    method: GET
    data_selector: streams
    params: {}
- name: tasks
  endpoint:
    path: /replication/tasks
    method: GET
    data_selector: tasks
    params: {}
- name: tags
  endpoint:
    path: /replication/tags
    method: GET
    data_selector: tags
    params: {}
- name: failover_group
  endpoint:
    path: /create_failover_group
    method: POST
    data_selector: result
    params:
      object_types: USERS, ROLES, WAREHOUSES, RESOURCE MONITORS, DATABASES
      allowed_databases: db1, db2
      allowed_accounts: myorg.myaccount2, myorg.myaccount3
      replication_schedule: 10 MINUTE
- name: connection
  endpoint:
    path: /services/data/vXX.X/sobjects/Connection
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: snapshot_policy
  endpoint:
    path: /create_snapshot_policy
    method: POST
    data_selector: policy_created
    params:
      schedule: 60 MINUTE
      expire_after_days: 90
      comment: Hourly backups expire after 90 days
- name: snapshot_set
  endpoint:
    path: /create_snapshot_set
    method: POST
    data_selector: set_created
    params:
      for: TABLE
      object_name: t1
      snapshot_policy: hourly_snapshot_policy
- name: table_snapshots
  endpoint:
    path: /snapshots/t1_snapshots
    method: SHOW
    data_selector: snapshot_id
- name: schema_snapshots
  endpoint:
    path: /snapshots/s1_snapshots
    method: SHOW
    data_selector: snapshot_id
- name: database_snapshots
  endpoint:
    path: /snapshots/d1_snapshots
    method: SHOW
    data_selector: snapshot_id
- name: snapshot_set
  endpoint:
    path: /create_snapshot_set
    method: POST
    data_selector: snapshot_set_response
    params: {}
- name: snapshot
  endpoint:
    path: /add_snapshot
    method: POST
    data_selector: snapshot_response
    params: {}
- name: restore_snapshot
  endpoint:
    path: /restore_snapshot
    method: POST
    data_selector: restore_response
    params: {}
- name: secrets
  endpoint:
    path: /secrets
    method: GET
    data_selector: secrets
- name: security_integrations
  endpoint:
    path: /security_integrations
    method: GET
    data_selector: integrations
- name: api_integrations
  endpoint:
    path: /api_integrations
    method: GET
    data_selector: integrations
- name: databases
  endpoint:
    path: /databases
    method: GET
    data_selector: databases
- name: tables
  endpoint:
    path: /tables
    method: GET
    data_selector: tables
- name: warehouses
  endpoint:
    path: /warehouses
    method: GET
    data_selector: warehouses
- name: roles
  endpoint:
    path: /roles
    method: GET
    data_selector: roles
- name: snapshot_policies
  endpoint:
    path: /sql-reference/info-schema/snapshot_policies
    method: GET
- name: snapshot_sets
  endpoint:
    path: /sql-reference/info-schema/snapshot_sets
    method: GET
- name: snapshots
  endpoint:
    path: /sql-reference/info-schema/snapshots
    method: GET
- name: snapshot_operation_history
  endpoint:
    path: /sql-reference/account-usage/snapshot_operation_history
    method: GET
- name: snapshot_storage_usage
  endpoint:
    path: /sql-reference/account-usage/snapshot_storage_usage
    method: GET
- name: failover_group
  endpoint:
    path: /en/user-guide/account-replication-config
    method: GET
    data_selector: replication
- name: storage_integration
  endpoint:
    path: /en/user-guide/data-load-snowpipe-auto-s3
    method: GET
    data_selector: storage_integration
- name: snowpipe
  endpoint:
    path: /services/data/vXX.X/sobjects/Snowpipe
    method: CREATE
    data_selector: pipe
    params:
      AUTO_INGEST: 'TRUE'
      AWS_SNS_TOPIC: <topic_arn>
      COPY INTO: snowpipe_db.public.mytable
      FROM: '@snowpipe_db.public.my_s3_stage'
      FILE_FORMAT: (TYPE = 'JSON')
- name: failover_group
  endpoint:
    path: /services/data/vXX.X/sobjects/FailoverGroup
    method: CREATE
    data_selector: failover_group
    params:
      OBJECT_TYPES: DATABASES, INTEGRATIONS
      ALLOWED_INTEGRATION_TYPES: STORAGE INTEGRATIONS
      ALLOWED_DATABASES: snowpipe_db
      ALLOWED_ACCOUNTS: myorg.my_account_2
- name: Git repository objects
  endpoint:
    path: /user-guide/account-replication-git-repositories
    method: GET
    data_selector: objects
    params: {}
- name: failover_group
  endpoint:
    path: /user-guide/account-replication-failover-failback
    method: GET
    data_selector: replication_groups
    params: {}
- name: UNDROP ACCOUNT
  endpoint:
    path: /undrop/account
    method: POST
- name: UNDROP DATABASE
  endpoint:
    path: /undrop/database
    method: POST
- name: UNDROP DYNAMIC TABLE
  endpoint:
    path: /undrop/dynamic_table
    method: POST
- name: UNDROP EXTERNAL VOLUME
  endpoint:
    path: /undrop/external_volume
    method: POST
- name: UNDROP ICEBERG TABLE
  endpoint:
    path: /undrop/iceberg_table
    method: POST
- name: UNDROP NOTEBOOK
  endpoint:
    path: /undrop/notebook
    method: POST
- name: UNDROP SCHEMA
  endpoint:
    path: /undrop/schema
    method: POST
- name: UNDROP SNAPSHOT
  endpoint:
    path: /undrop/snapshot
    method: POST
- name: UNDROP STREAMLIT
  endpoint:
    path: /undrop/streamlit
    method: POST
- name: UNDROP TABLE
  endpoint:
    path: /undrop/table
    method: POST
- name: UNDROP TAG
  endpoint:
    path: /undrop/tag
    method: POST
- name: tables
  endpoint:
    path: /show/tables
    method: GET
    data_selector: tables
    params: {}
- name: tables
  endpoint:
    path: /SHOW_TABLES
    method: GET
    data_selector: results
- name: replication_group_refresh_history
  endpoint:
    path: /snowflake.account_usage.replication_group_refresh_history
    method: GET
    data_selector: records
    params: {}
- name: replication_group_usage_history
  endpoint:
    path: /snowflake.account_usage.replication_group_usage_history
    method: GET
    data_selector: records
    params: {}
- name: schemas
  endpoint:
    path: /SHOW SCHEMAS
    method: GET
    data_selector: schemas
    params: {}
- name: databases
  endpoint:
    path: /show/databases
    method: GET
    data_selector: created_on
    params: {}
- name: connection
  endpoint:
    path: /services/data/vXX.X/sobjects/Connection
    method: CREATE
    data_selector: connections
    params: {}
- name: connection
  endpoint:
    path: /connections
    method: SHOW
    data_selector: CONNECTION_URL
    params: {}
- name: streams
  endpoint:
    path: /en/sql-reference/sql/show-streams
    method: GET
    data_selector: output
    params: {}
- name: account
  endpoint:
    path: /ALTER ACCOUNT
    method: GET
- name: session
  endpoint:
    path: /ALTER SESSION
    method: GET
- name: CURRENT_TIMESTAMP
  endpoint:
    path: /functions/current_timestamp
    method: GET
    data_selector: CURRENT_TIMESTAMP
    params: {}
- name: table_snapshots
  endpoint:
    path: /snapshots/table
    method: SHOW
    data_selector: snapshot_id
    params: {}
- name: schema_snapshots
  endpoint:
    path: /snapshots/schema
    method: SHOW
    data_selector: snapshot_id
    params: {}
- name: database_snapshots
  endpoint:
    path: /snapshots/database
    method: SHOW
    data_selector: snapshot_id
    params: {}
- name: create_table
  endpoint:
    path: /create_table
    method: POST
    data_selector: result
    params: {}
- name: snapshot_set
  endpoint:
    path: /create_snapshot_set
    method: POST
    data_selector: snapshot_set
    params: {}
- name: snapshot
  endpoint:
    path: /add_snapshot
    method: POST
    data_selector: snapshot
    params: {}
- name: restore_snapshot
  endpoint:
    path: /restore_snapshot
    method: POST
    data_selector: restored_snapshot
    params: {}
- name: CREATE TABLE  USING TEMPLATE
  endpoint:
    path: /create/table/using/template
    method: POST
- name: CREATE TABLE  LIKE
  endpoint:
    path: /create/table/like
    method: POST
- name: CREATE TABLE  CLONE
  endpoint:
    path: /create/table/clone
    method: POST
- name: CREATE TABLE  FROM ARCHIVE OF
  endpoint:
    path: /create/table/from/archive
    method: POST
- name: snapshot_policies
  endpoint:
    path: /sql-reference/info-schema/snapshot_policies
    method: GET
- name: snapshot_sets
  endpoint:
    path: /sql-reference/info-schema/snapshot_sets
    method: GET
- name: snapshots
  endpoint:
    path: /sql-reference/info-schema/snapshots
    method: GET
- name: snapshot_operation_history
  endpoint:
    path: /sql-reference/account-usage/snapshot_operation_history
    method: GET
- name: snapshot_storage_usage
  endpoint:
    path: /sql-reference/account-usage/snapshot_storage_usage
    method: GET
- name: mytable
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable
    method: GET
    data_selector: records
    params: {}
- name: mytable_2
  endpoint:
    path: /services/data/vXX.X/sobjects/mytable_2
    method: GET
    data_selector: records
    params: {}
- name: schema
  endpoint:
    path: /create_or_alter_schema
    method: POST
- name: account
  endpoint:
    path: /undrop/account
    method: POST
- name: database
  endpoint:
    path: /undrop/database
    method: POST
- name: dynamic_table
  endpoint:
    path: /undrop/dynamic_table
    method: POST
- name: external_volume
  endpoint:
    path: /undrop/external_volume
    method: POST
- name: iceberg_table
  endpoint:
    path: /undrop/iceberg_table
    method: POST
- name: notebook
  endpoint:
    path: /undrop/notebook
    method: POST
- name: schema
  endpoint:
    path: /undrop/schema
    method: POST
- name: snapshot
  endpoint:
    path: /undrop/snapshot
    method: POST
- name: streamlit
  endpoint:
    path: /undrop/streamlit
    method: POST
- name: table
  endpoint:
    path: /undrop/table
    method: POST
- name: tag
  endpoint:
    path: /undrop/tag
    method: POST
- name: show_tables
  endpoint:
    path: /show/tables
    method: GET
    data_selector: tables
    params: {}
- name: create_database
  endpoint:
    path: /en/sql-reference/sql/create-database
    method: GET
    data_selector: records
    params: {}
- name: tables
  endpoint:
    path: /SHOW_TABLES
    method: GET
    data_selector: tables
    params: {}
- name: schemas
  endpoint:
    path: /schemas
    method: GET
    data_selector: schemas
    params: {}
- name: notebook
  endpoint:
    path: /DROP NOTEBOOK
    method: POST
    data_selector: notebooks
    params: {}
- name: drop_table
  endpoint:
    path: /en/sql-reference/sql/drop-table
    method: GET
    data_selector: parameters
    params: {}
- name: databases
  endpoint:
    path: /SHOW_DATABASES
    method: GET
    data_selector: results
    params: {}
- name: DROP SCHEMA
  endpoint:
    path: /en/sql-reference/commands-database/DROP SCHEMA
    method: GET
    data_selector: parameters
    params: {}
- name: streams
  endpoint:
    path: /SHOW_STREAMS
    method: GET
    data_selector: null
    params: {}
- name: drop_database
  endpoint:
    path: /en/sql-reference/sql/drop-database
    method: DROP
    data_selector: ''
    params: {}
- name: account
  endpoint:
    path: /api/account
    method: GET
- name: database
  endpoint:
    path: /api/database
    method: GET
- name: accounts
  endpoint:
    path: /show_accounts
    method: GET
    data_selector: records
    params: {}
- name: date_time_data_types
  endpoint:
    path: /en/reference/sql-data-types-reference
    method: GET
    data_selector: data_types
    params: {}
- name: undrop_table
  endpoint:
    path: /undrop_table
    method: POST
    data_selector: status
    params:
      name: <name>
- name: undrop_schema
  endpoint:
    path: /undrop/schema
    method: POST
    data_selector: status
    params:
      name: <schema_name>
- name: CREATE TABLE
  endpoint:
    path: /en/sql-reference/sql/create-table
    method: GET
    data_selector: records
    params: {}
- name: CREATE OR ALTER TABLE
  endpoint:
    path: /en/sql-reference/sql/create-or-alter-table
    method: GET
    data_selector: records
    params: {}
- name: CREATE TABLE AS SELECT
  endpoint:
    path: /en/sql-reference/sql/create-table-as-select
    method: GET
    data_selector: records
    params: {}
- name: undrop_dynamic_table
  endpoint:
    path: /undrop_dynamic_table
    method: POST
    data_selector: response
    params:
      name: dynamic_table_name
- name: my_table
  endpoint:
    path: /create_or_alter_table
    method: POST
    data_selector: table_definition
- name: undrop_external_volume
  endpoint:
    path: /sql/reference/sql/undrop-external-volume
    method: GET
    data_selector: parameters
    params: {}
- name: schema
  endpoint:
    path: /schemas
    method: CREATE
    data_selector: schemas
    params: {}
- name: undrop_tag
  endpoint:
    path: /undrop-tag
    method: POST
    data_selector: result
    params:
      name: <tag_identifier>
- name: s1
  endpoint:
    path: /schemas/s1
    method: CREATE OR ALTER
    data_selector: properties
    params:
      WITH MANAGED ACCESS: true
      DATA_RETENTION_TIME_IN_DAYS: 5
      DEFAULT_DDL_COLLATION: de
- name: data_lifecycle
  endpoint:
    path: /user-guide/data-lifecycle
    method: GET
    data_selector: data
- name: create_database
  endpoint:
    path: /api/create_database
    method: POST
    data_selector: records
    params: {}
- name: database
  endpoint:
    path: /databases
    method: CREATE
    data_selector: database_creation
    params: {}
- name: DROP NOTEBOOK
  endpoint:
    path: /DROP NOTEBOOK
    method: GET
    data_selector: notebook
    params: {}
- name: drop_table
  endpoint:
    path: /en/sql-reference/sql/drop-table
    method: GET
    data_selector: parameters
    params: {}
- name: DROP SCHEMA
  endpoint:
    path: /en/sql-reference/sql/drop-schema
    method: GET
    data_selector: parameters
    params: {}
- name: tables
  endpoint:
    path: /SHOW TABLES
    method: GET
    data_selector: tables
- name: schemas
  endpoint:
    path: /SHOW SCHEMAS
    method: GET
    data_selector: schemas
- name: databases
  endpoint:
    path: /SHOW DATABASES
    method: GET
    data_selector: databases
- name: drop_database
  endpoint:
    path: /en/sql-reference/sql/drop-database
    method: POST
    data_selector: parameters
    params:
      name: <name>
      CASCADE | RESTRICT: CASCADE
- name: accounts
  endpoint:
    path: /en/sql-reference/commands-account/SHOW_ACCOUNTS
    method: GET
    data_selector: records
    params: {}
- name: loaddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/LOADDATA1
    method: SHOW
    data_selector: tables
    params: {}
- name: prodata1
  endpoint:
    path: /services/data/vXX.X/sobjects/PRODDATA1
    method: SHOW
    data_selector: tables
    params: {}
- name: undrop_notebook
  endpoint:
    path: /undrop_notebook
    method: POST
    data_selector: status
    params: {}
- name: tables_history
  endpoint:
    path: /tables/history
    method: GET
    data_selector: history_records
- name: undrop_table
  endpoint:
    path: /undrop-table
    method: GET
    data_selector: status
- name: Time Travel
  endpoint:
    path: /user-guide/data-time-travel
    method: GET
    data_selector: content
    params: {}
- name: undrop_schema
  endpoint:
    path: /undrop/schema
    method: POST
    data_selector: status
    params: {}
- name: tables
  endpoint:
    path: /sql-reference/sql/show-tables
    method: GET
    data_selector: retention_time
    params: {}
- name: schemas
  endpoint:
    path: /sql-reference/sql/show-schemas
    method: GET
    data_selector: retention_time
    params: {}
- name: databases
  endpoint:
    path: /sql-reference/sql/show-databases
    method: GET
    data_selector: retention_time
    params: {}
- name: streams
  endpoint:
    path: /sql-reference/sql/show-streams
    method: GET
    data_selector: stale_after
    params: {}
- name: history
  endpoint:
    path: /sql-reference/sql/show-databases/history
    method: GET
    data_selector: dropped_on
    params: {}
- name: undrop_database
  endpoint:
    path: /api/undrop_database
    method: POST
    data_selector: status
    params:
      name: database_name
- name: undrop_iceberg_table
  endpoint:
    path: /sql-reference/sql/undrop-iceberg-table
    method: POST
    data_selector: name
    params: {}
- name: loaddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/loaddata1
    method: GET
    data_selector: records
- name: prodata1
  endpoint:
    path: /services/data/vXX.X/sobjects/prodata1
    method: GET
    data_selector: records
- name: temporary_table
  endpoint:
    path: /en/user-guide/tables-temp-transient
    method: GET
    data_selector: data
    params: {}
- name: transient_table
  endpoint:
    path: /en/user-guide/tables-temp-transient
    method: GET
    data_selector: data
    params: {}
- name: undrop_dynamic_table
  endpoint:
    path: /undrop/dynamic/table
    method: POST
    data_selector: table
    params:
      name: <name>
- name: undrop_external_volume
  endpoint:
    path: /sql/undrop-external-volume
    method: POST
    data_selector: results
    params:
      name: <name>
- name: external_table
  endpoint:
    path: /services/data/vXX.X/external_tables
    method: GET
    data_selector: tables
    params: {}
- name: undrop_tag
  endpoint:
    path: /undrop/tag
    method: POST
    data_selector: result
    params:
      name: tag_name
- name: undrop_account
  endpoint:
    path: /undrop_account
    method: POST
    data_selector: response
    params:
      name: account_name
- name: table
  endpoint:
    path: /user-guide/ui-snowsight-data-databases-table
    method: GET
- name: test_table
  endpoint:
    path: /test_table
    method: CREATE
    data_selector: ''
    params: {}
- name: non_materialized_view
  endpoint:
    path: /en/user-guide/views-introduction
    method: GET
    data_selector: views
- name: materialized_view
  endpoint:
    path: /en/user-guide/views-materialized
    method: GET
    data_selector: materialized_views
- name: secure_view
  endpoint:
    path: /en/user-guide/views-secure
    method: GET
    data_selector: secure_views
- name: time_travel
  endpoint:
    path: /user-guide/data-time-travel
    method: GET
    data_selector: data
    params: {}
- name: tables
  endpoint:
    path: /sql-reference/sql/show-tables
    method: SHOW
    data_selector: retention_time
- name: schemas
  endpoint:
    path: /sql-reference/sql/show-schemas
    method: SHOW
    data_selector: retention_time
- name: databases
  endpoint:
    path: /sql-reference/sql/show-databases
    method: SHOW
    data_selector: retention_time
- name: streams
  endpoint:
    path: /sql-reference/sql/show-streams
    method: SHOW
    data_selector: stale_after
- name: tables_history
  endpoint:
    path: /SHOW_TABLES_HISTORY
    method: GET
    data_selector: records
- name: widgets_view
  endpoint:
    path: /widgets_view
    method: CREATE
    data_selector: definition
    params: {}
- name: loaddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/loaddata1
    method: GET
- name: proddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/proddata1
    method: GET
- name: time_travel
  endpoint:
    path: /user-guide/data-time-travel
    method: GET
    data_selector: data
    params: {}
- name: LOADDATA1
  endpoint:
    path: /mytestdb/public/LOADDATA1
    method: SHOW
    data_selector: history
    params: {}
- name: PRODDATA1
  endpoint:
    path: /mytestdb/public/PRODDATA1
    method: SHOW
    data_selector: history
    params: {}
- name: loaddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/LOADDATA1
    method: GET
    data_selector: records
    params: {}
- name: proddata1
  endpoint:
    path: /services/data/vXX.X/sobjects/PRODDATA1
    method: GET
    data_selector: records
    params: {}
- name: views
  endpoint:
    path: /views
    method: GET
    data_selector: records
- name: materialized_views
  endpoint:
    path: /materialized_views
    method: GET
    data_selector: records
- name: dynamic_tables
  endpoint:
    path: /dynamic_tables
    method: GET
    data_selector: records
- name: tables_history
  endpoint:
    path: /tables/history
    method: GET
    data_selector: history
    params: {}
- name: temporary_table
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: temporary_tables
- name: transient_table
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: transient_tables
- name: data_storage
  endpoint:
    path: /user-guide/tables-storage-considerations
    method: GET
    data_selector: data_storage_guidelines
    params: {}
- name: external_table
  endpoint:
    path: /create_external_table
    method: POST
    data_selector: external_table_definition
- name: table
  endpoint:
    path: /tables
    method: GET
    data_selector: tables
    params: {}
- name: tables
  endpoint:
    path: /en/sql-reference/info-schema/tables
    method: GET
    data_selector: rows
    params: {}
- name: test_table
  endpoint:
    path: /user-guide/search-optimization-service
    method: GET
    data_selector: queries
    params: {}
- name: views
  endpoint:
    path: /user-guide/views-introduction
    method: GET
    data_selector: views
- name: secure_view
  endpoint:
    path: /user-guide/views-secure
    method: GET
    data_selector: overview
- name: historical_data
  endpoint:
    path: /querying/historical_data
    method: GET
    data_selector: data
    params: {}
- name: cloning_objects
  endpoint:
    path: /cloning/historical_objects
    method: GET
    data_selector: data
    params: {}
- name: dropping_restoring_objects
  endpoint:
    path: /dropping/restoring_objects
    method: GET
    data_selector: data
    params: {}
- name: Time Travel and Fail-safe
  endpoint:
    path: /user-guide/data-time-travel
    method: GET
    data_selector: storage costs
    params: {}
- name: Temporary and Transient Tables
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: storage costs
    params: {}
- name: semantic_view
  endpoint:
    path: /user-guide/views-semantic
    method: GET
    data_selector: views
    params: {}
- name: views
  endpoint:
    path: /views
    method: GET
    data_selector: views
    params: {}
- name: materialized_views
  endpoint:
    path: /materialized_views
    method: GET
    data_selector: materialized_views
    params: {}
- name: semantic_views
  endpoint:
    path: /semantic_views
    method: GET
    data_selector: semantic_views
    params: {}
- name: databases
  endpoint:
    path: /api/databases
    method: GET
    data_selector: databases
- name: schemas
  endpoint:
    path: /api/schemas
    method: GET
    data_selector: schemas
- name: tables
  endpoint:
    path: /api/tables
    method: GET
    data_selector: tables
- name: event_tables
  endpoint:
    path: /api/event_tables
    method: GET
    data_selector: event_tables
- name: event_table
  endpoint:
    path: /databases/{database}/schemas/{schema}/event_tables
    method: GET
    data_selector: event_tables
- name: view
  endpoint:
    path: /databases/{database}/schemas/{schema}/views
    method: GET
    data_selector: views
- name: salespeople
  endpoint:
    path: /create-or-replace/table/salespeople
    method: CREATE
    data_selector: records
- name: salesorders
  endpoint:
    path: /create-or-replace/table/salesorders
    method: CREATE
    data_selector: records
- name: data_storage
  endpoint:
    path: /user-guide/tables-storage-considerations
    method: GET
    data_selector: content
- name: temporary_table
  endpoint:
    path: /user-guide/tables/temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: transient_table
  endpoint:
    path: /user-guide/tables/temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: COLUMNS
  endpoint:
    path: /en/sql-reference/info-schema/columns
    method: GET
    data_selector: columns
    params: {}
- name: tables
  endpoint:
    path: /en/sql-reference/info-schema/tables
    method: GET
    data_selector: rows
- name: temporary_table
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: transient_table
  endpoint:
    path: /user-guide/tables-transient
    method: GET
    data_selector: tables
    params: {}
- name: temporary_table
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: transient_table
  endpoint:
    path: /user-guide/tables-temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: Git in Snowflake
  endpoint:
    path: /developer-guide/builders/../git/git-overview
    method: GET
    data_selector: resources
- name: Snowflake CLI
  endpoint:
    path: /developer-guide/builders/../snowflake-cli/index
    method: GET
    data_selector: resources
- name: Python APIs
  endpoint:
    path: /developer-guide/builders/../snowflake-python-api/snowflake-python-overview
    method: GET
    data_selector: resources
- name: query_historical_data
  endpoint:
    path: /sql-reference/constructs/at-before
    method: GET
    data_selector: query_examples
    params: {}
- name: cloning_historical_objects
  endpoint:
    path: /sql-reference/sql/create-clone
    method: GET
    data_selector: clone_examples
    params: {}
- name: dropping_objects
  endpoint:
    path: /sql-reference/sql/drop-table
    method: GET
    data_selector: drop_examples
    params: {}
- name: restoring_objects
  endpoint:
    path: /sql-reference/sql/undrop-table
    method: GET
    data_selector: restore_examples
    params: {}
- name: fail_safe
  endpoint:
    path: /user-guide/data-failsafe
    method: GET
    data_selector: historical data storage
    params: {}
- name: Snowflake Notebooks
  endpoint:
    path: /en/developer-guide/snowflake-ml/notebooks-on-spcs
    method: GET
- name: Snowflake Feature Store
  endpoint:
    path: /en/developer-guide/snowflake-ml/feature-store/overview
    method: GET
- name: ML Jobs
  endpoint:
    path: /en/developer-guide/snowflake-ml/ml-jobs/overview
    method: GET
- name: Snowflake Model Registry
  endpoint:
    path: /en/developer-guide/snowflake-ml/model-registry/overview
    method: GET
- name: ML Observability
  endpoint:
    path: /en/developer-guide/snowflake-ml/model-registry/model-observability
    method: GET
- name: ML Lineage
  endpoint:
    path: /en/developer-guide/snowflake-ml/ml-lineage
    method: GET
- name: Snowflake Datasets
  endpoint:
    path: /en/developer-guide/snowflake-ml/dataset
    method: GET
- name: database
  endpoint:
    path: /databases
    method: GET
    data_selector: databases
- name: schema
  endpoint:
    path: /schemas
    method: GET
    data_selector: schemas
- name: table
  endpoint:
    path: /tables
    method: GET
    data_selector: tables
- name: event_table
  endpoint:
    path: /event_tables
    method: GET
    data_selector: event_tables
- name: event_table
  endpoint:
    path: /databases/my_db/schemas/my_schema/event_tables
    method: GET
    data_selector: event_tables
    params:
      like: my%
- name: view
  endpoint:
    path: /databases/my_db/schemas/my_schema/views
    method: GET
    data_selector: views
    params:
      like: my%
- name: temporary_tables
  endpoint:
    path: /fr/user-guide/tables-temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: transient_tables
  endpoint:
    path: /fr/user-guide/tables-temp-transient
    method: GET
    data_selector: tables
    params: {}
- name: accounts
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-accounts
    method: GET
- name: alerts
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-alerts
    method: GET
- name: data_loading
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-data-loading
    method: GET
- name: databases
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-databases
    method: GET
- name: functions
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-functions-procedures
    method: GET
- name: integrations
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-integrations
    method: GET
- name: network_policies
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-network-policies
    method: GET
- name: notebooks
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-notebooks
    method: GET
- name: streams
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-streams
    method: GET
- name: tasks
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-tasks
    method: GET
- name: users_roles
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-user-roles
    method: GET
- name: virtual_warehouses
  endpoint:
    path: /developer-guide/snowflake-python-api/snowflake-python-managing-warehouses
    method: GET
- name: temporary_table
  endpoint:
    path: /de/user-guide/tables-temp-transient
    method: GET
- name: transient_table
  endpoint:
    path: /de/user-guide/tables-temp-transient
    method: GET
- name: accounts
  endpoint:
    path: /v1/accounts
    method: GET
- name: users
  endpoint:
    path: /v1/users
    method: GET
- name: roles
  endpoint:
    path: /v1/roles
    method: GET
- name: databases
  endpoint:
    path: /v1/databases
    method: GET
- name: tutorial1_role
  endpoint:
    path: /services/data/vXX.X/sobjects/tutorial1_role
    method: GET
    data_selector: records
    params: {}
- name: temporary_table
  endpoint:
    path: /pt/user-guide/tables-temp-transient
    method: GET
    data_selector: tabelas temporrias
- name: transient_table
  endpoint:
    path: /pt/user-guide/tables-temp-transient
    method: GET
    data_selector: tabelas transitrias
- name: accounts
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts
    method: GET
    data_selector: records
- name: accounts_view
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts_view
    method: GET
    data_selector: records
- name: hello_snowflake_package
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_package
    method: POST
    data_selector: records
    params: {}
- name: hello_snowflake_app
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_app
    method: POST
    data_selector: records
    params: {}
- name: accounts
  endpoint:
    path: app/scripts/shared_content.sql
    method: INSERT
    data_selector: records
- name: accounts_view
  endpoint:
    path: code_schema.accounts_view
    method: SELECT
    data_selector: records
- name: hello_snowflake_streamlit
  endpoint:
    path: /streamlit
    method: CREATE
    data_selector: 'NULL'
    params: {}
- name: sample_product_data
  endpoint:
    path: /services/data/vXX.X/sobjects/sample_product_data
    method: GET
    data_selector: records
    params: {}
- name: tutorial_role
  endpoint:
    path: /services/data/tutorial_role
    method: POST
    data_selector: records
    params: {}
- name: tutorial_warehouse
  endpoint:
    path: /services/data/tutorial_warehouse
    method: POST
    data_selector: records
    params: {}
- name: tutorial_image_database
  endpoint:
    path: /services/data/tutorial_image_database
    method: POST
    data_selector: records
    params: {}
- name: tutorial_image_schema
  endpoint:
    path: /services/data/tutorial_image_schema
    method: POST
    data_selector: records
    params: {}
- name: tutorial_image_repo
  endpoint:
    path: /services/data/tutorial_image_repo
    method: POST
    data_selector: records
    params: {}
- name: na_spcs_tutorial_pkg
  endpoint:
    path: /services/data/na_spcs_tutorial_pkg
    method: POST
    data_selector: response
    params: {}
- name: na_spcs_tutorial_app
  endpoint:
    path: /services/data/na_spcs_tutorial_app
    method: POST
    data_selector: response
    params: {}
- name: na_spcs_tutorial_app
  endpoint:
    path: /services/data/v1.0/applications/na_spcs_tutorial_app
    method: GET
    data_selector: records
    params: {}
- name: setup_script
  endpoint:
    path: /setup_script.sql
    method: GET
    data_selector: scripts
    params: {}
- name: snowpark_container_services
  endpoint:
    path: /en/developer-guide/native-apps/container-workflow
    method: GET
- name: setup_script
  endpoint:
    path: /en/developer-guide/native-apps/creating-setup-script
    method: GET
    data_selector: script
    params: {}
- name: application_role
  endpoint:
    path: /api/v1/application_roles
    method: POST
    data_selector: roles
    params: {}
- name: sql_commands
  endpoint:
    path: /api/v1/sql_commands
    method: GET
    data_selector: commands
    params: {}
- name: manifest
  endpoint:
    path: /create/manifest
    method: POST
    data_selector: manifest_data
- name: accounts
  endpoint:
    path: /snowflake-python/managing-accounts
    method: GET
    data_selector: records
- name: alerts
  endpoint:
    path: /snowflake-python/managing-alerts
    method: GET
    data_selector: records
- name: data_loading
  endpoint:
    path: /snowflake-python/managing-data-loading
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /snowflake-python/managing-databases
    method: GET
    data_selector: records
- name: functions
  endpoint:
    path: /snowflake-python/managing-functions-procedures
    method: GET
    data_selector: records
- name: integrations
  endpoint:
    path: /snowflake-python/managing-integrations
    method: GET
    data_selector: records
- name: network_policies
  endpoint:
    path: /snowflake-python/managing-network-policies
    method: GET
    data_selector: records
- name: notebooks
  endpoint:
    path: /snowflake-python/managing-notebooks
    method: GET
    data_selector: records
- name: streams
  endpoint:
    path: /snowflake-python/managing-streams
    method: GET
    data_selector: records
- name: tasks
  endpoint:
    path: /snowflake-python/managing-tasks
    method: GET
    data_selector: records
- name: users_roles_grants
  endpoint:
    path: /snowflake-python/managing-user-roles
    method: GET
    data_selector: records
- name: virtual_warehouses
  endpoint:
    path: /snowflake-python/managing-warehouses
    method: GET
    data_selector: records
- name: restricted_callers_rights
  endpoint:
    path: restricted_callers_rights
    method: GET
    data_selector: enabled
    params: {}
- name: restricted_features
  endpoint:
    path: restricted_features
    method: GET
    data_selector: external_data
    params: {}
- name: accounts
  endpoint:
    path: /account
    method: GET
- name: users
  endpoint:
    path: /users
    method: GET
- name: roles
  endpoint:
    path: /roles
    method: GET
- name: warehouses
  endpoint:
    path: /warehouses
    method: GET
- name: databases
  endpoint:
    path: /databases
    method: GET
- name: schemas
  endpoint:
    path: /schemas
    method: GET
- name: tables
  endpoint:
    path: /tables
    method: GET
- name: stages
  endpoint:
    path: /stages
    method: GET
- name: notebooks
  endpoint:
    path: /notebooks
    method: GET
- name: hello_snowflake_package
  endpoint:
    path: /services/data/hello_snowflake_package
    method: POST
    data_selector: artifacts
- name: hello_snowflake_app
  endpoint:
    path: /services/data/hello_snowflake_app
    method: POST
    data_selector: artifacts
- name: accounts
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts
    method: GET
    data_selector: records
    params: {}
- name: accounts_view
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts_view
    method: GET
    data_selector: records
    params: {}
- name: hello_snowflake_package
  endpoint:
    path: /services/data/v1/application_packages
    method: POST
    data_selector: package
    params: {}
- name: hello_snowflake_app
  endpoint:
    path: /services/data/v1/applications
    method: POST
    data_selector: application
    params: {}
- name: accounts
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts
    method: INSERT
    data_selector: records
    params: {}
- name: accounts_view
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts_view
    method: SELECT
    data_selector: records
    params: {}
- name: hello_snowflake_streamlit
  endpoint:
    path: /streamlit
    method: CREATE
    data_selector: code_schema.hello_snowflake_streamlit
    params: {}
- name: tutorial_role
  endpoint:
    path: /services/data/v1/snowflake/roles/tutorial_role
    method: CREATE
    data_selector: ''
    params: {}
- name: tutorial_warehouse
  endpoint:
    path: /services/data/v1/snowflake/warehouses/tutorial_warehouse
    method: CREATE
    data_selector: ''
    params:
      WAREHOUSE_SIZE: X-SMALL
      AUTO_SUSPEND: 180
      AUTO_RESUME: true
      INITIALLY_SUSPENDED: false
- name: tutorial_image_database
  endpoint:
    path: /services/data/v1/snowflake/databases/tutorial_image_database
    method: CREATE
    data_selector: ''
    params: {}
- name: tutorial_image_schema
  endpoint:
    path: /services/data/v1/snowflake/schemas/tutorial_image_schema
    method: CREATE
    data_selector: ''
    params: {}
- name: tutorial_image_repo
  endpoint:
    path: /services/data/v1/snowflake/image_repositories/tutorial_image_repo
    method: CREATE
    data_selector: ''
    params: {}
- name: na_spcs_tutorial_app
  endpoint:
    path: /services/data/vXX.X/sobjects/na_spcs_tutorial_app
    method: CALL
    data_selector: status
- name: na_spcs_tutorial_app
  endpoint:
    path: /applications/na_spcs_tutorial_app
    method: GET
    data_selector: applications
    params: {}
- name: app_public.version_init
  endpoint:
    path: /app_public/version_init
    method: POST
    data_selector: result
    params: {}
- name: app_public.service_status
  endpoint:
    path: /app_public/service_status
    method: GET
    data_selector: status
    params: {}
- name: billing_events
  endpoint:
    path: /system/functions/SYSTEM$CREATE_BILLING_EVENT
    method: POST
    data_selector: result
- name: Snowpark Container Services
  endpoint:
    path: /snowpark/container/services
    method: GET
    data_selector: services
- name: privileges
  endpoint:
    path: /requesting-auto-privs
    method: GET
    data_selector: privileges
    params: {}
- name: external_services
  endpoint:
    path: /requesting-app-specs
    method: GET
    data_selector: external_services
    params: {}
- name: compute_pools
  endpoint:
    path: /SHOW COMPUTE POOLS
    method: GET
    data_selector: compute_pool_details
    params: {}
- name: account_usage_views
  endpoint:
    path: /ACCOUNT USAGE views
    method: GET
    data_selector: usage_details
    params: {}
- name: app_service
  endpoint:
    path: /containers/service1_spec.yaml
    method: CREATE SERVICE
    data_selector: service
    params: {}
- name: setup_script
  endpoint:
    path: /create_application
    method: POST
    data_selector: setup_script
    params: {}
- name: application_roles
  endpoint:
    path: /application_roles
    method: GET
    data_selector: roles
- name: sql_commands
  endpoint:
    path: /sql_commands
    method: GET
    data_selector: commands
- name: manifest_file
  endpoint:
    path: /manifest
    method: GET
    data_selector: manifest_data
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: execute_job_service
  endpoint:
    path: /sql-reference/sql/execute-job-service
    method: POST
    data_selector: null
    params: {}
- name: get_service_status
  endpoint:
    path: /sql-reference/functions/system_get_service_status
    method: CALL
    data_selector: null
    params: {}
- name: get_service_logs
  endpoint:
    path: /sql-reference/functions/system_get_service_logs
    method: CALL
    data_selector: null
    params: {}
- name: application_package
  endpoint:
    path: /api/v1/application_package
    method: CREATE
    data_selector: package_details
    params: {}
- name: my_external_access
  endpoint:
    path: /external_access_integration
    method: POST
    data_selector: integration_details
    params:
      privileges: USAGE
      object_type: EXTERNAL ACCESS INTEGRATION
      required_at_setup: true
      register_callback: config.REGISTER_EAI_CALLBACK
      configuration_callback: config.get_config_for_ref
- name: consumer_secret
  endpoint:
    path: /secret
    method: POST
    data_selector: secret_details
    params:
      privileges: READ
      object_type: SECRET
      register_callback: config.register_my_secret
      configuration_callback: config.get_config_for_ref
- name: external_function
  endpoint:
    path: /create_external_function
    method: POST
    data_selector: CREATE_STATEMENT
- name: image_repository
  endpoint:
    path: /create-image-repository
    method: POST
    data_selector: repository
    params: {}
- name: service_specification
  endpoint:
    path: /service-specification
    method: POST
    data_selector: specification
    params: {}
- name: application
  endpoint:
    path: /en/developer-guide/native-apps/native-apps-about
    method: GET
    data_selector: applications
    params: {}
- name: streamlit_app
  endpoint:
    path: /create/streamlit
    method: POST
    data_selector: streamlit_object
    params: {}
- name: Snowflake Cortex
  endpoint:
    path: /user-guide/snowflake-cortex/aisql
    method: GET
- name: required_compute_pools
  endpoint:
    path: /services/data/vXX.X/sobjects/RequiredComputePools
    method: GET
    data_selector: records
    params: {}
- name: connections
  endpoint:
    path: /services/data/vXX.X/sobjects/Connections
    method: GET
    data_selector: records
    params: {}
- name: custom_event_billing
  endpoint:
    path: /system/create_billing_event
    method: POST
    data_selector: event
    params: {}
- name: external_proc_batch
  endpoint:
    path: /system/create_billing_events
    method: POST
    data_selector: events
    params: {}
- name: Git Repository
  endpoint:
    path: /api/git/repositories
    method: GET
    data_selector: repositories
- name: consumer_account_objects
  endpoint:
    path: /api/v1/consumer_account_objects
    method: POST
- name: compute_pool
  endpoint:
    path: /services/data/v1/compute_pool
    method: POST
    data_selector: pool
    params:
      min_nodes: 1
      max_nodes: 1
      instance_family: standard_1
      auto_resume: true
- name: database_role
  endpoint:
    path: /sql-reference/sql/alter-database-role
    method: ALTER
    data_selector: roles
    params: {}
- name: create_share
  endpoint:
    path: /sql-reference/sql/create-share
    method: CREATE
    data_selector: shares
    params: {}
- name: grant_privileges
  endpoint:
    path: /sql-reference/sql/grant-privilege-share
    method: GRANT
    data_selector: privileges
    params: {}
- name: alter_share
  endpoint:
    path: /sql-reference/sql/alter-share
    method: ALTER
    data_selector: shares
    params: {}
- name: job_service
  endpoint:
    path: /sql-reference/sql/execute-job-service
    method: POST
    data_selector: commands
    params: {}
- name: service_status
  endpoint:
    path: /sql-reference/functions/system_get_service_status
    method: CALL
    data_selector: service_status
    params: {}
- name: service_logs
  endpoint:
    path: /sql-reference/functions/system_get_service_logs
    method: CALL
    data_selector: service_logs
    params: {}
- name: consumer_event_table
  endpoint:
    path: /EVENT_LOG/PUBLIC/CONSUMER_EVENT_TABLE
    method: SELECT
    data_selector: log messages and trace events
    params:
      app_name: YOUR_APP_NAME
- name: my_external_access
  endpoint:
    path: my_external_access
    method: GET
    data_selector: network_rules
    params:
      privileges:
      - USAGE
      object_type: EXTERNAL ACCESS INTEGRATION
      required_at_setup: true
      register_callback: config.REGISTER_EAI_CALLBACK
      configuration_callback: config.get_config_for_ref
- name: consumer_secret
  endpoint:
    path: consumer_secret
    method: GET
    data_selector: credentials
    params:
      privileges:
      - READ
      object_type: SECRET
      register_callback: config.register_my_secret
      configuration_callback: config.get_config_for_ref
- name: external_data
  endpoint:
    path: /SYSTEM$SET_APPLICATION_RESTRICTED_FEATURE_ACCESS
    method: SELECT
    data_selector: allowed_cloud_providers
    params: {}
- name: application_restricted_features
  endpoint:
    path: /SYSTEM$LIST_APPLICATION_RESTRICTED_FEATURES
    method: SELECT
    data_selector: JSON
    params: {}
- name: image_repository
  endpoint:
    path: /services/data/vXX.X/sobjects/ImageRepository
    method: POST
    data_selector: records
    params: {}
- name: application
  endpoint:
    path: /applications
    method: GET
    data_selector: applications
    params: {}
- name: account_usage
  endpoint:
    path: /en/sql-reference/account-usage/query_history
    method: GET
    data_selector: records
    params: {}
- name: query_history
  endpoint:
    path: /services/data/vXX.X/query_history
    method: GET
    data_selector: records
    params: {}
- name: ACCESS_HISTORY
  endpoint:
    path: /services/data/v1/sobjects/ACCESS_HISTORY
    method: GET
    data_selector: records
- name: required_compute_pools
  endpoint:
    path: marketplace.yml
    method: GET
    data_selector: required_compute_pools
    params: {}
- name: connections
  endpoint:
    path: marketplace.yml
    method: GET
    data_selector: connections
    params: {}
- name: declarative_native_apps
  endpoint:
    path: /declarative/native/apps
    method: GET
    data_selector: data
    params: {}
- name: git_repository
  endpoint:
    path: /sql-reference/sql/create-git-repository
    method: POST
    data_selector: repository
    params: {}
- name: git_operations
  endpoint:
    path: /sql-reference/sql/alter-git-repository
    method: POST
    data_selector: operations
    params: {}
- name: create_share
  endpoint:
    path: /sql-reference/sql/create-share
    method: POST
    data_selector: records
    params: {}
- name: grant_privileges
  endpoint:
    path: /sql-reference/sql/grant-privilege-share
    method: POST
    data_selector: records
    params: {}
- name: alter_share
  endpoint:
    path: /sql-reference/sql/alter-share
    method: POST
    data_selector: records
    params: {}
- name: show_grants
  endpoint:
    path: /sql-reference/sql/show-grants
    method: GET
    data_selector: records
    params: {}
- name: event_tracing
  endpoint:
    path: /en/developer-guide/native-apps/ui-consumer-enable-logging
    method: GET
    data_selector: log_messages
    params: {}
- name: event_sharing
  endpoint:
    path: /en/developer-guide/native-apps/ui-consumer-logging-enabling
    method: GET
    data_selector: event_data
    params: {}
- name: consumer_event_table
  endpoint:
    path: /EVENT_LOG/PUBLIC/CONSUMER_EVENT_TABLE
    method: SELECT
    data_selector: log_messages_and_trace_events
    params:
      application_name: YOUR_APP_NAME
- name: app_privileges
  endpoint:
    path: /manage_privileges
    method: POST
    data_selector: privileges
    params: {}
- name: object_access
  endpoint:
    path: /authorize_access
    method: POST
    data_selector: objects
    params: {}
- name: access_history
  endpoint:
    path: /en/sql-reference/account-usage/access_history
    method: GET
- name: aggregate_access_history
  endpoint:
    path: /en/sql-reference/account-usage/aggregate_access_history
    method: GET
- name: aggregate_query_history
  endpoint:
    path: /en/sql-reference/account-usage/aggregate_query_history
    method: GET
- name: external_data
  endpoint:
    path: /SYSTEM$SET_APPLICATION_RESTRICTED_FEATURE_ACCESS
    method: SELECT
    data_selector: allowed_cloud_providers
    params: {}
- name: application_restricted_features
  endpoint:
    path: /SYSTEM$LIST_APPLICATION_RESTRICTED_FEATURES
    method: SELECT
    data_selector: features
    params: {}
- name: access_history
  endpoint:
    path: /services/data/vXX.X/sobjects/AccessHistory
    method: GET
    data_selector: records
- name: QUERY_HISTORY
  endpoint:
    path: /en/sql-reference/account-usage/query_history
    method: GET
    data_selector: records
    params: {}
- name: query_history
  endpoint:
    path: /sql-reference/account-usage/query_history
    method: GET
    data_selector: queries
    params: {}
- name: aggregate_query_history
  endpoint:
    path: /sql-reference/account-usage/aggregate_query_history
    method: GET
    data_selector: queries
    params: {}
- name: query_history
  endpoint:
    path: /account_usage/query_history
    method: GET
    data_selector: records
    params: {}
- name: access_history
  endpoint:
    path: /en/sql-reference/account-usage/access_history
    method: GET
- name: aggregate_access_history
  endpoint:
    path: /en/sql-reference/account-usage/aggregate_access_history
    method: GET
- name: aggregate_query_history
  endpoint:
    path: /en/sql-reference/account-usage/aggregate_query_history
    method: GET
- name: anomalies_daily
  endpoint:
    path: /en/sql-reference/account-usage/anomalies_daily
    method: GET
- name: application_daily_usage_history
  endpoint:
    path: /en/sql-reference/account-usage/application_daily_usage_history
    method: GET
- name: copy_history
  endpoint:
    path: /en/sql-reference/account-usage/copy_history
    method: GET
- name: access_history
  endpoint:
    path: /services/data/vXX.X/ACCESS_HISTORY
    method: GET
    data_selector: records
    params: {}
- name: query_history
  endpoint:
    path: /sql-reference/account-usage/query_history
    method: GET
    data_selector: queries
- name: aggregate_query_history
  endpoint:
    path: /sql-reference/account-usage/aggregate_query_history
    method: GET
    data_selector: queries
- name: hello_snowflake_package
  endpoint:
    path: /services/data/vXX.X/hello_snowflake_package
    method: POST
    data_selector: artifacts
- name: hello_snowflake_app
  endpoint:
    path: /services/data/vXX.X/hello_snowflake_app
    method: POST
    data_selector: artifacts
- name: accounts
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts
    method: GET
    data_selector: records
    params: {}
- name: accounts_view
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts_view
    method: GET
    data_selector: records
    params: {}
- name: hello_snowflake_package
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_package
    method: POST
    data_selector: artifacts
- name: hello_snowflake_app
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_app
    method: POST
    data_selector: artifacts
- name: accounts
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts
    method: POST
    data_selector: records
    params: {}
- name: accounts_view
  endpoint:
    path: /services/data/vXX.X/sobjects/accounts_view
    method: GET
    data_selector: records
    params: {}
- name: hello_snowflake_package
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_package
    method: POST
    data_selector: records
    params: {}
- name: hello_snowflake_app
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_app
    method: POST
    data_selector: records
    params: {}
- name: hello_snowflake_package
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_package
    method: POST
    data_selector: records
- name: hello_snowflake_app
  endpoint:
    path: /services/data/vXX.X/sobjects/hello_snowflake_app
    method: POST
    data_selector: records
- name: accounts
  endpoint:
    path: /shared_data/accounts
    method: INSERT
    data_selector: records
    params: {}
- name: accounts_view
  endpoint:
    path: /code_schema/accounts_view
    method: SELECT
    data_selector: records
    params: {}
- name: hello_snowflake_streamlit
  endpoint:
    path: /streamlit
    method: CREATE
    data_selector: 'NULL'
    params: {}
- name: accounts
  endpoint:
    path: /shared_data/accounts
    method: INSERT
    data_selector: values
    params: {}
- name: accounts_view
  endpoint:
    path: /code_schema/accounts_view
    method: SELECT
    data_selector: queried_data
    params: {}
- name: hello_snowflake_streamlit
  endpoint:
    path: /streamlit
    method: CREATE
    data_selector: hello_snowflake_streamlit
- name: application
  endpoint:
    path: /sql/create-application
    method: POST
- name: application_package
  endpoint:
    path: /sql/create-application-package
    method: POST
- name: application_role
  endpoint:
    path: /sql/create-application-role
    method: POST
- name: app_specification
  endpoint:
    path: /sql/alter-application-set-app-spec
    method: POST
- name: additional_commands
  endpoint:
    path: /sql/create-versioned-schema
    method: POST
- name: Application
  endpoint:
    path: /en/sql-reference/sql/create-application
    method: POST
- name: Application Package
  endpoint:
    path: /en/sql-reference/sql/create-application-package
    method: POST
- name: Application Roles
  endpoint:
    path: /en/sql-reference/sql/create-application-role
    method: POST
- name: App Specification
  endpoint:
    path: /en/sql-reference/sql/alter-application-set-app-spec
    method: POST
- name: application_package
  endpoint:
    path: /api/v1/application/packages
    method: POST
    data_selector: applicationPackage
    params: {}
- name: consumer_application
  endpoint:
    path: /api/v1/consumer/applications
    method: GET
    data_selector: consumerApplications
    params: {}
- name: application_package
  endpoint:
    path: /api/v1/application_package
    method: POST
    data_selector: application
    params: {}
- name: manifest_file
  endpoint:
    path: /api/v1/manifest_file
    method: GET
    data_selector: manifest
    params: {}
- name: application_package
  endpoint:
    path: /native-apps/application-package
    method: POST
    data_selector: package
    params: {}
- name: consumer_application
  endpoint:
    path: /native-apps/consumer-application
    method: GET
    data_selector: application
    params: {}
- name: Numeric data types
  endpoint:
    path: /en/sql-reference/data-types-numeric
    method: GET
    data_selector: data types
    params: {}
- name: String & binary data types
  endpoint:
    path: /en/sql-reference/data-types-text
    method: GET
    data_selector: data types
    params: {}
- name: Logical data types
  endpoint:
    path: /en/sql-reference/data-types-logical
    method: GET
    data_selector: data types
    params: {}
- name: Date & time data types
  endpoint:
    path: /en/sql-reference/data-types-datetime
    method: GET
    data_selector: data types
    params: {}
- name: Semi-structured data types
  endpoint:
    path: /en/sql-reference/data-types-semistructured
    method: GET
    data_selector: data types
    params: {}
- name: Structured data types
  endpoint:
    path: /en/sql-reference/data-types-structured
    method: GET
    data_selector: data types
    params: {}
- name: Unstructured data types
  endpoint:
    path: /en/sql-reference/data-types-unstructured
    method: GET
    data_selector: data types
    params: {}
- name: Geospatial data types
  endpoint:
    path: /en/sql-reference/data-types-geospatial
    method: GET
    data_selector: data types
    params: {}
- name: Vector data types
  endpoint:
    path: /en/sql-reference/data-types-vector
    method: GET
    data_selector: data types
    params: {}
- name: Numeric data types
  endpoint:
    path: /en/sql-reference/data-types-numeric
    method: GET
    data_selector: records
    params: {}
- name: String & binary data types
  endpoint:
    path: /en/sql-reference/data-types-text
    method: GET
    data_selector: records
    params: {}
- name: Logical data types
  endpoint:
    path: /en/sql-reference/data-types-logical
    method: GET
    data_selector: records
    params: {}
- name: Date & time data types
  endpoint:
    path: /en/sql-reference/data-types-datetime
    method: GET
    data_selector: records
    params: {}
- name: Semi-structured data types
  endpoint:
    path: /en/sql-reference/data-types-semistructured
    method: GET
    data_selector: records
    params: {}
- name: Structured data types
  endpoint:
    path: /en/sql-reference/data-types-structured
    method: GET
    data_selector: records
    params: {}
- name: Unstructured data types
  endpoint:
    path: /en/sql-reference/data-types-unstructured
    method: GET
    data_selector: records
    params: {}
- name: Geospatial data types
  endpoint:
    path: /en/sql-reference/data-types-geospatial
    method: GET
    data_selector: records
    params: {}
- name: Vector data types
  endpoint:
    path: /en/sql-reference/data-types-vector
    method: GET
    data_selector: records
    params: {}
- name: Unsupported data types
  endpoint:
    path: /en/sql-reference/data-types-unsupported
    method: GET
    data_selector: records
    params: {}
- name: Conversion data types
  endpoint:
    path: /en/sql-reference/data-type-conversion
    method: GET
    data_selector: records
    params: {}
- name: fixed_point_numbers
  endpoint:
    path: /api/fixed_point_numbers
    method: GET
    data_selector: data
    params: {}
- name: floating_point_numbers
  endpoint:
    path: /api/floating_point_numbers
    method: GET
    data_selector: data
    params: {}
notes:
- Limitations of the SQL API for types of statements that are not supported.
- Snowflake provides several different methods to load data into Snowflake, such as
  by using Snowpipe, loading from cloud storage, or uploading files using Snowsight.
- Snowpark allows running Python, Java, and Scala code directly in Snowflake.
- Uses OAuth2 with refresh token  requires setup of connected app in Snowflake
- After completing the configuration to use private connectivity, access Snowsight
  using the specified URLs.
- Uses OAuth2 with refresh token  requires setup of connected app in Snowflake.
- Some objects may return nulls in deeply nested fields.
- Requires a Snowflake account and a user with the required roles (ACCOUNTADMIN and
  SYSADMIN).
- Trial account user has the required roles and a virtual warehouse (`compute_wh`).
- Snowflake supports multiple ways for you to connect to the service.
- The cloud platform you choose for each Snowflake account is completely independent
  from your other Snowflake accounts.
- Some data transfer billing charges may apply when loading data from files staged
  across different platforms.
- The Snowflake Edition that your organization chooses determines the unit costs for
  the credits and the data storage you use.
- 'On Demand: Usage-based pricing with no long-term licensing requirements.'
- 'Capacity: Discounted pricing based on an upfront Capacity commitment.'
- Snowflake deploys new releases each week.
- Users experience no downtime or disruption of service.
- All communication between clients, including all Snowflake connectors and drivers,
  and the server is protected through TLS.
- Snowflake is committed to meeting industry-standard regulatory compliance requirements.
- Support for PHI data (in compliance with HIPAA and HITRUST CSF regulations)  requires
  Business Critical Edition (or higher).
- Automatic data encryption by Snowflake using Snowflake-managed keys.
- Snowflake Time Travel (1 day standard for all accounts; additional days, up to 90,
  allowed with Snowflake Enterprise) for querying historical data in tables.
- Snowflake Fail-safe (7 days standard for all accounts) for disaster recovery of
  historical data.
- Snowflake bills a minimal amount for the on-disk storage used for the sample data
  in this tutorial.
- The tutorial provides steps to drop the database and minimize storage cost.
- A CSV file consists of 1 or more records, with 1 or more fields in each record,
  and sometimes a header record.
- Records and fields in each file are separated by delimiters.
- A running virtual warehouse consumes Snowflake credits.
- This tutorial is only available to users with a trial account.
- The COPY command skips any file containing an error and moves on to loading the
  next file.
- Files are staged to load into tables.
- Use file format option error_on_column_count_mismatch=false to ignore column count
  mismatch errors
- The COPY command skips files on data errors.
- Congratulations, you have successfully completed the tutorial.
- Ensure the Snowflake account is configured to use Amazon AWS.
- Use the COPY INTO command to load data into the target table.
- Snowflake does not enforce the primary key constraint. Rather, the constraint serves
  as metadata that identifies the natural key in the Information Schema.
- Temporary tables persist only for the duration of the user session and is not visible
  to other users.
- Temporary stages are automatically dropped at the end of the session.
- Uses COPY INTO to load data from staged files.
- Files are loaded into temporary tables that persist only for the duration of the
  user session.
- The sample database is read-only.
- Fix the errors in the records manually in the contacts3.csv file in your local environment.
- After you verify that you successfully copied data from your stage into the tables,
  you can remove data files from the internal stage.
- TPC-H consists of a suite of business-oriented ad hoc queries and concurrent data
  modifications.
- The queries and the data populating the database have been chosen to have broad
  industry-wide relevance.
- Uses COPY command to load data from S3 into Snowflake tables.
- Snowflake
- Snowflake supports using folders concept to organize a bucket.
- Reduce the number of files in each directory improves performance of COPY statements.
- Horizon Catalog provides a built-in set of compliance, security, privacy, discovery,
  and collaboration capabilities.
- Snowsight provides a unified experience for working with your Snowflake data by
  using SQL or Python.
- Temporary tables persist only for the duration of the user session and are not visible
  to other users.
- Some accounts only have access to specific features depending on the configuration.
- SnowSQL is a legacy command line client for connecting to Snowflake to execute SQL
  queries and perform all DDL and DML operations.
- Snowflake recommends that you begin transitioning from SnowSQL to Snowflake CLI.
- To use the Snowpark Python features of the VS Code extension, you must enable the
  extension setting Enable Public Preview Features.
- Snowflake Native App application object status is not available in Snowflake CLI
  version 3.0.0.
- The Snowflake Terraform provider is not supported in Microsoft Azure Government
  regions.
- Official Snowflake Support began exclusively with version 2.0.0 and later.
- Assurez-vous de remplir les prrequis pour utiliser nos applications qui se connectent
   Snowflake.
- Das untersttzende Schema enthlt wichtige Geschftsinformationen wie Kunden-, Auftrags-
  und Produktdaten.
- The term client as used in this article refers to any custom or third-party application
  using a Snowflake command-line client or API.
- Horizon Catalog lets organizations discover and govern data, apps, and models through
  a built-in set of compliance, security, privacy, discovery, and collaboration capabilities.
- Snowflake recommends you limit the size of query text (i.e. SQL statements) submitted
  through Snowflake clients to 1 MB per statement.
- This limit includes any literals, such as string literals or binary literals, that
  are part of the statement, whether as part of a WHERE clause, SET clause (in an
  UPDATE statement), etc.
- Some accounts only have access to Snowsight, and no longer have access to Classic
  Console.
- SnowSQL is a legacy command line client for connecting to Snowflake to execute SQL
  queries and perform all DDL and DML operations, including loading data into and
  unloading data out of database tables.
- Requires setup of OAuth2 with refresh token for secure access
- To use Snowpark Python features of the VS Code extension, you must enable the extension
  setting Enable Public Preview Features.
- Snowflake recommends upgrading to the most recent version of the Snowflake CLI.
- Warehouses are required for queries, as well as all DML operations, including loading
  data into tables.
- Openflow uses OAuth2 for authentication to Snowflake.
- Iceberg tables store their data and metadata files in an external cloud storage
  location.
- Snowflake does not provide Fail-safe storage for Iceberg tables.
- You must specify your Snowflake account identifier.
- You might need to specify the warehouse, database, schema, and role.
- Iceberg tables are available for all Snowflake accounts, on all cloud platforms
  and in all regions.
- Cross-cloud/cross-region tables are supported.
- Not available in government regions.
- Credential vending for internal catalogs is enabled by default when you create the
  catalog.
- Data can be loaded into Snowflake in a number of ways.
- Limit the size of query text to 1 MB per statement.
- Larger queries process normally but cannot be retried if they exceed the limit.
- Dynamic tables automatically refresh based on a defined query and target freshness.
- Combining tasks with table streams is a convenient and powerful way to continuously
  process new or changed data.
- A virtual warehouse must be running and in use for the session.
- While a warehouse is running, it consumes Snowflake credits.
- Only dbt Core projects are supported. dbt Cloud projects arent supported. dbt Projects
  on Snowflake runs dbt-core version 1.9.4 and dbt-snowflake version 1.9.2.
- Snowflake supports bulk unloading of data from a database table into flat, delimited
  text files.
- Storage lifecycle policies help optimize costs by automatically moving older data
  to more cost-effective archival tiers.
- You can archive data for a specific time before expiration, or expire it directly
  without archiving.
- Openflow uses industry-leading security features that help ensure you have the highest
  levels of security for your account, and users, and all the data you store in Snowflake.
- Easy-to-use interfaces
- Automated code conversion
- Accelerated migration timelines
- Path to leverage Snowflakes modern data platform capabilities
- To use listings and the Snowflake Marketplace, you need to agree to additional terms.
- Iceberg tables support Apache Parquet files.
- Snowflake supports Delta reader version 2 and can read all tables written by engines
  using Delta Lake version 2.2.0.
- Automated refresh is enabled by default for catalog-linked databases.
- Data can be shared using listings or direct shares.
- Clean rooms allow controlled access to shared data.
- Open Catalog is free for the first six months after general availability. Billing
  will begin in the second half of 2025.
- Snowflake offers a suite of AI features that use large language models to understand
  unstructured data.
- Snowflake ML provides functionality for you to build your own models.
- Dynamic tables simplify data transformation and pipeline management without requiring
  manual updates or custom scheduling.
- Dynamic tables use incremental processing for workloads that support it.
- Snowflake provides industry-leading features that maintain the privacy of individuals
  and sensitive data.
- A dbt project in a workspace cant have more than 20,000 files in its folder structure.
- Learn about organizations, which link the accounts owned by your business entity.
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Replication and failover/failback requires Business Critical (or higher).
- Storage lifecycle policies automatically manage data lifecycle for standard Snowflake
  tables.
- Snowflake executes these policies daily using shared compute resources.
- Both tools offer easy-to-use interfaces, automated code conversion, and accelerated
  migration timelines.
- You can share data with people who dont have Snowflake accounts by using Reader
  Accounts.
- All AI models run inside of Snowflakes security and governance perimeter.
- Snowflake never uses your Customer Data to train models made available to our customer
  base.
- Data Quality Monitoring and data metric functions allows the monitoring of the state
  and integrity of your data using system data metric functions and user-defined data
  metric functions.
- Column-level Security allows the application of a masking policy to a column within
  a table or view.
- Row-level Security allows the application of a row access policy to a table or view
  to determine which rows are visible in the query result.
- Introduction to object tagging allows the tracking of sensitive data for compliance,
  discovery, protection, and resource usage.
- Tag-based masking policies allows protecting column data by assigning a masking
  policy to a tag and then setting the tag on a database object or the Snowflake account.
- Sensitive data classification allows categorizing potentially personal and/or sensitive
  data to support compliance and privacy regulations.
- Access History allows the auditing of the user access history through the Account
  Usage ACCESS_HISTORY view.
- Object Dependencies allows the auditing of how one object references another object
  by its metadata.
- Para obter informaes sobre como configurar clientes, drivers, bibliotecas e aplicativos
  de terceiros para se conectar ao Snowflake, consulte Configurao de um cliente,
  driver, biblioteca ou aplicativo de terceiros para se conectar ao Snowflake
- Trial accounts cannot be canceled through the web interface. To cancel an account,
  you must contact Snowflake Support.
- Replication and failover/failback features require Business Critical (or higher)
  plan.
- Snowflake customers never directly create an organization; it's automatically created
  upon account creation.
- Snowflake provides a robust framework to manage costs.
- You can also obtain monthly usage statements and reconcile those statements with
  usage data in views.
- Using the ORGADMIN role in an ORGADMIN-enabled account is being phased out for multi-account
  organizations.
- , ,      Snowflake       , ,
       Snowflake    .
- Para obter informaes sobre como configurar clientes, drivers, bibliotecas e aplicativos
  de terceiros para se conectar ao Snowflake, consulte [Configurao de um cliente,
  driver, biblioteca ou aplicativo de terceiros para se conectar ao Snowflake](user-guide/gen-conn-config).
- Users can create additional accounts after the organization is created.
- Using the GLOBALORGADMIN role in the organization account to perform organization-level
  tasks is recommended.
- This feature requires Enterprise Edition (or higher).
- Creating the organization account incurs additional costs.
- Snowflake will send a notification email to customers at least three months prior
  to phasing out the ORGADMIN role.
- Available to the organization account, which requires Enterprise Edition or higher.
- Account administrators can only import organization user groups if the organization
  administrator has set the visibility of the group.
- Requires Enterprise Edition or higher
- Account administrators cannot add organization users directly to their regular account.
- Using the legacy account locator in an account identifier or account URL is still
  supported, though discouraged.
- Account identifiers are required in Snowflake wherever you need to specify the account
  you are using.
- The preferred account identifier consists of the name of the account prefixed by
  its organization.
- To override default values at the account level, you must be an account administrator
  (i.e. user granted the ACCOUNTADMIN role).
- Default values for session and object parameters can be overridden at each level
  in the parameter hierarchy.
- An organization administrator manages the lifecycle of every account that belongs
  to the organization.
- Administrators can create and manage Snowflake users through SQL or the web interface.
- Using private connectivity requires updating DNS records to include the private
  connectivity URL.
- The preferred method for identifying accounts is the account name within your organization.
- Dropping a user removes the user credentials from Snowflake.
- If a dropped users worksheets do not have sharing enabled, an administrator can
  recover up to 500 worksheets owned by the user.
- If private connectivity to the Snowflake service is enabled for your account, run
  the SYSTEM$GET_PRIVATELINK_CONFIG function to determine the private connectivity
  URL to use.
- The monthly costs for storing data in Snowflake is based on a flat rate per terabyte
  (TB).
- Snowflake automatically compresses all data stored in tables and uses the compressed
  file size to calculate the total storage used for an account.
- Users require a login name to log into Snowflake; if you dont explicitly provide
  a login name, Snowflake uses their user name as the login name.
- Requires setup of user roles and permissions in Snowflake
- Warehouses are only billed for credit usage while running.
- Charges for serverless features are calculated based on total usage of snowflake-managed
  compute resources measured in compute-hours.
- Functions not marked as preview features are generally available (GA) and can be
  used in production.
- Cortex AISQL Functions are optimized for throughput.
- Access to a model (whether by allowlist or RBAC) does not always mean that it can
  be used. It may still be subject to cross-region, deprecation, or other availability
  constraints.
- Model access controls only govern use of a model, and not the use of a feature itself,
  which may have its own access controls.
- Not all features support model access controls.
- Secondary roles can obscure permissions.
- Keep in mind that qualified model object identifiers are quoted and therefore case-sensitive.
- On-demand Snowflake accounts without a valid payment method are limited to 10 credits
  per day for Snowflake Cortex AISQL usage.
- Cortex AISQL functions can be grouped into AISQL functions and Helper functions.
- Cortex Guard is designed to filter possible unsafe and harmful responses.
- Access to a model (whether by allowlist or RBAC) does not always mean that it can
  be used.
- Model access controls only govern use of a model, and not the use of a feature itself.
- Cortex AISQL supports unstructured analytics on text and images with industry-leading
  LLMs.
- Functions like AI_COMPLETE and AI_CLASSIFY are purpose-built managed functions.
- Individual functions in the Cortex AISQL suite are Preview Features.
- Snowflake Cortex AI functions incur compute cost based on the number of tokens processed.
- You cant get granular usage information for requests made with the REST API.
- Updates to billing contact information might take several minutes to process.
- Pending updates appear as In-progress in Snowsight.
- The advanced chat-style (multi-message) form of COMPLETE is not currently supported
  in Snowflake ML Python.
- The advanced chat-style (multi-message) form of COMPLETE is not currently supported
  in Snowflake CLI.
- You can create secure access to specific network locations external to Snowflake.
- If you choose to use private connectivity, your Snowflake account must be Business
  Critical Edition (or later).
- Hybrid tables are optimized for low latency and high throughput using index-based
  random reads and writes.
- Hybrid tables typically have a larger storage footprint than standard tables.
- You can enable this access through an external access integration.
- Hybrid tables integrate seamlessly into the existing Snowflake architecture.
- Hybrid tables allow for faster results on short-running operational queries.
- Outbound private connectivity is available for various Snowflake features
- Each feature has its own prerequisites and configuration procedures
- Snowpark Container Services is available to accounts in AWS, Microsoft Azure, and
  Google Cloud Platform commercial regions.
- Private connectivity requires Business Critical or higher.
- Credits are billed per second, with a 60-second minimum.
- Aucun paiement requis pour le compte d'essai
- La priode dessai dure 30 jours ou jusqu puisement du solde dutilisation gratuite
- Die Testphase dauert 30 Tage (ab dem Anmeldedatum) oder bis Sie Ihre kostenlosen
  Credits aufgebraucht haben, je nachdem, was zuerst eintritt.
- Um ein gesperrtes Testkonto wieder zu aktivieren, mssen Sie eine Kreditkarte hinzufgen,
  die es in ein bezahltes Konto verwandelt.
- Aucune information de paiement ou autre information obligatoire nest requise.
- La priode dessai dure 30 jours.
- Evaluation accounts can be used for 30 days or until the free usage balance is exhausted.
- Contas de avaliao no podem ser canceladas por meio da interface da Web. Para
  cancelar uma conta, voc deve entrar em contato com o suporte Snowflake.
- Testkonten ohne gltige Zahlungsmethode sind auf etwa zehn Credits der Nutzung pro
  Tag von Snowflake Cortex-AISQL-Funktionen beschrnkt.
-    Snowflake        .
-         .
- A avaliao continua por 30 dias (a partir da data de inscrio) ou at que voc
  tenha esgotado seu saldo de uso gratuito, o que ocorrer primeiro.
- Adicionar um carto de crdito a uma conta de avaliao a converte em uma conta
  paga sem encerrar o perodo de avaliao.
- 'Records and fields in each file are separated by delimiters. The default delimiters
  are: Records: newline characters, Fields: commas.'
- There is a direct correlation between the fields in the files and the columns in
  the table you will be loading.
- The PUT command compresses files by default using gzip.
- The tutorial assumes you unpacked files into one of the specified directories.
- The command doesnt require a running warehouse to execute.
- The command only returns objects for which the current users current role has been
  granted at least one access privilege.
- Creating a database automatically sets it as the active/current database for the
  current session (equivalent to using the USE DATABASE command for the database).
- If a database with the same name already exists, an error is returned and the database
  is not created, unless the optional OR REPLACE keyword is specified in the command.
- Using OR REPLACE is the equivalent of using DROP DATABASE on the existing database
  and then creating a new database with the same name; however, the dropped database
  is not permanently removed from the system. Instead, it is retained in Time Travel.
- 'Databases created from shares differ from standard databases in the following ways:
  They do not have the PUBLIC or INFORMATION_SCHEMA schemas unless these schemas were
  explicitly granted to the share. They cannot be cloned. Properties, such as TRANSIENT
  and DATA_RETENTION_TIME_IN_DAYS, do not apply.'
- Database replication uses Snowflake-provided compute resources instead of your own
  virtual warehouse to copy objects and data.
- We recommend increasing the STATEMENT_TIMEOUT_IN_SECONDS value to 604800 (7 days,
  the maximum value) for the session in which you run the replication operation.
- Best practice for Database Replication and Failover recommends giving each secondary
  database the same name as its primary database.
- If the statement is replacing an existing table of the same name, then the grants
  are copied from the table being replaced.
- If you directly clone a table, any streams on that table are not cloned.
- Creating a database automatically sets it as the active/current database for the
  current session.
- Using OR REPLACE is equivalent to using DROP DATABASE on the existing database and
  then creating a new database with the same name; however, the dropped database is
  retained in Time Travel.
- Databases created from shares do not have the PUBLIC or INFORMATION_SCHEMA schemas
  unless these schemas were explicitly granted to the share.
- Specifies that the table persists only for the duration of the session that you
  created it in.
- If a table is not declared as TEMPORARY or TRANSIENT, the table is permanent.
- If you created a temporary table with the same name as another table in the schema,
  all queries and operations used on the table only affect the temporary table in
  the session, until you drop the temporary table.
- The default value for STATEMENT_TIMEOUT_IN_SECONDS is 172800 (2 days).
- It is recommended to increase STATEMENT_TIMEOUT_IN_SECONDS to 604800 (7 days) for
  the session when running replication operations.
- A schema cannot contain tables and/or views with the same name.
- Using OR REPLACE is the equivalent of using DROP TABLE on the existing table and
  then creating a new table with the same name; however, the dropped table is not
  permanently removed from the system.
- CREATE OR REPLACE *<object>* statements are atomic.
- Recreating or swapping a table drops its change data.
- The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive.
- Currently only supports permanent, temporary, and transient tables.
- Setting or unsetting a tag or policy on a table or column using a CREATE OR ALTER
  TABLE statement is not supported.
- Creates a new table in the current/specified schema, replaces an existing table,
  or alters an existing table.
- CREATE TABLE  LIKE for a table with an auto-increment sequence accessed through
  a data share is currently not supported.
- Some objects may return nulls in deeply nested fields
- Transient tables have a lower level of data protection than permanent tables, meaning
  that data in a transient table might be lost in the event of a system failure.
- Initial creation of a virtual warehouse might take some time to provision the compute
  resources, unless the warehouse is created initially in a SUSPENDED state.
- Using `OR REPLACE` is the equivalent of using DROP TABLE on the existing table and
  then creating a new table with the same name; however, the dropped table is not
  permanently removed from the system.
- The `OR REPLACE` and `IF NOT EXISTS` clauses are mutually exclusive.
- ANSI-reserved function names cannot be used as column names.
- Using this command requires the OWNERSHIP privilege on the source table.
- Filtering results using the WHERE clause helps you minimize costs by ensuring that
  Snowflake reads only the data that you require from archival storage.
- When change tracking is enabled, the table is locked for the duration of the operation.
- If the source table has clustering keys, then the new table has clustering keys.
- Creating a table with a masking policy on one or more table columns requires the
  POLICY_CONTEXT function.
- New columns can only be added to the end of the column list.
- The CREATE OR ALTER TABLE command currently does not guarantee atomicity.
- Creating a virtual warehouse automatically sets it as the warehouse in use for the
  current session.
- Using `OR REPLACE` is the equivalent of using [DROP WAREHOUSE](drop-warehouse) on
  the existing warehouse and then creating a new warehouse with the same name.
- PUT does not support uploading files onto an external stage.
- The command cannot be executed from the Worksheets page in either Snowflake web
  interface.
- To run this command with an external stage that uses a storage integration, you
  must use a role that has or inherits the USAGE privilege on the storage integration.
- 'Files must already be in one of the specified locations: Named internal stage,
  Named external stage, or External location.'
- PUT does not support uploading files onto an external stage. To upload files to
  an external stage, use the utilities provided by your cloud service.
- All files stored on internal stages for data loading and unloading operations are
  automatically encrypted using AES-256 strong encryption on the server side.
- Required only for loading from an external private/protected cloud storage location;
  not required for public buckets/containers
- We highly recommend the use of storage integrations. This option avoids the need
  to supply cloud storage credentials using the CREDENTIALS parameter when creating
  stages or loading data.
- Compression algorithm detected automatically, except for Brotli-compressed files,
  which cannot currently be detected automatically.
- Default compression is AUTO.
- Default date format is AUTO.
- Default time format is AUTO.
- Default timestamp format is AUTO.
- Default binary format is HEX.
- Default TRIM_SPACE is FALSE.
- Default MULTI_LINE is TRUE.
- Default NULL_IF is '\N'.
- Default ENABLE_OCTAL is FALSE.
- Default ALLOW_DUPLICATE is FALSE.
- Default STRIP_OUTER_ARRAY is FALSE.
- Default STRIP_NULL_VALUES is FALSE.
- Default REPLACE_INVALID_CHARACTERS is FALSE.
- Default IGNORE_UTF8_ERRORS is FALSE.
- Default SKIP_BYTE_ORDER_MARK is TRUE.
- Supported when the FROM value in the COPY statement is an external storage URI rather
  than an external stage name.
- Required only for loading from an external private/protected cloud storage location;
  not required for public buckets/containers.
- COPY commands contain complex syntax and sensitive information, such as credentials.
  In addition, they are executed frequently and are often stored in scripts or worksheets,
  which could lead to sensitive information being inadvertently exposed. The COPY
  command allows permanent (aka long-term) credentials to be used; however, for
  security reasons, do not use permanent credentials in COPY commands.
- If you must use permanent credentials, use external stages, for which credentials
  are entered once and securely stored, minimizing the potential for exposure.
- Snowflake recommends that you set BINARY_AS_TEXT to FALSE to avoid any potential
  conversion issues.
- Default for CLUSTER_AT_INGEST_TIME is FALSE
- Default for ENFORCE_LENGTH is TRUE
- Default for FORCE is FALSE
- Default for LOAD_MODE is FULL_INGEST
- Default for LOAD_UNCERTAIN_FILES is FALSE
- Default for MATCH_BY_COLUMN_NAME is NONE
- Default for ON_ERROR is ABORT_STATEMENT
- Default for PURGE is FALSE
- Default for RETURN_FAILED_ONLY is FALSE
- Default for TRUNCATECOLUMNS is FALSE
- Default trim space is FALSE.
- Default multi line is TRUE.
- Default null if is \N.
- Default enable octal is FALSE.
- Default allow duplicate is FALSE.
- Default strip outer array is FALSE.
- Default strip null values is FALSE.
- Default replace invalid characters is FALSE.
- Default ignore UTF8 errors is FALSE.
- Default skip byte order mark is TRUE.
- 'Some use cases are not fully supported and can lead to inconsistent or unexpected
  ON_ERROR behavior, including the following use cases: Specifying the DISTINCT keyword
  in SELECT statements. Using COPY with clustered tables.'
- A COPY job fails if Snowflake encounters an error on a partition transform, even
  if youve set ON_ERROR = CONTINUE.
- LOAD_MODE = ADD_FILES_COPY is not supported.
- When you load CSV data, if a stream is on the target table, the ON_ERROR copy option
  might not work as expected.
- The list of objects returned for an external stage might include one or more 'directory
  blobs'; essentially, paths that end in a forward slash character ('/').
- COPY statements that reference a stage can fail when the object list includes directory
  blobs. To avoid errors, we recommend using file pattern matching to identify the
  files for inclusion (i.e. the PATTERN clause) when the file list for a stage includes
  directory blobs.
- The VALIDATE function only returns output for COPY commands used to perform standard
  data loading; it does not support COPY commands that perform transformations during
  data loading.
- Unless you explicitly specify FORCE = TRUE as one of the copy options, the command
  ignores staged data files that were already loaded into the table. To reload the
  data, you must either specify FORCE = TRUE or modify the file and stage it again,
  which generates a new checksum.
- The COPY command does not validate data type conversions for Parquet files.
- VALIDATION_MODE isnt supported for Iceberg tables.
- Specifies LOAD_MODE = ADD_FILES_COPY, which tells Snowflake to copy the files into
  your external volume location.
- When using the pre-clustering feature, ensure that you do not disable the auto-clustering
  feature on the destination table. Disabling auto-clustering can lead to degraded
  query performance over time.
- This copy option doesnt work with MATCH_BY_COLUMN_NAME.
- 'Some use cases are not fully supported and can lead to inconsistent or unexpected
  ON_ERROR behavior, including the following use cases: Specifying the DISTINCT keyword
  in SELECT statements, Using COPY with clustered tables.'
- Unless you explicitly specify FORCE = TRUE as one of the copy options, the command
  ignores staged data files that were already loaded into the table.
- Default file format options include AUTO for COMPRESSION, comma for FIELD_DELIMITER,
  new line character for RECORD_DELIMITER, TRUE for MULTI_LINE, FALSE for PARSE_HEADER,
  0 for SKIP_HEADER, FALSE for SKIP_BLANK_LINES, AUTO for DATE_FORMAT, AUTO for TIME_FORMAT,
  AUTO for TIMESTAMP_FORMAT, HEX for BINARY_FORMAT, NONE for ESCAPE, backslash for
  ESCAPE_UNENCLOSED_FIELD, FALSE for TRIM_SPACE, NONE for FIELD_OPTIONALLY_ENCLOSED_BY,
  \N for NULL_IF, TRUE for ERROR_ON_COLUMN_COUNT_MISMATCH, FALSE for REPLACE_INVALID_CHARACTERS,
  TRUE for EMPTY_FIELD_AS_NULL, TRUE for SKIP_BYTE_ORDER_MARK.
- Uses Iceberg-compatible Parquet data
- Snowflake stores all data internally in the UTF-8 character set. The data is converted
  into UTF-8 before it is loaded into Snowflake.
- Creates a named file format that describes a set of staged data to access or load
  into Snowflake tables.
- Creates a new named file format if it doesnt already exist, or transforms an existing
  file format into the one defined in the statement.
- 'Caution: Recreating a file format (using CREATE OR REPLACE FILE FORMAT) breaks
  the association between the file format and any external table that references it.'
- Best practices, general guidelines, and important considerations for bulk data loading.
- Snowflake supports most of the standard operators defined in SQL:1999.
- When loading data, specifies the current compression algorithm for the data file.
- Boolean that specifies whether to remove leading and trailing white space from strings.
- Boolean that specifies whether to replace invalid UTF-8 characters with the Unicode
  replacement character.
- 'Caution: Recreating a file format breaks the association between the file format
  and any external table that references it.'
- Specifies the target table into which to insert rows.
- The VALUES clause is limited to 16,384 rows.
- Uses INSERT with OVERWRITE to rebuild the sf_employees table from employees after
  new records were added.
- NULL does not match NULL. In other words, if the subject is NULL and the pattern
  is NULL, that is not considered a match.
- 'SQL wildcards are supported in `pattern`: An underscore (`_`) matches any single
  character. A percent sign (`%`) matches any sequence of zero or more characters.'
- Wildcards in `pattern` include newline characters (`n`) in `subject` as matches.
- Pattern matching covers the entire string. To match a sequence anywhere within a
  string, start and end the pattern with `%`.
- There is no default escape character.
- When date_or_time_part is year, quarter, or month, the result day of the month might
  be different from the original day.
- You must have a Snowflake account and a user with the required roles (ACCOUNTADMIN
  and SYSADMIN) and access to a virtual warehouse.
- Using a single INSERT command, you can insert multiple rows into a table by specifying
  additional sets of values separated by commas in the VALUES clause.
- Snowflake requires an active virtual warehouse to load data and execute queries.
- The tutorial assumes you have downloaded and unzipped the sample data files.
- NULL does not match NULL.
- SQL wildcards are supported in `pattern`.
- Snowflake bentigt ein virtuelles Warehouse zum Laden und Abfragen von Daten.
- Beispieldatendateien im CSV-Format werden verwendet.
- When `date_or_time_part` is `year`, `quarter`, or `month`, the result day of the
  month might be different from the original day.
- Ensure proper permissions are set for API access
- Snowflake
- To explore the tutorials, a Snowflake account and specific user roles are required.
- Snowflake           .
-         .
- Snowflake charges a minimal amount for disk storage used for sample data.
- Snowflake requer um warehouse virtual para carregar os dados e executar consultas.
- O warehouse  inicialmente suspenso, mas a instruo DML tambm define AUTO_RESUME
  = true.
- Um arquivo CSV consiste em 1 ou mais registros, com 1 ou mais campos em cada registro,
  e s vezes um registro de cabealho.
- Os registros e campos de cada arquivo so separados por delimitadores.
- Snowflake bentigt ein virtuelles Warehouse, um die Daten laden und Abfragen ausfhren
  zu knnen.
- Ein aktives virtuelles Warehouse verbraucht Snowflake-Credits.
- Time Travel is automatically enabled with the standard, 1-day retention period.
- Changing the retention period may have unintended consequences.
- If the TIMESTAMP, OFFSET, or STATEMENT specified in the AT | BEFORE clause falls
  outside the data retention period for the table, the query fails and returns an
  error.
- The cloning operation for a database or schema fails if the specified Time Travel
  time is beyond the retention time of any current child of the entity.
- Restoring tables and schemas is only supported in the current schema or current
  database.
- Replication of some databases is not supported or might fail the refresh operation.
- Requires Business Critical Edition (or higher) for certain features.
- Snowflake
- 
- Requires Business Critical Edition (or higher).
- Refresh operations fail if the primary database includes a stream with an unsupported
  source object.
- COPY 
- You can only replicate the secret using a replication or failover group.
- 'If you replicate the integration first and then the secret, the operation is successful:
  all objects are replicated and there are no dangling references.'
- If you replicate the secret before the integration and the secret does not already
  exist in the target account, a 'placeholder secret' is added in the target account
  to prevent a dangling reference.
- When you decide to failover to make account1 as the source account, the secret and
  integration references match and the placeholder secret is not used.
- Data loss can occur when the most recent refresh operation for a secondary database
  is not completed prior to the failover operation.
-      30      .
- CSV  1  ,   1   .
-        .
- Target accounts do not have Tri-Secret Secure or private connectivity to the Snowflake
  service enabled by default.
- Only account administrators can create a replication or failover group using Snowsight.
- Both the source account and the target account must use the same connection type
  (public internet).
- Snowflake requires a virtual warehouse to load data and run queries.
- The tutorial provides steps to drop a database and minimize storage costs.
- Only a user with the ACCOUNTADMIN role can create a replication or failover group
  using Snowsight.
- Only a user with the ACCOUNTADMIN role can edit or drop a replication or failover
  group using Snowsight.
- Time Travel is automatically enabled with a standard retention period of 1 day.
- Account object replication and failover/failback require Business Critical Edition
  (or higher).
- The cloning operation for a database or schema fails if the specified Time Travel
  time is beyond the retention time of any current child (for example, a table) of
  the entity.
- If the specified Time Travel time is at or before the point in time when the object
  was created, the cloning operation fails.
- Ensure that the OAuth authorization server issues refresh tokens.
- Requires Business Critical Edition (or higher) for some features.
- Replication of external stages does not replicate files.
- To replicate files on an internal stage, a directory table must be enabled.
- Databases created from shares cannot be replicated.
- Auto-ingest enabled for pipes requires SNS topic setup
- Failover groups replicate storage integrations and databases
- Database and share replication are available to all accounts.
- Replication of other account objects & failover/failback require Business Critical
  Edition (or higher).
- If a privacy policy is assigned to a table or view in the source account, the policy
  needs to be replicated in the target account.
- Snowflake supports replication for Git repository clones up to 5 GB in size. Larger
  repositories currently arent supported.
- Replication charges are applied even if the initial replication or a refresh operation
  doesnt succeed.
- If you replicate the secret before the integration and the secret does not already
  exist in the target account, a placeholder secret is added in the target account
  to prevent a dangling reference.
- This features requires Business Critical Edition (or higher).
- Replicated streams can successfully track the change data for tables and views in
  the same database.
- Both the source account and the target account must use the same connection type
  (public internet). Otherwise, signing in to the target account fails.
- Currently, if your account uses private connectivity, you cant use Snowsight to
  create or modify groups or connection objects.
- A notification integration is required to send error notifications.
- The notification integration must have at least one verified email address in the
  DEFAULT_RECIPIENTS list.
- If the notification integration is not available, error notifications are not sent
  for refresh operation failures.
- When you upgrade an account to Business Critical Edition (or higher), it might take
  up to 12 hours for failover capabilities to become available.
- You must be signed in to the target account as a user with the ACCOUNTADMIN role.
- If private connectivity to the Snowflake service is enabled for your Snowflake account,
  you must create and manage a DNS CNAME record for your connection URL.
- Snapshots with retention lock and snapshots with legal holds are available for Business
  Critical Edition (or higher).
- Snapshots help organizations protect critical data against modification or deletion.
- If your account uses private connectivity, you cant use Snowsight to create, modify,
  or drop groups.
- Time Travel information for tables isnt stored as part of a snapshot.
- Applying a snapshot policy with a retention lock to a snapshot set is irreversible.
- You cant delete any snapshot from a snapshot set if a snapshot policy with retention
  lock is attached to that snapshot set.
- You cant delete a snapshot set that has a retention lock and contains unexpired
  snapshots.
- To create a snapshot set, use CREATE SNAPSHOT SET ...
- Snapshots are added using ALTER SNAPSHOT SET ... ADD SNAPSHOT.
- Snowflake does not replicate files on an external stage.
- Fail-safe is a data recovery service that is provided on a best effort basis and
  is intended only for use when all other recovery options have been attempted.
- Fail-safe is not provided as a means for accessing historical data after the Time
  Travel retention period has ended.
- The default type for tables is permanent. To define a table as temporary or transient,
  you must explicitly specify the type during table creation.
- Data in Snowflake is identified by timestamps that can differ slightly from the
  exact value of system time.
- The smallest time resolution for TIMESTAMP is milliseconds.
- If requested data is beyond the Time Travel retention period (default is 1 day),
  the statement fails.
- Replication for Git repository clones up to 5 GB in size is supported.
- On the secondary system, reading from the repository is allowed, but committing,
  fetching from, or pushing to the remote origin server is not allowed until promotion.
- Lists tables for which you have access privileges, including dropped tables within
  the Time Travel retention period.
- Snowsight lists the replication and failover groups for which your role has the
  MONITOR, OWNERSHIP, or REPLICATE privilege on.
- Refresh operation details are only available to users with the ACCOUNTADMIN role
  or the OWNERSHIP privilege on the group.
- If an account (or database or schema) has a large number of tables, searching can
  consume significant resources.
- Results are sorted by database name, schema name, and then table name.
- 'The notification integration must be one of the following types to send email notifications
  on refresh operation failures: EMAIL, QUEUE, or WEBHOOK.'
- Each secondary connection must have the same name as its primary connection.
- Users must be provisioned in the source account and on each target account if security
  integrations are not replicated.
- Snowflake recommends configuring your SAML 2.0-compliant identity provider (IdP)
  with the connection URL rather than an account URL so users are redirected to the
  correct account in case of failover.
- Only returns source object names for which the current users current role has been
  granted at least one access privilege.
- Currently, if your account uses private connectivity, you cant use Snowsight to
  modify target accounts for a connection.
- Currently, if your account uses private connectivity, you cant use Snowsight to
  drop a connection.
- The public preview functionality isnt certified for any specific regulations.
- Snowflake recommends using years between 1582 and 9999.
- Transient schemas and transient databases also retain the transient property after
  you restore them.
- Dynamic tables have their own data definition language (DDL) syntax for snapshots.
- Tasks restored from a snapshot are suspended and must be resumed.
- Snapshots created with a retention lock cant be deleted until the expiration period
  ends.
- INTERVAL is not a data type (that is, you cant define a table column to be of data
  type INTERVAL).
- Returns the current timestamp for the system in the local time zone.
- Fractional seconds are only displayed if they have been explicitly set in the TIMESTAMP_OUTPUT_FORMAT
  parameter for the session.
- You can only delete the oldest snapshot that doesnt have a legal hold.
- You cant delete any snapshot from a snapshot set if a snapshot policy with retention
  lock is attached.
- Snapshots can be created and managed through SQL commands.
- Legal holds can be applied to snapshots to prevent expiration.
- Fail-safe doesnt support tables that contain data ingested by Snowpipe Streaming
  Classic.
- The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY) are provided
  for compatibility with other databases.
- If you created a temporary table with the same name as another table in the schema,
  all queries and operations used on the table only affect the temporary table in
  the session.
- If you cancel a CREATE TABLE operation that retrieves data from archive storage,
  you might still incur retrieval costs.
- Temporary tables are dropped when the session in which they were created ends. Data
  stored in temporary tables is not recoverable after the table is dropped.
- Historical data in transient tables cant be recovered by Snowflake after the Time
  Travel retention period ends. Use transient tables only for data you can replicate
  or reproduce independently from Snowflake.
- Long-lived tables, such as fact tables, should always be defined as permanent to
  ensure they are fully protected by Fail-safe.
- You can define short-lived tables as transient to eliminate Fail-safe costs. For
  example, you might use transient tables for data with a lifetime of less than 1
  day, such as ETL work tables.
- Creates a new schema in the current database.
- Creating a schema automatically sets it as the active/current schema for the current
  session.
- If a schema with the same name already exists in the database, an error is returned
  unless the optional OR REPLACE keyword is specified.
- The identifier must start with an alphabetic character and cannot contain spaces
  or special characters unless the entire identifier string is enclosed in double
  quotes.
- If an account (or database or schema) has a large number of tables, then searching
  the entire account (or table or schema) can consume a significant amount of compute
  resources.
- 'Creating a new database automatically creates two schemas in the database: PUBLIC
  and INFORMATION_SCHEMA.'
- Databases created from shares differ from standard databases in several ways.
- The default value for STATEMENT_TIMEOUT_IN_SECONDS is 172800 (2 days). It is recommended
  to increase it to 604800 (7 days) for replication operations.
- The command doesn't require a running warehouse to execute.
- The command only returns objects for which the current user's current role has been
  granted at least one access privilege.
- Removes the specified notebook from the current/specified schema, but retains a
  version of the notebook so that it can be recovered using UNDROP NOTEBOOK.
- When you visit any website, it may store or retrieve information on your browser,
  mostly in the form of cookies.
- Dropping a table does not permanently remove it from the system. A version of the
  dropped table is retained in Time Travel for the number of days specified by the
  data retention period for the table.
- Only returns objects for which the current users current role has been granted
  at least one access privilege.
- The command only returns source object names for which the current users current
  role has been granted at least one access privilege.
- The value for LIMIT rows cant exceed 10000.
- Dropping a database does not permanently remove it from the system.
- 'Default: CASCADE'
- The command returns a maximum of ten thousand records for the specified object type.
- Notebooks can only be restored to the database and schema that contained the notebook
  at the time of deletion.
- If a notebook with the same name already exists, an error is returned.
- UNDROP relies on the Snowflake Time Travel feature.
- Snowflake supports formats for string constants used in manipulating dates, times,
  and timestamps.
- Tables can only be restored to the database and schema that contained the table
  at the time of deletion.
- If a table with the same name already exists, an error is returned.
- You cannot undrop a hybrid table.
- The setting of the TIMEZONE parameter affects the return value.
- The setting of the TIMESTAMP_TYPE_MAPPING parameter does not affect the return value.
- Do not use the returned value for precise time ordering between concurrent queries.
- A schema can only be restored to the database that contained the schema at the time
  of its deletion.
- If a schema with the same name already exists, an error is returned.
- Transient tables have a lower level of data protection than permanent tables.
- This command isnt supported for tables in a catalog-linked database.
- Restoring Iceberg tables is only supported in the current schema or current database,
  even if the table name is fully qualified.
- If an Iceberg table with the same name already exists, an error is returned.
- To undrop an Iceberg table whose external volume has been dropped, undrop the external
  volume first. You cant undrop the Iceberg table by creating a new external volume
  with same name as the dropped external volume.
- You cant restore a table that uses an external catalog if the associated catalog
  integration has been dropped.
- UNDROP relies on the Snowflake Time Travel feature. An object can be restored only
  if the object was deleted within the Data retention period. The default value is
  24 hours.
- Using OR REPLACE is the equivalent of using DROP TABLE on the existing table and
  then creating a new table with the same name.
- To undrop a dynamic table, you must be using a role that has OWNERSHIP privilege
  on that dynamic table.
- UNDROP relies on the Snowflake Time Travel feature. An object can be restored only
  if the object was deleted within the Data retention period.
- CREATE OR ALTER TABLE statements for existing tables can only be executed by a role
  with the OWNERSHIP privilege on the table.
- Restoring tags is only supported in the current schema or current database
- If a tag with the same name already exists, an error is returned
- UNDROP relies on the Snowflake Time Travel feature
- Only organization administrators can execute the command.
- Creates a new database in the system.
- CDP, which includes Time Travel and Fail-safe, is a standard set of features available
  to all Snowflake accounts at no additional cost.
- Storage is calculated and charged for data regardless of whether it is in the Active,
  Time Travel, or Fail-safe state.
- If a database with the same name already exists, an error is returned and the database
  is not created, unless the optional OR REPLACE keyword is specified.
- La priode de conservation standard est de 1 jour (24 heures) et est automatiquement
  active pour tous les comptes Snowflake.
- Une priode de conservation de 0 jour pour un objet dsactive la fonction Time Travel
  pour cet objet.
- Snowflake strongly recommends using the account replication feature to replicate
  and failover databases.
- If you modify the data retention period for a database or schema, the change only
  affects the active objects contained within the database or schema.
- The process of moving data from Time Travel to Fail-safe is performed in the background.
- Time Travel is automatically activated with a default retention period of 1 day.
- For longer retention periods of up to 90 days, an upgrade to Snowflake Enterprise
  Edition is required.
- After you drop a table, creating a table with the same name creates a new version
  of the table.
- Dropping a schema does not permanently remove it from the system. A version of the
  dropped schema is retained in Time Travel for the number of days specified by the
  DATA_RETENTION_TIME_IN_DAYS parameter for the schema.
- Gelschte Objekte knnen mithilfe der SHOW HISTORY-Befehle aufgelistet werden.
- Dropping a database does not permanently remove it from the system. A version of
  the dropped database is retained in Time Travel for the number of days specified
  by the DATA_RETENTION_TIME_IN_DAYS parameter for the database.
- Recommended to maintain at least a 1-day retention period for specific objects.
- An object can be restored only if the object was deleted within the Data retention
  period. The default value is 24 hours.
- 124
- Snowflake Enterprise Edition90
- Time Travel         .
-     1(24).
- Hybrid tables that belong to the specified schema are not undropped.
- Data retention period affects both active and Time Travel data.
- Changing the retention period impacts only active objects within a database or schema.
- O perodo de reteno padro  1 dia (24 horas) e  habilitado automaticamente para
  todas as contas Snowflake.
- Um perodo de reteno de 0 dias para um objeto efetivamente desativa o Time Travel
  para o objeto.
- A alterao do perodo de reteno de sua conta ou de objetos individuais altera
  o valor para todos os objetos de nvel inferior que no tenham um perodo de reteno
  explicitamente definido.
- No recomendamos alterar o perodo de reteno para 0 no nvel da conta.
- To undrop an Iceberg table whose external volume has been dropped, undrop the external
  volume first.
- Temporary tables only exist within the session in which they were created and persist
  only for the remainder of the session.
- Transient tables do not have a Fail-safe period.
- An object can be restored only if the object was deleted within the Data retention
  period.
- The default value is 24 hours.
- External tables are read-only.
- Querying data in an external table might be slower than querying data that you store
  natively in a table within Snowflake.
- Restoring tags is only supported in the current schema or current database.
- External tables dont support storage versioning (S3 versioning, Object Versioning
  in Google Cloud Storage, or versioning for Azure Storage).
- Snowflake omits query results for records that contain invalid UTF-8 data. After
  encountering invalid data, Snowflake continues to scan the file without returning
  an error message.
- To avoid missing records in your query results caused by invalid UTF-8 data, specify
  REPLACE_INVALID_CHARACTERS = TRUE for your file format.
- To work with tables in Snowsight, you must use a role with the relevant table privileges.
- Uses Continuous Data Protection features including Time Travel and Fail-safe
- The search optimization service can significantly improve the performance of certain
  types of lookup and analytical queries.
- Building the search access path can take significant time, depending on the size
  of the table.
- Views allow granting access to a subset of a table.
- Materialized Views can improve performance.
- A retention period of 0 days for an object disables Time Travel for that object.
- Modification de la priode de conservation affecte toutes les donnes actives ou
  celles actuellement conserves dans Time Travel.
- Les objets supprims ne sont pas affects par la modification de la priode de conservation.
- Time Travel is automatically enabled with a default retention period of 1 day.
- To set a retention period of more than 1 day, the Enterprise Edition is required.
- Secure views prevent users from possibly being exposed to data from rows of tables
  that are filtered by the view.
- Secure views should not be used for views that are defined solely for query convenience.
- When changing the data retention period for an object, the new retention period
  affects all active data and all data currently in Time Travel.
- To change the retention period of a deleted object, the object must be restored
  first.
- Uses Time Travel feature for restoring deleted tables.
- Materialized views require Enterprise Edition.
- Materialized views are designed to improve query performance for workloads composed
  of common, repeated query patterns.
- OBJECT retention period can be set to 0 to effectively deactivate Time Travel.
- The CREATE MATERIALIZED VIEW statement might take a substantial amount of time to
  complete.
- A materialized view can query only a single table.
- Joins, including self-joins, are not supported.
- A materialized view cannot query a materialized view, a non-materialized view, a
  hybrid table, a dynamic table, or a UDTF (user-defined table function).
- A materialized view cannot include UDFs, window functions, HAVING clauses, ORDER
  BY clause, LIMIT clause, or GROUP BY keys that are not within the SELECT list.
- Functions used in a materialized view must be deterministic.
- You cannot perform most DML operations on a materialized view itself.
- 
- 
- Adding a column to the base table does not suspend a materialized view created on
  that base table.
- If a base table is altered so that existing columns are changed or dropped, then
  all materialized views on that base table are suspended; the materialized views
  cannot be used or maintained.
- Renaming or swapping the base table can result in the materialized view pointing
  to a different base table than the base table used to create the materialized view.
- If a base table is dropped, the materialized view is suspended (but not automatically
  dropped).
- If you clone a schema or a database that contains a materialized view, then the
  materialized view is cloned.
- Materialized views impact your costs for both storage and compute resources.
- There are no tools to estimate the costs of maintaining materialized views.
- You can control the cost of maintaining materialized views by carefully choosing
  how many views to create, which tables to create them on, and each views definition.
- You can view the billing costs for maintaining materialized views using Snowsight,
  the Classic Console, or SQL.
- Currently, you cannot use Time Travel to query historical data for materialized
  views.
- Maintaining materialized views will consume credits.
- Changes to shared data can cause charges to the people who have materialized views.
- Time Travel      .
- Sharing semantic views is currently a private preview feature.
- Semantic views are not supported for replication.
- You must have the relevant privileges to access and manage the view, materialized
  view, or semantic view in Snowsight.
- Uses Time Travel to restore deleted tables.
- Views are simple and consume no storage.
- Materialized views provide fast results retrieval but consume storage.
- Dynamic tables can provide complex transformations with fine control on refresh.
- Recomendamos manter um valor de (pelo menos) 1 dia para qualquer objeto em particular.
- This topic provides important considerations when cloning objects in Snowflake,
  particularly databases, schemas, and non-temporary tables.
- Temporary tables only exist within the session in which they were created.
- Cloning is fast, but not instantaneous, particularly for large objects (for example,
  tables).
- To avoid conflicts in name resolution during a cloning operation, we suggest refraining
  from renaming objects to a name previously used by a dropped object until cloning
  is completed.
- Cloning operations require time to complete, particularly for large tables.
- If a child object (for example, a table) has a shorter data retention period than
  the data retention period for its parent object, the child objects historical data
  is moved out of Time Travel before the historical data of its parent object is moved
  out of Time Travel.
- CDP has an impact on storage costs based on the total amount of data stored and
  the length of time the data is stored.
- Storage is charged for data regardless of whether it is in the Active, Time Travel,
  or Fail-safe state.
- External tables are read-only. You cant perform data manipulation language (DML)
  operations on external tables.
- The view only displays objects for which the current role for the session has been
  granted access privileges.
- Search optimization does have effects on certain other table operations.
- Session policies require Enterprise Edition or higher.
- The minimum configurable idle timeout value for a session policy is 5 minutes.
- If a session policy is not set, Snowflake uses a default value of 240 minutes (four
  hours).
- Views allow the result of a query to be accessed as if it were a table.
- Non-materialized views are the most common type of view.
- Changing the retention period affects all data that is active, as well as any data
  currently in Time Travel.
- Time Travel allows querying previous versions of table data.
- Cloning can create duplicates of objects at specified points in history.
- Materialized views are automatically and transparently maintained by Snowflake.
- Aggregate functions that are allowed in materialized views still have some restrictions.
- You cannot RESUME a materialized view that has been suspended due to changes in
  the base table.
- If a base table is dropped, the materialized view is suspended but not automatically
  dropped.
- Remember that maintaining materialized views will consume credits.
- The larger the number of materialized views on a shared base table, the more important
  it is to update that base table efficiently to minimize the costs of maintaining
  materialized views.
- Storage fees are incurred for maintaining historical data during both the Time Travel
  and Fail-safe periods.
- You can manage databases, schemas, tables, and event tables with the Snowflake Python
  APIs.
- Materialized views provide the ability to pre-compute the dataset based on materialized
  view query.
- Dynamic tables materialize the results of a specified query and keep them up to
  date.
- Only the RENAME functionality of ALTER TABLE (event tables) is currently supported.
- ALTER VIEW is currently not supported.
- You cannot create hybrid tables that are temporary or transient.
- Temporary tables exist only in the session they were created in.
- Les tables transitoires nont pas de priode Fail-safe.
- Les donnes de ces tables ne peuvent pas tre rcupres aprs lexpiration de la
  priode de conservation Time Travel.
- Neben permanenten Tabellen, die der Standardtabellentyp beim Erstellen von Tabellen
  sind, untersttzt Snowflake auch das Definieren von temporren und transienten Tabellen.
- Temporre Tabellen sind nur in der Sitzung vorhanden, in der sie erstellt wurden,
  und sie bestehen nur fr die Dauer der Sitzung.
- Transiente Tabellen, die bestehen bleiben, bis sie explizit gelscht werden, und
  die allen Benutzern mit den entsprechenden Berechtigungen zur Verfgung stehen.
- If possible, refrain from executing DML transactions on the source object (or any
  of its children) until after the cloning operation completes.
- CDP includes Time Travel and Fail-safe features available at no additional cost.
- Storage is charged for data in Active, Time Travel, and Fail-safe states.
- Fail-safe
- Time Travel
- Snowflake
- Querying the sum(bytes) for a table does not represent the total storage usage.
- The view does not include tables that have been dropped.
- Temporary tables are session-based and removed after session ends.
- Transient tables are available until explicitly deleted.
-   Fail-safe                .
-       Snowflake           .
- Micro-partitioning is automatically performed on all Snowflake tables.
- Tables are transparently partitioned using the ordering of the data as it is inserted/loaded.
- As tabelas temporrias s existem dentro da sesso em que foram criadas e persistem
  apenas para o restante da sesso.
- As tabelas transitrias so semelhantes s tabelas permanentes, com a diferena
  fundamental de que no tm um perodo de Fail-safe.
- Multi-factor authentication will be mandatory for non-service users in a provider
  account.
- No tasks are required to enable Time Travel. It is automatically enabled with the
  standard, 1-day retention period.
- Changing the retention period for your account or individual objects changes the
  value for all lower-level objects that do not have a retention period explicitly
  set.
- By keeping your data assets, code, and configurations centrally managed and version-controlled,
  you can ensure consistency, simplify collaboration, and streamline rollbacks if
  needed.
- Snowflake supports a model that provides built-in observable data.
- Telemetry data must be emitted as your code executes.
- 'Consider integrating your external tools in one of the following ways: If your
  observability tools can read from external sources, point them to the event table.'
- If your tools use a push modelin which telemetry data must be sent to the toolconsider
  using a stored procedure with external access to regularly read telemetry data from
  the event table and emit it to your tool.
- The TIMESTAMP, OFFSET, or STATEMENT specified in the AT | BEFORE clause falls outside
  the data retention period for the table, the query fails and returns an error.
- Creating an object with the same name after dropping it does not restore the original
  object.
- Snowpark operations are executed lazily on the server, reducing the amount of data
  transferred between your client and the Snowflake database.
- Uses Snowflake's infrastructure for running Spark workloads.
- Fail-safe provides a (non-configurable) 7-day period for data recovery.
- Data recovery through Fail-safe may take several hours to several days.
- Snowflake ML is an integrated set of capabilities for end-to-end machine learning
  in a single platform.
- Snowflake captures observability data in a structure based on the OpenTelemetry
  standard.
- Requires setup of Snowflake credentials and connection parameters.
- Temporary tables do not exist beyond the session in which they are created.
- La priode Fail-safe nest configurable pour aucun type de table.
- Les tables transitoires et temporaires nont pas de priode Fail-safe.
- Nous recommandons dutiliser les tables transitoires seulement pour les donnes
  qui nont pas besoin dtre protges contre les pannes ou les donnes qui peuvent
  tre reconstruites  lextrieur de Snowflake.
- The Snowflake Python APIs library is supported in government regions, but some resource
  types are currently not available in government regions.
- Temporre Tabellen sind nur in der Sitzung vorhanden, in der sie erstellt wurden.
- Transiente Tabellen haben keine Fail-safe-Frist.
- 
- 
- The Snowflake REST APIs are under active development and are continuously expanding.
- Snowflake manages the underlying compute and storage for Streamlit apps.
- Streamlit apps are Snowflake objects and use Role-based Access Control (RBAC) to
  manage access to Streamlit apps.
- To run a Streamlit app, you must select a single virtual warehouse to run both the
  app itself and its queries.
-        .
-   Time Travel   1.
-   Fail-safe                 .
-     .
- Requires setup of the Snowflake CLI version 3.0.0 or greater.
- You must set a current warehouse.
- As tabelas temporrias no so visveis para outros usurios ou sesses.
- As tabelas transitrias no tm um perodo de Fail-safe.
- Users in a provider account are recommended to enroll in multi-factor authentication
  (MFA) if they do not have the TYPE property set to SERVICE.
- Non-service users who use federated authentication and single sign-on (SSO) must
  have MFA enabled as part of their authentication process.
- Application roles can only be used within the context of an app.
- The application package must include any files you want available to the setup script.
- Snowflake DevOps empowers developers to streamline and automate the software development
  lifecycle for their Snowflake environments.
- You can ensure consistency, simplify collaboration, and streamline rollbacks if
  needed by keeping your data assets, code, and configurations centrally managed and
  version-controlled in a remote Git repository.
- Snowflake supports telemetry data collection in an event table.
- Alerts can send notifications to email, cloud service provider queues, Slack, PagerDuty,
  and Microsoft Teams.
- You must grant the USAGE privilege on each schema to an application package for
  each schema you want to share with a consumer in an app.
- Post-deploy hooks must be written in an idempotent manner.
- You can use the telemetry data collected in your event table with other tools that
  support the OpenTelemetry data model.
- Using these tools, you can more thoroughly analyze software performance and behavior.
- Created a Streamlit object in the core schema.
- Allows the APP_PUBLIC role to access the Streamlit object.
- Snowpark operations are executed lazily on the server
- Custom code for UDFs is pushed to the Snowflake engine
- Decouples client and server, so that Spark code can run remotely against the Snowflake
  compute engine without your needing to manage a Spark cluster.
- Allows you to reuse open source Spark dataframes and Spark SQL code with minimal
  migrations or changes.
- Requires setup of connected app in Snowflake.
- A provider may need privileges to develop an app with containers.
- Snowflake ML is optimized for large-scale distributed feature engineering, model
  training and inference.
- Snowflake Feature Store supports automated, incremental refresh from batch and streaming
  data sources.
- The app with containers you created in this tutorial is a prerequisite for the upgrade
  tutorial.
- The app uses a compute pool and accrues credits in your account.
- An application package can only have two active versions at one time.
- A single version of an app can have up to 130 patches.
- Temporary tables or stages are not supported.
- Some Streamlit features are not supported.
- Snowflake Native Apps do not support failover for business continuity.
- Storage lifecycle policies arent supported in Snowflake Native Apps.
- A consumer can use the IN ACCOUNT clause of the SHOW COMPUTE POOLS command to see
  all compute pools in their account and the current state of the compute pool.
- Costs are not incurred when a compute pool is suspended.
- Queries that are run when the app is installed or upgraded.
- Queries that originate from a stored procedure owned by the app.
- Queries containing a non-secure view or function owned by the app.
- The setup script only support using SQL commands. Other languages are not supported.
- Application roles are not versioned.
- Application roles cannot be granted ownership of objects.
- The name of the manifest file must be manifest.yml.
- The manifest file must be uploaded to a named stage.
- Specifies whether the app is allowed to create executables with restricted callers
  rights.
- Streamlit in Snowflake requires a virtual warehouse to run a Streamlit app and to
  perform SQL queries.
- The WebSocket connection, which keeps the Streamlit apps virtual warehouse active,
  expires approximately 15 minutes after the apps last use.
- Requires Snowflake CLI version 3.0.0 or greater.
- Application roles and privileges must be set up correctly.
- Requires Snowflake CLI version 3.0.0 or greater
- Ensure all SQL commands are run in the same session
- To create an application package you must have the global CREATE APPLICATION PACKAGE
  privilege granted to your role.
- After creating an application package, use the SHOW APPLICATION PACKAGES command
  to view the list of available application packages.
- Users with the OWNERSHIP privilege on an application package can remove it from
  an account.
- Created a Streamlit app in your Snowflake Native App that displays shared data.
- Shared data content is not versioned, all versions of an app use the same data.
- Consumers cannot access shared content directly.
- Unrestricted callers rights are not supported for executables in an app.
- Cannot execute commands like SHOW ROLES, SHOW USERS, etc.
- Requires a Snowflake account that supports Snowpark Container Services.
- User must have ACCOUNTADMIN role to create the `tutorial_role`.
- This application uses a compute pool and may accrue credits.
- Streamlit apps in a Snowflake Native App run using a Snowflake warehouse.
- The `environment.yml` file must be at the same level as your main file of your Streamlit
  app.
- Each Streamlit app uses a unique subdomain.
- If you use strict firewalls, add *.snowflake.app to your firewall allowlist.
- Only models based on warehouses are currently supported.
- Providers must use the Snowflake Model Registry to share models with consumers.
- There are limitations on machine learning algorithms that are runnable in a Snowpark
  sandbox within a warehouse.
- Providers are responsible for all costs associated with event sharing on the provider
  side, including data ingestion and storage.
- The upgrade fails due to an intentional error in the setup script.
- Snowflake supports billable events that are emitted by calling the system function
  within a stored procedure in the application.
- Uses Stored Procedures for billing events
- Costs associated with apps with containers should be reviewed.
- App specifications allow consumers to control external endpoints connections.
- Feature policies allow consumers to prohibit certain object creations.
- All infrastructure costs are the responsibility of the consumer.
- Costs to consumers include provider software fees and infrastructure costs.
- Compute pools have cost implications. It is important to set values for the min_nodes,
  max_nodes, and instance_family properties to consume the correct amount of resources.
- Compute pools are account-level objects, as such their pool names must be unique
  within a consumer account.
- An app can create a maximum of five compute pools in a consumer account.
- Queries that are run when the app is installed or upgraded will redact information
  about the objects.
- Shared objects are read-only for an application object and installed Snowflake Native
  App.
- Your Privacy preference signal is honored.
- References to warehouses are supported.
- Quoted names for a service within an app are not supported.
- Services cannot be created in a versioned schema.
- The setup script only supports using SQL commands. Other languages are not supported.
- Database objects created by the setup script are internal to the app and are invisible
  and inaccessible to the consumer account directly.
- Application roles are the only type of role that can be created within an app.
- Application roles may only be safely dropped when you have dropped all versions
  of the app that use those roles.
- The name of the manifest file must be `manifest.yml`.
- Job services run synchronously and terminate when the code of the service exits.
- External functions require you to create an API Integration object.
- Snowflake recommends that providers create the image repository outside the application
  package.
- Shared data content is not versioned, which means that all versions of an app use
  the same data.
- Use caution when revoking permissions on shared objects from an application package
  or when dropping shared objects. If an installed version of the Snowflake Native
  App still requires access to those objects, the Snowflake Native App might become
  unstable or fail.
- Providers can create an app within the same account as the application package,
  so they can test the app before publishing it to consumers.
- Session debug mode can only be used in the session in which debug mode is set.
- Uses owners rights by default for executables.
- Restricted callers rights require explicit grants from consumers.
- The Snowflake Native App Framework is generally available on supported cloud platforms.
- It is your responsibility to ensure that no personal data, sensitive data, export-controlled
  data, or other regulated data is entered into any files included in your application
  package.
- Security requirements are subject to change as Snowflake continues to monitor new
  potential risks.
- The approval process required to publish an app on the Snowflake Marketplace is
  in addition to the automated security scan that is run when the DISTRIBUTION property
  of an application package is set to EXTERNAL.
- If you receive a rejection notification for the application package you submitted,
  make the recommended changes and resubmit your application package for approval.
- Providers must have an account to store shared events in each region where you want
  to support event sharing.
- Declarative Native Apps run in the consumer account, allowing the customer to manage
  their resource usage and costs.
- Uses SYSTEM$CREATE_BILLING_EVENT and SYSTEM$CREATE_BILLING_EVENTS functions for
  billing.
- Billing can be based on various usage events within application.
- The Snowflake Native SDK for Connectors currently supports only the pull-based pattern.
- The types of the arguments passed to the createBillingEvent function must correspond
  to the typed parameters expected by the SYSTEM$CREATE_BILLING_EVENT system function.
- Git repository integration allows synchronization with remote repositories.
- Automatically granting privileges simplifies app installation and configuration.
- Consumers can create feature policies to limit the objects an app can create.
- All Snowflake drivers support TLS to secure communications between the client and
  the Snowflake service.
- Use connection pools when possible.
- Use asynchronous queries cautiously.
- Use additional authentication optimizations.
- Disable query context caching.
- Provider sharing not enabled for all accounts
- Compute pools are account-level objects and must have unique names within the consumer
  account.
- Providers should set the AUTO_SUSPEND_SECS property to automatically suspend inactive
  compute pools.
- Privileges required to create a service in the consumer account include CREATE COMPUTE
  POOL and BIND SERVICE ENDPOINT.
- Services cannot not be created in a versioned schema.
- Notify any data consumers of a share that the name of the database role has changed.
- Access to any objects granted to the database role is revoked when dropped.
- This feature requires you to set up an event table in your account.
- Snowflake does not charge you to enable event sharing.
- Job services run synchronously.
- Job services cannot be executed within a version schema.
- The log and trace level of an app are defined by the provider before publishing
  an app.
- Consumers cannot change the log and trace levels for an app.
- External and Iceberg tables may pose data exfiltration risks to the consumer.
- External and Iceberg tables may incur additional costs related to egress and ingress
  usage.
- Consumers can discover and install apps published to the Snowflake Marketplace or
  shared using private listings.
- When the trial period ends for an app installed from a trial listing, Snowflake
  automatically suspends the app unless the consumer converts the app to a full listing.
- External image repositories are not supported.
- The DEVELOP object-level privilege is specific to a single application package.
- Debug mode can only be toggled on and off for an app created in development mode
  within the same account containing the application package.
- Session debug mode allows providers to view and modify all of the objects within
  the app and execute statements using the same privileges that the app has when installed
  in the consumer account.
- Latency for the view may be up to 45 minutes.
- Canceled queries are identified by their error_message text (`SQL execution canceled`),
  not by their execution_status value.
- Providers can create new versions or patches of an app and upgrade the app in the
  consumer account.
- Uses OAuth2 with refresh token  requires setup in Snowflake
- Creating a provider profile is not required for private listings.
- Declarative Native Apps allow providers to share and sell data products.
- Simplified creation experience similar to Secured Data Shares.
- Writing to the remote repository is supported only from Workspaces, Streamlit apps,
  and Snowflake notebooks.
- If you encounter errors when attempting to share data with consumers, the feature
  may not be enabled for your account.
- Requires setup of an event table to capture logs and events.
- Event sharing allows a provider to collect information about the app's performance.
- Log and trace levels are defined by the provider and cannot be changed by consumers.
- External and Iceberg tables may incur additional costs related to egress and ingress
  usage if the object store containing the table is not in the same region where the
  app is published.
- Access History requires Enterprise Edition (or higher).
- After the app is suspended it may not be possible to resume the app.
- The view displays data starting from February 22, 2021.
- Latency for the view may be up to 180 minutes (3 hours).
- The parent_query_id and root_query_id columns begin to record data starting on January
  15-16, 2024.
- Monitor individual or grouped queries executed by users in your account.
- Explore each step of an executed query in the query profile.
- Queries run when the app is installed or upgraded.
- Queries that originate from a child job of a stored procedure owned by the app.
- Pruning can help for queries that filter out a significant amount of data.
- MFA is intended for human users who authenticate with a password.
- Service users must use another form of authentication.
- MFA can be used for connecting to Snowflake via the Snowflake JDBC driver.
- By default, the Duo Push authentication mechanism is used when a user is enrolled
  in MFA; no changes to the JDBC connection string are required.
- Snowflake supports most SAML 2.0-compliant vendors as an IdP.
- Snowflake supports using key pair authentication for enhanced authentication security
  as an alternative to basic authentication.
- Snowflake recommends generating encrypted keys.
- For increased performance, filter queries on the query_start_time column and choose
  narrower time ranges.
- Details for queries executed more than seven days ago do not include User information
  due to the data retention policy for sessions.
- For queries that failed due to syntax or parsing errors, you see <redacted> instead
  of the SQL statement that was executed.
- 'If a query doesnt have query details, some possible causes include the following:
  The query is still running. When the query finishes running, you can view the query
  details and profile. Your role does not have privileges to view the query details.
  The query was run more than 14 days ago and query details and profile are no longer
  available. The query failed to run and therefore has no query profile. While the
  Snowflake platform is designed to preserve job details, the depth of job query detail
  and Query Profile metrics is on a best-effort basis and is not guaranteed for all
  queries.'
- MFA is intended for human users who authenticate with a password. Service users
  must use another form of authentication.
- Currently, strategies for implementing MFA for your organization vary depending
  on whether or not an account existed when the 2024_08 behavior change bundle was
  enabled.
- Supports most SAML 2.0-compliant vendors as an IdP
- Native Snowflake support for Okta and Microsoft AD FS
- Snowflake supports using key pair authentication for enhanced authentication security
  as an alternative to basic authentication, such as username and password.
- Snowflake supports rotating public keys in an effort to allow compliance with more
  robust security and governance postures.
- The Snowflake Native App Framework allows for the creation of applications that
  can share data and logic.
- Must grant the USAGE privilege on each schema to an application package for each
  schema you want to share with a consumer in an app.
- Must grant the SELECT privilege on the objects within the schema that you want to
  share.
- You must have Snowflake CLI version 3.0.0 or greater installed on your machine.
- You must run all SQL commands in the same SQL command session.
- The function defined in this external file matches the inline function defined in
  the setup script.
- This tutorial requires Snowflake CLI version 3.0.0 or greater.
- The tutorial uses both Snowflake CLI and the Snowsight web interface.
- All SQL commands must run in the same SQL command session.
- This statement creates a STREAMLIT object in the core schema.
- Utilise l'authentification multifactorielle (MFA) pour les utilisateurs hors services.
- Les utilisateurs des services doivent utiliser l'authentification par paire de cls
  ou OAuth.
- Available to accounts in all non-government AWS regions.
- Snowflake ncessite diffrents types dauthentification pour les utilisateurs du
  compte fournisseur.
- Snowflake recommends multi-factor authentication (MFA) for non-service users.
- Die Snowflake Native App Framework ist generell auf untersttzten Cloud-Plattformen
  verfgbar.
- Das Snowflake Native App Framework untersttzt auch eine verbesserte Benutzeroberflche
  fr Entwickler.
- Snowflake Native App Framework is available on widely supported cloud platforms.
- Applications can be shared with consumers through lists, which can be free or paid.
- 
- Snowflake
- Snowflake Native App Framework allows sharing data and business logic across Snowflake
  accounts.
- Applications can be published to Snowflake Marketplace or private listings.
- Applications are encapsulated in application packages containing data content and
  logic.
- A lgica de negcios de um aplicativo pode incluir um aplicativo Streamlit, procedimentos
  armazenados e funes escritas usando a API Snowpark, JavaScript e SQL.
- Um pacote de aplicativo encapsula o contedo de dados, a lgica do aplicativo, os
  metadados e o script de configurao exigidos por um aplicativo.
- Provedor  um usurio do Snowflake que deseja compartilhar contedo de dados e lgica
  de aplicativo com outros usurios do Snowflake.
- Consumidor  um usurio do Snowflake que deseja acessar o contedo de dados e a
  lgica do aplicativo compartilhado pelos provedores.
- Snowflake supports both fixed-point and floating-point numeric data types.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- 'USER_NOT_FOUND: The specified user does not exist.'
- 'ROLE_NOT_FOUND: The specified role does not exist.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '100080: Number of columns in file does not match that of the corresponding table'
- '100016: Field delimiter found while expecting record delimiter'
- Number of columns in file (11) does not match that of the corresponding table (10)
- Field delimiter '|' found while expecting record delimiter '\n'
- 'LOAD_FAILED: Check file format and data consistency.'
- '100080: Number of columns in file does not match that of the corresponding table.'
- '100016: Field delimiter found while expecting record delimiter.'
- Common connectivity issues and resolutions
- Troubleshooting steps
- Error messages
- Larger queries process normally, but you could not rerun or retry the larger queries,
  as Snowflake truncates queries larger than 1MB per statement before persisting them
  to the metadata store.
- Snowflake returns an error that includes the path of the duplicate file.
- You cant create a table if the Parquet metadata contains invalid UTF-8 characters.
- Snowflake truncates queries larger than 1MB before persisting them to the metadata
  store.
- duplicate files are present in the same snapshot
- you cant create a table if the Parquet metadata contains invalid UTF-8 characters
- Cannot disable the ORGADMIN role if it is the last account that has the role enabled.
- Currently, you cannot disable the ORGADMIN role if it is the last account that has
  the role enabled.
- 'CONFLICT: Name of the organization user group matches an existing role.'
- 'USER_CONFLICT: Existing user object corresponds to the same person as an organization
  user.'
- 'CONFLICT: User already exists in the regular account.'
- 'VISIBILITY_NOT_SET: Set visibility for the organization user group.'
- 'LOCKED_OUT: User is locked out of their account for a period of time after consecutive
  failed login attempts.'
- If the model powering this function is not allowed, the error message contains information
  about how to modify the allowlist.
- Model access errors may arise due to cross-region, deprecation, or other availability
  constraints.
- Cross-region availability constraints may result in error messages that seem similar
  to model access errors.
- Notification of successful and unsuccessful update attempts.
- Notification emails are sent for successful and unsuccessful update attempts.
- Cannot have more than five private endpoints per Snowflake account
- Cannot have more than one endpoint to the same AWS service or Azure subresource
- Limit of five private endpoints per Snowflake account.
- Cannot have more than one endpoint to the same AWS service or Azure subresource.
- Les comptes dessai ne peuvent pas tre rsilis via linterface Web
- Les comptes dessai ne peuvent pas tre rsilis via linterface Web.
- Contas de avaliao sem um mtodo de pagamento vlido esto limitadas a aproximadamente
  dez crditos de uso por dia das funes AISQL do Snowflake Cortex.
- 'LOADED: Indicates the files successfully loaded.'
- 'NULL: Indicates no errors were encountered during loading.'
- If a database with the same name already exists, an error is returned and the database
  is not created.
- CREATE TABLE statement requires CREATE TABLE privilege on the schema.
- CREATE OR ALTER execution failed. Partial updates may have been applied.
- CREATE TABLE privilege required on the schema.
- SELECT privilege required on queried tables and/or views only when cloning a table
  or executing CTAS statements.
- 'Insufficient privileges: Ensure the role has CREATE WAREHOUSE privilege.'
- 'Warehouse already exists: Use OR REPLACE to create or alter an existing warehouse.'
- Field delimiter ',' found while expecting record delimiter '\n'
- NULL result in a non-nullable column. Use quotes if an empty field should be interpreted
  as an empty string instead of a null
- End of record reached while expected to parse column '"MYTABLE"["QUOTA":3]'
- 'SQL compilation error: match_by_column_name is not supported with copy transform.'
- End of record reached while expected to parse column
- If set to FALSE, the load operation produces an error when invalid UTF-8 character
  encoding is detected.
- '100038 (22018): DML operation to table DEMO_INSERT_TYPE_MISMATCH failed on column
  V with error: Numeric value ''d'' is not recognized'
- 'NULL_ARGUMENT: Argument cannot be NULL.'
- '404 Not Found: Check the endpoint path'
- '403 Forbidden: Verify your permissions'
- '500 Internal Server Error: Retry after some time'
- If an object with the same name already exists, UNDROP fails.
- If an object with the same name already exists, UNDROP fails. You must rename the
  existing object, which then enables you to restore the previous version of the object.
- 'NOT_AUTHORIZED: You do not have permission to perform this action.'
- 'OBJECT_NOT_FOUND: The specified object does not exist.'
- Dangling references in the snapshot. Correct the errors before refreshing again.
- Fail-safe doesnt support tables that contain data ingested by Snowpipe Streaming
  Classic.
- Time travel data is not available for table <tablename>
- Error notifications not sent for refresh operation failures if the notification
  integration is not available.
- 'invalid OAuth access token: You must re-authenticate and consent to permissions
  to re-establish the connection.'
- The value for LIMIT rows cant exceed 10000.
- Snapshot cannot be deleted as it is not the oldest active snapshot in the snapshot
  set.
- 'SQL compilation error: Sequence used as a default value was not found.'
- Snapshot identifier not found.
- CREATE TABLE requires CREATE TABLE privilege on the Schema.
- SELECT is required on queried tables and/or views only when cloning a table or executing
  CTAS statements.
- APPLY privilege is required when applying a masking policy, row access policy, object
  tags, or storage lifecycle policy.
- LIMIT rows cannot exceed 10000.
- '401 Unauthorized: Recheck role privileges'
- Notebook mynotebook successfully restored.
- If a table with the same name already exists, an error is returned.
- 'CREATE_TABLE_PRIVILEGE_REQUIRED: Role must have CREATE TABLE privilege on the schema.'
- '001471 (42601): SQL compilation error: Column contains null values. Not null constraint
  cannot be added.'
- 'SQL compilation error: Cannot drop the table because of dependencies'
- The DROP operation fails if a session policy or password policy is set on a user
  or the account.
- Error if a table with the same name already exists.
- 'SCHEMA_ALREADY_EXISTS: If a schema with the same name already exists.'
- 'TIME_TRAVEL_EXPIRED: The object can only be restored if deleted within the data
  retention period.'
- 'Database with the same name already exists: an error is returned.'
- 'Error: An external volume with the same name already exists.'
- Error if a tag with the same name already exists.
- 'Changes in the referenced files in cloud storage dont invalidate the query results
  cache in the following circumstances, which lead to outdated query results: The
  automated refresh operation is turned off (that is, AUTO_REFRESH = FALSE) or isnt
  configured correctly.'
- Queries might run more slowly if the search access path is still being updated.
- Redaction of information in error messages related to secure views.
- Materialized views consume storage space.
- Snowflake does not allow DML operations on materialized views.
- 'SQL compilation error: Materialized View MY_MV is invalid. Invalidation reason:
  Division by zero'
-  UNDROP 
- If you want to use a suspended materialized view again, you must recreate it.
- 'SQL compilation error: Materialized View <name> is invalid.'
- 'Invalid reason: Division by zero'
-   DATA_RETENTION_TIME_IN_DAYS 0      .
- Temporary tables are purged once the session ends.
- Data in transient tables cannot be recovered after the Time Travel retention period
  passes.
- '002002 (42710): None: SQL compilation error: Object ''T_SALES'' already exists.'
- 'ProgrammingError occurred: "000707 (02000): None: Data is not available." with
  query id None.'
- '002003 (02000): SQL compilation error: Object ''SALES.PUBLIC.T_SALES'' does not
  exist.'
- Future grants of privileges on session policies are not supported.
- 'VIEW_DEFINITION_INVALID: Recreate the view with the new definition if changes are
  needed.'
- If you create a materialized view on a clustered table, consider removing any clustering
  on the base table.
- 'If the view has been suspended: Consider resuming the view by executing ALTER MATERIALIZED
  VIEW  RESUME.'
- '2003: Uncaught exception of type STATEMENT_ERROR on line 89 at position 0 : Uncaught
  exception of type STATEMENT_ERROR on line 19 at position 3 : SQL compilation error:
  Object TABLE_DOES_NOT_EXIST does not exist or not authorized.'
- Links in email notifications from apps do not correctly link into a private link
  accounts.
- Information about implementation details is redacted from the ACCESS_HISTORY view
  in certain contexts.
- '400 Bad Request: Check the structure of the manifest file.'
- '401 Unauthorized: Ensure proper authentication.'
- '404 Not Found: Verify the endpoint path.'
- You cannot remove an application package that is currently associated with a listing.
- Training performed on a providers dataset may not yield a model sufficiently effective
  for a consumers data.
- Object 'TABLE_DOES_NOT_EXIST' does not exist or not authorized.
- '401 Unauthorized: Check app specifications approval'
- 'INSTALLATION_FAILED: Consumer must grant required privileges'
- 'Queries run when the app is installed or upgraded: query_text and error_message
  fields are redacted.'
- 'Invalid manifest_version: Please ensure it is either 1 or 2.'
- Unrestricted callers rights are not supported for executables in an app.
- Cannot execute SHOW ROLES or SHOW USERS commands.
- 'Data exfiltration: Malicious apps could copy consumer data to external functions
  or logs.'
- 'Compute abuse: Apps could perform unauthorized tasks, such as cryptomining, at
  the consumers expense.'
- 'Ransomware: Apps could encrypt or corrupt consumer data, demanding payment for
  restoration.'
- 'Privilege escalation: Apps could attempt to gain unauthorized permissions within
  the consumers account.'
- If apps are not immediately actionable, they must document the expected workflow
  for a consumer, allowing consumers to fully install and configure the app.
- If a consumer does not set up an event table and make it the active event table
  before installing the app, trace event and log data is discarded.
- You cannot share historical events using event sharing.
- The app must be created in development mode.
- You must have the OWNERSHIP privilege on the app.
- You must have the DEVELOP privilege on the application package.
- 'SHARE_EVENTS_WITH_PROVIDER: True only when all event definitions are enabled.'
- 'Connection Error: Ensure the connection name is correct.'
- 'Connection test failed: Verify connection parameters'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - OAuth
  - Programmatic access tokens
  - Oauth
  - GLOBALORGADMIN
  - ORGADMIN
  - UserResource
  - UserCollection
  - Database
  - DatabaseResource
  - Schema
  - SchemaResource
  - Table
  - TableResource
  - EventTable
  - EventTableResource
  - Root
  - Session
  - app specification
  - reference
  - privileges
  - Service provider (SP)
  - Identity provider (IdP)
  - Service provider
  - Identity provider
  - TYPE
  - MFA
  - 
  - 
  - 
  -  (MFA)
  - SSO
  - Service User
  - Key Pair Authentication
client:
  base_url: https://docs.snowflake.com
  auth:
    type: oauth2
    flow: refresh_token
source_metadata: null
