resources:
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
- name: experimental_ls_chat_completion
  endpoint:
    path: /experimental/ls/inference/chat_completion
    method: GET
- name: experimental_ls_completion
  endpoint:
    path: /experimental/ls/inference/completion
    method: GET
- name: responses
  endpoint:
    path: /v1/responses
    method: GET
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
- name: experimental_ls_chat_completion
  endpoint:
    path: /experimental/ls/inference/chat_completion
    method: POST
- name: experimental_ls_completion
  endpoint:
    path: /experimental/ls/inference/completion
    method: POST
- name: responses
  endpoint:
    path: /v1/responses
    method: GET
- name: bielik_11b_v2_3_instruct
  endpoint:
    path: speakleash/bielik-11b-v2.3-instruct
    method: GET
    data_selector: records
    params: {}
- name: code_llama_13b_instruct
  endpoint:
    path: meta/codellama-13b-instruct
    method: GET
    data_selector: records
    params: {}
- name: code_llama_34b_instruct
  endpoint:
    path: meta/codellama-34b-instruct
    method: GET
    data_selector: records
    params: {}
- name: code_llama_70b_instruct
  endpoint:
    path: meta/codellama-70b-instruct
    method: GET
    data_selector: records
    params: {}
- name: deepseek_coder_v2_lite_instruct
  endpoint:
    path: deepseek-ai/deepseek-coder-v2-lite-instruct
    method: GET
    data_selector: records
    params: {}
- name: deepseek_r1
  endpoint:
    path: deepseek-ai/deepseek-r1
    method: GET
    data_selector: records
    params: {}
- name: deepseek_r1_distill_llama_8b
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: records
    params: {}
- name: deepseek_r1_distill_llama_70b
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-70b
    method: GET
    data_selector: records
    params: {}
- name: deepseek_r1_distill_llama_8b_rtx
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: records
    params: {}
- name: deepseek_r1_distill_qwen_32b
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-qwen-32b
    method: GET
    data_selector: records
    params: {}
- name: eurollm_9b_instruct
  endpoint:
    path: utter-project/eurollm-9b-instruct
    method: GET
    data_selector: records
    params: {}
- name: gemma_2_2b
  endpoint:
    path: google/gemma-2-2b-instruct
    method: GET
    data_selector: records
    params: {}
- name: gemma_2_9b
  endpoint:
    path: google/gemma-2-9b-it
    method: GET
    data_selector: records
    params: {}
- name: gemma2_9b_cpt_sahabat_ai_v1_instruct
  endpoint:
    path: gotocompany/gemma2-9b-cpt-sahabatai-v1-instruct
    method: GET
    data_selector: records
    params: {}
- name: gemma_3_1b_instruct
  endpoint:
    path: google/gemma-3-1b-it
    method: GET
    data_selector: records
    params: {}
- name: gpt_oss_20b
  endpoint:
    path: openai/gpt-oss-20b
    method: GET
    data_selector: records
    params: {}
- name: gpt_oss_120b
  endpoint:
    path: openai/gpt-oss-120b
    method: GET
    data_selector: records
    params: {}
- name: granite_3_3_8b_instruct
  endpoint:
    path: ibm-granite/granite-3.3-8b-instruct
    method: GET
    data_selector: records
    params: {}
- name: greenmind_medium_14b_r1
  endpoint:
    path: greennode/greenmind-medium-14b-r1
    method: GET
    data_selector: records
    params: {}
- name: Bielik 11B v2.3 Instruct
  endpoint:
    path: speakleash/bielik-11b-v2.3-instruct
    method: GET
    data_selector: records
    params: {}
- name: Code Llama 13B Instruct
  endpoint:
    path: meta/codellama-13b-instruct
    method: GET
    data_selector: records
    params: {}
- name: Code Llama 34B Instruct
  endpoint:
    path: meta/codellama-34b-instruct
    method: GET
    data_selector: records
    params: {}
- name: Code Llama 70B Instruct
  endpoint:
    path: meta/codellama-70b-instruct
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek Coder V2 Lite Instruct
  endpoint:
    path: deepseek-ai/deepseek-coder-v2-lite-instruct
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek R1
  endpoint:
    path: deepseek-ai/deepseek-r1
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek R1 Distill Llama 8B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek R1 Distill Llama 70B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-70b
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek R1 Distill Llama 8B RTX
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: records
    params: {}
- name: DeepSeek R1 Distill Qwen 32B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-qwen-32b
    method: GET
    data_selector: records
    params: {}
- name: EuroLLM 9B Instruct
  endpoint:
    path: utter-project/eurollm-9b-instruct
    method: GET
    data_selector: records
    params: {}
- name: Gemma 2 2B
  endpoint:
    path: google/gemma-2-2b-instruct
    method: GET
    data_selector: records
    params: {}
- name: Gemma 2 9B
  endpoint:
    path: google/gemma-2-9b-it
    method: GET
    data_selector: records
    params: {}
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: gotocompany/gemma2-9b-cpt-sahabatai-v1-instruct
    method: GET
    data_selector: records
    params: {}
- name: Gemma 3 1B Instruct
  endpoint:
    path: google/gemma-3-1b-it
    method: GET
    data_selector: records
    params: {}
- name: GPT-OSS-20B
  endpoint:
    path: openai/gpt-oss-20b
    method: GET
    data_selector: records
    params: {}
- name: GPT-OSS-120B
  endpoint:
    path: openai/gpt-oss-120b
    method: GET
    data_selector: records
    params: {}
- name: Granite 3.3 8B Instruct
  endpoint:
    path: ibm-granite/granite-3.3-8b-instruct
    method: GET
    data_selector: records
    params: {}
- name: GreenMind Medium 14B R1
  endpoint:
    path: greennode/greenmind-medium-14b-r1
    method: GET
    data_selector: records
    params: {}
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: kakaocorp/kanana-1.5-8b-instruct-2505
    method: GET
    data_selector: records
    params: {}
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: meta/llama-2-7b-chat
    method: GET
    data_selector: records
    params: {}
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: meta/llama-2-13b-chat
    method: GET
    data_selector: records
    params: {}
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: meta/llama-2-70b-chat
    method: GET
    data_selector: records
    params: {}
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: defog/llama-3-sqlcoder-8b
    method: GET
    data_selector: records
    params: {}
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: tokyotech-llm/llama-3-swallow-70b-instruct-v0.1
    method: GET
    data_selector: records
    params: {}
- name: Llama 3.1 8B Base
  endpoint:
    path: meta/llama-3.1-8b-base
    method: GET
    data_selector: records
    params: {}
- name: Llama 3.1 8B Instruct
  endpoint:
    path: meta/llama-3.1-8b-instruct
    method: GET
    data_selector: records
    params: {}
- name: llama_3_3_nemotron_super_49b_v1
  endpoint:
    path: /v1/containers/llama-3.3-nemotron-super-49b-v1
    method: GET
- name: llama_3_3_nemotron_super_49b_v1_5
  endpoint:
    path: /v1/containers/llama-3.3-nemotron-super-49b-v1.5
    method: GET
- name: Qwen2.5 72B Instruct
  endpoint:
    path: /qwen2.5/72B/instruct
    method: GET
    data_selector: records
- name: Qwen2.5 7B Instruct
  endpoint:
    path: /qwen2.5/7B/instruct
    method: GET
    data_selector: records
- name: Gemma 2 2B
  endpoint:
    path: /gemma2/2B
    method: GET
    data_selector: records
- name: Gemma 2 9B
  endpoint:
    path: /gemma2/9B
    method: GET
    data_selector: records
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: /gemma2/9B/cpt-sahabat-ai-v1/instruct
    method: GET
    data_selector: records
- name: Gemma 3 1B Instruct
  endpoint:
    path: /gemma3/1B/instruct
    method: GET
    data_selector: records
- name: GPT-OSS-20B
  endpoint:
    path: /gpt-oss/20B
    method: GET
    data_selector: records
- name: GPT-OSS-120B
  endpoint:
    path: /gpt-oss/120B
    method: GET
    data_selector: records
- name: Granite 3.3 8B Instruct
  endpoint:
    path: /granite3.3/8B/instruct
    method: GET
    data_selector: records
- name: GreenMind Medium 14B R1
  endpoint:
    path: /greenmind/medium/14B/R1
    method: GET
    data_selector: records
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: /kanana/1.5/8B/instruct/2505
    method: GET
    data_selector: records
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: /meta/llama2/7B/chat
    method: GET
    data_selector: records
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: /meta/llama2/13B/chat
    method: GET
    data_selector: records
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: /meta/llama2/70B/chat
    method: GET
    data_selector: records
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: /llama3/sqlcoder/8B
    method: GET
    data_selector: records
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: /llama3/swallow/70B/instruct/v0.1
    method: GET
    data_selector: records
- name: Llama 3 Taiwan 70B Instruct
  endpoint:
    path: /llama3/taiwan/70B/instruct
    method: GET
    data_selector: records
- name: Llama 3.1 8B Base
  endpoint:
    path: /llama/3.1/8b/base
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 8B Instruct
  endpoint:
    path: /llama/3.1/8b/instruct
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 8B Instruct RTX
  endpoint:
    path: /llama/3.1/8b/instruct/rtx
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 Nemotron Nano 4B V1.1
  endpoint:
    path: /llama/3.1/nemotron/nano/4b/v1.1
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 Nemotron Nano 8B V1
  endpoint:
    path: /llama/3.1/nemotron/nano/8b/v1
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 Nemotron Ultra 253B V1
  endpoint:
    path: /llama/3.1/nemotron/ultra/253b/v1
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Bielik 11B v2.3 Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Code Llama 13B Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Code Llama 34B Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Code Llama 70B Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek Coder V2 Lite Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek R1
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek R1 Distill Llama 8B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek R1 Distill Llama 70B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek R1 Distill Llama 8B RTX
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek R1 Distill Qwen 32B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek-R1-Distill-Qwen-7B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DeepSeek-R1-Distill-Qwen-14B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: EuroLLM 9B Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Qwen3 Next 80B A3B Thinking
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Qwen3-32B NIM for DGX Spark
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Qwen2.5 Coder 32B Instruct
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Llama 3.1 405B Instruct
  endpoint:
    path: /llama/3.1/405b/instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Nemotron 70B Instruct
  endpoint:
    path: /llama/3.1/nemotron/70b/instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Swallow 8B Instruct v0.1
  endpoint:
    path: /llama/3.1/swallow/8b/instruct/v0.1
    method: GET
    data_selector: records
- name: Llama 3.1 Swallow 70B Instruct v0.1
  endpoint:
    path: /llama/3.1/swallow/70b/instruct/v0.1
    method: GET
    data_selector: records
- name: Llama 3.1 Typhoon 2 8B Instruct
  endpoint:
    path: /llama/3.1/typhoon/2/8b/instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Typhoon 2 70B Instruct
  endpoint:
    path: /llama/3.1/typhoon/2/70b/instruct
    method: GET
    data_selector: records
- name: Llama 3.3 70B Instruct
  endpoint:
    path: /llama/3.3/70b/instruct
    method: GET
    data_selector: records
- name: Meta Llama 3 8B Instruct
  endpoint:
    path: /meta/llama/3/8b/instruct
    method: GET
    data_selector: records
- name: Llama 3.3 Nemotron Super 49B V1
  endpoint:
    path: /llama/3.3/nemotron/super/49b/v1
    method: GET
    data_selector: records
- name: Qwen2.5 72B Instruct
  endpoint:
    path: /qwen2.5/72b/instruct
    method: GET
    data_selector: records
- name: Qwen2.5 7B Instruct
  endpoint:
    path: /qwen2.5/7b/instruct
    method: GET
    data_selector: records
- name: Gemma 2 2B
  endpoint:
    path: /gemma2/2b
    method: GET
    data_selector: records
- name: Gemma 2 9B
  endpoint:
    path: /gemma2/9b
    method: GET
    data_selector: records
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: /gemma2/9b/cpt/sahabat-ai/v1/instruct
    method: GET
    data_selector: records
- name: Gemma 3 1B Instruct
  endpoint:
    path: /gemma3/1b/instruct
    method: GET
    data_selector: records
- name: GPT-OSS-20B
  endpoint:
    path: /gpt-oss/20b
    method: GET
    data_selector: records
- name: GPT-OSS-120B
  endpoint:
    path: /gpt-oss/120b
    method: GET
    data_selector: records
- name: Granite 3.3 8B Instruct
  endpoint:
    path: /granite/3.3/8b/instruct
    method: GET
    data_selector: records
- name: GreenMind Medium 14B R1
  endpoint:
    path: /greenmind/medium/14b/r1
    method: GET
    data_selector: records
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: /kanana/1.5/8b/instruct/2505
    method: GET
    data_selector: records
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: /meta/llama2/7b/chat
    method: GET
    data_selector: records
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: /meta/llama2/13b/chat
    method: GET
    data_selector: records
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: /meta/llama2/70b/chat
    method: GET
    data_selector: records
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: /llama3/sqlcoder/8b
    method: GET
    data_selector: records
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: /llama3/swallow/70b/instruct/v0.1
    method: GET
    data_selector: records
- name: Llama 3 Taiwan 70B Instruct
  endpoint:
    path: /llama3/taiwan/70b/instruct
    method: GET
    data_selector: records
- name: Llama 3.3 Nemotron Super 49B V1.5
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Meta Llama 3 70B Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral 7B Instruct V0.3
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo Minitron 8B 8K Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo 12B Instruct RTX
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo 12B Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral Small 24b Instruct 2501
  endpoint:
    path: /generic-configuration
    method: GET
    data_selector: records
- name: Llama 3.1 8B Base
  endpoint:
    path: /llama-3.1-8b-base
    method: GET
    data_selector: records
- name: Llama 3.1 8B Instruct
  endpoint:
    path: /llama-3.1-8b-instruct
    method: GET
    data_selector: records
- name: Llama 3.1 8B Instruct RTX
  endpoint:
    path: /llama-3.1-8b-instruct-rtx
    method: GET
    data_selector: records
- name: Llama 3.1 Nemotron Nano 4B V1.1
  endpoint:
    path: /llama-3.1-nemotron-nano-4b-v1.1
    method: GET
    data_selector: records
- name: Llama 3.1 Nemotron Nano 8B V1
  endpoint:
    path: /llama-3.1-nemotron-nano-8b-v1
    method: GET
    data_selector: records
- name: Llama 3.1 Nemotron Ultra 253B V1
  endpoint:
    path: /llama-3.1-nemotron-ultra-253b-v1
    method: GET
    data_selector: records
- name: Llama 3.2 1B Instruct
  endpoint:
    path: /llama/3.2/1b/instruct
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.2 3B Instruct
  endpoint:
    path: /llama/3.2/3b/instruct
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Llama 3.1 70B Instruct
  endpoint:
    path: /llama/3.1/70b/instruct
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mixtral 8x7B Instruct
  endpoint:
    path: /mixtral/8x7b/instruct
    method: GET
    data_selector: records
    params: {}
- name: Mixtral 8x22B Instruct
  endpoint:
    path: /mixtral/8x22b/instruct
    method: GET
    data_selector: records
    params: {}
- name: Nemotron 4 340B Instruct
  endpoint:
    path: /nemotron/4/340b/instruct
    method: GET
    data_selector: records
    params: {}
- name: Nemotron 4 340B Reward
  endpoint:
    path: /nemotron/4/340b/reward
    method: GET
    data_selector: records
    params: {}
- name: NVIDIA-Nemotron-Nano-9B-v2
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2
    method: GET
    data_selector: records
    params: {}
- name: NVIDIA-Nemotron-Nano-9B-v2-DGX-Spark
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2/dgx/spark
    method: GET
    data_selector: records
    params: {}
- name: Phi 3 Mini 4K Instruct
  endpoint:
    path: /phi/3/mini/4k/instruct
    method: GET
    data_selector: records
    params: {}
- name: Phi 4 Mini Instruct
  endpoint:
    path: /phi/4/mini/instruct
    method: GET
    data_selector: records
    params: {}
- name: Phind Codellama 34B V2 Instruct
  endpoint:
    path: /phind/codellama/34b/v2/instruct
    method: GET
    data_selector: records
    params: {}
- name: Riva Translate 4B Instruct
  endpoint:
    path: /riva/translate/4b/instruct
    method: GET
    data_selector: records
    params: {}
- name: Sarvam - M
  endpoint:
    path: /sarvam/m
    method: GET
    data_selector: records
    params: {}
- name: SILMA 9B Instruct
  endpoint:
    path: /silma/9b/instruct
    method: GET
    data_selector: records
    params: {}
- name: StarCoder2 7B
  endpoint:
    path: /starcoder2/7b
    method: GET
    data_selector: records
    params: {}
- name: StarCoderBase 15.5B
  endpoint:
    path: /starcoderbase/15.5b
    method: GET
    data_selector: records
    params: {}
- name: Llama 3.3 Nemotron Super 49B V1.5
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Meta Llama 3 70B Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral 7B Instruct V0.3
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo Minitron 8B 8K Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo 12B Instruct RTX
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral NeMo 12B Instruct
  endpoint:
    path: /optimized-configurations
    method: GET
    data_selector: records
- name: Mistral Small 24b Instruct 2501
  endpoint:
    path: /generic-configuration
    method: GET
    data_selector: records
- name: optimized_configurations
  endpoint:
    path: /api/optimized_configurations
    method: GET
    data_selector: models
    params: {}
- name: generic_configuration
  endpoint:
    path: /api/generic_configuration
    method: GET
    data_selector: configurations
    params: {}
- name: supported_profiles
  endpoint:
    path: /api/supported_profiles
    method: GET
    data_selector: profiles
    params: {}
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
- name: Mixtral 8x7B Instruct
  endpoint:
    path: /mixtral/8x7b/instruct
    method: GET
    data_selector: optimized_configurations
- name: Mixtral 8x22B Instruct
  endpoint:
    path: /mixtral/8x22b/instruct
    method: GET
    data_selector: optimized_configurations
- name: Nemotron 4 340B Instruct
  endpoint:
    path: /nemotron/4/340b/instruct
    method: GET
    data_selector: optimized_configurations
- name: Nemotron 4 340B Reward
  endpoint:
    path: /nemotron/4/340b/reward
    method: GET
    data_selector: optimized_configurations
- name: NVIDIA-Nemotron-Nano-9B-v2
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2
    method: GET
    data_selector: test_configurations
- name: NVIDIA-Nemotron-Nano-9B-v2-DGX-Spark
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2/dgx/spark
    method: GET
    data_selector: optimized_configurations
- name: Phi 3 Mini 4K Instruct
  endpoint:
    path: /phi/3/mini/4k/instruct
    method: GET
    data_selector: optimized_configurations
- name: Phi 4 Mini Instruct
  endpoint:
    path: /phi/4/mini/instruct
    method: GET
    data_selector: generic_configuration
- name: Phind Codellama 34B V2 Instruct
  endpoint:
    path: /phind/codellama/34b/v2/instruct
    method: GET
    data_selector: optimized_configurations
- name: Riva Translate 4B Instruct
  endpoint:
    path: /riva/translate/4b/instruct
    method: GET
    data_selector: generic_configuration
- name: Sarvam - M
  endpoint:
    path: /sarvam/m
    method: GET
    data_selector: generic_configuration
- name: SILMA 9B Instruct v1.0
  endpoint:
    path: /silma/9b/instruct/v1.0
    method: GET
    data_selector: generic_configuration
- name: StarCoder2 7B
  endpoint:
    path: /starcoder2/7b
    method: GET
    data_selector: optimized_configurations
- name: StarCoderBase 15.5B
  endpoint:
    path: /starcoderbase/15.5b
    method: GET
    data_selector: generic_configuration
- name: optimized_configurations
  endpoint:
    path: /optimized/configurations
    method: GET
    data_selector: models
    params: {}
- name: generic_configuration
  endpoint:
    path: /generic/configuration
    method: GET
    data_selector: config
    params: {}
- name: supported_vllm_profiles
  endpoint:
    path: /supported/vllm/profiles
    method: GET
    data_selector: profiles
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completion
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: status
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: model
  endpoint:
    path: /v1/models
    method: GET
- name: model_store
  endpoint:
    path: /create-model-store
    method: POST
    data_selector: model-repo
    params:
      repository: nim/nvidia/llm-nim
      tag: latest
- name: download_cache
  endpoint:
    path: /download-to-cache
    method: POST
    data_selector: cache
    params:
      profile: 09e2f8e68f78ce94bf79d15b40a21333cea5d09dbe01ede63f6c957f4fcfab7b
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: null
    params: {}
- name: multi-node-deployment
  endpoint:
    path: /v1/health/ready
    method: GET
- name: nim-llm
  endpoint:
    path: /nim-llm-<version_number>.tgz
    method: GET
- name: chat_completions
  endpoint:
    path: chat/completions
    method: POST
    data_selector: choices[0].message.content
- name: model_repository
  endpoint:
    path: /create-model-store
    method: POST
    data_selector: repository
    params: {}
- name: multiNode
  endpoint:
    params:
      multiNode.enabled: 'false'
      multiNode.clusterStartTimeout: 300
      multiNode.gpusPerNode: 1
      multiNode.workers: 1
      multiNode.leaderWorkerSet.enabled: true
      multiNode.optimized.enabled: true
- name: tool_calling
  endpoint:
    path: /function-calling
    method: GET
    data_selector: tools
- name: model_profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: MODEL PROFILES
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /api/lora/adapters
    method: GET
    data_selector: adapters
    params: {}
- name: Downloading LoRA Adapters
  endpoint:
    path: /api/lora/download
    method: POST
    data_selector: download_info
    params: {}
- name: detailed thinking on
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices[0].message
- name: detailed thinking off
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices[0].message
- name: custom_guided_decoding_backends
  endpoint:
    path: /path/to/mounted/custom/backends/directory
    method: POST
    data_selector: backend
    params: {}
- name: LoRA adapters
  endpoint:
    path: /api/loRA_adapters
    method: GET
    data_selector: records
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /services/data/vXX.X/lora/adapters
    method: GET
    data_selector: adapters
    params: {}
- name: custom_guided_decoding_backends
  endpoint:
    path: /path/to/mounted/custom/backends/directory
    method: POST
    data_selector: backend
    params: {}
- name: custom_guided_decoding_backends
  endpoint:
    path: /custom-guided-decoding-backends
    method: GET
    data_selector: response
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: guided_grammar
  endpoint:
    path: /v1/guided_grammar
    method: POST
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
    data_selector: stats
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
    data_selector: stats
- name: LoRA Adapters
  endpoint:
    path: /lora/adapters
    method: GET
    data_selector: adapters
    params: {}
- name: LoRA Setup Overview
  endpoint:
    path: /lora/setup
    method: GET
    data_selector: setup
    params: {}
- name: Obtain LoRA Models
  endpoint:
    path: /lora/obtain
    method: GET
    data_selector: models
    params: {}
- name: LoRA adapters
  endpoint:
    path: /nim/meta/llama3-8b-instruct-lora
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: ''
    params: {}
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
- name: experimental_ls_chat_completion
  endpoint:
    path: /experimental/ls/inference/chat_completion
    method: POST
- name: experimental_ls_completion
  endpoint:
    path: /experimental/ls/inference/completion
    method: POST
- name: chat_completion
  endpoint:
    path: /inference/chat_completion
    method: POST
    data_selector: completion_message
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
- name: list-model-profiles
  endpoint:
    path: list-model-profiles
    method: GET
- name: download-to-cache
  endpoint:
    path: download-to-cache
    method: POST
- name: create-model-store
  endpoint:
    path: create-model-store
    method: POST
- name: nim-llm-check-cache-env
  endpoint:
    path: nim-llm-check-cache-env
    method: GET
- name: nim-llm-set-cache-env
  endpoint:
    path: nim-llm-set-cache-env
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: ''
    params: {}
- name: health_ready
  endpoint:
    path: /v1/health/ready
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
- name: chat_completion
  endpoint:
    path: /inference/chat_completion
    method: POST
    data_selector: response_data
    params: {}
- name: list-model-profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: profiles
    params: {}
- name: download-to-cache
  endpoint:
    path: /download-to-cache
    method: GET
    data_selector: download
    params: {}
- name: create-model-store
  endpoint:
    path: /create-model-store
    method: GET
    data_selector: model_store
    params: {}
- name: nim-llm-check-cache-env
  endpoint:
    path: /nim-llm-check-cache-env
    method: GET
    data_selector: cache_check
    params: {}
- name: nim-llm-set-cache-env
  endpoint:
    path: /nim-llm-set-cache-env
    method: GET
    data_selector: cache_env
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: model
    params: {}
- name: metrics
  endpoint:
    path: /v1/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: Llama 3 8B Instruct
  endpoint:
    path: /models/llama3-8b-instruct
    method: GET
- name: Llama 3 70B Instruct
  endpoint:
    path: /models/llama3-70b-instruct
    method: GET
- name: Mistral-7B-Instruct-v0.3
  endpoint:
    path: /models/mistral-7b-instruct-v0.3
    method: GET
- name: Mixtral-8x7B-v0.1
  endpoint:
    path: /models/mixtral-8x7b-v0.1
    method: GET
- name: Mixtral-8x22B-v0.1
  endpoint:
    path: /models/mixtral-8x22b-v0.1
    method: GET
- name: bielik_11b_v2_3_instruct
  endpoint:
    path: /orgs/nim/teams/speakleash/containers/bielik-11b-v2.3-instruct
    method: GET
    data_selector: model
    params: {}
- name: code_llama_13b_instruct
  endpoint:
    path: /orgs/nim/teams/meta/containers/codellama-13b-instruct
    method: GET
    data_selector: model
    params: {}
- name: code_llama_34b_instruct
  endpoint:
    path: /orgs/nim/teams/meta/containers/codellama-34b-instruct
    method: GET
    data_selector: model
    params: {}
- name: code_llama_70b_instruct
  endpoint:
    path: /orgs/nim/teams/meta/containers/codellama-70b-instruct
    method: GET
    data_selector: model
    params: {}
- name: deepseek_coder_v2_lite_instruct
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-coder-v2-lite-instruct
    method: GET
    data_selector: model
    params: {}
- name: deepseek_r1
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-r1
    method: GET
    data_selector: model
    params: {}
- name: deepseek_r1_distill_llama_8b
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: model
    params: {}
- name: deepseek_r1_distill_llama_70b
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-70b
    method: GET
    data_selector: model
    params: {}
- name: deepseek_r1_distill_llama_8b_rtx
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: model
    params: {}
- name: deepseek_r1_distill_qwen_32b
  endpoint:
    path: /orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-qwen-32b
    method: GET
    data_selector: model
    params: {}
- name: eurollm_9b_instruct
  endpoint:
    path: /orgs/nim/teams/utter-project/containers/eurollm-9b-instruct
    method: GET
    data_selector: model
    params: {}
- name: stockmark-2-100b-instruct
  endpoint:
    path: stockmark/stockmark-2-100b-instruct
    method: GET
- name: teuken-7b-instruct-commercial-v0.4
  endpoint:
    path: opengpt-x/teuken-7b-instruct-commercial-v0.4
    method: GET
- name: Llama 3 SQLCoder 8B Instruct
  endpoint:
    path: /support-matrix.html#llama-3-sqlcoder-8b-instruct
    method: GET
- name: Mistral NeMo Minitron 8B 8K Instruct
  endpoint:
    path: /support-matrix.html#mistral-nemo-minitron-8b-8k-instruct
    method: GET
- name: Nemotron 4 340B Instruct 128K
  endpoint:
    path: /support-matrix.html#nemotron-4-340b-instruct-128k
    method: GET
- name: Phi 3 Mini 4K Instruct
  endpoint:
    path: /support-matrix.html#phi-3-mini-4k-instruct
    method: GET
- name: Phind Codellama 34B v2
  endpoint:
    path: /support-matrix.html#phind-codellama-34b-v2
    method: GET
- name: Llama 3 8B Instruct
  endpoint:
    path: /language_models/llama3_8b_instruct
    method: GET
- name: Llama 3 70B Instruct
  endpoint:
    path: /language_models/llama3_70b_instruct
    method: GET
- name: Mistral-7B-Instruct-v0.3
  endpoint:
    path: /language_models/mistral_7b_instruct_v0_3
    method: GET
- name: Mixtral-8x7B-v0.1
  endpoint:
    path: /language_models/mixtral_8x7b_v0_1
    method: GET
- name: Mixtral-8x22B-v0.1
  endpoint:
    path: /language_models/mixtral_8x22b_v0_1
    method: GET
- name: Qwen2.5 72B Instruct
  endpoint:
    path: /qwen2.5/72B/instruct
    method: GET
- name: Qwen2.5 7B Instruct
  endpoint:
    path: /qwen2.5/7B/instruct
    method: GET
- name: Gemma 2 2B
  endpoint:
    path: /gemma2/2B
    method: GET
- name: Gemma 2 9B
  endpoint:
    path: /gemma2/9B
    method: GET
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: /gemma2/9B/cpt/sahabat-ai/v1/instruct
    method: GET
- name: Gemma 3 1B Instruct
  endpoint:
    path: /gemma3/1B/instruct
    method: GET
- name: GPT-OSS-20B
  endpoint:
    path: /gpt-oss/20B
    method: GET
- name: GPT-OSS-120B
  endpoint:
    path: /gpt-oss/120B
    method: GET
- name: Granite 3.3 8B Instruct
  endpoint:
    path: /granite3.3/8B/instruct
    method: GET
- name: GreenMind Medium 14B R1
  endpoint:
    path: /greenmind/medium/14B/r1
    method: GET
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: /kanana/1.5/8B/instruct/2505
    method: GET
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: /meta/llama/2/7B/chat
    method: GET
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: /meta/llama/2/13B/chat
    method: GET
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: /meta/llama/2/70B/chat
    method: GET
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: /llama3/sqlcoder/8B
    method: GET
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: /llama3/swallow/70B/instruct/v0.1
    method: GET
- name: Llama 3 Taiwan 70B Instruct
  endpoint:
    path: /llama3/taiwan/70B/instruct
    method: GET
- name: Llama 3.1 8B Base
  endpoint:
    path: /model/lama-3.1-8b-base
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.1 8B Instruct
  endpoint:
    path: /model/lama-3.1-8b-instruct
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.1 8B Instruct RTX
  endpoint:
    path: /model/lama-3.1-8b-instruct-rtx
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.1 Nemotron Nano 4B V1.1
  endpoint:
    path: /model/lama-3.1-nemotron-nano-4b-v1.1
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.1 Nemotron Nano 8B V1
  endpoint:
    path: /model/lama-3.1-nemotron-nano-8b-v1
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.1 Nemotron Ultra 253B V1
  endpoint:
    path: /model/lama-3.1-nemotron-ultra-253b-v1
    method: GET
    data_selector: configurations
    params: {}
- name: Llama 3.2 1B Instruct
  endpoint:
    path: /llama/3.2/1B/instruct
    method: GET
    data_selector: optimized_configurations
- name: Llama 3.2 3B Instruct
  endpoint:
    path: /llama/3.2/3B/instruct
    method: GET
    data_selector: optimized_configurations
- name: Llama 3.1 70B Instruct
  endpoint:
    path: /llama/3.1/70B/instruct
    method: GET
    data_selector: optimized_configurations
- name: Llama 3.1 405B Instruct
  endpoint:
    path: /llama-3-1-405b-instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Nemotron 70B Instruct
  endpoint:
    path: /llama-3-1-nemotron-70b-instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Swallow 8B Instruct v0.1
  endpoint:
    path: /llama-3-1-swallow-8b-instruct-v0-1
    method: GET
    data_selector: records
- name: Llama 3.1 Swallow 70B Instruct v0.1
  endpoint:
    path: /llama-3-1-swallow-70b-instruct-v0-1
    method: GET
    data_selector: records
- name: Llama 3.1 Typhoon 2 8B Instruct
  endpoint:
    path: /llama-3-1-typhoon-2-8b-instruct
    method: GET
    data_selector: records
- name: Llama 3.1 Typhoon 2 70B Instruct
  endpoint:
    path: /llama-3-1-typhoon-2-70b-instruct
    method: GET
    data_selector: records
- name: Llama 3.3 70B Instruct
  endpoint:
    path: /llama-3-3-70b-instruct
    method: GET
    data_selector: records
- name: Meta Llama 3 8B Instruct
  endpoint:
    path: /meta-llama-3-8b-instruct
    method: GET
    data_selector: records
- name: Llama 3.3 Nemotron Super 49B V1
  endpoint:
    path: /llama-3-3-nemotron-super-49b-v1
    method: GET
    data_selector: records
- name: Llama 3.3 Nemotron Super 49B V1.5
  endpoint:
    path: /models/llama3.3
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Meta Llama 3 70B Instruct
  endpoint:
    path: /models/meta_llama3
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mistral 7B Instruct V0.3
  endpoint:
    path: /models/mistral_7b
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mistral NeMo Minitron 8B 8K Instruct
  endpoint:
    path: /models/mistral_nemo_minitron_8b
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mistral NeMo 12B Instruct RTX
  endpoint:
    path: /models/mistral_nemo_12b_rtx
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mistral NeMo 12B Instruct
  endpoint:
    path: /models/mistral_nemo_12b
    method: GET
    data_selector: optimized_configurations
    params: {}
- name: Mistral Small 24b Instruct 2501
  endpoint:
    path: /models/mistral_small_24b
    method: GET
    data_selector: generic_configuration
    params: {}
- name: Bielik 11B v2.3 Instruct
  endpoint:
    path: speakleash/bielik-11b-v2.3-instruct
    method: GET
    data_selector: model
    params: {}
- name: Code Llama 13B Instruct
  endpoint:
    path: meta/codellama-13b-instruct
    method: GET
    data_selector: model
    params: {}
- name: Code Llama 34B Instruct
  endpoint:
    path: meta/codellama-34b-instruct
    method: GET
    data_selector: model
    params: {}
- name: Code Llama 70B Instruct
  endpoint:
    path: meta/codellama-70b-instruct
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek Coder V2 Lite Instruct
  endpoint:
    path: deepseek-ai/deepseek-coder-v2-lite-instruct
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek R1
  endpoint:
    path: deepseek-ai/deepseek-r1
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek R1 Distill Llama 8B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek R1 Distill Llama 70B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-70b
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek R1 Distill Llama 8B RTX
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-llama-8b
    method: GET
    data_selector: model
    params: {}
- name: DeepSeek R1 Distill Qwen 32B
  endpoint:
    path: deepseek-ai/deepseek-r1-distill-qwen-32b
    method: GET
    data_selector: model
    params: {}
- name: EuroLLM 9B Instruct
  endpoint:
    path: utter-project/eurollm-9b-instruct
    method: GET
    data_selector: model
    params: {}
- name: Gemma 2 2B
  endpoint:
    path: google/gemma-2-2b-instruct
    method: GET
    data_selector: model
    params: {}
- name: Gemma 2 9B
  endpoint:
    path: google/gemma-2-9b-it
    method: GET
    data_selector: model
    params: {}
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: gotocompany/gemma2-9b-cpt-sahabatai-v1-instruct
    method: GET
    data_selector: model
    params: {}
- name: Gemma 3 1B Instruct
  endpoint:
    path: google/gemma-3-1b-it
    method: GET
    data_selector: model
    params: {}
- name: GPT-OSS-20B
  endpoint:
    path: openai/gpt-oss-20b
    method: GET
    data_selector: model
    params: {}
- name: GPT-OSS-120B
  endpoint:
    path: openai/gpt-oss-120b
    method: GET
    data_selector: model
    params: {}
- name: Granite 3.3 8B Instruct
  endpoint:
    path: ibm-granite/granite-3.3-8b-instruct
    method: GET
    data_selector: model
    params: {}
- name: GreenMind Medium 14B R1
  endpoint:
    path: greennode/greenmind-medium-14b-r1
    method: GET
    data_selector: model
    params: {}
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: kakaocorp/kanana-1.5-8b-instruct-2505
    method: GET
    data_selector: model
    params: {}
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: meta/llama-2-7b-chat
    method: GET
    data_selector: model
    params: {}
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: meta/llama-2-13b-chat
    method: GET
    data_selector: model
    params: {}
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: meta/llama-2-70b-chat
    method: GET
    data_selector: model
    params: {}
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: defog/llama-3-sqlcoder-8b
    method: GET
    data_selector: model
    params: {}
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: tokyotech-llm/llama-3-swallow-70b-instruct-v0.1
    method: GET
    data_selector: model
    params: {}
- name: Llama 3.1 8B Base
  endpoint:
    path: meta/llama-3.1-8b-base
    method: GET
    data_selector: model
    params: {}
- name: Llama 3.1 8B Instruct
  endpoint:
    path: meta/llama-3.1-8b-instruct
    method: GET
    data_selector: model
    params: {}
- name: Mixtral 8x7B Instruct
  endpoint:
    path: /mixtral/8x7b/instruct
    method: GET
    data_selector: records
- name: Mixtral 8x22B Instruct
  endpoint:
    path: /mixtral/8x22b/instruct
    method: GET
    data_selector: records
- name: Nemotron 4 340B Instruct
  endpoint:
    path: /nemotron/4/340b/instruct
    method: GET
    data_selector: records
- name: Nemotron 4 340B Reward
  endpoint:
    path: /nemotron/4/340b/reward
    method: GET
    data_selector: records
- name: NVIDIA-Nemotron-Nano-9B-v2
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2
    method: GET
    data_selector: records
- name: NVIDIA-Nemotron-Nano-9B-v2-DGX-Spark
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2/dgx/spark
    method: GET
    data_selector: records
- name: Phi 3 Mini 4K Instruct
  endpoint:
    path: /phi/3/mini/4k/instruct
    method: GET
    data_selector: records
- name: Phi 4 Mini Instruct
  endpoint:
    path: /phi/4/mini/instruct
    method: GET
    data_selector: records
- name: Phind Codellama 34B V2 Instruct
  endpoint:
    path: /phind/codellama/34b/v2/instruct
    method: GET
    data_selector: records
- name: Riva Translate 4B Instruct
  endpoint:
    path: /riva/translate/4b/instruct
    method: GET
    data_selector: records
- name: Sarvam - M
  endpoint:
    path: /sarvam/m
    method: GET
    data_selector: records
- name: SILMA 9B Instruct v1.0
  endpoint:
    path: /silma/9b/instruct/v1.0
    method: GET
    data_selector: records
- name: StarCoder2 7B
  endpoint:
    path: /starcoder2/7b
    method: GET
    data_selector: records
- name: StarCoderBase 15.5B
  endpoint:
    path: /starcoderbase/15.5b
    method: GET
    data_selector: records
- name: optimized_configurations
  endpoint:
    path: /optimized_configurations
    method: GET
    data_selector: models
- name: generic_configuration
  endpoint:
    path: /generic_configuration
    method: GET
    data_selector: configurations
- name: supported_vllm_profiles
  endpoint:
    path: /supported_vllm_profiles
    method: GET
    data_selector: profiles
- name: Qwen2.5 72B Instruct
  endpoint:
    path: /qwen2.5/72B/instruct
    method: GET
- name: Qwen2.5 7B Instruct
  endpoint:
    path: /qwen2.5/7B/instruct
    method: GET
- name: Gemma 2 2B
  endpoint:
    path: /gemma2/2B
    method: GET
- name: Gemma 2 9B
  endpoint:
    path: /gemma2/9B
    method: GET
- name: Gemma2 9B CPT Sahabat-AI v1 Instruct
  endpoint:
    path: /gemma2/9B/cpt/sahabat-ai/v1/instruct
    method: GET
- name: Gemma 3 1B Instruct
  endpoint:
    path: /gemma3/1B/instruct
    method: GET
- name: GPT-OSS-20B
  endpoint:
    path: /gpt/oss/20B
    method: GET
- name: GPT-OSS-120B
  endpoint:
    path: /gpt/oss/120B
    method: GET
- name: Granite 3.3 8B Instruct
  endpoint:
    path: /granite/3.3/8B/instruct
    method: GET
- name: GreenMind Medium 14B R1
  endpoint:
    path: /greenmind/medium/14B/R1
    method: GET
- name: Kanana 1.5 8B Instruct 2505
  endpoint:
    path: /kanana/1.5/8B/instruct/2505
    method: GET
- name: (Meta) Llama 2 7B Chat
  endpoint:
    path: /llama2/7B/chat
    method: GET
- name: (Meta) Llama 2 13B Chat
  endpoint:
    path: /llama2/13B/chat
    method: GET
- name: (Meta) Llama 2 70B Chat
  endpoint:
    path: /llama2/70B/chat
    method: GET
- name: Llama 3 SQLCoder 8B
  endpoint:
    path: /llama3/sqlcoder/8B
    method: GET
- name: Llama 3 Swallow 70B Instruct V0.1
  endpoint:
    path: /llama3/swallow/70B/instruct/v0.1
    method: GET
- name: Llama 3 Taiwan 70B Instruct
  endpoint:
    path: /llama3/taiwan/70B/instruct
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: Llama 3.1 8B Base
  endpoint:
    path: /models/llama-3-1-8b-base
    method: GET
    data_selector: optimizations
    params: {}
- name: Llama 3.1 8B Instruct
  endpoint:
    path: /models/llama-3-1-8b-instruct
    method: GET
    data_selector: optimizations
    params: {}
- name: Llama 3.1 8B Instruct RTX
  endpoint:
    path: /models/llama-3-1-8b-instruct-rtx
    method: GET
    data_selector: optimizations
    params: {}
- name: Llama 3.1 Nemotron Nano 4B V1.1
  endpoint:
    path: /models/llama-3-1-nemotron-nano-4b-v1-1
    method: GET
    data_selector: optimizations
    params: {}
- name: Llama 3.1 Nemotron Nano 8B V1
  endpoint:
    path: /models/llama-3-1-nemotron-nano-8b-v1
    method: GET
    data_selector: optimizations
    params: {}
- name: Llama 3.1 Nemotron Ultra 253B V1
  endpoint:
    path: /models/llama-3-1-nemotron-ultra-253b-v1
    method: GET
    data_selector: optimizations
    params: {}
- name: nim-llm
  endpoint:
    path: /nim-llm
    method: GET
- name: Llama 3.1 405B Instruct
  endpoint:
    path: /llama/3.1/405B/instruct
    method: GET
    data_selector: optimizedConfigurations
- name: Llama 3.1 Nemotron 70B Instruct
  endpoint:
    path: /llama/3.1/nemotron/70B/instruct
    method: GET
    data_selector: optimizedConfigurations
- name: Llama 3.1 Swallow 8B Instruct v0.1
  endpoint:
    path: /llama/3.1/swallow/8B/instruct/v0.1
    method: GET
    data_selector: genericConfiguration
- name: Llama 3.1 Swallow 70B Instruct v0.1
  endpoint:
    path: /llama/3.1/swallow/70B/instruct/v0.1
    method: GET
    data_selector: genericConfiguration
- name: Llama 3.1 Typhoon 2 8B Instruct
  endpoint:
    path: /llama/3.1/typhoon/2/8B/instruct
    method: GET
    data_selector: optimizedConfigurations
- name: Llama 3.1 Typhoon 2 70B Instruct
  endpoint:
    path: /llama/3.1/typhoon/2/70B/instruct
    method: GET
    data_selector: genericConfiguration
- name: Llama 3.3 70B Instruct
  endpoint:
    path: /llama/3.3/70B/instruct
    method: GET
    data_selector: optimizedConfigurations
- name: Meta Llama 3 8B Instruct
  endpoint:
    path: /meta/llama/3/8B/instruct
    method: GET
    data_selector: optimizedConfigurations
- name: Llama 3.3 Nemotron Super 49B V1
  endpoint:
    path: /llama/3.3/nemotron/super/49B/V1
    method: GET
    data_selector: optimizedConfigurations
- name: model_store
  endpoint:
    path: /create-model-store
    method: POST
    data_selector: model-repo
    params: {}
- name: air_gap_nim_cache
  endpoint:
    path: /opt/nim/.cache
    method: GET
    data_selector: cache
    params: {}
- name: multi-node deployment
  endpoint:
    path: /v1/health/ready
    method: GET
- name: Mixtral 8x7B Instruct
  endpoint:
    path: /mixtral/8x7b/instruct
    method: GET
    data_selector: records
- name: Mixtral 8x22B Instruct
  endpoint:
    path: /mixtral/8x22b/instruct
    method: GET
    data_selector: records
- name: Nemotron 4 340B Instruct
  endpoint:
    path: /nemotron/4/340b/instruct
    method: GET
    data_selector: records
- name: Nemotron 4 340B Reward
  endpoint:
    path: /nemotron/4/340b/reward
    method: GET
    data_selector: records
- name: NVIDIA-Nemotron-Nano-9B-v2
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2
    method: GET
    data_selector: records
- name: NVIDIA-Nemotron-Nano-9B-v2-DGX-Spark
  endpoint:
    path: /nvidia/nemotron/nano/9b/v2/dgx/spark
    method: GET
    data_selector: records
- name: Phi 3 Mini 4K Instruct
  endpoint:
    path: /phi/3/mini/4k/instruct
    method: GET
    data_selector: records
- name: Phi 4 Mini Instruct
  endpoint:
    path: /phi/4/mini/instruct
    method: GET
    data_selector: records
- name: Phind Codellama 34B V2 Instruct
  endpoint:
    path: /phind/codellama/34b/v2/instruct
    method: GET
    data_selector: records
- name: Riva Translate 4B Instruct
  endpoint:
    path: /riva/translate/4b/instruct
    method: GET
    data_selector: records
- name: Sarvam - M
  endpoint:
    path: /sarvam/m
    method: GET
    data_selector: records
- name: SILMA 9B Instruct
  endpoint:
    path: /silma/9b/instruct
    method: GET
    data_selector: records
- name: StarCoder2 7B
  endpoint:
    path: /starcoder2/7b
    method: GET
    data_selector: records
- name: StarCoderBase 15.5B
  endpoint:
    path: /starcoderbase/15.5b
    method: GET
    data_selector: records
- name: model_profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: MODEL PROFILES
- name: optimized_configurations
  endpoint:
    path: /optimized/configurations
    method: GET
    data_selector: records
- name: generic_configuration
  endpoint:
    path: /generic/configuration
    method: GET
    data_selector: records
- name: supported_vllm_profiles
  endpoint:
    path: /supported/vllm/profiles
    method: GET
    data_selector: records
- name: custom_fine_tuned_model
  endpoint:
    path: /path/to/customized/llama
    method: POST
    data_selector: model
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completion
  endpoint:
    path: /chat/completions
    method: POST
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: health
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: status
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: custom_guided_decoding_backends
  endpoint:
    path: /custom-guided-decoding-backends
    method: GET
    data_selector: backends
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: function_calling
  endpoint:
    path: /function-calling
    method: POST
- name: nim-llm
  endpoint:
    path: /nim-llm
    method: GET
- name: LoRA Adapters
  endpoint:
    path: /api/lora/adapters
    method: GET
    data_selector: adapters
- name: LoRA Adapters
  endpoint:
    path: /model/download-version
    method: POST
    data_selector: models
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /registry/model/download-version
    method: GET
    data_selector: models
    params: {}
- name: create-model-store
  endpoint:
    path: /create-model-store
    method: POST
    data_selector: model-store
    params: {}
- name: download-to-cache
  endpoint:
    path: /download-to-cache
    method: POST
    data_selector: cache
    params: {}
- name: multi-node-deployment
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: log
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /api/lora/adapters
    method: GET
    data_selector: adapters
    params: {}
- name: multiNode
  endpoint:
    path: /multiNode
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: model_profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: MODEL PROFILES
    params: {}
- name: optimized
  endpoint:
    path: /optimization/targets
    method: GET
- name: quantization
  endpoint:
    path: /quantization
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
    data_selector: stats
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
    data_selector: stats
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
- name: detailed thinking on
  endpoint:
    path: /detailed_thinking_on
    method: POST
    data_selector: choices[0].message
- name: detailed thinking off
  endpoint:
    path: /detailed_thinking_off
    method: POST
    data_selector: choices[0].message
- name: no think prompt
  endpoint:
    path: /no_think
    method: POST
    data_selector: choices[0].message
- name: custom_guided_decoding_backend
  endpoint:
    path: /custom-guided-decoding-backends
    method: POST
    data_selector: response
    params: {}
- name: repository_override
  endpoint:
    path: /repository_override
    method: GET
    data_selector: repository
    params: {}
- name: s3_support
  endpoint:
    path: /s3_support
    method: GET
    data_selector: s3
    params: {}
- name: model
  endpoint:
    path: /<namespace>.compat.objectstorage.<region>.oraclecloud.com
    method: GET
    data_selector: models
    params: {}
- name: chat_completion
  endpoint:
    path: /inference/chat_completion
    method: POST
    data_selector: completion_message
    params: {}
- name: tool_call
  endpoint:
    path: /function_call
    method: POST
    data_selector: tool_calls
- name: list-model-profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: profiles
    params: {}
- name: download-to-cache
  endpoint:
    path: /download-to-cache
    method: POST
    data_selector: info
    params: {}
- name: create-model-store
  endpoint:
    path: /create-model-store
    method: POST
    data_selector: status
    params: {}
- name: nim-llm-check-cache-env
  endpoint:
    path: /nim-llm-check-cache-env
    method: GET
    data_selector: cache_status
    params: {}
- name: nim-llm-set-cache-env
  endpoint:
    path: /nim-llm-set-cache-env
    method: GET
    data_selector: env_variables
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /api/loRA_adapters
    method: GET
    data_selector: adapters
    params: {}
- name: NIM Configuration
  endpoint:
    path: /api/nim_configuration
    method: GET
    data_selector: configuration
    params: {}
- name: LoRA_adapters
  endpoint:
    path: /nim/adapters
    method: GET
    data_selector: adapters
- name: LoRA adapters
  endpoint:
    path: /ngc/registry/model/download-version
    method: POST
    data_selector: downloads
    params: {}
- name: LoRA Adapters
  endpoint:
    path: /api/lora/adapters
    method: GET
    data_selector: adapters
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
- name: repository_override
  endpoint:
    path: /repository/override
    method: GET
    data_selector: repository_data
    params: {}
- name: s3_support
  endpoint:
    path: /s3/support
    method: GET
    data_selector: s3_data
    params: {}
- name: chat_completion
  endpoint:
    path: /inference/chat_completion
    method: POST
    data_selector: response
    params: {}
- name: list-model-profiles
  endpoint:
    path: /list-model-profiles
    method: GET
    data_selector: MODEL PROFILES
- name: download-to-cache
  endpoint:
    path: /download-to-cache
    method: GET
    data_selector: INFO
    params: {}
- name: create-model-store
  endpoint:
    path: /create-model-store
    method: GET
    data_selector: INFO
    params: {}
- name: nim-llm-check-cache-env
  endpoint:
    path: /nim-llm-check-cache-env
    method: GET
    data_selector: WARNING
    params: {}
- name: nim-llm-set-cache-env
  endpoint:
    path: /nim-llm-set-cache-env
    method: GET
    data_selector: export
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: model
    params: {}
notes:
- NVIDIA cannot guarantee the security of any models hosted on non-NVIDIA systems
  such as HuggingFace.
- Malicious or insecure models can result in serious security risks up to and including
  full remote code execution.
- NIM abstracts away model inference internals such as execution engine and runtime
  operations.
- NIM offers the fastest path to inference with unparalleled performance.
- NIMs are packaged as container images on a per model/model family basis.
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- NIM microservices have production-grade runtimes including ongoing security updates.
- Uses KV Cache reuse for improved performance
- Some top-level parameters can trigger log warnings.
- SGLang profiles may run out of memory (OOM) under high load. Set `NIM_KVCACHE_PERCENT=0.7`
  to help mitigate the issue.
- On the TRT-LLM backend, setting `temperature=0` enforces greedy decoding, making
  `repetition_penalty` ineffective.
- 'When calling the `v1/metadata` API, the following fields under `modelInfo` are
  missing: `repository_override` and `selectedModelProfileId`.'
- For all version 1.14 NIMs, you must disable KV cache by setting `NIM_ENABLE_KV_CACHE_REUSE=0`
  when `NIM_GUIDED_DECODING_BACKEND` is set to `lm-format-enforcer` or a custom backend.
- An incorrect backend name is treated as a custom backend.
- NIM automatically downloads the model from NGC, leveraging a local filesystem cache
  if available.
- Code Llama FP8 profiles are not released due to accuracy degradations.
- LoRA is not supported.
- The min_p sampling parameter is not compatible with Deepseek and will be set to
  0.0
- DeepSeek models require setting --trust-remote-code. This is handled automatically
  in DeepSeek NIMs.
- Only profiles matching specific hardware topologies are supported for the DeepSeek
  R1 model.
- DeepSeek-R1 profiles disable DP attention by default to avoid crashes at higher
  concurrency.
- DeepSeek Coder V2 Lite Instruct does not support kv_cache_reuse for vLLM.
- DeepSeek R1 Distill Llama 70B does not include pre-built engines for TP8, A10G,
  and H100.
- To deploy DeepSeek R1 Distill Qwen 32B, set NIM_MAX_MODEL_LEN = 32768 or less when
  using vLLM profile.
- Using a trtllm_buildable profile with a fine-tuned model can crash on H100.
- Recommend at least 80GB of CPU memory for DeepSeek R1 Distill Qwen 32B.
- When running vLLM engine with GPU that has smaller memory, may run into ValueError
  of model max sequence length larger than maximum KV cache storage.
- When running vLLM engine with A10G, may run into ValueError of model max sequence
  length larger than maximum KV cache storage.
- kv_cache_reuse is not supported.
- suffix parameter is not supported in API call.
- LoRA not supported for vLLM and TRT-LLM buildable.
- Accuracy degradation observed for profiles with specific configurations.
- Performance degradation observed at higher concurrencies with TRT-LLM engines compared
  to the previous release.
- For all version 1.14 NIMs, you must disable KV cache by setting `NIM_ENABLE_KV_CACHE_REUSE=0`
  when `NIM_GUIDED_DECODING_BACKEND` is set to `lm-format-enforcer` or a custom backend.
  An incorrect backend name is treated as a custom backend.
- 'Performance degradation observed for the following profiles: vllm-b200-bf16-1 and
  vllm-b200-bf16-2.'
- LoRA is not supported on L40S with TRT-LLM.
- H100 and L40s LoRA profiles can hang with high (>2000) ISL values.
- For the LoRA enabled profiles, TTFT can be worse with the pre-built engines compared
  to the vLLM fallback while throughput is better. If TTFT is critical, please consider
  using the vLLM fallback.
- 'For requests that consume the maximum sequence length generation (for example,
  requests that use ignore_eos: True), generation time can be very long and the request
  can consume the available KV cache causing future requests to stall. You should
  reduce concurrency under these conditions.'
- Performance degradation observed for BF16 profiles for ISL=5000 OSL=500 when concurrency
  > 100.
- Performance degradation observed at higher concurrencies >= 50 with TRT-LLM engines
  compared to previous release.
- Performance degradation observed (compared to OS vLLM) for vLLM profiles vllm-h200-bf16-2
  and vllm-rtx6000_blackwell_sv-bf16-2.
- 'Llama 3.1 models vLLM profiles fail with ValueError: Unknown RoPE scaling type
  extended.'
- Llama 3.1 FP8 requires NVIDIA driver version >= 550.
- LoRA isnt supported on 8 x GPU configuration.
- The vllm-fp16-tp2 profile has been validated and is known to work on H100 x 2 and
  A100 x 2 configurations. Other GPUs might encounter a 'CUDA out of memory' issue.
- Tool calling is not supported for Mistral NeMo Minitron 8B 8K Instruct and Mistral
  Small 24b Instruct 2501 models.
- The model requires at least 48GB of VRAM but cannot be launched on a single 48GB
  GPU such as L40S. Single-GPU deployment is only supported on GPUs with 80GB or more
  of VRAM (for example, A100 80GB or H100 80GB).
- Multi-turn tool calling is not supported for smaller parameter models (less than
  or equal to 3 billion parameters).
- The default context length allocation might result in Out of Memory (OOM) errors.
  For the A100 40GB GPU, set NIM_MAX_NUM_SEQS to 64. For the A10G GPU, set NIM_MAX_NUM_SEQS
  to 4. Other GPUs have suitable default values.
- Deploying NIM with NIM_LOG_LEVEL=CRITICAL causes the start process to hang. Use
  WARNING, DEBUG or INFO instead.
- FP8 profiles are not released due to accuracy degradations.
- The `min_p` sampling parameter is not compatible with Deepseek and will be set to
  `0.0`
- DeepSeek models require setting `--trust-remote-code`. This is handled automatically
  in DeepSeek NIMs.
- 'Only profiles matching the following hardware topologies are supported for the
  DeepSeek R1 model: 2 nodes of 8xH100, 1 node of 8xH200.'
- DeepSeek-R1 profiles disable DP attention by default to avoid crashes at higher
  concurrency. To turn on DP attention you can set `NIM_ENABLE_DP_ATTENTION`.
- BF16 profiles require at least 64GB GPU memory to launch.
- When running vLLM engine with GPU that has smaller memory, may run into ValueError
  of model max sequence length larger than maximum KV cache storage. Set `NIM_MAX_MODEL_LEN
  = 32768` or less when using vLLM profile.
- Recommend at least 80GB of CPU memory.
- When running vLLM engine with GPU memory less than 48GB, may run into ValueError
  of model max sequence length larger than maximum KV cache storage. Set `NIM_MAX_MODEL_LEN
  = 32768` to enable vLLM profile.
- When running vLLM engine with A10G, may run into ValueError of model max sequence
  length larger than maximum KV cache storage. Set `NIM_MAX_MODEL_LEN = 32768` to
  enable vLLM profile.
- The `/v1/health` and `/v1/metrics` [API endpoints](api-reference.html) return incorrect
  response values and empty response schemas instead of the expected health status
  and metrics data.
- Deploying a fine-tuned model fails for some TRT-LLM profiles when TP is greater
  than 1.
- SGLang is not supported.
- NVIDIA cannot guarantee the security of any models hosted on non-NVIDIA systems
  such as HuggingFace. Malicious or insecure models can result in serious security
  risks up to and including full remote code execution.
- NIM validates the required files in each folder structure.
- Tokenizer files are used for inference, and each backend defaults to nonoptimal
  tokenizers if these files are not present.
- NIM validates the required files in each folder structure. If one or more required
  files are missing, NIM raises an exception and prompts you to provide additional
  files.
- When NIM_MAX_IMAGES_PER_PROMPT is set to 0, the VLM will process only text inputs
  and ignore any image data in the prompt, effectively operating as a text-only language
  model.
- NIM validates the required files in each folder structure. If one or more required
  files are missing, NIM raises an exception and prompts you to provide additional
  files. Tokenizer files are used for inference, and each backend defaults to nonoptimal
  tokenizers if these files are not present. Refer to Troubleshooting for more details.
- Users should ensure that the tensor parallel and pipeline parallel sizes configured
  at start-up time using NIM_TENSOR_PARALLEL_SIZE and NIM_PIPELINE_PARALLEL_SIZE matches
  the TRTLLM checkpoint and engine ranks. Otherwise, NIM throws an exception to prompt
  users to fix the inference configurations.
- Users should ensure that the tensor parallel and pipeline parallel sizes configured
  at start-up time matches the TRTLLM checkpoint and engine ranks.
- NVIDIA NIM for LLMs supports Multi-Instance GPU (MIG) mode to partition supported
  NVIDIA GPUs into multiple isolated instances.
- This feature works best with smaller parameter models (less than or equal to 8 billion
  parameters).
- Any NVIDIA GPU should be, but is not guaranteed to be, able to run this model with
  sufficient GPU memory or multiple, homogeneous NVIDIA GPUs with sufficient aggregate
  memory, compute capability >= 7.0 (8.0 for bfloat16), and at least one GPU with
  95% or greater free memory.
- Any NVIDIA GPU should be able to run this model with sufficient GPU memory or multiple,
  homogeneous NVIDIA GPUs with sufficient aggregate memory, compute capability >=
  7.0 (8.0 for bfloat16), and at least one GPU with 95% or greater free memory.
- Any NVIDIA GPU should be able to run this model with sufficient GPU memory or multiple,
  homogeneous NVIDIA GPUs.
- Some model/GPU combinations, including vGPU, are optimized.
- Use the latest tag to always get the newest version of a NIM.
- Use the /v1/health/ready endpoint to check server readiness.
- You should set NIM_RELAX_MEM_CONSTRAINTS=1 when you deploy this NIM on WSL2 due
  to high memory usage.
- Any NVIDIA GPU should be able to run this model with sufficient GPU memory or multiple,
  homogeneous NVIDIA GPUs with sufficient aggregate memory.
- NIM automatically selects the most suitable profile based on your system specification.
- Use the /v1/health/ready endpoint to check if the server is ready.
- Uses API key for authentication  requires joining the NVIDIA Developer Program
  or obtaining an NVIDIA AI Enterprise license.
- Logs should not be relied upon for server readiness.
- Uses custom metrics API for autoscaling
- LeaderWorkerSets are recommended for multi-node deployments
- Uses NVIDIA API key for authentication.
- Do NOT provide the NGC_API_KEY after running create-model-store.
- Ensure model-store has permission to write inside the container.
- Use the /v1/health/ready endpoint to check if the server is ready to accept requests.
- Multi-node deployment requires coordinating the creation of NIM containers across
  multiple different nodes.
- Recommended approach for orchestration is to use Kubernetes with the nim-deploy
  Helm chart.
- 'Ensure your proxy allows outbound connections to all required endpoints: authn.nvidia.com,
  api.ngc.nvidia.com, xfiles.ngc.nvidia.com, huggingface.co, cas-bridge.xethub.hf.co.'
- Verify that the proxy supports HTTP 1.1 connections.
- LLM-specific NIM containers provide profiles that validate accuracy and performance
  characteristics across different hardware configurations (model/GPU combinations).
- Generic model profiles can be identified by the presence of local_build, vllm or
  sglang in the profile name.
- NIM_MODEL_NAME and HF_TOKEN are required for the multi-LLM compatible NIM container
  to download Hugging Face model URIs.
- fp8 models are chosen by default when available.
- All quantized engines are rigorously tested to meet the same accuracy criteria as
  the default fp16 engines.
- NIM_FT_MODEL must be a path to a directory containing a Hugging Face checkpoint
  or a TRTLLM checkpoint.
- Typically, TRTLLM checkpoints constructed using TRTLLM conversion scripts result
  in a partial engine configuration with only the pretrained configuration options.
- Reward models are used to score the outputs of another large language model.
- Detailed reasoning is on by default for this model.
- Support for custom guided decoding backends is experimental.
- Custom guided decoding backends are not supported with SGLang.
- Supports serving models in an air gap system (also known as air wall, air-gapping,
  or disconnected network)
- Run a NIM with no internet connection, and with no connection to the NGC registry
  or Hugging Face Hub
- Uses NIM for air gap deployment  requires setup of model repository
- 'To check server readiness for a multi-node deployment, perform a /v1/health/ready
  probe and evaluate the response. A successful probe will generate the following
  log entry: [INFO 2025-02-13 02:36:24.635 health.py:43] Health request successful.'
- Requires Kubernetes version >1.26
- DeepSeek R1 multi-node deployment (LLM NIM version 1.7.0) requires Helm version
  1.7.0
- For LLM-specific NIMs, do not set tool calling environment variables externally.
  If an LLM-specific NIM container supports tool calling, it is enabled automatically.
- Proxy must allow connections to external services like authn.nvidia.com, api.ngc.nvidia.com,
  xfiles.ngc.nvidia.com, huggingface.co, cas-bridge.xethub.hf.co.
- If using self-signed certificate, must mount the certificate and set SSL_CERT_FILE.
- Thinking budget control is supported on NVIDIA-Nemotron-Nano-9B-v2 only for version
  1.12.
- Thinking budget control is not supported with SGLang.
- Thinking budget control cannot be used with other structured generation backends
  at the moment.
- Optimized profiles leverage GPU specific TensorRT-LLM options in order to achieve
  optimal throughput and latency.
- Quantization leads to reduced memory requirements and therefore significant improvements
  in both latency and throughput.
- Requires NVIDIA NIM for LLMs version 1.0.2 or later.
- NIM_MODEL_NAME must be set to the local folder path for deploying a fine-tuned model.
- The TRTLLM checkpoint root directory should include Hugging Face tokenizer and configuration
  files.
- By default, these metrics are available at http://localhost:8000/v1/metrics.
- Reward models are used to score outputs of another large language model for fine-tuning
  or filtering datasets.
- NIM for LLMs supports LoRA PEFT adapters trained by the NVIDIA NeMo framework and
  Hugging Face Transformers libraries.
- LoRA model weights are tied to a particular base model.
- Generates long chain-of-thought style responses with explicit thinking tokens.
- Generates more concise responses without extended chain-of-thought or thinking tokens.
- LoRA model weights are tied to a particular base model. Deploy only LoRA models
  that were tuned with the same model as is being served by NIM.
- The underlying NIM that you use must match the base model of the LoRAs.
- NIM_PEFT_REFRESH_INTERVAL is set to 3600 seconds
- If an LLM-specific NIM container supports tool calling, it is enabled automatically.
- 'Per-request metrics are disabled by default. To enable them, set the environment
  variable: export NIM_PER_REQ_METRICS_ENABLE=1.'
- --gpus all only works if your configuration has the same number of GPUs as specified
  for the model in Supported Models for NVIDIA NIM for LLMs.
- Passing --shm-size=16GB to docker run is recommended for multi-GPU setups.
- Provide your NGC API key for authentication, and specify a custom model repository
  for air-gapped or mirrored environments.
- scheme defaults to 'http'.
- The string that terminates the thinking phase. Leave unset to use the default `</think>`
  tag.
- Set to `1` to enable reward score collection from the models response.
- Set to `0` to view logs of request details to `v1/completions` and `v1/chat/completions`.
  These logs contain sensitive attributes of the request including `prompt`, `sampling_params`,
  and `prompt_token_ids`. You should be aware that these attributes are exposed to
  the container logs when you set this to `0`.
- Deterministic mode is only supported on TRT-LLM buildable profiles.
- There may be a slight degradation in performance metrics, including latency and
  throughput, due to the additional constraints required to ensure determinism.
- You can enable key-value (KV) cache reuse by setting the environment variable `NIM_ENABLE_KV_CACHE_REUSE`
  to `1`.
- NVIDIA NIM for LLMs can offload KV cache blocks to host memory to improve cache
  reuse.
- 'The value for NIM_REPOSITORY_OVERRIDE follows the structure: <repository_type>://<repository_location>'
- Supported Repository Types include 's3'.
- LoRA adapters must be stored in their own directory under LOCAL_PEFT_DIRECTORY.
- Names of loaded LoRA adapters must match their directory names.
- This feature is supported only on the multi-LLM NIM container.
- NIM_PEFT_REFRESH_INTERVAL is set to 3600 seconds.
- Default value for NIM_PEFT_REFRESH_INTERVAL is None, meaning that once LoRAs are
  loaded from NIM_PEFT_SOURCE at start time, NIM for LLMs does not check NIM_PEFT_SOURCE
  again.
- We recommend that you use `guided_json` for optimal performance and reliability.
- Using the `outlines` backend for regular expressions might cause the regular expressions
  to fail to compile.
- Support for the Llama Stack API in NIMs is experimental.
- The Llama Stack API supports tool calling.
- Per-request metrics are disabled by default.
- 'To enable them, set the environment variable: export NIM_PER_REQ_METRICS_ENABLE=1.'
- NIM throws this exception because it cannot filter the supported backends if the
  model architecture is missing.
- If you must use the default port number for other processes, then change the host
  IP port using the -p parameter for docker run.
- It is recommended to set NIM_SERVED_MODEL_NAME to a custom model name to avoid having
  a long path sent in every request.
- Some NIMs are built with packages that vary from the standard base Docker container.
- These NIM container variants have important, underlying differences from NIMs built
  with the standard base container.
- The container docker run command doesnt support the -u $(id -u) parameter.
- Most of these variables are not used with an SGLang backend.
- KV Cache reuse (NIM_ENABLE_KV_CACHE_REUSE) is now enabled by default for improved
  performance.
- 'Only profiles matching the following hardware topologies are supported for the
  DeepSeek R1 model: 2 nodes of 8xH100, 1 node of 8xH200'
- Set `NIM_MAX_MODEL_LEN = 32768` or less when using vLLM profile.
- If you set [`NIM_MANIFEST_ALLOW_UNSAFE`](configuration.html#environment-variables)
  to `1`, deployment fails.
- Set `NIM_DISABLE_CUDA_GRAPH=1` when running BF16 profiles.
- Avoid using the `logit_bias` parameter with this model because the results are unpredictable.
- Stored responses are not persisted to disk, so all stored data is lost on server
  restart.
- Enabling response storage causes a memory leak because responses are not automatically
  cleaned up.
- When running vLLM engine with GPU memory less than 48GB, may run into ValueError
  of model max sequence length larger than maximum KV cache storage.
- 'Performance degradation observed for the following profiles: `tensorrt_llm-h100-bf16-8`,
  `tensorrt_llm-h100_nvl-bf16-8`, and `tensorrt_llm-h100-bf16-8-latency`.'
- --gpus all only works if your configuration has the same number of GPUs as specified
  for the model
- Passing --shm-size=16GB to docker run is recommended for multi-GPU setups
- Thinking budget control helps manage latency and cost for reasoning models.
- The log level of the NIM for LLMs service. Possible values are `DEFAULT`, `TRACE`,
  `DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.
- Set to `1` to enable JSON-formatted logs. By default, human-readable text logs are
  enabled.
- Set to `1` to enable OpenTelemetry instrumentation in NIMs.
- Set to `0` to enable the TRT-LLM PyTorch RT backend. Additionally, this feature
  requires `NIM_USE_TRTLLM_LEGACY_BACKEND` to be disabled.
- Set to `0` to disable the TRT-LLM legacy backend.
- Set to `1` to enable per-request metrics in API responses. When enabled, `/v1/completions`
  and `/v1/chat/completions` responses will include a `stats` field with detailed
  timing and token metrics for each request.
- logit_bias is not available for any model using the TRT-LLM backend.
- Empty metrics values on multi-GPU TensorRT-LLM model. Metrics items gpu_cache_usage_perc,
  num_request_max, num_requests_running, num_requests_waiting, and prompt_tokens_total
  wont be reported for multi-GPU TensorRT-LLM model, because TensorRT-LLM currently
  doesnt expose iteration statistics in orchestrator mode.
- You can enable key-value (KV) cache reuse by setting the environment variable NIM_ENABLE_KV_CACHE_REUSE
  to 1.
- This documentation contains the release notes for NVIDIA NIM for Large Language
  Models (LLMs).
- NVIDIA NIM for LLMs 1.12.0 introduces significant enhancements and bug fixes across
  multiple areas, improving support for TensorRT-LLM backend, updating dependencies,
  and enhancing guided decoding and LoRA functionalities.
- NIM_REPOSITORY_OVERRIDE replaces the default repository location and protocol used
  to access the repository.
- Deploying with KServe can require changing permissions for the cache directory.
- GH200 NVIDIA driver <560.35.03 can cause a segmentation fault or hanging during
  deployment. Fixed in GPU driver 560.35.03
- Optimized engines (TRT-LLM) arent supported with NVIDIA vGPU. To use optimized
  engines, use GPU Passthrough.
- Prompts with Unicode characters in the range from 0x0e0020 to 0x0e007f can produce
  unpredictable responses. NVIDIA recommends that you filter these characters out
  of prompts before submitting the prompt to an LLM.
- The container may crash when building local TensorRT LLM engines if there isnt
  enough host memory. If that happens, try setting NIM_LOW_MEMORY_MODE=1.
- Out-of-Bounds (OOB) sequence length with tensorrt_llm-local_build is 8K. Use the
  NIM_MAX_MODEL_LEN environment variable to modify the sequence length within the
  range of values supported by a model.
- NIM does not support Multi-instance GPU mode (MIG).
- 'vGPU related issues: trtllm_buildable profiles might encounter an Out of Memory
  (OOM) error on vGPU systems, which can be fixed via NIM_LOW_MEMORY_MODE=1 flag.'
- 'When using vGPU systems with trtllm_buildable profiles, you might still encounter
  a broken connection error. For example, client_loop: send disconnect: Broken pipe.'
- vLLM for A100 and H200 is not supported.
- NIM with vLLM backend may intermittently enter a state where the API returns a 'Service
  in unhealthy' message. This is a known issue with vLLM. You must restart the NIM
  in this case.
- You cant deploy fp8 quantized engines on H100-NVL GPUs with deterministic generation
  mode on.
- INT4/INT8 quantized profiles are not supported for Blackwell GPUs.
- When using Native TLS Stack to download the model, you should set --ulimit nofile=1048576
  in the docker run command.
- Air Gap Deployments of a model like Llama 3.3 Nemotron Super 49B that uses the model
  directory option might not work if the model directory is in the HuggingFace format.
  Switch to using NIM_FT_MODEL in those cases.
- Llama-3.1-Nemotron-Ultra-253B-v1 does not work on H100s and A100s. Use H200s and
  B200s to deploy successfully.
- BF16 profiles require at least 64GB GPU memory to launch. For example, `vllm-bf16-tp1-pp1`
  profile does not launch successfully on a single L20 or other supported GPUs with
  GPU memory less than 80GB.
- If you send more than 15 concurrent requests with detailed thinking on, the container
  may crash.
- Deploying with KServe fails. As a workaround, try increasing the CPU memory to at
  least 77GB in the runtime YAML file.
- Buildable TRT-LLM BF16 TP4 LoRA profiles on A100 and H100 can fail due to not enough
  host memory. You can work around this problem by setting `NIM_LOW_MEMORY_MODE=1`.
- The alternative option to use vLLM is not supported due to poor performance.
- Tool calling is not supported.
- This model requires at least 48GB of VRAM but cannot be launched on a single 48GB
  GPU such as L40S. Single-GPU deployment is only supported on GPUs with 80GB or more
  of VRAM (for example, A100 80GB or H100 80GB).
- This model is optimized for Arabic language contexts. While the model does process
  input in other languages, you may experience inconsistencies or reduced accuracy
  in content generated for non-Arabic languages.
- Does not support the chat endpoint.
- Unlike the OpenAI API, the Llama Stack API supports only the tool choices 'auto',
  'required', or None.
- The Granite-3.3-2b-instruct model is not compatible with the `TensorRT-LLM` backend.
  This model can only be deployed using the `SGLang` or `vLLM` backends.
- Guided Decoding might not work for `Llama-3.3-70b-instruct` with `TensorRT-LLM`
  backend.
- Llama-3.3-nemotron-super-49b-v1 is only supported with `TensorRT-LLM` backend.
- For llama-3.3-nemotron-super-49b-v1, Concurrent requests > 15 with detailed thinking
  on leads to a crash.
- If `sglang` inference enters OOM at higher-workloads, try setting `NIM_KVCACHE_PERCENT`
  to something lower than 0.85.
- '`xgrammar` guided decoding with `sglang` for `NIM_TENSOR_PARALLEL_SIZE > 1` is
  not functioning as expected because of a known issue in `xgrammar`.'
- Models with `nemo` format checkpoints cannot be deployed.
- 'If you see the RuntimeError of `NCCL error: unhandled system error`, increase the
  `--shm-size` to resolve the issue.'
- NIM requires TensorRT-LLM engine files to be present in the model directory, either
  locally or after downloading and caching from a remote URI.
- In case of local model deployment in LLM-NIMs, the curl requests expect model name
  to be the absolute path to the local model.
- 'For air gap deployment, add the following parameters to the docker run command:
  -e NIM_DISABLE_MODEL_DOWNLOAD=1 -v <local-model-path>:<model-weight-path> -e NIM_MODEL_PATH=<model-weight-path>.'
- The `min_p` sampling parameter is not compatible with Deepseek and will be set to
  `0.0`.
- Some models do not include pre-built engines for certain configurations.
- BF16 profiles require specific GPU memory to launch.
- Structured generation has unexpected behavior due to CoT output.
- Performance degradation observed for certain profiles when running with specific
  GPUs.
- Concurrent requests may be blocked when running NIM with large `max_tokens` value.
- Insufficient memory for KV cache and LoRA cache might result in Out of Memory (OOM)
  errors.
- LoRA is not supported for vLLM and TRT-LLM buildable.
- Logarithmic Probabilities (`logprobs`) support with echo requires TRTLLM engine
  to be built with `--gather_generation_logits`.
- Empty metrics values on multi-GPU TensorRT-LLM model will not be reported for specific
  metrics.
- The model does not include pre-built engines for certain configurations.
- When running with smaller memory, may run into ValueError of model max sequence
  length larger than maximum KV cache storage.
- When running with GPU memory less than 48GB, may run into ValueError of model max
  sequence length larger than maximum KV cache storage.
- When using A10G, may run into ValueError of model max sequence length larger than
  maximum KV cache storage.
- Some models can only be deployed using the vLLM backend.
- Custom decoding is not supported.
- Tool calling is done sequentially (not in parallel).
- For L40S GPUs, set `NIM_KVCACHE_PERCENT=0.8` to resolve out of memory (OOM) errors.
- A Harmony parser error occurs intermittently during streaming with tool calling.
- When passing the payload using the Responses API, background fill is disabled.
- Performance degradation observed for certain profiles at higher concurrencies.
- Cache engines built for supervised fine-tuning (SFT) models dont work.
- Accuracy degradation observed for certain profiles.
- Parallel tool calling is not supported.
- Due to a bug in automatic profile selection, the vLLM profile is always chosen as
  the default. To use a TRT-LLM profile instead, refer to Profile Selection.
- The `list-model-profiles` command returns errors for Multi-LLM compatible NIMs.
- 'When deploying models, you may get `Download Error - Too many open files`. To resolve
  the error, do the following: For Docker deployments, add the `--ulimit nofile=65536:65536`
  option. For Kubernetes deployments, run the following commands: sudo mkdir -p /etc/systemd/system/containerd.service.d;
  echo ''[Service]'' | sudo tee /etc/systemd/system/containerd.service.d/override.conf;
  echo ''LimitNOFILE=65536'' | sudo tee -a /etc/systemd/system/containerd.service.d/override.conf;
  sudo systemctl daemon-reload; sudo systemctl restart containerd; sudo systemctl
  restart kubelet.'
- Models with 8 billion parameters require `NIM_KVCACHE_PERCENT=0.8` for `tp=1` profiles.
- '`NIM_ENABLE_PROMPT_LOGPROBS=1` is not supported for the TRTLLM backend.'
- Smaller parameter models (<= 3 billion parameters) can have tool calling enabled
  but are highly inaccurate due to their limited parameter count. We dont recommend
  using such models for tool calling use cases.
- If `ignore_eos=true`, the model ignores EOS tokens and keeps generating until a
  custom stop token is encountered or the max token limit is reached (if not set,
  the default context window size is 128 tokens). For VLLM and simple queries, we
  recommend using `ignore_eos=false` (default).
- For [tool calling](function-calling.html), `tool_choice` is always set to `auto`.
- The `vllm-fp16-tp2` profile has been validated and is known to work on H100 x 2
  and A100 x 2 configurations. Other GPUs might encounter a 'CUDA out of memory' issue.
- Creating chat completion with non-existing model returns a 500 when it should return
  a 404.
- NIM doesnt include support for TRT-LLM buildable profiles.
- Added host-based KV cache offloading support to improve memory efficiency when KV
  cache reuse is enabled.
- Added support for running NIM behind an SSL forward proxy.
- Added reward model support with custom reward string and logits range.
- Air Gap Deployments of a model like Llama 3.3 Nemotron Super 49B that uses the model
  directory option might not work if the model directory is in the HuggingFace format.
- 'Code Llama: FP8 profiles are not released due to accuracy degradations. LoRA is
  not supported.'
- 'Deepseek: The `min_p` sampling parameter is not compatible with Deepseek and will
  be set to `0.0`. LoRA, Guided Decoding, and FT (fine-tuning) are not supported.
  DeepSeek models require setting `--trust-remote-code`. This is handled automatically
  in DeepSeek NIMs.'
- 'DeepSeek R1: Only profiles matching specific hardware topologies are supported.
  DeepSeek-R1 profiles disable DP attention by default to avoid crashes at higher
  concurrency.'
- 'DeepSeek R1 Distill Qwen 32B: BF16 profiles require at least 64GB GPU memory to
  launch.'
- 'DeepSeek R1 Distill Qwen 14B: When running vLLM engine with GPU memory less than
  48GB, may run into ValueError of model max sequence length larger than maximum KV
  cache storage.'
- 'Llama 3.1 Typhoon 2 8B Instruct: Performance degradation observed on TRT-LLM profiles
  when ISL>OSL and concurrency is 100 or 250 for specific GPUs.'
- 'Llama 3.3 70B Instruct: At least 400GB of CPU memory is required. Concurrent requests
  are blocked when running NIM with the `-e NIM_MAX_MODEL_LENGTH` option and a large
  `max_tokens` value in the request.'
- 'Llama 3.1 models: vLLM profiles fail with `ValueError: Unknown RoPE scaling type
  extended`. Requires NVIDIA driver version >= 550.'
- 'Sarvam - M: This model requires at least 48GB of VRAM but cannot be launched on
  a single 48GB GPU such as L40S.'
- Code Llama LoRA is not supported.
- Deepseek min_p sampling parameter is not compatible with Deepseek and will be set
  to 0.0
- 'The following are not supported for DeepSeek models: LoRA, Guided Decoding, FT
  (fine-tuning).'
- DeepSeek-R1 profiles disable DP attention by default to avoid crashes at higher
  concurrency. To turn on DP attention you can set NIM_ENABLE_DP_ATTENTION.
- DeepSeek R1 Distill Llama 70B model does not include pre-built engines for TP8,
  A10G, and H100.
- To deploy DeepSeek R1 Distill Llama 70B, set -e NIM_MAX_MODEL_LEN = 131072.
- BF16 profiles require at least 64GB GPU memory to launch for DeepSeek R1 Distill
  Qwen 32B.
- Structured generation has unexpected behavior due to CoT output for DeepSeek R1
  Distill Qwen 32B.
- When running vLLM engine with GPU that has smaller memory, may run into ValueError
  of model max sequence length larger than maximum KV cache storage. Set NIM_MAX_MODEL_LEN
  = 32768 or less when using vLLM profile.
- Using a trtllm_buildable profile with a fine-tuned model can crash on H100 for DeepSeek
  R1 Distill Qwen 32B.
- The vLLM backend is not supported on Llama Nemotron models.
- Many models return a 500 error when using structured generation with context-free
  grammar.
- The `TRACE` log level for `NIM_LOG_LEVEL` is not supported.
- Some stop words might not work as expected and could appear in the output.
- vLLM is not supported on A100 and H200.
- Structured generation using regular expressions may produce unexpected responses.
- The maximum supported context length may decrease based on memory availability.
- The `top_logprobs` parameter is not supported.
- All models return a 500 error when `logprobs=2`, `echo=true`, and `stream=false`
  are set.
- On GH200, NVIDIA driver versions <560.35.03 can cause segmentation faults or hangs
  during deployment. Fixed in GPU driver 560.35.03.
- For all pre-built engines, gather_context_logits is not enabled.
- The tool_choice is not supported.
- This model requires at least 48GB of VRAM but cannot be launched on a single 48GB
  GPU such as L40S.
- Single-GPU deployment is only supported on GPUs with 80GB or more of VRAM.
- This model is optimized for Arabic language contexts.
- The suffix parameter isnt supported in API calls.
- Deployment fails on H100 with vLLM (TP1, PP1) at 250 concurrent requests.
- Using FP32 checkpoints for the NIM_FT_MODEL variable or local build isnt supported.
- Setting NIM_TOKENIZER_MODE=slow is not supported.
- The SGLang backend is not supported.
- 'LoRA is not supported for the following models: Gemma 2 9B, Llama 3.1 Swallow 8B
  Instruct v0.1.'
- Prompts with Unicode characters in the range from 0x0e0020 to 0x0e007f can produce
  unpredictable responses.
- When launching a Docker container for the DeepSeek R1 model, dont use the `-u $(id
  -u)` parameter or else you may encounter launch errors.
- For llama-3.3-nemotron-super-49b-v1, Concurrent requests > 15 with detailed thinking
  on leads to a crash. Recommend using the LLM-specific NIM - llama-3.3-nemotron-super-49b-v1/tags.
- If `sglang` inference enters OOM at higher workloads, try setting `NIM_KVCACHE_PERCENT`
  to something lower than 0.85. `sglang` requires significantly more memory than other
  backends.
- '`xgrammar` guided decoding with `sglang` for `NIM_TENSOR_PARALLEL_SIZE > 1` is
  not functioning as expected because of a known issue in `xgrammar`. Please restrict
  to `NIM_TENSOR_PARALLEL_SIZE = 1` for `sglang` backend.'
- The `suffix` parameter isnt supported in API calls.
- LoRA is not supported for Llama 3.1 405B Instruct
- vLLM profiles are not supported for Llama 3.1 405B Instruct
- Throughput optimized profiles are not supported on A100 FP16 and H100 FP16 for Llama
  3.1 405B Instruct
- CUDA out of memory issue for Llama2 70b v1.0.3
- Llama 3.1 FP8 requires NVIDIA driver version >= 550
- All models return a 500 when setting `logprobs=2`, `echo=true`, and `stream=false`;
  they should return a 200.
- StarCoder2 7B might return a KV cache no new block error. Set `NIM_MAX_MODEL_LEN
  = 4096` to enable all profiles.
- When UVM is disabled, the TRT-LLM profile is selected.
- Filenames should not contain spaces if a custom fine-tuned model directory is provided.
- The 'fast_outlines' guided decoding backend will fail with requests that force the
  model to generate emoji.
- All models return a 500 when setting `logprobs=2`, `echo=true`, and `stream=false`.
- 'LoRA is not supported for the following models: Gemma 2 9B, Llama 3.1 Swallow 8B
  Instruct v0.1, Llama 3.1 Swallow 70B Instruct v0.1'
- Structured generation has unexpected behavior due to CoT output. Despite this, `guided_json`
  parameter exhibits normal functionality when used with a JSON schema prompt.
- Does not support the System role in a chat or completions API call.
- 'Performance degradation observed on TRT-LLM profiles when ISL>OSL and concurrency
  is 100 or 250 for the following GPUs: H200, A100, and L40S.'
- The `/v1/health` and `/v1/metrics` API endpoints return incorrect response values
  and empty response schemas instead of the expected health status and metrics data.
- The model might occasionally bypass its typical thinking patterns for certain queries,
  especially in multi-turn conversations.
- You cannot deploy this model using KServe.
- Listing the profiles for this model when the local cache is enabled can result in
  log warnings, which do not impact NIM functionality.
- Logs for this model can contain spurious warnings. You can safely ignore them.
- At least 400GB of CPU memory is required.
- Concurrent requests are blocked when running NIM with the `-e NIM_MAX_MODEL_LENGTH`
  option and a large `max_tokens` value in the request.
- Accuracy was noted to be lower than the expected range with profiles `vllm-bf16-tp4-pp1-lora`
  and `vllm-bf16-tp8-pp1`.
- Insufficient memory for KV cache and LoRA cache might result in Out of Memory (OOM)
  errors. Make sure the hardware is appropriately sized based on the memory requirements
  for the workload. Long context and LoRA workloads should use larger TP configurations.
- gather_context_logits is not enabled by default. If you require logits output, specify
  it in your TRT-LLM configuration when using the `trtllm_buildable` feature by setting
  the environment variable `NIM_ENABLE_PROMPT_LOGPROBS`.
- 'Performance degradation observed (compared to OS vLLM) for the following TRT-LLM
  LoRA profiles: `tensorrt_llm-b200-fp8-tp1-pp1-throughput-lora` and `tensorrt_llm-b200-bf16-tp1-pp1-throughput-lora`.'
- 'Performance degradation observed (compared to OS vLLM) for the following vLLM profiles:
  `vllm-b200-bf16-1` and `vllm-a100_sxm4_40gb-bf16-1`.'
- 'When making requests that consume the maximum sequence length generation (such
  as using `ignore_eos: True`), generation time might be significantly longer and
  can exhaust the available KV cache, causing future requests to stall. In this scenario,
  we recommend that you reduce concurrency.'
- Currently, LoRA is not supported for this model.
- Currently, tool calling is not supported.
- 'Accuracy degradation observed for the following profiles: `vllm-a100-bf16-1` and
  `vllm-h200-bf16-2`.'
- TRT-LLM BF16 TP16 buildable profile cannot be deployed on A100.
- Throughput optimized profiles are not supported on A100 FP16 and H100 FP16.
- vLLM profiles are not supported.
- 'Accuracy degradation observed for the following profiles: H200 TRT-LLM, B200 FP8,
  TP2, LoRa.'
- The performance of vLLM LoRA on L40s TP88 is significantly suboptimal.
- Create chat completion with non-existing model returns a 500 when it should return
  a 404.
- 'vLLM profiles fail with `ValueError: Unknown RoPE scaling type extended`.'
- requires NVIDIA driver version >= 550.
- The `vllm-fp16-tp2` profile has been validated and is known to work on H100 x 2
  and A100 x 2 configurations. Other GPUs might encounter a CUDA out of memory issue.
- Deployment fails for vLLM profiles when `NIM_ENABLE_KV_CACHE_REUSE=1`.
- Using FP32 checkpoints for the `NIM_FT_MODEL` variable or local build isnt supported.
- The "fast_outlines" guided decoding backend will fail with requests that force the
  model to generate emoji.
- Nemotron4 models require use of slow tokenizers. fast tokenizers causes accuracy
  degradation.
- LoRA is not supported for Llama 3.1 405B Instruct.
- vLLM profiles are not supported for Llama 3.1 405B Instruct.
- When `repetition_penalty=2`, the response time for larger models is greater. Use
  `repetition_penalty=1` on larger models.
- Llama 3.1 8B Instruct H100 and L40s LoRA profiles can hang with high (>2000) ISL
  values.
- Cache deployment fails for air-gapped system or read-only volume for multi-GPU vLLM
  profile
- Docker container launch errors if using `-u $(id -u)` parameter.
- The `min_p` sampling parameter is set to `0.0` for Deepseek.
- LoRA, Guided Decoding, and FT (fine-tuning) are not supported for DeepSeek models.
- DeepSeek models require `--trust-remote-code`.
- Only specific hardware topologies are supported for DeepSeek R1 model.
- NIM does not support Multi-instance GPU mode (MIG)
- P-Tuning isnt supported.
- Llama3 70b v1.0.0 - LoRA is not supported on non-optimized 8 x GPU configuration
- Empty metrics values on multi-GPU TensorRT-LLM model
- '`No tokenizer found` error when running PEFT  this warning can be safely ignored.'
- PB releases provide stable, secure AI frameworks and SDKs for mission-critical applications.
- It is essential to launch the container as the root user, otherwise permission issues
  are likely to occur.
- 'Theres an incorrect warning regarding checksums when running the 1.3 NIM. For
  example: `Profile 0462612f0f2de63b2d423bc3863030835c0fbdbc13b531868670cc416e030029
  is not fully defined with checksums`. It is safe to ignore this warning.'
- StarCoder2 7B might run into a KV cache 'no new block' error. Set NIM_MAX_MODEL_LEN
  = 4096 to enable all profiles.
- NVIDIA AI Enterprise (NVAIE) infrastructure does not support the RTX 4090 GPU.
- When UVM is disabled, the TRT-LLM profile is selected. In the previous releases,
  when UVM was disabled, the vLLM profile was selected.
- '[StarCoderBase 15.5B](supported-models.html#starcoderbase-15-5b) does not support
  the chat endpoint.'
- '[Llama 3.3 70B Instruct](supported-models.html#llama-3-3-70b-instruct) requires
  at least 400GB of CPU memory.'
- '[DeepSeek-R1-Distill-Llama-70B](supported-models.html#deepseek-r1-distill-llama-70b)
  does not include pre-built engines for TP8, A10G, and H100. To deploy, set `-e NIM_MAX_MODEL_LEN
  = 131072`.'
- Gemma-2-2b does not support the System role in a chat or completions API call.
- Users with systems that do not have pre-built optimized engines available should
  see substantial speed ups over previous versions of NIM, but may experience slower
  start times on first deployment due to the local compilation process.
- NIM validates the required files in each folder structure. If one or more required
  files are missing, NIM raises an exception and prompts you to provide additional
  files. Tokenizer files are used for inference, and each backend defaults to nonoptimal
  tokenizers if these files are not present.
- Users should ensure that the tensor parallel and pipeline parallel sizes configured
  at start-up time using NIM_TENSOR_PARALLEL_SIZE and NIM_PIPELINE_PARALLEL_SIZE matches
  the TRTLLM checkpoint and engine ranks.
- This model supports LoRA and vGPU.
- Use the /v1/health/ready endpoint to check if the server is ready to accept inference
  requests.
- Uses NVIDIA Developer Program or NVIDIA AI Enterprise license for access
- Set NIM_RELAX_MEM_CONSTRAINTS=1 when deploying on WSL2 due to high memory usage
- Server is ready when /v1/health/ready returns a 200 status code.
- Compute capability >= 7.0 (8.0 for bfloat16) and at least one GPU with 95% or greater
  free memory.
- Supports serving models in an air gap system with no internet connection
- Ensure model-store has permission to write inside the container
- NIM supports serving models in an air gap system with no internet connection.
- Downloading the model will take quite a long time. Give it as much time as ends
  up being needed.
- Proxy server address must include the scheme and port, e.g., https://proxy.company.com:3128.
- If the proxy requires authentication, use the format https://$user:$pass@proxy_host:$port.
- TRTLLM checkpoints should include Hugging Face tokenizer and configuration files.
- Reward models are often used to score the outputs of another large language model
  for fine-tuning that model or filtering synthetically created datasets.
- 'Use system prompt to control reasoning mode: ''detailed thinking on'' for detailed
  responses, ''detailed thinking off'' for concise responses.'
- Use /v1/health/ready to check server readiness.
- For inference requests use /v1/models.
- To run NVIDIA NIM for LLMs, set NIM_RELAX_MEM_CONSTRAINTS=1 when deploying on WSL2
  due to high memory usage.
- Models can be large, and a cluster operator can quickly fill disk space when downloading
  them.
- By default, metrics are available at http://localhost:8000/v1/metrics
- Do NOT provide the NGC_API_KEY (after running `download-to-cache`)
- When you use `create-model-store` with vLLM profiles, set the `NIM_MODEL_PROFILE`
  environment variable to `vllm`.
- 'To check server readiness for a multi-node deployment, perform a `/v1/health/ready`
  probe and evaluate the response. A successful probe will generate the following
  log entry: `[INFO 2025-02-13 02:36:24.635 health.py:43] Health request successful.`'
- Only optimized profiles are supported for multi-node deployment at this time.
- The default value for NIM_PEFT_REFRESH_INTERVAL is None.
- If NIM_PEFT_REFRESH_INTERVAL is set to 10, NIM for LLMs checks NIM_PEFT_SOURCE for
  new and removed models every 10 seconds.
- Ensure you have the proxy server address and port.
- If the proxy requires authentication, provide the username and password.
- If using a self-signed certificate, include the CA certificate file.
- xgrammar does not support all configurations of guided decoding.
- NIM will choose a profile automatically according to the rules laid out in Automatic
  Profile Selection.
- LLM-specific NIM containers provide profiles that validate accuracy and performance
  characteristics across different hardware configurations.
- Uses NGC API key for authentication.
- Default value for NIM_BUDGET_CONTROL_THINKING_STOP_STRING is </think>
- Default value for NIM_REWARD_MODEL is None
- Default value for NIM_REWARD_MODEL_STRING is None
- Default value for NIM_REWARD_LOGITS_RANGE is None
- Default value for NIM_LOG_LEVEL is DEFAULT
- Default value for NIM_JSONL_LOGGING is None
- Default value for NIM_DISABLE_LOG_REQUESTS is 1
- Default value for NIM_ENABLE_OTEL is None
- Default value for OTEL_TRACES_EXPORTER is console
- Default value for OTEL_METRICS_EXPORTER is console
- Default value for OTEL_LOGS_EXPORTER is console
- Default value for NIM_OTEL_SERVICE_NAME is None
- Default value for NIM_DISABLE_TRTLLM_PYTORCH_RT is 1
- Default value for NIM_USE_TRTLLM_LEGACY_BACKEND is 1
- Default value for NIM_PER_REQ_METRICS_ENABLE is 0
- When deterministic mode is enabled, the generated text remains consistent across
  multiple inference runs.
- Enabling key-value (KV) cache reuse improves inference speed when more than 90%
  of the initial prompt is identical across multiple requests.
- KV cache reuse speeds up time to first token (TTFT) starting with the second request.
- NIM_REPOSITORY_OVERRIDE can be set to override repository location.
- Feature is supported only on the multi-LLM NIM container.
- To enable tool calling, launch the server with the following environment variables
  for LLM NIMs.
- If the chat completion response contains an empty `tool_call` field but the function
  call is present in the `content` field, the output of the LLM was not post-processed
  into a tool call successfully.
- 'The following environment variables arent currently supported: NIM_CUSTOM_GUIDED_DECODING_BACKENDS,
  NIM_CUSTOM_MODEL_NAME, NIM_DISABLE_CUDA_GRAPH: Defaults to False, NIM_DISABLE_OVERLAP_SCHEDULING,
  NIM_ENABLE_DP_ATTENTION, NIM_ENABLE_KV_CACHE_HOST_OFFLOAD, NIM_ENABLE_PROMPT_LOGPROBS,
  NIM_FORCE_DETERMINISTIC, NIM_FORCE_TRUST_REMOTE_CODE: Defaults to True, NIM_FT_MODEL,
  NIM_GUIDED_DECODING_BACKEND, NIM_JSONL_LOGGING, NIM_KV_CACHE_HOST_MEM_FRACTION,
  NIM_LOW_MEMORY_MODE, NIM_MANIFEST_ALLOW_UNSAFE: No longer required, NIM_MAX_CPU_LORAS,
  NIM_MAX_GPU_LORAS, NIM_NUM_KV_CACHE_SEQ_LENS, NIM_PEFT_REFRESH_INTERVAL, NIM_PEFT_SOURCE,
  NIM_RELAX_MEM_CONSTRAINTS, NIM_REWARD_LOGITS_RANGE, NIM_REWARD_MODEL, NIM_REWARD_MODEL_STRING,
  NIM_SCHEDULER_POLICY, NIM_SERVED_MODEL_NAME: Only a single name is supported, NIM_TOKENIZER_MODE:
  Defaults to fast mode, SSL_CERT_FILE: Use NIM_SSL_CERT_PATH instead'
- 'The container docker run command doesnt support the -u $(id -u) parameter. For
  air gap deployment, add the following parameters to the docker run command: -e NIM_DISABLE_MODEL_DOWNLOAD=1,
  -v <local-model-path>:<model-weight-path>, -e NIM_MODEL_PATH=<model-weight-path>'
- The names of the loaded LoRA adapters must match their directory names.
- NIM_PEFT_REFRESH_INTERVAL checks NIM_PEFT_SOURCE for newly added models every hour
- When enabled, API responses will include a stats field with detailed timing and
  token metrics.
- --shm-size=16GB is recommended for multi-GPU setups.
- Provide your NGC API key for authentication.
- Specify a custom model repository for air-gapped or mirrored environments.
- The string that terminates the thinking phase. Leave unset to use the default </think>
  tag.
- Set to 1 to enable reward score collection from the models response.
- The log level of the NIM for LLMs service. Possible values are DEFAULT, TRACE, DEBUG,
  INFO, WARNING, ERROR, and CRITICAL.
- Set to 1 to enable JSON-formatted logs. By default, human-readable text logs are
  enabled.
- Set to 0 to view logs of request details to v1/completions and v1/chat/completions.
- Set to 1 to enable OpenTelemetry instrumentation in NIMs.
- Set to 0 to enable the TRT-LLM PyTorch RT backend.
- Set to 0 to disable the TRT-LLM legacy backend.
- Set to 1 to enable per-request metrics in API responses.
- Enabling deterministic generation requires setting the environment variable NIM_FORCE_DETERMINISTIC=1.
- Offloading to host memory helps keep more KV cache blocks available for reuse.
- NIM_REPOSITORY_OVERRIDE replaces the default repository location
- Supports S3 as a repository type
- The Llama Stack API supports tool choices 'auto', 'required', or None.
- For air gap deployment, add specific parameters to the docker run command.
- 'For air gap deployment, add the following parameters to the docker run command:
  -e NIM_DISABLE_MODEL_DOWNLOAD=1, -v <local-model-path>:<model-weight-path>, -e NIM_MODEL_PATH=<model-weight-path>'
errors:
- 500 status code (or a 200 status code and a BadRequest error) when logprobs is set
  to 0 in the request.
- '500: All models return a 500 when setting `logprobs=2`, `echo=true`, and `stream=false`;
  they should return a 200.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '500: The server returns a 500 status code (or a 200 status code and a `BadRequest`
  error) when `logprobs` is set to `0` in the request.'
- Create chat completion with non-existing model returns a 500 when it should return
  a 404.
- 'ValueError: Unknown RoPE scaling type extended.'
- NIM throws an exception to prompt users to fix the inference configurations.
- '200: Success'
- '401: Unauthorized'
- '400 Bad Request: Check request parameters.'
- '401 Unauthorized: Recheck API key.'
- '404 Not Found: Check endpoint path.'
- 'BadRequestError: Field required'
- '400 BadRequestError: Field required'
- 'Too many open files: Increase the limit, for example to 1,048,576 file descriptors.'
- GLOO or NCCL related interface errors may occur if the deployment configuration
  is incorrect.
- 'Too many open files: Increase file descriptor limits to avoid download failures.'
- 'NIM_DISABLE_TRTLLM_PYTORCH_RT: Set to `0` to enable the TRT-LLM PyTorch RT backend.'
- 'NIM_USE_TRTLLM_LEGACY_BACKEND: Set to `0` to disable the TRT-LLM legacy backend.'
- 'ValueError: Invalid repository ID or local directory specified: /models/llama3-8b-instruct/.
  Missing HuggingFace configuration file named config.json.'
- 'ValueError: Model architecture is None. Please check for valid model architecture
  in HF configuration file or contact NIM support.'
- 'ValueError: Found unknown format of weights in /path/to/model. Expected one of
  [''hf-safetensor'', ''trtllm-engine'', ''trtllm-ckpt'', ''gguf''].'
- 'logprobs: Not supported'
- 'suffix: Not supported'
- 'Guided decoding: Not supported'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '500: When setting `logprobs=2`, `echo=true`, and `stream=false`; they should return
  a 200.'
- 'ValueError: Unknown RoPE scaling type extended'
- '500: All models return a 500 when setting logprobs=2, echo=true, and stream=false;
  they should return a 200.'
- 'Download Error - Too many open files: For Docker deployments, add the --ulimit
  nofile=65536:65536 option.'
- '500: The server returns a 500 status code (or a 200 status code and a BadRequest
  error) when `logprobs` is set to `0` in the request.'
- '200: Create chat completion with non-existing model returns a 500 when it should
  return a 404.'
- All models return a 500 when setting `logprobs=2`, `echo=true`, and `stream=false`;
  they should return a 200.
- Deploying with KServe can require changing permissions for the cache directory.
- Function calling and structured generation is not supported for pipeline parallelism
  greater than 1.
- 'Logarithmic Probabilities (`logprobs`) support with echo: TRTLLM engine needs to
  be built explicitly with [`--gather_generation_logits`](https://nvidia.github.io/TensorRT-LLM/commands/trtllm-build.html)'
- '`logit_bias` is not available for any model using the TRT-LLM backend.'
- 'Echo and role configuration: Not supported'
- 'Reward: Not supported'
- 'Llama API: Not supported'
- 'nvext: Not supported'
- 500 status code (or a 200 status code and a BadRequest error) when `logprobs` is
  set to `0` in the request.
- '500: Deploying NIM with `NIM_LOG_LEVEL=CRITICAL` causes the start process to hang.'
- '500: The server returns a 500 status code when setting logprobs=2, echo=true, and
  stream=false.'
- 'Service in unhealthy: NIM with vLLM backend may intermittently enter a state where
  the API returns a ''Service in unhealthy'' message.'
- '500 Internal Server Error: All models return a 500 when setting logprobs=2, echo=true,
  and stream=false.'
- NIM does not support Multi-instance GPU mode (MIG)
- All models return a 500 when setting logprobs=2, echo=true, and stream=false; they
  should return a 200.
- '500 Internal Server Error: Check settings for logprobs and echo'
- 'Service in unhealthy: Restart the NIM'
- '500: Create chat completion with non-existing model returns a 500 when it should
  return a 404.'
- 'CUDA out of memory: Other GPUs might encounter a CUDA out of memory issue.'
- '500: Returns a 500 when setting logprobs=2, echo=true, and stream=false; it should
  return 200.'
- 'Out of Memory (OOM): trtllm_buildable profiles might encounter an Out of Memory
  (OOM) error on vGPU systems, which can be fixed via NIM_LOW_MEMORY_MODE=1 flag.'
- 'Service in unhealthy: NIM with vLLM backend may intermittently enter a state where
  the API return a ''Service in unhealthy'' message.'
- No tokenizer found error when running PEFT
- 'Unauthorized: Check API key or permissions'
- '400: Missing ''messages'' or ''prompt'' field'
- '401 Unauthorized: Check NGC_API_KEY.'
- '404 Not Found: Verify endpoint paths.'
- '401 Unauthorized: Check your API key.'
- 'Too many open files: Increase the limit to 1,048,576 file descriptors.'
- Unknown weight format detected.
auth_info:
  mentioned_objects:
  - NGC_API_KEY
client:
  base_url: http://0.0.0.0:8000/v1
  headers:
    accept: application/json
    Content-Type: application/json
source_metadata: null
