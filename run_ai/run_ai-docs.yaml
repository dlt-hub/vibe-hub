resources:
- name: access_rules
  endpoint:
    path: /v1/access_rules
    method: DELETE
    data_selector: rules
    params: {}
- name: tokens
  endpoint:
    path: /v1/tokens
    method: POST
    data_selector: token
    params: {}
- name: ai_initiatives
  endpoint:
    path: /api/v1/ai_initiatives
    method: GET
    data_selector: data
- name: policies
  endpoint:
    path: /policies
    method: GET
    data_selector: policies
    params: {}
- name: workspaces
  endpoint:
    path: /workspaces
    method: GET
    data_selector: records
- name: training
  endpoint:
    path: /training
    method: GET
    data_selector: records
- name: inference
  endpoint:
    path: /inference
    method: GET
    data_selector: records
- name: workloads
  endpoint:
    path: /api/v1/workloads
    method: GET
    data_selector: workloads
- name: bio-nemo-four-gpus
  endpoint:
    path: /saas/workloads-in-nvidia-run-ai/assets/bio-nemo-four-gpus
    method: GET
    data_selector: templates
    params: {}
- name: jupyter-lab-one-gpu
  endpoint:
    path: /saas/workloads-in-nvidia-run-ai/assets/jupyter-lab-one-gpu
    method: GET
    data_selector: templates
    params: {}
- name: nemo-four-gpus
  endpoint:
    path: /saas/workloads-in-nvidia-run-ai/assets/nemo-four-gpus
    method: GET
    data_selector: templates
    params: {}
- name: pytorch-one-gpu
  endpoint:
    path: /saas/workloads-in-nvidia-run-ai/assets/pytorch-one-gpu
    method: GET
    data_selector: templates
    params: {}
- name: workloads
  endpoint:
    path: /api/v1/workloads
    method: GET
    data_selector: items
- name: clusters
  endpoint:
    path: /api/v1/clusters
    method: GET
    data_selector: clusters
    params: {}
- name: workspaces
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/workload-types
    method: GET
    data_selector: workload_types
    params: {}
- name: training
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/workloads
    method: GET
    data_selector: workloads
    params: {}
- name: inference
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/using-inference
    method: GET
    data_selector: inference_workloads
    params: {}
- name: workspaces
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/using-workspaces/running-workspace
    method: GET
    data_selector: records
    params: {}
- name: training
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/using-training/standard-training/train-models
    method: GET
    data_selector: records
    params: {}
- name: inference
  endpoint:
    path: /self-hosted/workloads-in-nvidia-run-ai/using-inference
    method: GET
    data_selector: records
    params: {}
- name: workloads
  endpoint:
    path: /api/v1/workloads
    method: GET
    data_selector: workloads
    params: {}
- name: workload_assets
  endpoint:
    path: /api/v2/workload_assets
    method: GET
    data_selector: data
    params: {}
- name: Templates
  endpoint:
    path: /api-docs.run.ai/2.22/tag/Workload-templates
    method: GET
    data_selector: templates
    params: {}
notes:
- Requires setup of connected app in api
- NVIDIA Run:ai is installed on the customer's data science GPU clusters.
- The cluster requires an outbound connection to the NVIDIA Run:ai cloud.
- Integrate with your organization's identity provider for streamlined authentication
  (Single Sign On) and role-based access control (RBAC).
- From cluster v2.22 onward
- Support for exchanging external IdP tokens via API.
- Uses OAuth2 with refresh token
- Last updated 1 month ago
- Uses OAuth2 with refresh token.
- OAuth2 with refresh token is required for authentication
- Requires setup of OAuth2 for authentication
- Multi-GPU training and distributed training are two distinct concepts. Multi-GPU
  training uses multiple GPUs within a single node, whereas distributed training spans
  multiple nodes and typically requires coordination between them.
- Some objects may return nulls in deeply nested fields
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Flexible workload templates is disabled by default and requires Flexible workload
  submission to be enabled.
- Migrating existing templates (legacy) to flexible workload templates will be supported
  in a future release.
- Templates (legacy) is deprecated and will be removed in a future release.
- Refer to the documentation for details on authentication.
- Enable resource quota based on CPU resources.
- Enable GPU memory limit for workloads.
- Ensure to set up OAuth2 for authentication.
- NVIDIA Run:ai may extend the support periods and/or otherwise amend this Product
  Support Levels Policy from time to time at its own discretion.
- NVIDIA Run:ai versioning follows Semantic Version (SemVer) numbering scheme, “Ma.Mi.Pa”
- 'A supported NVIDIA Run:ai environment is built from: NVIDIA Run:ai Control Plane
  of a version equal to or greater than the versions of each of the NVIDIA Run:ai
  Clusters.'
- Uses OAuth2 with refresh token — requires setup of connected app in NVIDIA
- If your deployment is approaching the end of support, we recommend planning an upgrade
  to ensure continued security and functionality.
- NVIDIA Run:ai follows a structured product lifecycle to ensure customers receive
  regular updates, security patches, and long-term stability for their deployments.
- Enterprise-grade authentication — integrate with your organization's identity provider
  for streamlined authentication (Single Sign On) and role-based access control (RBAC).
- New Features and Enhancements - Highlights major updates introduced in each version,
  including new capabilities, UI improvements, and changes to system behavior.
- Hotfixes - Lists patches applied to released versions, including critical fixes
  and behavior corrections.
- Requires setup of connected app in API
- NVIDIA Run:ai enhances visibility and simplifies management by monitoring, presenting
  and orchestrating all AI workloads in the clusters it is installed.
- Multi-GPU training and distributed training are two distinct concepts.
- Workload management and scheduling features available
- Status of workloads can be monitored through the UI
- Integration requires authentication setup with OAuth2.
- Enable the CPU analytics dashboard for better visibility of CPU intensive workloads.
- Enable the consumption dashboard to view consumption by segment.
- Enable advanced GPU device-level metrics on the Nodes table.
- Improve dashboard performance with enhanced queries.
- Uses OAuth2 for authentication.
- Requires setup of connected app in NVIDIA API
- 'Versioning: NVIDIA Run:ai versioning follows Semantic Version (SemVer) numbering
  scheme, "Ma.Mi.Pa", where: Ma.Mi is a major version that contains new features,
  bug fixes and security updates ("Major Version"). Pa is a patch level version that
  is focused on bug fixes and security updates. NVIDIA Run:ai version release dates
  are listed in the NVIDIA Run:ai product documentation.'
- NVIDIA Run:ai is a GPU orchestration and optimization platform that helps organizations
  maximize compute utilization for AI workloads.
errors:
- 'CVE-2025-47907: High severity security vulnerability'
- 'CVE-2025-22868: High severity security vulnerability'
- 'CVE-2025-7425: High severity security vulnerability'
- 'CVE-2025-49796: High severity security vulnerability'
- 'CVE-2025-30749: High severity security vulnerability'
- 'CVE-2025-22869: High severity security vulnerability'
- 'CVE-2025-30204: High severity security vulnerability'
- 'CVE-2025-22874: High severity security vulnerability'
- '400 Bad Request: Required fields missing in request.'
- '500 Internal Server Error: Check telemetry type for node.'
- '401 Unauthorized: Check your credentials'
- '403 Forbidden: You do not have permission to access this resource'
- '401 Unauthorized: Check OAuth token and permissions'
- '401 Unauthorized: Check your credentials.'
- '404 Not Found: Verify the endpoint path.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Cluster connectivity issues: Verify network access to the cluster'
- 'Workload in ''Initializing'' status: Check container image registry access'
- 'Workload has been pending: Check required quota and project availability'
- '401 Unauthorized: Ensure credentials are correct.'
- '404 Not Found: Check endpoint path and parameters.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
client:
  base_url: https://run-ai-docs.nvidia.com
  auth:
    type: oauth2
source_metadata: null
