resources:
- name: personal_api_keys_verify
  endpoint:
    path: /api/v1/personal-api-keys/verify
    method: GET
- name: agent_api_keys_verify
  endpoint:
    path: /api/v1/agent-api-keys/verify
    method: GET
- name: Get all Agent API Keys
  endpoint:
    path: /reference/apikeyservice_getallagentapikeys
    method: GET
- name: Create an Agent API Key
  endpoint:
    path: /reference/apikeyservice_createagentapikey
    method: POST
- name: Verify an Agent API Key
  endpoint:
    path: /reference/apikeyservice_verifyagentapikey
    method: GET
- name: Delete an Agent API Key
  endpoint:
    path: /reference/apikeyservice_deleteagentapikey
    method: DELETE
- name: Get all Personal API Keys
  endpoint:
    path: /reference/apikeyservice_getallpersonalapikeys
    method: GET
- name: Create a Personal API Key
  endpoint:
    path: /reference/apikeyservice_createpersonalapikey
    method: POST
- name: Verify a Personal API Key
  endpoint:
    path: /reference/apikeyservice_verifypersonalapikey
    method: GET
- name: Delete a Personal API Key
  endpoint:
    path: /reference/apikeyservice_deletepersonalapikey
    method: DELETE
- name: workflow_status
  endpoint:
    path: /api/v1/workflows/{workflow_id}/status
    method: GET
- name: metric
  endpoint:
    path: /api/v1/metric
    method: POST
- name: metric_runs
  endpoint:
    path: /api/v1/metric/runs
    method: POST
- name: metric_scorecard
  endpoint:
    path: /api/v1/metric/scorecard
    method: POST
- name: table_level_metric_names
  endpoint:
    path: /api/v1/metrics/table-level-names
    method: GET
- name: validate_metric
  endpoint:
    path: /api/v1/metric/validate
    method: GET
- name: metric_configuration
  endpoint:
    path: /api/v1/metric/configuration
    method: GET
- name: metric_revisions
  endpoint:
    path: /api/v1/metric/revisions
    method: GET
- name: debug_rows
  endpoint:
    path: /api/v1/debug/rows
    method: GET
- name: debug_queries
  endpoint:
    path: /api/v1/debug/queries
    method: GET
- name: agent_api_keys
  endpoint:
    path: /agent-api-keys
    method: GET
- name: agent
  endpoint:
    path: /agent
    method: POST
- name: some snowflake db1 using aws secrets manager for password
  endpoint:
    path: /
    method: POST
    data_selector: connectionFactory
    params: {}
- name: bigeye-agent
  endpoint:
    path: docker.io/bigeyedata/agent:latest
    method: GET
- name: agent_health
  endpoint:
    path: /api/v1/agent/health
    method: GET
- name: agent_health_ping
  endpoint:
    path: /api/v1/agent/health/ping
    method: GET
- name: sql_server_kerberos
  endpoint:
    path: /app/config/krb5.conf
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: tables
  endpoint:
    path: /SNOWFLAKE.ACCOUNT_USAGE.TABLES
    method: GET
    data_selector: records
- name: access_history
  endpoint:
    path: /SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY
    method: GET
    data_selector: records
- name: copy_history
  endpoint:
    path: /SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY
    method: GET
    data_selector: records
- name: query_history
  endpoint:
    path: /SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
    method: GET
    data_selector: records
- name: bigquery
  endpoint:
    path: /bigquery
    method: GET
- name: database
  endpoint:
    path: /databases
    method: GET
    data_selector: databases
- name: bigeyeuser
  endpoint:
    path: /login
    method: POST
    data_selector: user
    params:
      username: bigeyeuser
      password: <password>
- name: bufferpools
  endpoint:
    path: /syscat/bufferpools
    method: GET
- name: colchecks
  endpoint:
    path: /syscat/colchecks
    method: GET
- name: columns
  endpoint:
    path: /syscat/columns
    method: GET
- name: tabconst
  endpoint:
    path: /syscat/tabconst
    method: GET
- name: checks
  endpoint:
    path: /syscat/checks
    method: GET
- name: colauth
  endpoint:
    path: /syscat/colauth
    method: GET
- name: dbauth
  endpoint:
    path: /syscat/dbauth
    method: GET
- name: wrappers
  endpoint:
    path: /syscat/wrappers
    method: GET
- name: indexcoluse
  endpoint:
    path: /syscat/indexcoluse
    method: GET
- name: indexauth
  endpoint:
    path: /syscat/indexauth
    method: GET
- name: indexes
  endpoint:
    path: /syscat/indexes
    method: GET
- name: packageauth
  endpoint:
    path: /syscat/packageauth
    method: GET
- name: statements
  endpoint:
    path: /syscat/statements
    method: GET
- name: packages
  endpoint:
    path: /syscat/packages
    method: GET
- name: keycoluse
  endpoint:
    path: /syscat/keycoluse
    method: GET
- name: procparms
  endpoint:
    path: /syscat/procparms
    method: GET
- name: references
  endpoint:
    path: /syscat/references
    method: GET
- name: tables
  endpoint:
    path: /syscat/tables
    method: GET
- name: schemaauth
  endpoint:
    path: /syscat/schemaauth
    method: GET
- name: schemata
  endpoint:
    path: /syscat/schemata
    method: GET
- name: procedures
  endpoint:
    path: /syscat/procedures
    method: GET
- name: tabauth
  endpoint:
    path: /syscat/tabauth
    method: GET
- name: tbspaceauth
  endpoint:
    path: /syscat/tbspaceauth
    method: GET
- name: tablespaces
  endpoint:
    path: /syscat/tablespaces
    method: GET
- name: triggers
  endpoint:
    path: /syscat/triggers
    method: GET
- name: datatypes
  endpoint:
    path: /syscat/datatypes
    method: GET
- name: functions
  endpoint:
    path: /syscat/functions
    method: GET
- name: views
  endpoint:
    path: /syscat/views
    method: GET
- name: servers
  endpoint:
    path: /syscat/servers
    method: GET
- name: Bigeye
  endpoint:
    path: /add_source
    method: POST
- name: information_schema.tables
  endpoint:
    path: /information_schema/tables
    method: GET
    data_selector: records
- name: information_schema.columns
  endpoint:
    path: /information_schema/columns
    method: GET
    data_selector: records
- name: database
  endpoint:
    path: /path/to/database/endpoint
    method: GET
    data_selector: records
    params: {}
- name: all_arguments
  endpoint:
    path: /SYS.all_arguments
    method: GET
- name: all_clusters
  endpoint:
    path: /SYS.all_clusters
    method: GET
- name: all_col_privs
  endpoint:
    path: /SYS.all_col_privs
    method: GET
- name: all_constraints
  endpoint:
    path: /SYS.all_constraints
    method: GET
- name: all_db_links
  endpoint:
    path: /SYS.all_db_links
    method: GET
- name: all_errors
  endpoint:
    path: /SYS.all_errors
    method: GET
- name: all_indexes
  endpoint:
    path: /SYS.all_indexes
    method: GET
- name: all_objects
  endpoint:
    path: /SYS.all_objects
    method: GET
- name: all_sequences
  endpoint:
    path: /SYS.all_sequences
    method: GET
- name: all_tables
  endpoint:
    path: /SYS.all_tables
    method: GET
- name: all_views
  endpoint:
    path: /SYS.all_views
    method: GET
- name: your-database
  endpoint:
    path: /your-database
    method: GET
    data_selector: records
- name: data_source
  endpoint:
    path: /add/source
    method: POST
- name: warehouse
  endpoint:
    path: /api/warehouses
    method: GET
    data_selector: warehouses
- name: role
  endpoint:
    path: /api/roles
    method: GET
    data_selector: roles
- name: trino_connection
  endpoint:
    path: /add_source
    method: POST
    data_selector: connection_details
    params: {}
- name: sql_server_read_only_user
  endpoint:
    path: /docs/connect-microsoft-sql-server#1-create-a-sql-server-read-only-user
    method: GET
- name: table_lineage
  endpoint:
    path: /system/access/table_lineage
    method: GET
- name: column_lineage
  endpoint:
    path: /system/access/column_lineage
    method: GET
- name: db.connection.url
  endpoint:
    path: jdbc:oracle:thin:@//<host>:1521/<service_name>
    method: GET
- name: db.connection.driver_class
  endpoint:
    path: oracle.jdbc.OracleDriver
    method: GET
- name: db.connection.username
  endpoint:
    path: bigeye_user
    method: GET
- name: db.connection.password
  endpoint:
    path: your_password
    method: GET
- name: db.catalog.name.include
  endpoint:
    path: SALES,MARKETING
    method: GET
- name: db.catalog.name.exclude
  endpoint:
    path: TEST
    method: GET
- name: db.query.fetchsize
  endpoint:
    path: '500'
    method: GET
- name: db.metadata.usedbaviews
  endpoint:
    path: 'true'
    method: GET
- name: db.metadata.uselocalviews
  endpoint:
    path: 'false'
    method: GET
- name: process.package.source
  endpoint:
    path: 'true'
    method: GET
- name: save.package.source
  endpoint:
    path: 'true'
    method: GET
- name: oracle.version
  endpoint:
    path: 19c
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: workspaces
  endpoint:
    path: /api/workspaces
    method: POST
- name: groups
  endpoint:
    path: /api/groups
    method: GET
    data_selector: groups
    params: {}
- name: roles
  endpoint:
    path: /roles
    method: GET
    data_selector: roles
    params: {}
- name: data_dimensions
  endpoint:
    path: /api/data_dimensions
    method: GET
    data_selector: data_dimensions
- name: agent
  endpoint:
    path: /agent
    method: POST
- name: connector
  endpoint:
    path: /add-connector
    method: POST
    data_selector: records
    params: {}
- name: connectors
  endpoint:
    path: /add-connector
    method: POST
- name: collection
  endpoint:
    path: /collections
    method: GET
    data_selector: collections
    params: {}
- name: table_preview
  endpoint:
    path: /table/preview
    method: GET
- name: tags
  endpoint:
    path: /tags
    method: GET
    data_selector: tags
    params: {}
- name: metrics
  endpoint:
    path: /metrics/deploy
    method: POST
    data_selector: metrics
    params: {}
- name: row_creation_time
  endpoint:
    path: /docs/row-creation-time
    method: GET
    data_selector: metrics
    params: {}
- name: debug_query
  endpoint:
    path: /debug/query
    method: GET
- name: thresholds
  endpoint:
    path: /docs/thresholds
    method: GET
- name: custom_rule
  endpoint:
    path: /docs/custom-rules
    method: GET
    data_selector: rules
    params: {}
- name: deltas
  endpoint:
    path: /deltas
    method: POST
    data_selector: delta
    params: {}
- name: delta_results
  endpoint:
    path: /api/delta/results
    method: GET
    data_selector: records
    params: {}
- name: virtual_table
  endpoint:
    path: /api/virtual-tables
    method: POST
    data_selector: virtualTables
    params: {}
- name: notifications
  endpoint:
    path: /api/chat.postMessage
    method: POST
    data_selector: message
    params: {}
- name: webhook_metric
  endpoint:
    path: /metrics
    method: POST
    data_selector: payload
- name: webhook_collection
  endpoint:
    path: /collections
    method: POST
    data_selector: payload
- name: issue-webhook
  endpoint:
    path: /api/x_617994_bigeye/bigeye/issue-webhook
    method: POST
- name: issues
  endpoint:
    path: /api/issues
    method: GET
- name: issue_status
  endpoint:
    path: /change-the-issue-status
    method: POST
    data_selector: status
    params: {}
- name: DIM_METRIC
  endpoint:
    path: /services/data/vXX.X/sobjects/DIM_METRIC
    method: GET
    data_selector: records
- name: DIM_ISSUE
  endpoint:
    path: /services/data/vXX.X/sobjects/DIM_ISSUE
    method: GET
    data_selector: records
- name: DIM_DELTA
  endpoint:
    path: /services/data/vXX.X/sobjects/DIM_DELTA
    method: GET
    data_selector: records
- name: DIM_COLLECTION
  endpoint:
    path: /services/data/vXX.X/sobjects/DIM_COLLECTION
    method: GET
    data_selector: records
- name: FACT_METRIC_RUN
  endpoint:
    path: /services/data/vXX.X/sobjects/FACT_METRIC_RUN
    method: GET
    data_selector: records
- name: FACT_DELTA_RUN
  endpoint:
    path: /facts/delta_run
    method: GET
    data_selector: records
- name: FACT_COLLECTION_METRIC
  endpoint:
    path: /facts/collection_metric
    method: GET
    data_selector: records
- name: AGG_WORKSPACE_MONTHLY
  endpoint:
    path: /aggregates/workspace_monthly
    method: GET
    data_selector: records
- name: Tableau
  endpoint:
    path: /connect/tableau
    method: GET
- name: Tableau Workbooks
  endpoint:
    path: /api/metadata
    method: GET
    data_selector: workbooks
- name: run_metrics
  endpoint:
    path: /run_metrics
    method: POST
    data_selector: result
    params:
      warehouse_id: 446
      schema_name: analytics
      table_name: users
- name: run_metrics_by_collection
  endpoint:
    path: /run_metrics_by_collection
    method: POST
    data_selector: result
    params:
      collection_id: 3602
      circuit_breaker_mode: true
- name: dbt_core_sync
  endpoint:
    path: /dbt_core_sync
    method: POST
    data_selector: result
    params:
      workspace_id: <bigeye_workspace_id>
      target_path: path_to/target
- name: dbt_jobs
  endpoint:
    path: /api/dbt/jobs
    method: GET
    data_selector: jobs
- name: collections
  endpoint:
    path: /api/collections
    method: POST
    data_selector: collection
    params: {}
- name: metrics
  endpoint:
    path: /api/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: collections
  endpoint:
    path: /api/collections
    method: POST
    data_selector: data
    params: {}
- name: issues
  endpoint:
    path: /api/issues
    method: POST
    data_selector: data
    params: {}
- name: dbt_results
  endpoint:
    path: /api/dbt_results
    method: POST
    data_selector: results
    params: {}
- name: dbt_results
  endpoint:
    path: /dbt_results
    method: GET
    data_selector: results
    params: {}
- name: Referential Integrity Check
  endpoint:
    path: /api/referential-integrity/check
    method: POST
    data_selector: results
    params: {}
- name: deltas
  endpoint:
    path: /api/v1/deltas
    method: POST
    data_selector: deltas
    params: {}
- name: table_deployments
  endpoint:
    path: /bigconfig/table_deployments
    method: POST
    data_selector: deployments
    params: {}
- name: metrics
  endpoint:
    path: /api/v1/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: schema_changes
  endpoint:
    path: /api/v1/schema/changes
    method: GET
    data_selector: changes
    params: {}
- name: get_table_lineage
  endpoint:
    path: /dataset/<table_id>
    method: GET
    data_selector: dataNodeId
- name: create_data_node
  endpoint:
    path: /api/v1/lineage/nodes
    method: POST
    data_selector: id
- name: list_table_lineage
  endpoint:
    path: /api/v1/lineage/nodes/{data_node_id}/relationships
    method: GET
    data_selector: upstream, downstream
- name: create_table_relationship
  endpoint:
    path: /api/v1/lineage/relationships
    method: POST
    data_selector: ''
- name: mute_metrics
  endpoint:
    path: /api/v1/tables/992/snooze
    method: POST
    data_selector: ''
- name: unmute_metrics
  endpoint:
    path: /api/v1/tables/992/unsnooze
    method: POST
    data_selector: ''
- name: pipeline_reliability
  endpoint:
    path: /metrics/pipeline_reliability
    method: GET
    data_selector: metrics
    params: {}
- name: uniqueness
  endpoint:
    path: /metrics/uniqueness
    method: GET
    data_selector: metrics
    params: {}
- name: completeness
  endpoint:
    path: /metrics/completeness
    method: GET
    data_selector: metrics
    params: {}
- name: distributions
  endpoint:
    path: /metrics/distributions
    method: GET
    data_selector: metrics
    params: {}
- name: validity
  endpoint:
    path: /metrics/validity
    method: GET
    data_selector: metrics
    params: {}
- name: contact_information
  endpoint:
    path: /contact_information
    method: GET
    data_selector: metrics
    params: {}
- name: financial
  endpoint:
    path: /financial
    method: GET
    data_selector: metrics
    params: {}
- name: time
  endpoint:
    path: /time
    method: GET
    data_selector: metrics
    params: {}
- name: geolocation
  endpoint:
    path: /geolocation
    method: GET
    data_selector: metrics
    params: {}
- name: user_specified
  endpoint:
    path: /user_specified
    method: GET
    data_selector: metrics
    params: {}
- name: single_sign_on
  endpoint:
    path: /settings/single-sign-on
    method: POST
    data_selector: response
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
notes:
- Company prefix is the first part of the Bigeye URL (e.g. 'app' for app.bigeye.com)
- To use API Key authentication, prefix your API Key with 'apikey '
- 'Minimum HW size: 4 CPU, 16GB memory, 25GB disk space on /var is recommended.'
- Must be an Admin in Bigeye to create an Agent API Key
- Must be an Admin in Bigeye to create Agent API Key.
- If all requests on your VM for the Bigeye agent are required to go through a proxy,
  additional steps are required to get everything installed successfully.
- A secret can be used for the agent configuration.
- Certificate given to Bigeye in “Step 2. Upload agent authentication key” does not
  match.
- Work with Bigeye support to resolve this (config/mtls_ca.pem md5 should be checked
  and in Bigeye’s workflow service)
- Network connectivity between the Bigeye agent and the Bigeye workflow service does
  not work.
- Check that workflowUrl is set correctly in ./config/agent.yaml.
- Check that the port is open, ie nmap -Pn -p 443 <workflowUrl from above step>.
- Nmap should return “open” as the state of the port and not “filtered”.
- Something in the networking infrastructure is closing the connection between the
  agent and Bigeye SaaS.
- Work with your security / networking team to find where this is happening.
- Yaml formatting error in ./config/agent.yaml.
- Typically this will be improper indentation or a missing quote in the agent config
  file.
- The agent is detecting that an 'HTTP' connection is being established on the server
  side, but we are using GRPC for communication which is HTTP2.
- Ensure that the proxy can handle HTTP2 connections or route agent connections directly
  to Bigeye infrastructure and do not go through the proxy.
- The agent has run out of memory and will crash.
- Either add more memory to the host or constrain the amount of memory allocated for
  the agent as described in the documentation.
- Teradata requires downloading their JDBC driver directly.
- The agent can accept a flag to coerce it to prefer ipv4 addresses. Set the following
  JDK_JAVA_OPTIONS environment variable in the bigeye docker-compose.yaml file.
- Check that workflowUrl is set correctly in `./config/agent.yaml`
- Check that the port is open, ie nmap -Pn -p 443 <workflowUrl from above step>
- Nmap should return “open” as the state of the port and not “filtered”
- Ensure that the proxy can handle HTTP2 connections.
- Nmap should return 'open' as the state of the port and not 'filtered'
- Work with your security / networking team to find where this is happening
- Yaml formatting error in `./config/agent.yaml`
- Ensure that the proxy can handle HTTP2 connections or route agent connections directly
  to Bigeye infrastructure and do not go through the proxy
- Either add more memory to the host or constrain the amount of memory allocated for
  the agent
- Teradata requires downloading their JDBC driver directly
- The CA certificates file names must end in .pem (i.e. custom_ca.pem).
- If you are unable to utilize an Init Container, you may also use Persistent Volume
  Storage to mount custom JDBC drivers.
- Direct connection to SQL Server using Kerberos authentication is not currently supported.
- You must have krb5.conf and keytab files ready for Kerberos authentication.
- Starting with agent version 1.14.0, Agent API Keys are the supported way of authenticating
  with Bigeye's systems.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Initial Agent release
- Initial public release, supporting row level deltas and join rules
- Add the BQ key file to the agent_config directory and reference it below for the
  agent
- Snowflake is deprecating basic authentication for Snowflake users in November 2025.
- Create a dedicated user for Bigeye and grant SELECT permissions to the tables that
  the user must access.
- The password must contain at least one uppercase, one lowercase, and one numeric
  character.
- Requires setup of a dedicated user for Bigeye with appropriate permissions.
- Bigeye requires a read-only user for connecting to your database.
- It can take up to 24 hours for the profiling to complete and your autometrics and
  autothresholds to populate.
- This setup assumes that your data and ingest jobs are in the same project.
- Bigeye supports SQL Server connections with Kerberos authentication using the data
  source agent.
- Since Bigeye monitoring uses multiple sessions during connections, ensure that you
  have an appropriate number of connections configured.
- You can enforce limits to database resources by creating a profile for the Bigeye
  user. Since connections can sit idle, enforce a low IDLE_TIME.
- If your Oracle instance is deployed in the cloud, allow all of Bigeye's Static IP
  addresses access.
- Ensure the Bigeye user has the necessary permissions to see and query the tables.
- Fully automated column level lineage for modern and legacy data sources.
- Create a read-only SQL Server user
- The user or service principal must have read access to system tables for lineage
  computation.
- Ideal for POC or limited-scope deployments
- Not recommended for use cases requiring full dynamic execution support or complex
  transformations
- Connector supports version 1.54.7 or higher with Enterprise Mode feature enabled.
- Job metadata queries are made per job, which may impact runtime for large environments.
- Enterprises can organize their work in Bigeye using workspaces and groups to ensure
  that each team manages and monitors their own data independently.
- Several Bigeye API endpoints require the workspace to be specified in the request
  body or as an HTTP header.
- Users can belong to one or more groups.
- Admin group cannot be deleted.
- Data Dimensions are available as part of the Bigeye platform and accessible for
  Admins to edit in Settings.
- Bigeye is built to industry-leading security standards and follows enterprise-grade
  practices to deliver data observability while protecting your data and ensuring
  compliance.
- Personal API Key is required for Admin level user in Bigeye.
- Agent API Key is associated with the company, not an individual user.
- The personal API key for an Admin level user in Bigeye can be generated in Bigeye.
- The agent API key is associated with the company, not an individual user.
- The Bigeye Agent Installer is hosted in a public s3 bucket in AWS.
- The binary produced by pyinstaller depends on GLIBC and the version of GLIBC it
  depends on is based on the OS that the executable gets generated. This error indicates
  the the version of GLIBC used is not supported by the OS.
- The Python environment is unable to verify the SSL certificate of the server you're
  trying to connect to. Python might not automatically use the updated certificates.
- The firewall rules should NOT strip any Authorization headers for the below mentioned
  host/domain names.
- Partial Support for custom jars on Kubernetes.
- Uses API key for authentication
- Requires setup of connected app in Bigeye
- The Bigeye catalog is automatically reindexed every 24 hours to identify and display
  any new datasets available and any schema changes that occurred.
- Catalog popularity is available for Snowflake, BigQuery and Redshift sources.
- Ensure your service account is setup correctly so Bigeye can access source metadata.
- The only type changes captured are fundamental type changes such as boolean to string
  or string to numeric. Bigeye does not capture differences in precision. For example,
  a varchar(20) altered to varchar(50) will not be presented as a schema change in
  Bigeye.
- Data Restricted mode is disabled.
- Catalog data preview is enabled.
- Tags are not global and cannot be promoted or shared across workspaces.
- Bigeye enables you to monitor your data by deploying Metrics or Custom Rules on
  your data.
- Bigeye monitors metrics for database tables or columns and alert you if they go
  out of bounds.
- You cannot create a new collection from Configure metrics modal.
- Row Creation Time is recommended for incremental tables.
- Using Row Creation Time and a Lookback Window filters the rows queried each metric
  run to the most recent data.
- Metadata-based Freshness and Volume are only available if you are using the Enterprise
  Edition of Snowflake and not on Snowflake Standard Edition.
- Only one Freshness metric and one Volume metric can be deployed per table.
- Only one Freshness (data) metric and one Volume (data) metric can be deployed per
  table.
- HSLV requires a timestamp.
- Default training window is 21 days.
- Autothresholds pull in new data and retune models every 24 hours.
- By default, Autothresholds include three weeks of data in training history.
- Bigeye provides various means to customizing the monitors you place on your data
- Make sure you fully qualify your table names
- Custom Rules can only send notifications through Collections
- Clicking on Deltas in the left navigation menu shows a list of existing Deltas.
- Learn more about how to create and set schedules for deltas
- Requires setup of connected app in api
- 'If a virtual table query can no longer be run successfully, for example due to
  an underlying data change, metric runs on the table will return with “Failed: connection”.
  In this case, it’s necessary to delete the existing virtual table and create a new
  table with a valid definition.'
- Bigeye integrates bidirectionally with Jira to enable flexible data quality issue
  tracking.
- Bigeye currently does not directly integrate with Microsoft Teams, so the notification
  text you see will be an email rendered in the channel.
- Enabling a ServiceNow integration for a Bigeye instance requires configuring system
  properties in ServiceNow.
- Incident management is key to fixing data and pipeline problems quickly to minimize
  business impact.
- Data lineage allows you to visualize and track how the current issue is impacting
  your data pipeline.
- bigAI can analyze your ETL jobs and suggest improvements to make them more reliable
  and performant.
- Preventing problems from occurring in the future is the best possible way to improve
  data reliability and ensure trust from your business users.
- User must have permission to access the metadata API.
- Verify that your Tableau site has connected successfully and Bigeye has recognized
  the workbooks.
- The Bigeye Data Health Agent uses a batch process, and each customer can decide
  their own batch schedule. The standard scheduling period is every 60 minutes.
- 'Minimum HW size: 1 CPU, 2GB mem. If the Bigeye agent and data health integration
  will be running on the same host, minimum HW requirements are 4CPU, 16GB mem'
- The integration may be deployed as a standard Docker container instance using docker
  compose run.
- Kubernetes is available on different cloud platforms; e.g., AWS EKS, Azure AKS,
  GCP GKE.
- An additional parameter called bigeye_workspace_id needs to be added to the credential
  when multiple workspaces are in place.
- When using an API key for Bigeye, you do not need the parameters bigeye_username
  or bigeye_password.
- The current integration only supports a single Bigeye workspace.
- The user from data.world must have write permissions to the ddw-catalogs dataset.
- You must have an API token for an integration.
- Requires a dedicated service user in dbt Cloud for Bigeye with read-only access.
- The Bigeye extension utilizes the side panel of the browser for optimum user experience.
  The side panel, and therefore the Bigeye extension, is only available in chrome
  versions 116+ and edge versions 119+.
- A Collection in Bigeye is a grouping of metrics that will send you an alert if any
  of the metrics go out of bounds.
- Bigeye users can mute metrics and collections, and acknowledge issues or close issues
  with feedback directly from Slack notifications.
- Uses OAuth2 with refresh token — requires setup of connected app in Bigeye
- Tracks the performance and outcomes of dbt runs and tests
- Template checks if each row satisfies the referential integrity condition and returns
  a True or False result.
- Requires setup of GitHub Actions for automated workflows
- The workspace ID is only required if your Bigeye user has access to more than 1
  workspace.
- Workspace ID is required if user has access to more than 1 workspace.
- The workspace ID is only required if user has access to more than 1 workspace.
- API calls are restricted by Bigeye access level.
- API supports various metrics for monitoring data quality.
- Bigeye supports cron-based scheduling in Universal Coordinated Time (UTC), which
  is consistent throughout the year and does not update for Daylight Saving Time (DST).
- Bigeye will not execute a schedule more frequently than every 5 minutes.
- Bigeye also does not support cron predefined values such as @weekly.
- Bigeye is set up for local authentication by default, whereby all credentials are
  stored by Bigeye in the application database.
- Bigeye is compatible with identity providers that support OpenID Connect.
- 'When verifying the permissions for the app inside of Ping, make sure that the Identity
  Token has at least the following pieces of information: email, name or firstName/lastName,
  aud (Audience), iss (Issuer), exp (Expiration).'
- Ensure API permission for User.Read is set up to allow Bigeye to read user profiles.
- Ensure to substitute your email domain values correctly.
- Your Bigeye URL may differ from app.bigeye.com.
- Bigeye OIDC only supports connecting via a Client ID and Client Secret. It does
  not support via a private JWT token at this time.
- Bigeye will create accounts as needed for users, but it will not automatically add
  them to any workspaces. To control user access into Bigeye, we recommend applying
  group membership policies inside of your identity provider.
- When users are deactivated or removed inside of your identity provider for any reason,
  they are not removed from Bigeye. However, they will no longer be able to log in
  through your identity provider.
- User sessions are revalidated every hour with your identity provider. If a user's
  access is removed from Bigeye through your group policy, they will maintain access
  until their session is revalidated.
- Only Bigeye admins can map identity provider groups to Bigeye groups.
- SSO should already be enabled in Bigeye.
- Identity provider group mapping should already be enabled in Bigeye.
- Migrating metrics must not be confused with editing metrics.
- There can be some data loss during this process, such as issues for metrics from
  the source warehouse.
errors:
- '401: Authentication failed'
- 'PERMISSION_DENIED: Request unauthorized: Verify your Agent API Key is valid.'
- 'SSLException: Certificate given to Bigeye does not match: Work with Bigeye support
  to resolve.'
- 'DEADLINE_EXCEEDED: Network connectivity between the Bigeye agent and the Bigeye
  workflow service does not work.'
- 'UNAVAILABLE: error reading from server: EOF: Yaml formatting error in agent.yaml.'
- 'java.net.UnknownHostException: app-workflows.bigeye.com: Name or service not known:
  Set the JDK_JAVA_OPTIONS environment variable to prefer IPv4 addresses.'
- 'PERMISSION_DENIED: Request unauthorized'
- 'CANCELLED: Something in the networking infrastructure is closing the connection
  between the agent and Bigeye SaaS.'
- 'jdbc timeout / error reaching host:port: Networking connectivity between the agent
  and the data source is not set up.'
- 'UNAVAILABLE: Yaml formatting error in ./config/agent.yaml.'
- 'Http2Exception: The agent is detecting that an ''HTTP'' connection is being established
  on the server side.'
- 'OutOfMemoryError: The agent has run out of memory and will crash.'
- 'UnknownHostException: app-workflows.bigeye.com: Name or service not known.'
- 'DEADLINE_EXCEEDED: Network connectivity between the Bigeye agent and the Bigeye
  workflow service does not work'
- 'io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error
  code: CANCEL'
- 'io.grpc.StatusRuntimeException: UNAVAILABLE: error reading from server: EOF'
- 'Http2Exception: First received frame was not SETTINGS.'
- 'Exception: java.lang.OutOfMemoryError: The agent has run out of memory and will
  crash.'
- 'java.sql.SQLException: Unable to load class: com.teradata.jdbc.TeraDriver'
- 'java.net.UnknownHostException: app-workflows.bigeye.com: Name or service not known'
- 'CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL'
- 'jdbc timeout / error reaching host:port: Networking connectivity between the agent
  and the data source is not set up'
- 'UNAVAILABLE: error reading from server: EOF'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Ensure proper permissions for the user or service principal.'
- '401 Unauthorized: Recheck API key or permissions'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- '401 Unauthorized: Check your GitHub token and permissions'
- '404 Not Found: Ensure the endpoint is correct and accessible'
- '400: A workspace ID must be supplied'
- '401 Unauthorized: Ensure valid OAuth token.'
- '404 Not Found: Check the endpoint path.'
- '429 Too Many Requests: Rate limit exceeded.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - User Profile
  - GITHUB_TOKEN
  - BIGEYE_SBX_CONF
  - ApiAuth
  - DatawatchClient
  - LineageController
  - DataNodeType
client:
  base_url: https://app.bigeye.com
  auth:
    type: apikey
source_metadata: null
