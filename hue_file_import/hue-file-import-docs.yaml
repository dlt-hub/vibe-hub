resources:
- name: upload_file
  endpoint:
    path: /api/v1/importer/upload/file/
    method: POST
    data_selector: file_path
    params: {}
- name: guess_metadata
  endpoint:
    path: /api/v1/importer/file/guess_metadata/
    method: GET
    data_selector: ''
    params: {}
- name: guess_header
  endpoint:
    path: /api/v1/importer/file/guess_header/
    method: GET
    data_selector: ''
    params: {}
- name: preview_file
  endpoint:
    path: /api/v1/importer/file/preview/
    method: GET
    data_selector: ''
    params: {}
- name: sql_type_mapping
  endpoint:
    path: /api/v1/importer/sql_type_mapping/
    method: GET
    data_selector: sqlTypes
    params:
      sql_dialect: hive
- name: upload_file
  endpoint:
    path: /api/v1/importer/upload/file/
    method: POST
    data_selector: uploadResult
- name: guess_metadata
  endpoint:
    path: /api/v1/importer/file/guess_metadata/
    method: GET
    data_selector: metadata
- name: guess_header
  endpoint:
    path: /api/v1/importer/file/guess_header/
    method: GET
    data_selector: headerResult
- name: file_preview
  endpoint:
    path: /api/v1/importer/file/preview/
    method: GET
    data_selector: preview
- name: database
  endpoint:
    path: /admin/database
    method: GET
    data_selector: records
    params:
      host: localhost
      port: 3306
      engine: mysql
      user: hue
      password: secretpassword
      name: hue
- name: server
  endpoint:
    path: /admin/server
    method: GET
    data_selector: records
    params:
      http_host: 0.0.0.0
      http_port: 8888
- name: app_blacklist
  endpoint:
    path: /desktop
    method: GET
    data_selector: app_blacklist
    params: {}
- name: SAML
  endpoint:
    path: /opt/cloudera/security/saml/idp-openam-metadata.xml
    method: GET
- name: hiveserver2
  endpoint:
    path: /hiveserver2
    method: GET
    data_selector: records
    params: {}
- name: llap
  endpoint:
    path: /llap
    method: GET
    data_selector: records
    params: {}
- name: impala
  endpoint:
    path: /impala
    method: GET
    data_selector: records
    params: {}
- name: mysql
  endpoint:
    path: /api/rest/mysql
    method: GET
    data_selector: records
    params: {}
- name: presto
  endpoint:
    path: /api/rest/presto
    method: GET
    data_selector: records
    params: {}
- name: impala
  endpoint:
    path: /api/rest/impala
    method: GET
    data_selector: records
    params: {}
- name: postgresql
  endpoint:
    path: /api/rest/postgresql
    method: GET
    data_selector: records
    params: {}
- name: oracle
  endpoint:
    path: /api/rest/oracle
    method: GET
    data_selector: records
    params: {}
- name: aws_athena
  endpoint:
    path: /api/rest/athena
    method: GET
    data_selector: records
    params: {}
- name: sparksql
  endpoint:
    path: /sparksql
    method: GET
- name: default
  endpoint:
    path: /
    method: GET
    data_selector: clusters
    params: {}
- name: mysql
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: results
    params:
      incremental: updated_at
- name: presto
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: results
    params: {}
- name: trino
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: results
    params: {}
- name: hbase_clusters
  endpoint:
    path: /hbase/clusters
    method: GET
    data_selector: clusters
- name: aws_accounts
  endpoint:
    path: /aws/accounts
    method: GET
    data_selector: accounts
- name: azure_account
  endpoint:
    path: /azure/accounts
    method: GET
    data_selector: accounts
- name: gc_accounts
  endpoint:
    path: /gc/accounts
    method: GET
    data_selector: accounts
- name: sentry_service
  endpoint:
    path: /sentry/service
    method: GET
    data_selector: service
- name: hbase_clusters
  endpoint:
    path: /hbase
    method: GET
    data_selector: clusters
    params:
      hbase_clusters: (Cluster|localhost:9090)
- name: hdfs
  endpoint:
    path: /hadoop/hdfs
    method: GET
- name: ozone
  endpoint:
    path: /ozone
    method: GET
- name: azure_account
  endpoint:
    path: /azure_account
    method: GET
    data_selector: records
- name: s3_account
  endpoint:
    path: /s3_account
    method: GET
    data_selector: records
- name: hbase_clusters
  endpoint:
    path: /hbase_clusters
    method: GET
    data_selector: records
- name: livy_server
  endpoint:
    path: /livy_server
    method: GET
    data_selector: records
- name: mysql
  endpoint:
    path: /interpreter/mysql
    method: POST
    data_selector: records
    params: {}
- name: presto
  endpoint:
    path: /interpreter/presto
    method: POST
    data_selector: records
    params: {}
- name: aws
  endpoint:
    path: /aws/accounts
    method: GET
    data_selector: accounts
- name: azure
  endpoint:
    path: /azure/accounts
    method: GET
    data_selector: accounts
- name: gcs
  endpoint:
    path: /gcs/accounts
    method: GET
    data_selector: accounts
- name: hbase
  endpoint:
    path: /hbase/clusters
    method: GET
    data_selector: clusters
- name: metadata
  endpoint:
    path: /metadata/catalog
    method: GET
    data_selector: catalog
- name: sentry
  endpoint:
    path: /sentry/service
    method: GET
    data_selector: service
- name: yarn
  endpoint:
    path: /yarn/clusters
    method: GET
    data_selector: clusters
- name: hive
  endpoint:
    path: /services/data/vXX.X/sobjects/Hive
    method: GET
    data_selector: records
    params: {}
- name: is_alive
  endpoint:
    path: /desktop/debug/is_alive
    method: GET
- name: sessions
  endpoint:
    path: /sessions
    method: POST
    data_selector: id
- name: session_statements
  endpoint:
    path: /sessions/{session_id}/statements
    method: POST
    data_selector: id
- name: sessions
  endpoint:
    path: /sessions
    method: GET
    data_selector: sessions
- name: create_session
  endpoint:
    path: /sessions
    method: POST
    data_selector: id
- name: session_statements
  endpoint:
    path: /sessions/{session_id}/statements
    method: POST
    data_selector: id
- name: check_statement
  endpoint:
    path: /sessions/{session_id}/statements/{statement_id}
    method: GET
    data_selector: output
- name: delete_session
  endpoint:
    path: /sessions/{session_id}
    method: DELETE
    data_selector: msg
- name: sessions
  endpoint:
    path: /sessions/0/statements
    method: POST
    data_selector: code
    params: {}
- name: sessions
  endpoint:
    path: /sessions
    method: GET
    data_selector: sessions
- name: create_session
  endpoint:
    path: /sessions
    method: POST
    data_selector: id
- name: check_session
  endpoint:
    path: /sessions/{id}
    method: GET
    data_selector: state
- name: execute_statement
  endpoint:
    path: /sessions/{id}/statements
    method: POST
    data_selector: id
- name: hive
  endpoint:
    path: /hive
    method: GET
    data_selector: records
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: records
- name: sql_parsers
  endpoint:
    path: /administrator/configuration/connectors/
    method: GET
    data_selector: parsers
    params: {}
- name: sql_scratchpad
  endpoint:
    path: /
    method: GET
- name: hive_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: impala_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: calcite_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: phoenix_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: flink_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: ksql_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: presto_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: dask_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: generic_autocomplete_parser
  endpoint:
    path: /developer/api/rest/importer/
    method: POST
    data_selector: locations
- name: execute_query
  endpoint:
    path: /api/v1/editor/execute/hive
    method: POST
    data_selector: result
    params:
      statement: SHOW TABLES
- name: check_status
  endpoint:
    path: /api/v1/editor/check_status
    method: POST
    data_selector: query_status
    params:
      operationId: history_uuid
- name: fetch_result_data
  endpoint:
    path: /api/v1/editor/fetch_result_data
    method: POST
    data_selector: result
    params:
      operationId: history_uuid
- name: list_databases
  endpoint:
    path: /api/v1/editor/autocomplete/
    method: POST
    data_selector: databases
    params:
      snippet:
        type: hive
- name: upload_file
  endpoint:
    path: /api/v1/importer/upload/file/
    method: POST
    data_selector: file_path
- name: guess_metadata
  endpoint:
    path: /api/v1/importer/file/guess_metadata/
    method: GET
    data_selector: null
- name: guess_header
  endpoint:
    path: /api/v1/importer/file/guess_header/
    method: GET
    data_selector: null
- name: preview_file
  endpoint:
    path: /api/v1/importer/file/preview/
    method: GET
    data_selector: null
- name: create_notebook
  endpoint:
    path: /api/v1/query/create_notebook
    method: POST
- name: token_auth
  endpoint:
    path: /api/v1/token/auth
    method: POST
- name: query_autocomplete
  endpoint:
    path: /api/v1/query/autocomplete
    method: POST
- name: check_status
  endpoint:
    path: /editor/check_status
    method: POST
    data_selector: query_status
    params: {}
- name: fetch_result_data
  endpoint:
    path: /editor/fetch_result_data
    method: POST
    data_selector: result
    params: {}
- name: autocomplete
  endpoint:
    path: /editor/autocomplete
    method: POST
    data_selector: databases
    params: {}
- name: describe_database
  endpoint:
    path: /editor/describe/<DB>
    method: POST
    data_selector: ''
    params:
      source_type: mysql
- name: list_filesystems
  endpoint:
    path: /storage/filesystems
    method: GET
    data_selector: ''
    params: {}
- name: upload_file
  endpoint:
    path: /api/v1/importer/upload/file/
    method: POST
    data_selector: file_path
- name: guess_metadata
  endpoint:
    path: /api/v1/importer/file/guess_metadata/
    method: GET
    data_selector: type
- name: guess_header
  endpoint:
    path: /api/v1/importer/file/guess_header/
    method: GET
    data_selector: has_header
- name: preview_file
  endpoint:
    path: /api/v1/importer/file/preview/
    method: GET
    data_selector: type
- name: sql_type_mapping
  endpoint:
    path: /api/v1/importer/sql_type_mapping/
    method: GET
    data_selector: response
    params:
      sql_dialect: hive
- name: remote_storage_files
  endpoint:
    path: /compose/storage/list
    method: GET
    data_selector: files
- name: Livy CSRF option
  endpoint:
    path: /livy/csrf
    method: GET
    data_selector: records
- name: Authentication backends
  endpoint:
    path: /auth/backends
    method: GET
    data_selector: records
- name: SparkSQL Livy connector
  endpoint:
    path: /spark/livy
    method: GET
    data_selector: records
- name: dashboard
  endpoint:
    path: /services/data/v4.2/dashboard
    method: GET
    data_selector: records
    params: {}
- name: editor
  endpoint:
    path: /services/data/v4.2/editor
    method: GET
    data_selector: records
    params: {}
- name: job
  endpoint:
    path: /api/jobs
    method: GET
    data_selector: jobs
- name: workflow
  endpoint:
    path: /api/workflows
    method: GET
    data_selector: workflows
- name: job
  endpoint:
    path: /services/data/v4.0.0/jobs
    method: GET
- name: workflow
  endpoint:
    path: /services/data/v4.0.0/workflows
    method: GET
- name: directories
  endpoint:
    path: /api/directories
    method: GET
    data_selector: records
- name: documents
  endpoint:
    path: /api/documents
    method: GET
    data_selector: records
- name: table_partition
  endpoint:
    path: /get/table/partition
    method: GET
    data_selector: partitions
- name: table_metadata
  endpoint:
    path: /get/table/metadata
    method: GET
    data_selector: metadata
- name: subworkflows
  endpoint:
    path: /api/subworkflows
    method: GET
    data_selector: subworkflows
    params: {}
- name: workflows
  endpoint:
    path: /api/workflows
    method: GET
    data_selector: workflows
    params: {}
notes:
- All API endpoints require authentication.
- For endpoints that work with remote files, the API uses the file system permissions
  associated with the authenticated user's session.
- Hue employs some Python modules which use native code and requires certain development
  libraries be installed on your system.
- If you are using Python 3.11, set PYTHON_VER before the build.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Restarting the server is currently required after for taking ini changes into consideration
- Uses Gunicorn as the webserver.
- Comma separated list of apps to not load at server startup.
- Add key and cert files even if not encrypting assertions.
- Connectors are also configurable via the public REST API.
- HPL/SQL uses the [beeswax] config like Hive uses.
- Existing HBase tables need to be mapped to views
- Tables are seeing as uppercase by Phoenix. When getting started, it is simpler to
  just create the table via Phoenix.
- Phoenix follows Apache Calcite. Feel free to help improve the SQL autocomplete support
  for it.
- The UI (and the underlying SQLAlchemy API) cannot distinguish between ‘ANY namespace’
  and ‘empty/Default’ namespace
- 'After enabling the above flags, if a `django.db.utils.OperationalError: (1054,
  "Unknown column ''useradmin_huepermission.connector_id'' in ''field list''" )` error
  comes, then try **changing the DB name** in the hue.ini under `[[database]]` because
  there is no upgrade path and run the migrate command `./build/env/bin/hue migrate`.'
- 'SqlAlchemy interface requires the Hive connector which does not work out of the
  box because of the issue #150.'
- Properties need to be URL quoted (e.g. with `urllib.quote_plus(...)` in Python).
- 'After enabling the above flags, if a `django.db.utils.OperationalError: (1054,
  "Unknown column ''useradmin_huepermission.connector_id'' in ''field list''")` error
  comes, then try **changing the DB name** in the hue.ini under `[[database]]` because
  there is no upgrade path and run the migrate command `./build/env/bin/hue migrate`.'
- HA is supported by pointing to the HttpFs service instead of the NameNode.
- WebHDFS proxy user setting is required for Hue.
- Hue can read and write files on the Ozone filesystem, similar to HDFS.
- Connectors are configurable via the public REST API.
- Requires setup of the hue.ini file for configuration.
- Requires ODBC configuration for Azure SQL Database and other SQL interfaces.
- Hue's file browser can now allow users to explore, manage, and upload data in an
  ADLS v1 or ADLS v2 (ABFS), in addition to HDFS, Ozone and S3.
- The region should be set to the AWS region corresponding to the S3 account. By default,
  this region will be set to ‘us-east-1’.
- If you use framed transport, you cannot use doAs impersonation, because SASL does
  not work with Thrift framed transport.
- 'After enabling the above flags, if a django.db.utils.OperationalError: (1054, "Unknown
  column ''useradmin_huepermission.connector_id'' in ''field list''") error comes,
  then try changing the DB name in the hue.ini under [[database]] because there is
  no upgrade path and run the migrate command ./build/env/bin/hue migrate.'
- The region should be set to the AWS region corresponding to the S3 account.
- By default, this region will be set to ‘us-east-1’.
- Hue connects to any database or warehouse via native Thrift or SqlAlchemy connectors.
- The dialect should be added to the Python system or Hue Python virtual environment.
- Hue supports one or two Yarn clusters (two for HA).
- Create a role in the Sentry app through Hue.
- A limit to the number of rows that can be downloaded from a query before it is truncated.
  A value of -1 means there will be no limit.
- A limit to the number of bytes that can be downloaded from a query before it is
  truncated. A value of -1 means there will be no limit.
- Global setting to allow or disable end user downloads in all Hue.
- The download feature in the file browser can be disabled separately.
- The editor supports some global settings.
- Some features are experimental.
- This interface provides asynchronous executions and progress reports.
- Hue must be behind a load balancer proxying static files.
- Adding more Hue instances behind the load balancer will increase performances by
  50 concurrent users.
- Database backend should be such as MySql/Postgres/Oracle. Hue does not work on SQLite
  as it makes concurrent write calls to the database.
- Hue must be behind a load balancer proxying static files. e.g. NGINX is used for
  the containers, Cloudera Hue ships with HTTPD.
- There are some memory fragmentation issues in Python that manifest in Hue. Check
  the memory usage of Hue periodically.
- Hue loads and merges all of the files with extension .ini located in the /etc/hue
  directory.
- If users on your cluster have problems running Hue, you can often find error messages
  in these log files.
- By default, Hue is configured to use an embedded SQLite database so that it starts
  but many errors will come up due to the lack of transactions.
- Currently in Beta.
- Slack users need to be a Hue user to have permission to access the features from
  the App.
- Livy will automatically inactive idle sessions after 1 hour (configurable)
- Sessions ids are incrementing numbers starting from 0.
- Livy will automatically inactive idle sessions after 1 hour (configurable).
- 'Persistence: Hue needs an existing database with transactions like MySQL to support
  concurrent requests and also not lose the registered users, saved queries, sharing
  permissions… when the server gets stopped.'
- 'Persistence: Hue needs an existing database with transactions like MySQL to support
  concurrent requests.'
- Set DESKTOP_DEBUG=1 as an environment variable if you want logs to go to stderr
  as well as to the respective log files.
- After upgrading the version of Hue, running ./build/env/bin/hue migrate will make
  sure the database has the correct tables and fields.
- Users authenticate with the same credentials as they would do in the Browser login
  page.
- JWT token is returned and needs to be passed as a bearer in the headers for all
  the API calls.
- Token is obtained via username and password authentication.
- Refresh REST API with more details on the authentication
- Support impersonation in the python driver
- keytab_reinit_frequency is ignored in hue.ini
- Removing incremental refresh
- Hue v4.7.1, released May 20th 2020 - HUE-9356
- The continuous integration now checks for deadlinks on docs.gethue.com
- Refactor some of the old dev content
- Improve instructions on how to re-generate the js SQL parser
- No major core changes
- Support multi-authentication with LDAP
- Uses OAuth2 with refresh token — requires setup of connected app in hue
- Some objects may return nulls in deeply nested fields
- Configuration doesn't work in Hue 4
- Allow any URL base path in embedded mode
- Editor is missing regular ssh action
- Toggle hiding/showing the code snippets
- Avoid microsecond comparison for last_modified field MySQL < 5.6 doesn't support
  microsecond precision.
- Gracefully fail if the input file is invalid for an external table
- Avoid double bar chart rendering on window resize
- Hue is an open source Web UI for easily doing Big Data analysis with Hadoop.
- Supports nesting of workflows
- Empty documents will break the editor.
- Some errors are not reseted at query execution
- Hue is an open source UI for Hadoop and its satellite projects.
- LDAP backend should use search/bind semantics
- LdapSync of groups in AD with more than 30 chars in the group name fails
- Hue 2.4.0 is compatible with CDH4.3 (Cloudera's Distribution Including Apache Hadoop
  4.3).
- Hue 2.0.1 runs on CentOS versions 5 to 6, and Ubuntu 10.04 to 11.10.
- Hue 2.0.1 is compatible with CDH4.
- Hue doesn't build on Ubuntu 11.10
- Document cyrus-sasl-gssapi requirement
- Hue 1.2.0 only works with CDH3b4 or newer. CDH2 and earlier versions of CDH3 will
  not work.
- The included version of Hive has been upgraded from 0.5 to 0.7. This version of
  Hive has changed the format for the metastore. Upgrading Hue 1.2 will require you
  to upgrade your metastore version using the upgrade scripts included with Hive.
- Hue 1.1.0 only works with CDH3b3 or newer. CDH2 and earlier versions of CDH3 will
  not work.
- The default port for the namenode plugins has changed from 9090 to 10090.
- 'HUE 0.9.1 is mainly a bug-fix release. Most notably: A lot of installation related
  issues have been corrected.'
- The JFrame filtering system now supports garbage collection of individual elements,
  which fixes memory leak on partial refresh.
- IE compatibility has improved.
- User manual has been updated.
- After a successful upload with Linux's flash player (which may warn you about hanging
  your computer), the upload screen does not clear. Click the red (x) button to clear
  it.
- Running on Internet Explorer 8 has a memory leak issue, which can cause instability
  with prolonged use.
- FileBrowser upload does not work well with big files (larger than 200MB).
- Pagination in the Beeswax application is now available in the UI.
- The tarball build now succeeds if a previous version's Cloudera Desktop plugins
  are installed.
- Error messages are propagated correctly in Python 2.4.
- Desktop can now run on port 80.
- Package dependencies on python-dev are now more accurate.
- Creating a 'home directory' while creating the user in the user manager no longer
  produces an error.
- Dates and times are now displayed in a specific timezone, which is configurable.
- Inactive users are not able to login.
- The Job Designer appropriately configures hadoop.tmp.dir; this fixes some permissions
  denied errors.
- File upload is more robust.
errors:
- '400 Bad Request: Invalid file format or size'
- '500 Internal Server Error: Server-side error'
- '400 Bad Request: Invalid SQL dialect'
- 'Results have expired: This can occur if the user session is lost due to load balancing
  configuration.'
- 'Invalid session Id: This may occur if the TCP connection is balanced away from
  the previously selected HiveServer2 or Impalad instance.'
- 'database is locked: Ensure you connect to a transactional database.'
- '401 unauthorized: Given token not valid for any token type'
- '401 Unauthorized: Given token not valid for any token type'
- '401 Unauthorized: Recheck username or password'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '404: Not Found'
- '403: Forbidden'
- '404 Not Found: Check the endpoint path'
- '500 Internal Server Error: Contact support'
- After a successful upload with Linux's flash player (which may warn you about hanging
  your computer), the upload screen does not clear. Click the red (x) button to clear
  it.
auth_info:
  mentioned_objects: []
client:
  base_url: http://gethue.com
  headers:
    Accept: application/json
source_metadata: null
