resources:
- name: jobs
  endpoint:
    path: /jobs/team-a
    method: GET
    data_selector: jobs
    params: {}
- name: Run:ai cluster
  endpoint:
    path: /
    method: GET
    data_selector: ''
    params: {}
- name: Helm chart values
  endpoint:
    path: /helm/chart/values
    method: GET
- name: uninstall_cluster
  endpoint:
    path: /uninstall/runai-cluster
    method: POST
- name: installation
  endpoint:
    path: /install/additional-clusters
    method: GET
    data_selector: details
    params: {}
- name: control_plane_upgrade
  endpoint:
    path: /upgrade/control-plane
    method: POST
- name: Cluster hardware requirements
  endpoint:
    path: /cluster/hardware-requirements
    method: GET
    data_selector: resources
    params: {}
- name: OpenShift requirements
  endpoint:
    path: /openshift/requirements
    method: GET
    data_selector: resources
    params: {}
- name: PostgreSQL database
  endpoint:
    path: /external/postgresql/database
    method: GET
    data_selector: resources
    params: {}
- name: runai-reg-creds
  endpoint:
    path: /runai-reg-creds
    method: POST
    data_selector: credentials
    params:
      namespace: runai-backend
- name: runai-backend
  endpoint:
    path: /runai-backend
    method: GET
    data_selector: projects
    params: {}
- name: Run:ai Projects
  endpoint:
    path: /platform-admin/aiinitiatives/org/projects/
    method: GET
- name: runai_backend
  endpoint:
    path: /runai-backend/control-plane
    method: POST
    data_selector: values
    params:
      version: ~2.20.0
- name: clusters
  endpoint:
    path: /clusters
    method: GET
    data_selector: clusters
    params: {}
- name: s3FileSystemImage
  endpoint:
    path: /workload-controller/s3FileSystemImage
    method: GET
    data_selector: name
    params: {}
- name: gitSyncImage
  endpoint:
    path: /workload-controller/gitSyncImage
    method: GET
    data_selector: name
    params: {}
- name: permissions
  endpoint:
    path: /permissions
    method: GET
    data_selector: permissions
    params: {}
- name: runaiconfig_backup
  endpoint:
    path: /runai/config
    method: GET
    data_selector: spec
    params: {}
- name: backend
  endpoint:
    path: /runai-backend-backend
    method: GET
    data_selector: deployment_info
- name: frontend
  endpoint:
    path: /runai-backend-frontend
    method: GET
    data_selector: deployment_info
- name: grafana
  endpoint:
    path: /runai-backend-grafana
    method: GET
    data_selector: deployment_info
- name: thanos_query
  endpoint:
    path: /thanos/query
    method: POST
    data_selector: metrics
    params:
      resources:
        limits:
          memory: 3G
          cpu: '1'
        requests:
          memory: 3G
          cpu: '1'
- name: thanos_receive
  endpoint:
    path: /thanos/receive
    method: POST
    data_selector: metrics
    params:
      resources:
        limits:
          memory: 6G
          cpu: '1'
        requests:
          memory: 6G
          cpu: '1'
- name: scheduling_services
  endpoint:
    path: /runai/schedulingServices
    method: POST
    data_selector: resource_requests
    params:
      resources:
        limits:
          cpu: 1000m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 512Mi
- name: recommended_large_clusters
  endpoint:
    path: /runai/largeClusters
    method: POST
    data_selector: resource_requests
    params:
      resources:
        limits:
          cpu: '2'
          memory: 2Gi
        requests:
          cpu: '1'
          memory: 1Gi
- name: email_notifications
  endpoint:
    path: /enable/email/notifications
    method: POST
    data_selector: response
    params: {}
- name: system_notifications
  endpoint:
    path: /configure/system/notifications
    method: POST
    data_selector: response
    params: {}
- name: node_roles
  endpoint:
    path: /configure/node_roles
    method: GET
    data_selector: roles
    params: {}
- name: Port Forwarding
  endpoint:
    path: /kubernetes/port-forwarding
    method: POST
    data_selector: portForwardingDetails
    params: {}
- name: NodePort
  endpoint:
    path: /kubernetes/nodeport
    method: POST
    data_selector: nodePortDetails
    params: {}
- name: LoadBalancer
  endpoint:
    path: /kubernetes/loadbalancer
    method: POST
    data_selector: loadBalancerDetails
    params: {}
- name: node_labels
  endpoint:
    path: /node-pools
    method: GET
    data_selector: node_pools
    params: {}
- name: persistent_volume_claim
  endpoint:
    path: /kubectl/label/persistentvolumeclaims
    method: POST
    data_selector: PVC
    params: {}
- name: node_pool
  endpoint:
    path: /nodePools
    method: GET
    data_selector: nodePools
    params: {}
- name: node_affinity
  endpoint:
    path: /nodeAffinity
    method: GET
    data_selector: nodeAffinity
    params: {}
- name: namespace_annotation
  endpoint:
    path: /kubectl/annotate/ns/{namespace} runai/enforce-scheduler-name=true
    method: POST
    data_selector: annotations
    params: {}
- name: custom_alert
  endpoint:
    path: /api/custom_alerts
    method: POST
    data_selector: alert
    params: {}
- name: events_history
  endpoint:
    path: /audit-log
    method: GET
    data_selector: events
- name: users
  endpoint:
    path: /users
    method: GET
    data_selector: users
    params: {}
- name: users
  endpoint:
    path: /api/users
    method: GET
- name: access_rules
  endpoint:
    path: /api/access_rules
    method: GET
- name: roles
  endpoint:
    path: /api/roles
    method: GET
    data_selector: roles
    params: {}
- name: access_rules
  endpoint:
    path: /access
    method: GET
    data_selector: access_rules
    params: {}
- name: access_rule
  endpoint:
    path: /api/access_rules
    method: GET
- name: prometheus_logging
  endpoint:
    path: /runaiconfig/runai
    method: EDIT
    data_selector: spec.prometheus.spec.logLevel
    params:
      logLevel: debug
- name: scheduler_logging
  endpoint:
    path: /runaiconfig/runai
    method: EDIT
    data_selector: runai-scheduler.args.verbosity
    params:
      verbosity: 6
- name: users
  endpoint:
    path: /api/users
    method: GET
    data_selector: users
    params: {}
- name: applications
  endpoint:
    path: /applications
    method: GET
- name: roles
  endpoint:
    path: /roles
    method: GET
    data_selector: roles
    params: {}
- name: roles
  endpoint:
    path: /api/roles
    method: GET
    data_selector: roles
    params: {}
- name: access_rules
  endpoint:
    path: /access
    method: GET
    data_selector: access_rules
    params: {}
- name: projects
  endpoint:
    path: /api/projects
    method: GET
    data_selector: projects
    params: {}
- name: departments
  endpoint:
    path: /api/departments
    method: GET
    data_selector: departments
    params: {}
- name: node_pools
  endpoint:
    path: /api/node_pools
    method: GET
    data_selector: node_pools
    params: {}
- name: projects
  endpoint:
    path: /api/projects
    method: GET
    data_selector: projects
- name: departments
  endpoint:
    path: /api/departments
    method: GET
    data_selector: departments
- name: scheduling_rules
  endpoint:
    path: /api/scheduling/rules
    method: GET
    data_selector: rules
    params: {}
- name: projects
  endpoint:
    path: /api/docs#tag/Projects/operation/create_project
    method: GET
- name: nodes
  endpoint:
    path: /api/nodes
    method: GET
    data_selector: nodes
    params: {}
- name: node_pools
  endpoint:
    path: /api/node_pools
    method: GET
    data_selector: node_pools
- name: workloads
  endpoint:
    path: /api/workloads
    method: GET
    data_selector: workload_list
- name: workspaces
  endpoint:
    path: /workspaces
    method: GET
- name: training
  endpoint:
    path: /training
    method: GET
- name: inference
  endpoint:
    path: /inference
    method: GET
- name: workloads
  endpoint:
    path: /api/workloads
    method: GET
- name: environments
  endpoint:
    path: /environments
    method: GET
    data_selector: records
- name: data_sources
  endpoint:
    path: /datasources
    method: GET
    data_selector: records
- name: compute_resources
  endpoint:
    path: /compute
    method: GET
    data_selector: records
- name: credentials
  endpoint:
    path: /credentials
    method: GET
    data_selector: records
- name: environments
  endpoint:
    path: /api/environments
    method: GET
    data_selector: environments
    params: {}
- name: data_source
  endpoint:
    path: /api/docs#tag/Datasources
    method: GET
- name: data_volumes
  endpoint:
    path: /api/data_volumes
    method: GET
- name: compute_resource
  endpoint:
    path: /api/compute_resources
    method: GET
    data_selector: resources
    params: {}
- name: credentials
  endpoint:
    path: /api/credentials
    method: GET
    data_selector: credentials
    params: {}
- name: workspace_template
  endpoint:
    path: /workspace/templates
    method: GET
    data_selector: templates
    params: {}
- name: templates
  endpoint:
    path: /api/templates
    method: GET
    data_selector: templates
    params: {}
- name: workload_policies
  endpoint:
    path: /api/policies
    method: GET
- name: workload
  endpoint:
    path: /api/workloads
    method: POST
    data_selector: spec
    params: {}
- name: policy
  endpoint:
    path: /api/policies
    method: GET
    data_selector: rules
    params: {}
- name: workload
  endpoint:
    path: /workloads
    method: GET
    data_selector: instances
- name: interactive_policy
  endpoint:
    path: /apis/run.ai/v2alpha1/interactivepolicies
    method: GET
    data_selector: items
    params: {}
- name: training_policy
  endpoint:
    path: /apis/run.ai/v2alpha1/trainingpolicies
    method: GET
    data_selector: items
    params: {}
- name: workspace
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: ''
    params: {}
- name: workspaces
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: ''
    params: {}
- name: projects
  endpoint:
    path: /api/v1/projects
    method: GET
    data_selector: ''
    params: {}
- name: clusters
  endpoint:
    path: /api/v1/clusters
    method: GET
    data_selector: ''
    params: {}
- name: workspace
  endpoint:
    path: /workspace/nginx-test
    method: POST
- name: job
  endpoint:
    path: /job/nginx-test
    method: SUBMIT
- name: inference
  endpoint:
    path: /api/v1/workloads/inferences
    method: POST
    data_selector: ''
    params: {}
- name: training_jobs
  endpoint:
    path: /training
    method: submit
    data_selector: jobs
    params: {}
- name: workspaces
  endpoint:
    path: /workspaces
    method: GET
- name: training
  endpoint:
    path: /training
    method: GET
- name: inference
  endpoint:
    path: /inference
    method: GET
- name: workload
  endpoint:
    path: /api/workloads
    method: GET
    data_selector: workloads
- name: environments
  endpoint:
    path: /environments
    method: GET
    data_selector: environments
    params: {}
- name: environments
  endpoint:
    path: /api/environments
    method: GET
    data_selector: data
    params: {}
- name: NFS
  endpoint:
    path: /nfs
    method: POST
    data_selector: data
    params: {}
- name: PVC
  endpoint:
    path: /pvc
    method: POST
    data_selector: data
    params: {}
- name: S3 Bucket
  endpoint:
    path: /s3
    method: POST
    data_selector: data
    params: {}
- name: Git
  endpoint:
    path: /git
    method: POST
    data_selector: data
    params: {}
- name: Host path
  endpoint:
    path: /hostpath
    method: POST
    data_selector: data
    params: {}
- name: ConfigMap
  endpoint:
    path: /configmap
    method: POST
    data_selector: data
    params: {}
- name: Secret
  endpoint:
    path: /secret
    method: POST
    data_selector: data
    params: {}
- name: data_volumes
  endpoint:
    path: /api/v2.20/data_volumes
    method: GET
    data_selector: data
    params: {}
- name: compute_resource
  endpoint:
    path: /api/v1/compute_resources
    method: GET
    data_selector: compute_resources
    params: {}
- name: credentials
  endpoint:
    path: /api/credentials
    method: GET
    data_selector: credentials
    params: {}
- name: templates_table
  endpoint:
    path: /workspace/templates
    method: GET
    data_selector: templates
    params: {}
- name: workload
  endpoint:
    path: /api/workloads
    method: POST
    data_selector: workspace
    params: {}
- name: workspaces
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: spec
    params: {}
- name: workload
  endpoint:
    path: /api/v1/workloads
    method: POST
    data_selector: workloads
    params: {}
- name: trainings
  endpoint:
    path: /api/v1/workloads/trainings
    method: POST
    data_selector: spec
    params: {}
- name: workload
  endpoint:
    path: /api/v1/workloads
    method: POST
    data_selector: workload
    params: {}
- name: environment
  endpoint:
    path: /api/v1/environments
    method: GET
    data_selector: environments
    params: {}
- name: distributed_workload
  endpoint:
    path: /workloads/distributed
    method: POST
    data_selector: spec
    params: {}
- name: inference_workload
  endpoint:
    path: /workloads/inference
    method: POST
- name: kubeconfig
  endpoint:
    path: ./kube/config
    method: SET
    data_selector: contexts
    params: {}
- name: attach_job
  endpoint:
    path: /attach
    method: POST
    data_selector: output
    params:
      job-name: <job-name>
      no-stdin: 'false'
      no-tty: 'false'
      pod: string
      loglevel: info
- name: runai_bash
  endpoint:
    path: /runai/bash
    method: POST
    data_selector: output
    params: {}
- name: project
  endpoint:
    path: /config/project/<project-name>
    method: POST
- name: cluster
  endpoint:
    path: /config/cluster/<cluster-name>
    method: POST
- name: delete_workload
  endpoint:
    path: /runai/delete
    method: DELETE
    data_selector: output
    params:
      job-name: <job-name>
      all: --all
      loglevel: --loglevel
      project: --project
- name: job
  endpoint:
    path: /describe/job
    method: GET
    data_selector: Workload properties and status
    params:
      job-name: <job-name>
- name: node
  endpoint:
    path: /describe/node
    method: GET
    data_selector: Node properties
    params:
      node-name: <node-name>
- name: exec_command
  endpoint:
    path: /runai/exec
    method: POST
    data_selector: output
    params: {}
- name: jobs
  endpoint:
    path: runai list jobs
    method: GET
    data_selector: Workloads
    params:
      all_projects: true
- name: projects
  endpoint:
    path: runai list projects
    method: GET
    data_selector: Projects
    params: {}
- name: clusters
  endpoint:
    path: runai list clusters
    method: GET
    data_selector: Clusters
    params: {}
- name: nodes
  endpoint:
    path: runai list nodes
    method: GET
    data_selector: Nodes
    params:
      node_name: optional
- name: logout
  endpoint:
    path: /runai/logout
    method: POST
    data_selector: output
    params: {}
- name: logs
  endpoint:
    path: /runai/logs
    method: GET
    data_selector: logs
    params: {}
- name: resume_job
  endpoint:
    path: /runai/resume
    method: POST
    data_selector: job
    params: {}
- name: job
  endpoint:
    path: /submit
    method: POST
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: mpi_job
  endpoint:
    path: /submit-dist/mpi
    method: POST
    data_selector: job
    params: {}
- name: distributed_job
  endpoint:
    path: /submit-dist/pytorch
    method: POST
    data_selector: job_details
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: distributed_job
  endpoint:
    path: /submit-dist/tf
    method: POST
    data_selector: job
    params: {}
- name: distributed_job
  endpoint:
    path: /submit-dist/xgboost
    method: POST
    data_selector: job
    params: {}
- name: suspend_job
  endpoint:
    path: /suspend
    method: POST
    data_selector: output
    params:
      job-name: <job-name>
      project: <project-name>
- name: nodes
  endpoint:
    path: /top/node
    method: GET
- name: environment_variables
  endpoint:
    path: /environment/variables
    method: GET
    data_selector: variables
    params: {}
- name: email_notifications
  endpoint:
    path: /admin/config/notifications/
    method: GET
    data_selector: notifications
    params: {}
- name: kubernetes_secret
  endpoint:
    path: /kubernetes/secrets
    method: POST
    data_selector: secrets
    params: {}
- name: dynamic_gpu_fractions
  endpoint:
    path: /api/v1/dynamic_gpu_fractions
    method: GET
    data_selector: dynamicGpuFractions
    params: {}
- name: multi_gpu_dynamic_fractions
  endpoint:
    path: /api/v1/multi_gpu_dynamic_fractions
    method: GET
    data_selector: multiGpuDynamicFractions
    params: {}
- name: time_slicing_modes
  endpoint:
    path: /gpu/time-slicing/modes
    method: GET
    data_selector: modes
- name: time_slicing_plan
  endpoint:
    path: /gpu/time-slicing/plan
    method: GET
    data_selector: plan
- name: enable_time_slicing
  endpoint:
    path: /gpu/time-slicing/enable
    method: POST
    data_selector: enable
- name: dynamic GPU fractions
  endpoint:
    path: /dynamic-gpu-fractions
    method: GET
- name: Node Level Scheduler
  endpoint:
    path: /node-level-scheduler
    method: GET
- name: runaiconfig
  endpoint:
    path: /runaiconfig
    method: PATCH
- name: job_submission
  endpoint:
    path: /submit
    method: POST
    data_selector: job
    params:
      cpu: --cpu
      memory: --memory
- name: workspaces
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: spec
    params: {}
- name: workspaces
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: spec
    params: {}
- name: inference
  endpoint:
    path: /api/v1/workloads/inferences
    method: POST
    data_selector: spec
    params: {}
- name: workspace
  endpoint:
    path: /api/v1/workloads/workspaces
    method: POST
    data_selector: spec
    params: {}
- name: workload
  endpoint:
    path: /workload
    method: POST
    data_selector: workload
    params: {}
- name: workspace
  endpoint:
    path: /workspace
    method: GET
    data_selector: workspaces
    params: {}
- name: workload
  endpoint:
    path: /workspace/submit
    method: POST
    data_selector: workloads
    params:
      image: runai.jfrog.io/demo/pycharm-demo
- name: workspace_list
  endpoint:
    path: /workspace/list
    method: GET
    data_selector: workloads
    params: {}
- name: workspace_port_forward
  endpoint:
    path: /workspace/port-forward
    method: POST
    data_selector: port_forward
    params:
      workload: build-remote
      port_map: '2222:22'
- name: workload
  endpoint:
    path: /workspace/submit
    method: POST
    data_selector: workload
    params: {}
- name: workspace_status
  endpoint:
    path: /workspace/list
    method: GET
    data_selector: status
    params: {}
- name: ssh_connection
  endpoint:
    path: /workspace/port-forward
    method: POST
    data_selector: connection
    params:
      port: '2222:22'
- name: logs_directory
  endpoint:
    path: /mnt/nfs_share/john/logs/fit
    method: GET
    data_selector: logs
    params: {}
- name: workload
  endpoint:
    path: /workspace
    method: POST
    data_selector: workload_status
- name: clusters
  endpoint:
    path: /v1/k8s/clusters
    method: GET
    data_selector: data
    params: {}
- name: metrics
  endpoint:
    path: /api/metrics
    method: GET
    data_selector: data
    params: {}
- name: telemetry
  endpoint:
    path: /api/telemetry
    method: GET
    data_selector: data
    params: {}
- name: runai_active_job_cpu_requested_cores
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_REQUEST_CORES
- name: runai_active_job_memory_requested_bytes
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_MEMORY_REQUEST_BYTES
- name: runai_cluster_cpu_utilization
  endpoint:
    path: /api/v2/clusters/{clusterUuid}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_UTILIZATION
- name: runai_cluster_memory_utilization
  endpoint:
    path: /api/v2/clusters/{clusterUuid}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_MEMORY_UTILIZATION
- name: runai_gpu_utilization_per_gpu
  endpoint:
    path: /api/v1/workloads/{workloadId}/pods/{podId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: GPU_UTILIZATION_PER_GPU
- name: runai_gpu_utilization_per_workload
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: GPU_UTILIZATION
- name: runai_requested_gpu_memory_mb_per_workload
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: GPU_MEMORY_REQUEST_BYTES
- name: runai_job_cpu_usage
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_USAGE_CORES
- name: runai_job_memory_used_bytes
  endpoint:
    path: /api/v1/workloads/{workloadId}/metrics
    method: GET
    data_selector: metrics
    params:
      metricType: CPU_MEMORY_USAGE_BYTES
- name: Run:ai Cluster
  endpoint:
    path: /runai/cluster
    method: GET
- name: Run:ai Control Plane
  endpoint:
    path: /runai/control-plane
    method: GET
- name: runaiconfig
  endpoint:
    path: /runaiconfig
    method: GET
- name: helm_upgrade_instructions
  endpoint:
    path: /upgrade/instructions
    method: GET
    data_selector: instructions
    params: {}
- name: control_plane
  endpoint:
    path: /control-plane
    method: POST
    data_selector: data
    params: {}
- name: cluster
  endpoint:
    path: /clusters
    method: GET
    data_selector: clusters
    params: {}
- name: runai-backend
  endpoint:
    path: /oc/new-project/runai-backend
    method: POST
    data_selector: created
    params: {}
- name: control_plane_install
  endpoint:
    path: /control-plane
    method: GET
    data_selector: install_details
- name: projects
  endpoint:
    path: /projects
    method: POST
    data_selector: namespace
    params: {}
- name: upgrade_control_plane
  endpoint:
    path: /upgrade/control-plane
    method: GET
    data_selector: values
    params: {}
- name: upgrade_cluster
  endpoint:
    path: /upgrade/cluster
    method: GET
    data_selector: instructions
    params: {}
- name: node_pool_labels
  endpoint:
    path: /node-pools
    method: POST
    data_selector: labels
    params:
      key: run.ai/type
      value: <TYPE_VALUE>
- name: schedulingServices
  endpoint:
    path: /api/v1/schedulingServices
    method: POST
    data_selector: resources
    params:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 512Mi
- name: syncServices
  endpoint:
    path: /api/v1/syncServices
    method: POST
    data_selector: resources
    params:
      limits:
        cpu: '2'
        memory: 2Gi
      requests:
        cpu: '1'
        memory: 1Gi
- name: workloadServices
  endpoint:
    path: /api/v1/workloadServices
    method: POST
    data_selector: resources
    params: {}
- name: Access Rules
  endpoint:
    path: /access_rules
    method: GET
    data_selector: rules
    params: {}
- name: AlertmanagerConfig
  endpoint:
    path: /api/v1/alertmanagerconfig
    method: POST
    data_selector: metadata
    params: {}
- name: Alertmanager
  endpoint:
    path: /api/v1/alertmanager
    method: POST
    data_selector: metadata
    params: {}
- name: Service
  endpoint:
    path: /api/v1/service
    method: POST
    data_selector: metadata
    params: {}
- name: email_receiver
  endpoint:
    path: /alertmanager/configuration
    method: PATCH
    data_selector: receivers
    params: {}
- name: alert_route
  endpoint:
    path: /alertmanager/routes
    method: PATCH
    data_selector: route
    params: {}
- name: audit_log
  endpoint:
    path: /v1/k8s/audit
    method: GET
    data_selector: ''
- name: node_pool
  endpoint:
    path: /api/docs/#/NodePools/createNodePool
    method: POST
    data_selector: node_pool
- name: interactive_workload
  endpoint:
    path: /interactiveWorkload
    method: POST
- name: training_workload
  endpoint:
    path: /trainingWorkload
    method: POST
- name: distributed_workload
  endpoint:
    path: /distributedWorkload
    method: POST
- name: inference_workload
  endpoint:
    path: /inferenceWorkload
    method: POST
- name: interactive_policy
  endpoint:
    path: /v2alpha1/interactivepolicy
    method: POST
    data_selector: metadata.name
    params: {}
- name: training_policy
  endpoint:
    path: /v2alpha1/trainingpolicy
    method: POST
    data_selector: metadata.name
    params: {}
- name: workspaces_policy
  endpoint:
    path: /workspaces/policy
    method: GET
- name: kubernetes_secret
  endpoint:
    path: /kubernetes/secrets
    method: POST
    data_selector: secrets
    params: {}
- name: workload
  endpoint:
    path: /workloads
    method: POST
    data_selector: workloads
    params: {}
- name: departments
  endpoint:
    path: /api/v2/departments
    method: GET
- name: departments
  endpoint:
    path: /departments
    method: GET
    data_selector: departments
    params: {}
- name: projects
  endpoint:
    path: /projects
    method: GET
    data_selector: projects
    params: {}
- name: GPU Dashboard
  endpoint:
    path: /gpu/dashboard
    method: GET
- name: CPU Dashboard
  endpoint:
    path: /cpu/dashboard
    method: GET
- name: Analytics Dashboard
  endpoint:
    path: /analytics/dashboard
    method: GET
- name: Consumption Dashboard
  endpoint:
    path: /consumption/dashboard
    method: GET
- name: Quota Management Dashboard
  endpoint:
    path: /quota/dashboard
    method: GET
- name: jobs
  endpoint:
    path: /api/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: Docker registry
- name: Access key
- name: User name and password
- name: Cluster Health
  endpoint:
    path: /cluster_health
    method: GET
    data_selector: status
    params: {}
- name: RunaiAgentClusterInfoPushRateLow
  endpoint:
    path: /RunaiAgentClusterInfoPushRateLow/
    method: GET
- name: RunaiAgentPullRateLow
  endpoint:
    path: /RunaiAgentPullRateLow/
    method: GET
- name: RunaiContainerMemoryUsageCritical
  endpoint:
    path: /RunaiContainerMemoryUsageCritical/
    method: GET
- name: RunaiContainerMemoryUsageWarning
  endpoint:
    path: /RunaiContainerMemoryUsageWarning/
    method: GET
- name: RunaiContainerRestarting
  endpoint:
    path: /RunaiContainerRestarting/
    method: GET
- name: RunaiCpuUsageWarning
  endpoint:
    path: /RunaiCpuUsageWarning/
    method: GET
- name: RunaiCriticalProblem
  endpoint:
    path: /RunaiCriticalProblem/
    method: GET
- name: RunaiDaemonSetRolloutStuck
  endpoint:
    path: /RunaiDaemonSetRolloutStuck/
    method: GET
- name: RunaiDaemonSetUnavailableOnNodes
  endpoint:
    path: /RunaiDaemonSetUnavailableOnNodes/
    method: GET
- name: RunaiDeploymentInsufficientReplicas
  endpoint:
    path: /RunaiDeploymentInsufficientReplicas/
    method: GET
- name: RunaiDeploymentNoAvailableReplicas
  endpoint:
    path: /RunaiDeploymentNoAvailableReplicas/
    method: GET
- name: RunaiDeploymentUnavailableReplicas
  endpoint:
    path: /RunaiDeploymentUnavailableReplicas/
    method: GET
- name: RunaiProjectControllerReconcileFailure
  endpoint:
    path: /RunaiProjectControllerReconcileFailure/
    method: GET
- name: RunaiStatefulSetInsufficientReplicas
  endpoint:
    path: /RunaiStatefulSetInsufficientReplicas/
    method: GET
- name: RunaiStatefulSetNoAvailableReplicas
  endpoint:
    path: /RunaiStatefulSetNoAvailableReplicas/
    method: GET
notes:
- By default, researcher APIs are unauthenticated.
- Use the token for subsequent API calls.
- The NVIDIA Run:ai docs are moving! Visit the new documentation site for the latest
  features.
- Integrate with your organization's identity provider for streamlined authentication
  (Single Sign On) and role-based access control (RBAC).
- Run:ai only synchronizes metadata and operational metrics (e.g., workloads, nodes)
  with the control plane. No proprietary data, model artifacts, or user data sets
  are ever transmitted, ensuring full data privacy and security.
- The NVIDIA Run:ai docs are moving!
- Documentation for versions 2.19 and below will remain on this site.
- Self-hosted installation supports both connected and air-gapped.
- Visit NVIDIA Run:ai documentation for the latest features starting from version
  2.20.
- Researchers, or AI practitioners, use Run:ai to submit Workloads.
- Developers can access Run:ai through various programmatic interfaces.
- Run:ai does not recommend using API endpoints and fields marked as `deprecated`
  and will not add functionality to them.
- The self-hosted option is for organizations that cannot use a SaaS solution due
  to data leakage concerns.
- 'Requires a Fully Qualified Domain Name (FQDN) to install Run:ai Cluster (ex: runai.mycorp.local).'
- Ensure the inbound and outbound rules are correctly applied to your firewall.
- Ensure that all Kubernetes nodes can communicate with each other across all necessary
  ports.
- There are a number of matters to consider prior to installing using Helm.
- Run:ai cluster must be installed in a namespace named runai.
- A TLS private and public keys are required for HTTP access to the cluster.
- The Run:ai cluster installation can be customized to support your environment via
  Helm values files or Helm install flags.
- Run:ai Projects are implemented as Kubernetes namespaces.
- Setting the createNamespaces flag to false moves the responsibility of creating
  namespaces to match Run:ai Projects to the administrator.
- It is highly recommended to upgrade the Kubernetes version together with the Run:ai
  cluster version, to ensure compatibility with latest supported version of your Kubernetes
  distribution
- The latest releases of the Run:ai cluster require Helm 3.14 or above.
- Run:ai cluster version 2.13 (old release) does not support migration of the configured
  Helm values.
- An Application secret key is required to connect the cluster to the Run:ai platform.
- The machine running the installation script (typically the Kubernetes master) must
  have at least 50GB of free space.
- Docker installed.
- Run:ai requires Helm 3.14 or later.
- Nodes are required to be synchronized by time using NTP (Network Time Protocol)
  for proper system functionality.
- The Run:ai control plane requires a default storage class to create persistent volume
  claims for Run:ai storage.
- For a simple (nonproduction) storage class example see Kubernetes Local Storage
  Class.
- Run:ai assumes the existence of a Docker registry for images.
- If lacking resources, the Run:ai nodes will move to another, non-labeled node.
- Do not select the Kubernetes master as a runai-system node.
- Make sure you have followed the Control Plane prerequisites and preparations.
- Use the --dry-run flag to gain an understanding of what is being installed before
  the actual installation.
- For self-hosted deployments, Kubernetes Ingress Controller and Cluster Fully Qualified
  Domain Name (FQDN) requirements are only necessary when the Run:ai Control Plane
  and Run:ai Cluster reside on separate Kubernetes clusters.
- The first Run:ai cluster is typically installed on the same Kubernetes cluster as
  the Run:ai control plane.
- Run:ai supports multiple clusters per single control plane.
- Setting the `createNamespaces` flag to `false` moves the responsibility of creating
  namespaces to match Run:ai Projects to the administrator.
- Create additional Users.
- Set up Project-based Researcher Access Control.
- Set up Researchers to work with the Run:ai Command-line interface (CLI).
- If you are installing an air-gapped version of Run:ai, the Run:ai tar file contains
  the helm binary.
- The helm repository name has changed from 'runai-backend/runai-backend' to 'runai-backend/control-plane'.
- 'To uninstall the cluster see: cluster delete'
- 'To delete the control plane, run: helm uninstall runai-backend -n runai-backend'
- In OpenShift environments, adding a cluster connecting to a remote control plane
  currently requires the assistance of customer support.
- OpenShift must be configured with a trusted certificate.
- OpenShift must have a configured identity provider (Idp).
- The Run:ai control plane, when installed without a Run:ai cluster, does not require
  the NVIDIA prerequisites.
- The Run:ai control plane, when installed without a Run:ai cluster, does not require
  the Inference prerequisites.
- The following section provides IT with the information needed to prepare for a Run:ai
  installation.
- Requires setup of a Docker registry for images
- Must have at least 20GB of free disk space to run the setup script
- Uses Helm for installation
- PostgreSQL is included as a sub-chart
- The last namespace (`runai-scale-adjust`) is only required if the cluster is a cloud
  cluster and is configured for auto-scaling.
- When creating a new cluster, select the OpenShift target platform.
- Do not add the helm repository and do not run helm repo update.
- When creating a new Run:ai Project, Run:ai creates a namespace by the name of 'runai-<PROJECT-NAME>'.
- Create additional Run:ai Users.
- The helm repository name has changed from runai-backend/runai-backend to runai-backend/control-plane.
- Run:ai CLI needs to be installed on the Researcher's machine.
- To submit workloads with Run:ai, the Researcher must be provided with a Project
  that contains a GPU quota.
- The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send
  deep learning workloads, acquire GPU-based containers, list jobs, etc.
- When enabled, Researcher authentication requires additional setup when installing
  the CLI.
- 'Before starting, make sure you have the following: Access to the Kubernetes cluster
  where Run:ai is deployed with the necessary permissions, Access to the Run:ai Platform'
- Advanced cluster configurations can be used to tailor your Run:ai cluster deployment
  to meet specific operational requirements and optimize resource management.
- For air-gapped environments, replace default sidecar images for Git and S3 integrations.
- Security customizations allow organizations to tailor Run:ai to their specific needs.
- Shared storage is a critical component in AI and machine learning workflows, particularly
  in scenarios involving distributed training and shared datasets.
- While Run:ai Platform supports a variety of remote data sources, such as Git and
  S3, it is often more efficient to keep data close to the compute resources.
- Run:ai lists all available storage classes in the Kubernetes cluster, making it
  easy for users to select the appropriate storage.
- Run:ai can be installed in an isolated network.
- The organization creates a local certificate which serves as the root certificate
  for the organization.
- The public key of the local certificate authority is required.
- Create the runai namespace if it does not exist.
- Add the public key to the runai namespace.
- In case you're using OpenShift, add the public key also to the openshift-monitoring
  namespace.
- Run:ai enables AI practitioners to integrate with S3 or Git as data sources.
- When using a custom CA, sidecar containers used for S3 or Git integrations do not
  automatically inherit the CA configured at the cluster level.
- The CLI is supported on Mac and Linux.
- The Kubernetes command-line interface must be installed and configured to access
  your cluster.
- In self-hosted environment, use the control-plane URL instead of app.run.ai
- Run:ai uses an internal PostgreSQL database.
- Run:ai also supports an external PostgreSQL database.
- The purpose of this document is to configure Run:ai such that it will continue to
  provide service even if parts of the system are down.
- A frequent fail scenario is a physical node in the system becoming non-responsive
  due to physical problems or lack of resources.
- Run:ai uses three third parties which are managed as Kubernetes StatefulSets
- Keycloak sets a minimum of 3 pods and will scale to more on transaction load
- To increase the number of replicas, use helm flags for backend, frontend, and grafana.
- Under Kubernetes, each of the Run:ai containers has default resource requirements
  that reflect an average customer load.
- In large clusters (100 nodes or 1500 GPUs or more), we recommend certain configurations
  for SchedulingServices and SyncServices groups.
- To reduce strain on the network when sending metrics, configure Prometheus to send
  information in larger bulks.
- System administrator needs to enable and setup email notifications.
- System notifications allow administrators to update users with events occurring
  within the Run:ai platform.
- Run:ai system-level services run on dedicated CPU-only nodes.
- Workloads that do not request GPU resources are executed on CPU-only nodes.
- To ensure high availability and prevent a single point of failure, it is recommended
  to configure at least three system nodes in your cluster.
- By default, Kubernetes master nodes are configured to prevent workloads from running
  on them as a best-practice measure to safeguard control plane stability.
- Exposing container ports requires declaring them when starting the container.
- Run:ai syntax is similar to Docker.
- Workspaces require access to the container via URLs.
- Host-based routing allows all workspaces to run at the root path.
- Node affinity allows assigning a Project to run on specific nodes.
- Label the target nodes with the label `run.ai/type`.
- Add the key run.ai/type and the value <TYPE_VALUE> in the Kubernetes labels section.
- PVCs are namespace-specific.
- Creating a PVC within the run:ai namespace propagates it to all run:ai Projects.
- Run:ai allows administrators to group specific nodes into a node pool.
- Node affinity can be used by labeling a node or a set of nodes.
- Node Pool can only use one label (key & value) at a time.
- When submitting a workload, if you choose a node pool label and a node affinity
  (node type) label which does not intersect, the Run:ai scheduler will not be able
  to schedule that workload as it represents an empty nodes group.
- Workload deletion protection ensures that only users who created a workload can
  delete or modify them.
- The protection feature is implemented at the cluster level.
- Add PVCs in advance to be used when creating a PVC-type data source via the Run:ai
  UI.
- Add ConfigMaps in advance to be used when creating a ConfigMap-type data source
  via the Run:ai UI.
- 'To authorize Run:ai to use the ConfigMap, label it: run.ai/cluster-wide: "true"'
- 'The ConfigMap must have a label of run.ai/resource: <resource-name>'
- 'To configure Run:ai Scheduler as the default for a specific namespace, add annotation
  ''runai/enforce-scheduler-name: true''.'
- Deploying Run:ai in mission-critical environments requires proper monitoring and
  maintenance of resources to ensure workloads run and are deployed as expected.
- Run:ai does not require any special configurations at this level.
- Administrative access to the Kubernetes cluster, including permissions to run `kubectl`
  commands.
- Familiarity with Kubernetes concepts such as nodes, taints, and workloads.
- The Run:ai software installed and configured within your Kubernetes cluster.
- While training workloads can be automatically migrated, it is recommended to plan
  maintenance and manually manage this process for a faster response, as it may take
  time for Kubernetes to detect a node failure.
- Run:ai uses Prometheus for externalizing metrics and providing visibility to end-users.
- The Run:ai Cluster installation includes Prometheus or can connect to an existing
  Prometheus instance used in your organization.
- A Kubernetes cluster with the necessary permissions is required.
- Up and running Run:ai environment, including Prometheus Operator is necessary.
- The Event history table consists of various columns including Subject, Event, Status,
  and more.
- The Event history table saves events for the last 90 days but presents up to the
  last 30 days of information.
- RBAC at Run:ai is configured using access rules.
- Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with
  a single pair of credentials to multiple, independent software systems.
- Ensure you have the SAML XML Metadata available from your identity provider.
- To avoid losing access, removing the identity provider must be carried out by a
  local user.
- Users can be managed locally, or via the Identity provider, while assigned with
  Access Rules to manage its permissions.
- Single Sign-On users are managed by the identity provider and appear once they have
  signed in to Run:ai.
- The client secret is visible only at the time of creation. It cannot be recovered
  but can be regenerated.
- Regenerating a client secret revokes the previous one.
- The Roles table displays a list of predefined roles available to users in the Run:ai
  platform.
- It is not possible to create additional rules or edit or delete existing rules.
- Admin becomes System Admin with full access to all managed objects and scopes
- Research Manager is not automatically assigned to all projects, but to projects
  set by the relevant Admin when assigning this role to a user, group or app
- To allow the Department Admin to assign a Researcher role to a user, group or app,
  the Department Admin must have VECD permissions for jobs and workspaces
- Users with the role of Editor are assigned to the same scope they had before the
  upgrade
- Flexible management of access rules for specific user, application, project, or
  department.
- Export to CSV is limited to 20,000 rows.
- As of Run:ai version 2.16, you only need to perform these steps when accessing Run:ai
  from the command-line interface.
- As of Run:ai version 2.18, you only need to perform these steps when if using the
  older command-line interface.
- Ensure to delete client-certificate-data and client-key-data from kubeconfig file
  to prevent unauthorized access.
- The identity of the user in the container determines its access to resources.
- 'By default, if you run: runai submit -i ubuntu --attach --interactive -- bash,
  then run id, you will see the root user.'
- The flags --run-as-user and --prevent-privilege-escalation are voluntary and not
  enforced by the system.
- Data saved in the temporary home directory will not be saved when the container
  exits.
- Ensure that you have administrator-level access to the Kubernetes cluster where
  Run:ai is installed.
- The Run:ai Administrator Command-Line Interface (CLI) must be installed.
- Enabling verbose logging can significantly increase disk space usage. Monitor your
  storage capacity and adjust the verbosity level as necessary.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Run:ai Self-hosted installation contains an internal database. To diagnose database
  issues, you can run the database in debug mode.
- Verbose scheduler logs consume a significant amount of disk space.
- SSO users are managed by the identity provider and appear once they have signed
  in to Run:ai.
- The temporary password is visible only at the time of user’s creation, and must
  be changed after the first sign-in
- To ensure administrative operations are always available, at least one local user
  with System Administrator role should exist.
- The Roles table displays a list of predefined roles available to users in the Run:ai
  platform. It is not possible to create additional rules or edit or delete existing
  rules.
- To preserve backwards compatibility, users with the role of Research Manager are
  assigned to all current projects, but not to new projects
- To preserve backwards compatibility, users with the role of Editor are assigned
  to the same scope they had before the upgrade
- Flexible management of access rules directly for a specific user, application, project,
  or department.
- AI initiatives require compute resources and a methodology to effectively and efficiently
  use those compute resources and split them among the different AI initiatives stakeholders.
- Run:ai allows grouping and assigning resources to initiatives.
- Departments are disabled, by default. If you cannot see Departments in the menu,
  then it must be enabled by your Administrator, under General settings → Resources
  → Departments.
- It is not possible to create additional nodes, or edit, or delete existing nodes.
- Only users with relevant permissions can view the table.
- By default, the Run:ai platform includes a single node pool named ‘default’.
- Setup SSO with SAML
- Setup SSO with OpenID Connect
- Setup SSO with OpenShift
- Workspaces support a variety of machine learning frameworks
- Inference workloads are considered production-critical
- The Workloads table can be found under Workload manager in the Run:ai platform.
- An environment consists of a configuration that simplifies how workloads are submitted
  and can be used by AI practitioners when they submit their workloads.
- Environment creation is limited to specific roles
- llm-server and chatbot-ui environments cannot be edited
- Data volume creation is limited to specific roles
- Data volume sharing is limited to specific roles
- The policy submission to the entire account scope is supported via API only
- Workload policies are disabled by default. If you cannot see Workload policies in
  the menu, then it must be enabled by your administrator, under General settings
  → Workloads → Policies
- A workload submission request cannot exclude the default/cpu resource, as this key
  is included in the locked rules under the instances section.
- Policies on this page cannot be added to platform 2.16 or higher that have the New
  Policy Manager enabled.
- Support for third-party integrations varies.
- Integration is supported out of the box with Run:ai.
- Run:ai provides integrated support with Jupyter Notebooks.
- Run:ai allows using a docker registry as a Credentials asset.
- Karpenter optimizes the cloud cost of a customer’s cluster by moving workloads between
  different node types.
- Karpenter’s main goal is cost optimization.
- Dashboard data updates once an hour.
- Reports is enabled by default for SaaS tenants. In order to enable the feature for
  tenants, additional configuration must be added.
- Reports must be saved in a storage solution compatible with S3.
- It is highly recommended to schedule an onboarding session for Researchers with
  a Run:ai customer success professional.
- Most quickstarts rely on an image called `runai.jfrog.io/demo/quickstart`. The image
  is based on TensorFlow Release 20-08.
- Workload of type Workspace will not end automatically.
- To complete this Quickstart, the Infrastructure Administrator will need to configure
  a wildcard certificate to Run:ai as described here.
- To complete this Quickstart, the Platform Administrator will need to provide you
  with Researcher access to Project in Run:ai named 'team-a'.
- To use the API, you will need to obtain a token.
- The image quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest
  is pre-configured for Visual Studio Code.
- The above API snippet will only work with Run:ai clusters of 2.18 and above.
- Your cluster should have 4 GPUs on 2 machines with 2 GPUs each.
- Researcher access to two Projects named 'team-a' and 'team-b'.
- Each project should be assigned an exact quota of 1 GPU.
- Two team-b Jobs have immediately displaced team-a.
- As the quotas are equal (1 for each Project, the remaining pending Jobs will get
  scheduled one by one alternating between Projects, regardless of the time in which
  they were submitted.
- Run:ai enhances visibility and simplifies management by monitoring, presenting and
  orchestrating all AI workloads in the clusters it is installed on.
- Run:ai workloads are submitted via the Run:ai platform.
- Third-party workloads are submitted via third-party applications that use the Run:ai
  Scheduler.
- Workloads may be pending if there are issues with resource allocation.
- Workload status can be checked for troubleshooting.
- The creation of assets is possible only via API and the Run:ai UI
- The submission of workloads using assets, is possible only via the Run:ai UI
- The Environment table provides a list of all the environment defined in the platform
  and allows you to manage them.
- Data volumes can be shared across projects, departments, or clusters.
- Working with data volumes is currently available using the API.
- The name must be unique.
- Limit value must be equal to or higher than the request.
- Credentials are crucial for the security of AI workloads and the resources they
  require, as they restrict access to authorized users, verify identities, and ensure
  secure interactions.
- You have created a project or have one created for you.
- The project has an assigned quota of at least 1 GPU.
- Default values may be displayed automatically based on policies set by the administrator.
- Multi-GPU training and distributed training are two distinct concepts.
- Workload policies may affect field defaults and available actions.
- An inference workload provides the setup and configuration needed to deploy your
  trained model for real-time or batch predictions.
- S3 data sources are not supported for inference workloads.
- Selecting the Inference type is disabled by default. If you cannot see it in the
  menu, then it must be enabled by your Administrator, under General settings → Workloads
  → Models.
- The CLI is only available for Run:ai cluster version 2.18 and up.
- A tool for managing Run:ai workloads and monitoring available resources. It provides
  researchers with comprehensive control over their AI development environment.
- Setting kubeconfig is not required in order to use the CLI. This command is used
  to enable third-party workloads under Run:ai authorization.
- Get a bash session inside a running Job
- This command is a shortcut to runai exec
- Set a default Project or Cluster
- Once you delete a Workload, its entire data will be gone.
- Output format can be json, yaml, or wide. Default is 'wide'.
- Set logging level to debug, info, warn, or error (default is 'info').
- To execute a bash command, you can also use the shorthand runai bash.
- When Researcher Authentication is enabled, you will need to login to Run:ai using
  your username and password before accessing resources.
- Log out from Run:ai
- Show the logs of a Job.
- Forward one or more local ports to the selected job or a pod within the job.
- Resume a suspended Job will return it to the queue for scheduling.
- Some objects like Contact may return nulls in deeply nested fields
- To use distributed training you need to have installed the Kubeflow MPI Operator.
- To use distributed training you need to have installed the Pytorch operator as specified
  in Distributed training.
- To use distributed training you need to have installed the TensorFlow operator as
  specified in Distributed training.
- Default clean pod policy is 'Running'.
- Job will be retried 6 times before failing.
- Suspending a Running Job will stop the Job and will not allow it to be scheduled
  until it is resumed using `runai resume`.
- Suspend and resume do not work with MPI and Inference.
- Show list of Nodes (machines), their capacity and utilization.
- The command must be run with sudo permissions.
- Show the version of the Run:ai command-line interface
- Training data is usually significantly large and is read-only in nature.
- The best practice is to store the training data on a shared file system.
- Inputs to machine learning models and artifacts of training sessions are better
  stored in a shared file system.
- Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas
  and into over-quota as long as computing resources are available.
- Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter
  optimization runs.
- Gather the commands you ran inside the interactive Job into a single script.
- Your training script must be flexible enough to support variance in execution without
  changing the code.
- It is important to save the checkpoints to network storage and not the machine itself.
- When your workload resumes, it can, in all probability, be allocated to a different
  node (machine) than the original node.
- exit with status "1" is important for the Job to return later.
- By default, you will have 30 seconds to save your checkpoints.
- Run:ai provides pre-defined environment variables for Job identification.
- Environment variables include JOB_NAME, JOB_UUID, RUNAI_NUM_OF_GPUS, and NODE_NAME.
- Users receive notifications about their jobs that transition from one status to
  another.
- Users get warning notifications before workload termination due to project-defined
  timeouts.
- Secrets are base64 encoded
- Secrets are stored in the scope of a namespace and will not be accessible from other
  namespaces.
- To connect the secret to the new Workload, use the CLI command runai workspace submit
  -e <ENV-VARIABLE>=SECRET:<secret-name>,<secret-key>
- Uses OAuth2 with OpenID Connect
- Dynamic GPU fractions is disabled by default in the Run:ai UI.
- To use dynamic GPU fractions, it must be enabled by your Administrator, under General
  Settings → Resources → GPU resource optimization.
- While the Node Level Scheduler applies to all workload types, it will best optimize
  the performance of burstable workloads.
- Burstable workloads are always susceptible to an OOM Kill signal if the owner of
  the excess memory requires it back.
- Using interactive workloads with notebooks is the best use case for burstable workloads
  and Node Level Scheduler.
- GPU resource optimization is disabled by default. It must be enabled by your Administrator,
  under General Settings → Resources → GPU resource optimization.
- Decreasing the lease time makes time-slicing less accurate.
- Increasing the lease time makes the system more accurate, but each workload is less
  responsive.
- Run:ai’s GPU memory swap helps administrators and AI practitioners to further increase
  the utilization of their existing GPU hardware.
- The workload MUST use dynamic GPU fractions.
- A pod created before the GPU memory swap feature was enabled in that cluster cannot
  be scheduled to a swap-enabled node.
- GPU memory swap cannot be enabled if the Run:ai strict or fair time-slicing is used.
- When submitting a Job, you can request a guaranteed amount of CPUs and memory using
  the --cpu and --memory flags.
- The amount of Memory your Job will receive is guaranteed to be the number defined
  using the --memory flag.
- If your Job tries to allocate more than the amount stated in the --memory-limit
  flag it will receive an out-of-memory exception.
- Requires setup of a project with a quota of at least 0.5 GPU.
- Dynamic GPU fractions is disabled by default in the Run:ai UI. To use dynamic GPU
  fractions, it must be enabled by your Administrator, under General Settings → Resources
  → GPU resource optimization.
- The above API snippet runs with Run:ai clusters of 2.18 and above only.
- You will need your image to run an SSH server (e.g OpenSSH).
- The image is configured to use the root user and password for SSH.
- Workload starts and sshd server on port 22
- Connection is redirected to the local machine (127.0.0.1) on port 2222
- 'Under PyCharm | Preferences go to: Project | Python Interpreter'
- Add a new SSH Interpreter.
- As Host, use the IP address above. Change the port to the above and use the Username
  `root`
- You will be prompted for a password. Enter `root`
- Apply settings and run the code via this interpreter. You will see your project
  uploaded to the container and running remotely.
- Job starts an sshd server on port 22.
- The connection is redirected to the local machine (127.0.0.1) on port 2222.
- As Host, use localhost. Change the port to the above (2222) and use the Username
  root.
- You will be prompted for a password. Enter root.
- Make sure to set the correct path of the Python binary. In our case it's /usr/local/bin/python.
- Apply your settings.
- 'Under PyCharm configuration set the following environment variables:'
- DISPLAY - set environment variable you copied before
- HOME - In our case it's /root. This is required for the X11 authentication to work.
- Uses OAuth2 with refresh token — requires setup of connected app in Run:ai
- The logs directory must be saved on a Network File Server such that it can be accessed
  by the TensorBoard Job.
- A single TensorBoard Job can be used to view multiple deep learning Jobs, provided
  it has access to the logs directory for these Jobs.
- All clusters in the tenant must be version 2.20 and onward.
- Run:ai APIs are accessed using bearer tokens.
- Replace <ACCESS-TOKEN> with the bearer token
- To enable advanced metrics at the Pod level, please contact Run:ai customer support.
- The Run:ai command line interface provides limited support for Kubernetes Workloads.
- Endpoints and fields specified in the API reference are officially supported.
- Run:ai is installed in its own Kubernetes namespace named runai.
- Workloads are run in the context of Run:ai Projects.
- Version 2.17 has been deprecated
- Improved *Credentials* creation. A Run:ai scope can now be added to credentials.
- Node pools are now enabled by default. There is no need to enable the feature in
  the settings.
- Improved the behavior of any workload time limit so that the time limit will affect
  existing workloads that were created before the time limit was configured.
- Improved workspaces time limits. Workspaces that reach a time limit will now transition
  to a state of `stopped`.
- Fixed an issue with allowing a fractional GPU value of 0 when submitting jobs via
  YAML.
- Fixed an issue with policies where the `canEdit` rule is not validated properly
  for itemized fields.
- Fixed an issue where `runai bash` does not wait for pods to be ready.
- Fixed an issue with Prometheus permissions in OpenShift environments.
- Fixed an issue that affected the dashboard where projects created with fractional
  GPUs display the number of GPUs rounded down to nearest whole number.
- Added environment variables for customizable QPS and burst support.
- Added the ability to support running multiple Prometheus replicas.
- Quota management is now enabled by default.
- If the data detailed below is not in line with your organization's policy, you can
  choose to install the Run:ai self-hosted version.
- 'Run:ai consists of two components: the Run:ai Cluster and the Run:ai Control plane.'
- Self-hosted installation is priced differently.
- Verify certified vendor and correct version of Kubernetes.
- Different Kubernetes flavors have slightly different setup instructions for NVIDIA
  GPU Operator.
- Install and configure NGINX Ingress Controller.
- Install Prometheus.
- Provide a trusted domain name accessible only inside the organization.
- Run:ai allows scheduling of Jobs with PVCs.
- Run:ai on GKE has only been tested with GPU Operator version 22.9 and up.
- To customize specific aspects of the cluster installation see customize cluster
  installation.
- Using these instructions to customize your cluster is optional.
- The cluster installation has changed in version 2.15 such that no values file is
  needed and old customizations do not have to be copied.
- The reason for this process is that Run:ai 2.9 cluster installation no longer installs
  pre-requisites. As such ownership of dependencies such as Prometheus will be undefined
  if a helm upgrade is run.
- The script must be run with ROOT privileges.
- Inbound ports 6443,443,8080 must be allowed.
- The script supports Kubernetes 1.24 or later.
- Use http rather than https.
- Use the IP and not a domain name.
- The machine running the installation script must have at least 50GB of free space.
- The Run:ai control plane requires a domain name (FQDN).
- If your network is air-gapped, you will need to provide the Run:ai control-plane
  and cluster with information about the local certificate authority.
- In air-gapped environments, you must prepare the public key of your local certificate
  authority.
- Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes
  to stop working.
- Replace <VERSION> with the Run:ai control plane version.
- Domain name described here.
- 'If you modify one of the usernames or passwords (KeyCloak, PostgreSQL, Grafana)
  after Run:ai is already installed, perform the following steps to apply the change:
  1. Modify the username/password within the relevant component as well (KeyCloak,
  PostgreSQL, Grafana). 2. Run `helm upgrade` for Run:ai with the right values, and
  restart the relevant Run:ai pods so they can fetch the new username/password.'
- Do not add the helm repository and do not run `helm repo update`.
- Use the `--dry-run` flag to gain an understanding of what is being installed before
  the actual installation.
- Run:ai data is stored in Kubernetes persistent volumes (PVs). Prior to Run:ai 2.12,
  PVs are owned by the Run:ai installation. Thus, uninstalling the `runai-backend`
  helm chart may delete all of your data.
- From version 2.12 forward, PVs are owned the customer and are independent of the
  Run:ai installation.
- OpenShift requires a trusted certificate.
- OpenShift must have a configured identity provider.
- You must create the namespace 'runai-backend' before installing.
- Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes
  to stop working (specifically if Kubernetes API Server is configured on 443 instead
  of the default 6443).
- Helm Charts require Kubernetes administrator permissions.
- To install a specific version, add --version <version> to the install command.
- You must have Cluster Administrator rights to install these dependencies.
- The exact configuration details must be worked together with Run:ai customer support.
- When you log in, you do so in the context of a specific cluster. When you switch
  to a different cluster, you will be prompted to log in to that cluster.
- 'Caution: Setting the ''createNamespaces'' flag to ''false'' moves the responsibility
  of creating namespaces to match Run:ai Projects to the administrator.'
- Run:ai data is stored in Kubernetes persistent volumes (PVs). Prior to Run:ai 2.12,
  PVs are owned by the Run:ai installation. Thus, uninstalling the runai-backend helm
  chart may delete all of your data.
- From version 2.12 forward, PVs are owned the customer and are independent of the
  Run:ai installation. As such, they are subject to storage class reclaim policy.
- Enable Run:ai access control is at the Project level.
- Integrate with the organization's Identity Provider to provide single sign-on for
  Run:ai
- Run:ai has configuration flags that control specific behavioral aspects of Run:ai.
- The purpose of this document is to provide security officers with the ability to
  review what cluster-wide access Run:ai requires.
- Version 2.9 introduces Workspaces which allow the Researcher to build AI models
  interactively.
- By default, if you run 'runai submit -i ubuntu --attach --interactive -- bash',
  then run 'id', you will see the root user.
- The --create-home-dir flag is set by default to true when the --run-as-user flag
  is used, and false if not.
- Most cloud clusters allow configuring node labels for the node pools in the cluster.
- You will need to have the public key of the local certificate authority.
- As part of the installation instructions you need to create a secret for runai-backend-tls.
  Use the local certificate authority instead.
- The SaaS version of Run:ai does not store any data.
- The Run:ai control plane is running on a single Kubernetes namespace named runai-backend.
- Keycloak sets a minimum of 3 pods and will scale to more on transaction load.
- It is not possible to configure an internal PostgreSQL to scale horizontally.
- If you have chosen to mark some of the nodes as Run:ai System Workers, the new replicas
  will attempt to use these nodes first.
- Recommended configuration for SchedulingServices and SyncServices for large clusters
  includes requests of 1 CPU and 1Gi memory, limits of 2 CPU and 2Gi memory.
- Run:ai session should timeout after 1 hour of inactivity.
- You cannot edit access rules. To change an access rule, you need to delete the rule,
  then create a new rule to replace it.
- Run:ai access control is at the Project level.
- This requires several steps to configure access control for Researchers.
- No configuration is needed. Instead, Run:ai assumes that an Identity Provider has
  been defined at the OpenShift level.
- Single sign-on is only available with SaaS installations where the tenant has been
  created post-January 2022 or any Self-hosted installation of release 2.0.58 or later.
- Run:ai SSO does not support single logout. As such, logging out from Run:ai will
  not log you out from other systems.
- Nodes (Machines) that are part of the cluster are susceptible to occasional downtime.
- Worker Nodes are where machine learning workloads run.
- In a production installation, Run:ai software runs on one or more Run:ai system
  nodes.
- As a best practice, it's best to have more than one such node so that during planned
  maintenance or unplanned downtime of a single node, the other node will take over.
- Add your smtp password as a secret
- Different receivers can be configured using Alertmanager receiver-integration-settings
- The Event History table saves logged operations for the last 90 days.
- Use the limit and offset properties to retrieve all audit log entries.
- Researcher authentication requires additional setup when installing the CLI.
- Create a PVC within the run:ai namespace to propagate it to all run:ai Projects.
- Run:ai makes it easy to run machine learning workloads effectively on Kubernetes.
- Workload statuses include Creating, Pending, Initializing, Running, Degraded, Deleting,
  Stopped, Failed, and Completed.
- Example Policy provides a workspace policy structure.
- Environment variables can be set with names and values.
- Policies can be applied using kubectl commands.
- The termination after preemption policy can be set cluster-wide or per project.
- Uses OAuth2 with refresh token — requires setup of connected app in Runai
- Some objects may return nulls in deeply nested fields
- Not all the configurable fields available are listed in the example below.
- Secrets are stored in the scope of a namespace and will not be accessible from other
  namespaces
- Inference is considered a high-priority workload as it is customer-facing.
- Running an Inference workload (within the Project's quota) will preempt any Run:ai
  Workload marked as Training.
- Roles provide a way for administrators to group and identify collections of permissions
  that administrators assign to subjects.
- Permissions define the actions that can be performed on managed entities.
- By default, all nodes in a cluster are part of the Default node pool.
- The scheduler will try to schedule the Job to the first node pool. If not successful
  the scheduler will try the second node pool in the list.
- When the node pools flag is enabled, over-quota is effective and calculated per
  node pool.
- 'Best practice: As a rule, the sum of the Projects'' allocations should be equal
  to the number of GPUs in the cluster.'
- If the Submit Job button is disabled or does not exist, then your cluster is not
  installed or configured to connect to the cluster
- To configure Credentials you need to make sure Workspaces are enabled.
- Existing secrets can't be used more than once.
- The previous steps can be used if you installed the cluster and the status is stuck
  in Waiting to connect for a long time.
- Upgrade fails with 'Ingress already exists'
- Pods are failing due to certificate issues
- No metrics are showing on dashboards at https://<company-name>.run.ai/dashboards/now
- Verify that there are no errors. If there are connectivity-related errors you may
  need to check your firewall for outbound connections.
- The cluster-sync pod in the runai namespace might not be functioning properly.
- No information or partial information from the cluster is being synced back to the
  Run:ai Control Plane.
- The runai-agent pod may be too loaded, is slow in processing data (possible in very
  big clusters), or the runai-agent pod itself in the runai namespace may not be functioning
  properly.
- No information or partial information from the Run:ai Control Plane is being synced
  to the cluster.
- If it seems that the runai-agent pod is functioning properly, but the cluster is
  very big and loaded, it is possible that the agent is taking time to process the
  data coming from the Run:ai Control Plane.
- A Run:ai container is using more than 90% of its Memory limit.
- The container might go out of memory (OOM) and crash.
- A Run:ai container is using more than 80% of its Memory limit.
- A Run:ai container has restarted more than twice in the last 10 minutes.
- The container may be unavailable and affect Run:ai system feature functionalities.
errors:
- When using Kubernetes 1.23, Data Source of PVC type does not work using a Storage
  Class with the property `volumeBindingMode` equals to `WaitForFirstConsumer`.
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- 'Invalid signature in response from identity provider: Mitigation - Check for a
  ''Certificate expired'' error and update the SAML metadata file to include a valid
  certificate.'
- '401 - We’re having trouble identifying your account because your email is incorrect
  or can’t be found: Mitigation - Validate the user’s email attribute is mapped correctly.'
- '403 - Sorry, we can’t let you see this page: Mitigation - Validate either the user
  or its related group/s are assigned with access rules and validate the user’s groups
  attribute is mapped correctly.'
- 403 - Sorry, we can’t let you see this page. Something about permissions…
- 401 - We’re having trouble identifying your account because your email is incorrect
  or can’t be found.
- Unexpected error when authenticating with identity provider
- Client not found
- unauthorized_client
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- Karpenter may try to scale up new nodes to make workloads schedulable, which may
  exceed their quota parameters.
- 'Cluster connectivity issues: Verify network access to the cluster.'
- 'Workload in ''Initializing'' status: Check access to Container image registry.'
- 'Workload has been pending: Check required quota.'
- Error if the container does not exist or has not been in a running state yet
- 'Invalid project name: Ensure the project exists'
- 'Invalid cluster name: Ensure the cluster exists'
- The Workload will be deleted and not available via the command runai list jobs.
- Job may or may not start immediately, depending on available resources.
- 'Invalid job name: Ensure the job name is correct.'
- 'Job not found: The specified job does not exist.'
- 'RUN-11005: Incorrect error messages when trying to run `runai` CLI commands in
  an OpenShift environment.'
- 'RUN-11009: Incorrect error message when a user without permissions tries to delete
  another user.'
- 'RUN-10333: Fixed an issue with allowing a fractional GPU value of 0 when submitting
  jobs via YAML.'
- 'RUN-9920: Fixed an issue with policies where the `canEdit` rule is not validated
  properly for itemized fields.'
- 'RUN-9912: Fixed an issue where `runai bash` does not wait for pods to be ready.'
- 'RUN-9902: Fixed an issue with Prometheus permissions in OpenShift environments.'
- 'RUN-9326: Fixed an issue that affected the dashboard where projects created with
  fractional GPUs, display the number of GPUs rounded down to nearest whole number.'
- '400 Bad Request: Ensure the policy YAML is correctly formatted.'
- '404 Not Found: Check if the specified policy exists.'
- 'Error: rendered manifests contain a resource that already exists. Unable to continue
  with install: IngressClass "nginx" in namespace "" exists'
- 'Error: rendered manifests contain a resource that already exists. Unable to continue
  with install:...'
- 'Critical: No information or partial information from the cluster is being synced
  back to the Run:ai Control Plane.'
- If the property does not exist, add it.
- If the above instructions did not correct the issue, contact Run:ai support.
- 'Critical: A Run:ai container is using more than 90% of its Memory limit.'
- 'Warning: The expected result should be one or more pods where the restart count
  >= 2.'
auth_info:
  mentioned_objects:
  - Client Application
  - Bearer Token
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Project-based Researcher Access Control
  - Kubernetes configuration file
  - Discovery URL
  - ClientID
  - Client Secret
  - client ID
  - client secret
  - Access Rules
  - API
  - Application
  - SAML 2.0
  - identity service
  - SAML protocol
  - client applications
  - Identity Provider
  - XML Metadata
client:
  base_url: https://docshub.run.ai
  auth:
    type: oauth2
source_metadata: null
