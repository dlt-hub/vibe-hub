resources:
- name: topics
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: clusters
  endpoint:
    path: /api/v1/clusters
    method: GET
    data_selector: clusters
    params: {}
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: topics
- name: topic_metadata
  endpoint:
    path: /topics/{topic_name}
    method: GET
    data_selector: name
- name: produce_messages
  endpoint:
    path: /topics/{topic_name}
    method: POST
    data_selector: offsets
- name: partitions
  endpoint:
    path: /topics/{topic_name}/partitions
    method: GET
    data_selector: partitions
    params: {}
- name: partition_metadata
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: partition
    params: {}
- name: partition_offsets
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}/offsets
    method: GET
    data_selector: offsets
    params: {}
- name: produce_to_partition
  endpoint:
    path: /topics/{topic_name}/partitions/{partition_id}
    method: POST
    data_selector: offsets
    params: {}
- name: consumers
  endpoint:
    path: /consumers/
    method: POST
    data_selector: ''
    params: {}
- name: consumer_positions
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions
    method: POST
    data_selector: offsets
    params: {}
- name: consumer_positions_beginning
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions/beginning
    method: POST
    data_selector: partitions
    params: {}
- name: consumer_positions_end
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/positions/end
    method: POST
    data_selector: partitions
    params: {}
- name: consumer_records
  endpoint:
    path: /consumers/{group_name}/instances/{instance}/records
    method: GET
    data_selector: records
    params: {}
- name: brokers
  endpoint:
    path: /brokers
    method: GET
    data_selector: brokers
- name: cells
  endpoint:
    path: /clusters/{cluster_id}/cells
    method: GET
    data_selector: cells
    params:
      cluster_id: string
- name: clusters
  endpoint:
    path: /clusters
    method: GET
    data_selector: data
    params: {}
- name: cluster
  endpoint:
    path: /clusters/{cluster_id}
    method: GET
    data_selector: metadata
    params: {}
- name: broker_configs
  endpoint:
    path: /clusters/{cluster_id}/broker-configs
    method: GET
    data_selector: data
    params: {}
- name: alter_broker_configs
  endpoint:
    path: /clusters/{cluster_id}/broker-configs:alter
    method: POST
    data_selector: data
    params: {}
- name: get_broker_config
  endpoint:
    path: /clusters/{cluster_id}/broker-configs/{name}
    method: GET
    data_selector: data
    params: {}
- name: update_broker_config
  endpoint:
    path: /clusters/{cluster_id}/broker-configs/{name}
    method: PUT
    data_selector: ''
    params: {}
- name: dynamic_broker_configs
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/configs
    method: GET
    data_selector: data
    params: {}
- name: broker_configs
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs
    method: GET
    data_selector: data
    params: {}
- name: update_broker_configs
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs:alter
    method: POST
    data_selector: data
    params: {}
- name: get_broker_config
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs/{name}
    method: GET
    data_selector: metadata
    params: {}
- name: update_specific_broker_config
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs/{name}
    method: PUT
    data_selector: value
    params: {}
- name: reset_broker_config
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/configs/{name}
    method: DELETE
- name: list_topic_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs
    method: GET
- name: batch_alter_topic_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs:alter
    method: POST
- name: Get Topic Config
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs/{name}
    method: GET
    data_selector: metadata
    params: {}
- name: Update Topic Config
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs/{name}
    method: PUT
    data_selector: null
    params: {}
- name: Reset Topic Config
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/configs/{name}
    method: DELETE
    data_selector: null
    params: {}
- name: list_all_topic_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/-/configs
    method: GET
    data_selector: data
    params: {}
- name: list_new_topic_default_configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
    data_selector: data
    params: {}
- name: brokers
  endpoint:
    path: /clusters/{cluster_id}/brokers
    method: GET
    data_selector: data
    params: {}
- name: broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
    data_selector: metadata
    params: {}
- name: remove_broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: DELETE
    data_selector: metadata
    params: {}
- name: partition_replicas
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/partition-replicas
    method: GET
    data_selector: data
    params: {}
- name: partition_replicas
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/partition-replicas
    method: GET
    data_selector: data
    params: {}
- name: delete_brokers
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: POST
    data_selector: data
    params: {}
- name: replicas
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replicas
    method: GET
    data_selector: data
    params: {}
- name: replica
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replicas/{broker_id}
    method: GET
    data_selector: metadata
    params: {}
- name: acls_batch
  endpoint:
    path: /clusters/{cluster_id}/acls:batch
    method: POST
    data_selector: data
    params: {}
- name: acls_list
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: GET
    data_selector: data
    params: {}
- name: acls_create
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: POST
    data_selector: ''
    params: {}
- name: acls_delete
  endpoint:
    path: /clusters/{cluster_id}/acls
    method: DELETE
    data_selector: ''
    params: {}
- name: consumer_group
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups
    method: GET
    data_selector: data
    params: {}
- name: get_consumer_group
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}
    method: GET
    data_selector: data
    params: {}
- name: list_consumers
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers
    method: GET
    data_selector: data
    params: {}
- name: consumer_group_lag_summary
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lag-summary
    method: GET
    data_selector: data
    params: {}
- name: consumer_lags
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lags
    method: GET
    data_selector: data
    params: {}
- name: consumer_lag
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/lags/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: data
    params: {}
- name: consumer
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}
    method: GET
    data_selector: data
    params: {}
- name: consumer_assignments
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}/assignments
    method: GET
    data_selector: data
    params: {}
- name: consumer_assignment
  endpoint:
    path: /clusters/{cluster_id}/consumer-groups/{consumer_group_id}/consumers/{consumer_id}/assignments/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: data
    params: {}
- name: partitions
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions
    method: GET
    data_selector: data
- name: partition
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}
    method: GET
    data_selector: metadata
- name: replica_reassignments
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/reassignment
    method: GET
    data_selector: data
- name: replica_reassignments_by_topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/reassignment
    method: GET
    data_selector: data
    params:
      cluster_id: string
      topic_name: string
- name: replica_reassignments_by_partition
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/reassignment
    method: GET
    data_selector: data
    params:
      cluster_id: string
      topic_name: string
      partition_id: integer
- name: list_topics
  endpoint:
    path: /clusters/{cluster_id}/topics
    method: GET
    data_selector: data
    params:
      cluster_id: ''
- name: create_topic
  endpoint:
    path: /clusters/{cluster_id}/topics
    method: POST
    data_selector: ''
    params:
      cluster_id: ''
- name: get_topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: GET
    data_selector: ''
    params:
      cluster_id: ''
      topic_name: ''
- name: update_partition_count
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: PATCH
    data_selector: topic
    params: {}
- name: delete_topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}
    method: DELETE
    data_selector: topic
    params: {}
- name: produce_records
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/records
    method: POST
    data_selector: records
    params: {}
- name: cluster_links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
    data_selector: data
    params: {}
- name: create_cluster_link
  endpoint:
    path: /clusters/{cluster_id}/links
    method: POST
    data_selector: data
    params: {}
- name: describe_cluster_link
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: GET
    data_selector: data
    params: {}
- name: delete_cluster_link
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: DELETE
- name: list_cluster_link_configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs
    method: GET
- name: describe_cluster_link_config
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: GET
- name: list_mirror_topics
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors
    method: GET
- name: list_all_mirror_topics
  endpoint:
    path: /clusters/{cluster_id}/links/-/mirrors
    method: GET
- name: describe_mirror_topic
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors/{mirror_topic_name}
    method: GET
- name: promote_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:promote
    method: POST
- name: failover_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:failover
    method: POST
- name: pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
- name: pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
    data_selector: null
    params: {}
- name: resume_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:resume
    method: POST
    data_selector: null
    params: {}
- name: reverse_and_start_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:reverse-and-start-mirror
    method: POST
    data_selector: null
    params: {}
- name: reverse_and_pause_mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:reverse-and-pause-mirror
    method: POST
    data_selector: mirror_topic_names
- name: truncate_and_restore
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:truncate-and-restore
    method: POST
    data_selector: mirror_topic_names
- name: share_groups
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: share_group
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      group_id: string
- name: share_group_consumers
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers
    method: GET
    data_selector: data
    params:
      cluster_id: string
      group_id: string
- name: share_group_consumer
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers/{consumer_id}
    method: GET
    data_selector: data
    params: {}
- name: share_group_consumer_assignments
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/share-groups/{group_id}/consumers/{consumer_id}/assignments
    method: GET
    data_selector: data
    params: {}
- name: balancer_status
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
    data_selector: status
    params:
      cluster_id: string
- name: any_uneven_load_status
  endpoint:
    path: /clusters/{cluster_id}/balancer/any-uneven-load
    method: GET
    data_selector: status
    params:
      cluster_id: string
- name: broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: broker_tasks_specific
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks
    method: GET
    data_selector: data
    params:
      cluster_id: string
      broker_id: integer
- name: broker_tasks_type
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks/{task_type}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      task_type: string
- name: broker_task
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks/{task_type}
    method: GET
    data_selector: data
    params: {}
- name: broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: broker_replica_exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions/{broker_id}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      broker_id: integer
- name: create_broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:create
    method: POST
    data_selector: data
    params:
      cluster_id: string
- name: delete_broker_replica_exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:delete
    method: POST
    data_selector: data
    params:
      cluster_id: string
- name: remove_broker_tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: GET
    data_selector: data
    params:
      cluster_id: string
- name: get_remove_broker_task
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks/{broker_id}
    method: GET
    data_selector: data
    params:
      cluster_id: string
      broker_id: integer
- name: unregister_broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: POST
    data_selector: data
    params:
      cluster_id: string
      broker_id: integer
- name: replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
- name: partition_replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
      topic_name: topic_name
- name: single_partition_replica_status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replica-status
    method: GET
    data_selector: data
    params:
      cluster_id: cluster_id
      topic_name: topic_name
      partition_id: partition_id
- name: clusters
  endpoint:
    path: /kafka/v3/clusters
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/topics
    method: GET
    data_selector: data
    params: {}
- name: create_topic
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/topics
    method: POST
    data_selector: data
    params: {}
- name: delete_topic
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/topics/{topic_name}
    method: DELETE
    data_selector: data
    params: {}
- name: brokers
  endpoint:
    path: /api/v2/brokers
    method: GET
- name: consumers
  endpoint:
    path: /api/v2/consumers
    method: GET
- name: topics
  endpoint:
    path: /api/v2/topics
    method: GET
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: clusters
  endpoint:
    path: /cmk/v2/clusters
    method: POST
    data_selector: spec
    params: {}
- name: topics
  endpoint:
    path: /kafka/v3/clusters/{cluster_id}/topics
    method: POST
    data_selector: topic_name
    params: {}
- name: connectors
  endpoint:
    path: /connect/v1/environments/{environment_id}/clusters/{cluster_id}/connectors
    method: POST
    data_selector: name
    params: {}
- name: DatagenSourceConnector_users
  endpoint:
    path: /connect/v1/environments/{environment_id}/clusters/{kafka_cluster_id}/connectors/{connector_name}
    method: DELETE
- name: users
  endpoint:
    path: /kafka/v3/clusters/{kafka_cluster_id}/topics/{topic_name}
    method: DELETE
- name: users_mask
  endpoint:
    path: /kafka/v3/clusters/{kafka_cluster_id}/topics/{topic_name}
    method: DELETE
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: pageviews
  endpoint:
    path: /topics/pageviews
    method: GET
    data_selector: messages
    params: {}
- name: users
  endpoint:
    path: /topics/users
    method: GET
    data_selector: messages
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: connectors
  endpoint:
    path: /connectors
    method: GET
    data_selector: connectors
    params: {}
- name: tasks
  endpoint:
    path: /tasks
    method: GET
    data_selector: tasks
    params: {}
- name: workers
  endpoint:
    path: /workers
    method: GET
    data_selector: workers
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Confluent Server Brokers
  endpoint:
    path: /security/authentication/oauth-oidc/configure-cs.html
    method: GET
- name: Confluent Schema Registry
  endpoint:
    path: /security/authentication/oauth-oidc/configure-sr.html
    method: GET
- name: Metadata Service
  endpoint:
    path: /security/authentication/oauth-oidc/configure-mds.html
    method: GET
- name: Kafka Connect
  endpoint:
    path: /security/authentication/oauth-oidc/configure-connect.html
    method: GET
- name: Confluent Control Center
  endpoint:
    path: /security/authentication/oauth-oidc/configure-c3.html
    method: GET
- name: REST Proxy
  endpoint:
    path: /security/authentication/oauth-oidc/configure-rest-proxy.html
    method: GET
- name: controller
  endpoint:
    path: /etc/kafka/my-controller.properties
    method: POST
    data_selector: controller_properties
    params: {}
- name: broker-0
  endpoint:
    path: /etc/kafka/broker-0.properties
    method: POST
    data_selector: broker_properties
    params: {}
- name: broker-1
  endpoint:
    path: /etc/kafka/broker-1.properties
    method: POST
    data_selector: broker_properties
    params: {}
- name: broker-2
  endpoint:
    path: /etc/kafka/broker-2.properties
    method: POST
    data_selector: broker_properties
    params: {}
- name: controller
  endpoint:
    path: /etc/kafka/my-controller.properties
    method: GET
    data_selector: properties
    params: {}
- name: broker-0
  endpoint:
    path: /etc/kafka/broker-0.properties
    method: GET
    data_selector: properties
    params: {}
- name: broker-1
  endpoint:
    path: /etc/kafka/broker-1.properties
    method: GET
    data_selector: properties
    params: {}
- name: broker-2
  endpoint:
    path: /etc/kafka/broker-2.properties
    method: GET
    data_selector: properties
    params: {}
- name: metrics
  endpoint:
    path: /api/v1/otlp
    method: POST
    data_selector: metrics
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: wikipedia.parsed
  endpoint:
    path: https://stream.wikimedia.org/v2/stream/recentchange
    method: GET
- name: wikipedia.parsed.count-by-domain
  endpoint: {}
- name: WIKIPEDIABOT
  endpoint: {}
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: recentchange
  endpoint:
    path: /v2/stream/recentchange
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka_brokers
  endpoint:
    path: /kafka/brokers
    method: GET
- name: kafka_topics
  endpoint:
    path: /kafka/topics
    method: GET
- name: users_topic
  endpoint:
    path: /topics/users
    method: POST
    data_selector: records
- name: my_avro_consumer
  endpoint:
    path: /consumers/my_avro_consumer
    method: POST
    data_selector: instance_id
- name: subscription
  endpoint:
    path: /consumers/my_avro_consumer/instances/my_consumer_instance/subscription
    method: POST
    data_selector: null
- name: records
  endpoint:
    path: /consumers/my_avro_consumer/instances/my_consumer_instance/records
    method: GET
    data_selector: null
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: users
  endpoint:
    path: /subjects/users-value/versions
    method: POST
    data_selector: schema
    params: {}
- name: users-value
  endpoint:
    path: /subjects/users-value/versions
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /kafka/v3/clusters/{KAFKA_CLUSTER_ID}/topics
    method: POST
    data_selector: data
    params:
      topic_name: dev_users
      partitions_count: 64
      replication_factor: 2
      configs:
      - name: cleanup.policy
        value: compact
      - name: compression.type
        value: gzip
- name: hosted_monitoring_query
  endpoint:
    path: /v2/metrics/hosted-monitoring/query
    method: POST
    data_selector: data
- name: cloud_query
  endpoint:
    path: /v2/metrics/cloud/query
    method: POST
    data_selector: data
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: topic
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: consumer_group
  endpoint:
    path: /v1/consumer-groups
    method: GET
    data_selector: consumerGroups
    params: {}
- name: Orchestrated Installation
  endpoint:
    path: /operator/current/overview.html
    method: GET
- name: Manual Installation
  endpoint:
    path: /installation_cp/zip-tar.html#prod-kafka-cli-install
    method: GET
- name: Development Environments
  endpoint:
    path: /current/installation/overview.html
    method: GET
- name: Confluent Signing Keys
  endpoint:
    path: /current/installation/overview.html
    method: GET
- name: GPG Key Revoke
  endpoint:
    path: /deb/8.1/archive.key
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: production_environments
  endpoint:
    path: installing_cp/zip-tar.html#prod-kafka-cli-install
    method: GET
- name: development_environments
  endpoint:
    path: ../get-started/platform-quickstart.html#quickstart
    method: GET
- name: Revoke in GnuPGP
  endpoint:
    path: /revoke/gnupgp
    method: POST
    data_selector: result
- name: Revoke in Debian Linux Based Distributions
  endpoint:
    path: /revoke/debian
    method: POST
    data_selector: result
- name: Revoke in Redhat Linux Based Distributions
  endpoint:
    path: /revoke/redhat
    method: POST
    data_selector: result
- name: Control Center-Normal mode
  endpoint:
    path: /control-center/normal-mode
    method: GET
    data_selector: records
    params: {}
- name: Control Center-Reduced infrastructure mode
  endpoint:
    path: /control-center/reduced-infrastructure
    method: GET
    data_selector: records
    params: {}
- name: Broker
  endpoint:
    path: /broker
    method: GET
    data_selector: records
    params: {}
- name: KRaft controller
  endpoint:
    path: /kraft-controller
    method: GET
    data_selector: records
    params: {}
- name: Connect
  endpoint:
    path: /connect
    method: GET
    data_selector: records
    params: {}
- name: Replicator
  endpoint:
    path: /replicator
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksqldb
    method: GET
    data_selector: records
    params: {}
- name: REST Proxy
  endpoint:
    path: /rest-proxy
    method: GET
    data_selector: records
    params: {}
- name: Schema Registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: records
    params: {}
- name: Confluent Platform and Apache Kafka compatibility
  endpoint:
    path: /platform/versions-interoperability
    method: GET
- name: Confluent Platform patch versions
  endpoint:
    path: /platform/patch-versions
    method: GET
- name: Operating systems
  endpoint:
    path: /platform/operating-systems
    method: GET
- name: Java support
  endpoint:
    path: /platform/java-support
    method: GET
- name: Scala support
  endpoint:
    path: /platform/scala-support
    method: GET
- name: Customer-Managed Confluent Platform for Confluent Cloud compatibility
  endpoint:
    path: /platform/customer-managed-cloud-compatibility
    method: GET
- name: Confluent Control Center
  endpoint:
    path: /control-center/current/overview.html
    method: GET
    data_selector: versions
    params: {}
- name: Docker Images
  endpoint:
    path: /docker/image-reference.html
    method: GET
    data_selector: releases
    params: {}
- name: streams
  endpoint:
    path: /streams
    method: GET
    data_selector: data
    params: {}
- name: broker
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
- name: controller
  endpoint:
    path: /etc/kafka/controller.properties
    method: GET
- name: server
  endpoint:
    path: /etc/kafka/server.properties
    method: GET
- name: Cluster Status
  endpoint:
    path: /ksqldb/cluster/status
    method: GET
- name: Info
  endpoint:
    path: /ksqldb/info
    method: GET
- name: kafka
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
    data_selector: properties
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka_broker
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
- name: kafka_controller
  endpoint:
    path: /etc/kafka/controller.properties
    method: GET
- name: kafka_server
  endpoint:
    path: /etc/kafka/server.properties
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: broker
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
- name: controller
  endpoint:
    path: /etc/kafka/controller.properties
    method: GET
- name: server
  endpoint:
    path: /etc/kafka/server.properties
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: confluent-platform
  endpoint:
    path: /deb/8.1
    method: GET
    data_selector: packages
    params: {}
- name: confluent-security
  endpoint:
    path: /deb/8.1/security
    method: GET
    data_selector: packages
    params: {}
- name: confluent-community
  endpoint:
    path: /deb/community-2.13
    method: GET
    data_selector: packages
    params: {}
- name: kafka_broker
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
    data_selector: properties
    params: {}
- name: kafka_controller
  endpoint:
    path: /etc/kafka/controller.properties
    method: GET
    data_selector: properties
    params: {}
- name: kafka_server
  endpoint:
    path: /etc/kafka/server.properties
    method: GET
    data_selector: properties
    params: {}
- name: Confluent Repository
  endpoint:
    path: /rpm/8.1
    method: GET
    data_selector: records
    params: {}
- name: Confluent Clients Repository
  endpoint:
    path: /clients/rpm/centos/$releasever/$basearch
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /streams/data
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params:
      incremental: updated_at
- name: cluster_status
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params:
      incremental: updated_at
- name: kafka
  endpoint:
    path: /cp-kafka
    method: GET
    data_selector: records
    params: {}
- name: schema_registry
  endpoint:
    path: /cp-schema-registry
    method: GET
    data_selector: records
    params: {}
- name: kafka_connect
  endpoint:
    path: /cp-kafka-connect
    method: GET
    data_selector: records
    params: {}
- name: ksqldb_server
  endpoint:
    path: /cp-ksqldb-server
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB_server
  endpoint:
    path: /ksqldb
    method: POST
    data_selector: queries
    params:
      bootstrap_servers: localhost:9092
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cp-kafka
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: cp-server
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: cp-schema-registry
  endpoint:
    path: /services/data/vXX.X/sobjects/SchemaRegistry
    method: GET
    data_selector: records
- name: cp-kafka-connect
  endpoint:
    path: /services/data/vXX.X/sobjects/KafkaConnect
    method: GET
    data_selector: records
- name: cp-ksqldb-server
  endpoint:
    path: /services/data/vXX.X/sobjects/KsqlDB
    method: GET
    data_selector: records
- name: ksql
  endpoint:
    path: /ksql
    method: POST
    data_selector: queries
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka
  endpoint:
    path: /cp-kafka
    method: GET
- name: schema_registry
  endpoint:
    path: /cp-schema-registry
    method: GET
- name: kafka_connect
  endpoint:
    path: /cp-kafka-connect
    method: GET
- name: ksqldb_server
  endpoint:
    path: /cp-ksqldb-server
    method: GET
- name: ksql
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params:
      KSQL_BOOTSTRAP_SERVERS: localhost:9092
- name: replicator
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: cp-base-java
  endpoint:
    path: https://github.com/confluentinc/common-docker/tree/master/base-java
    method: GET
    data_selector: packages
    params: {}
- name: cp-base-lite
  endpoint:
    path: https://github.com/confluentinc/common-docker/tree/master/base-lite
    method: GET
    data_selector: packages
    params: {}
- name: cp-kafka
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka
    method: GET
    data_selector: packages
    params: {}
- name: cp-server
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-server
    method: GET
    data_selector: packages
    params: {}
- name: cp-schema-registry
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-schema-registry
    method: GET
    data_selector: packages
    params: {}
- name: cp-kafka-connect
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka-connect
    method: GET
    data_selector: packages
    params: {}
- name: cp-ksqldb-server
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-ksqldb-server
    method: GET
    data_selector: packages
    params: {}
- name: cp-kafka-rest
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kafka-rest
    method: GET
    data_selector: packages
    params: {}
- name: confluent-cli
  endpoint:
    path: https://hub.docker.com/r/confluentinc/confluent-cli
    method: GET
    data_selector: packages
    params: {}
- name: cp-enterprise-control-center-next-gen
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-enterprise-control-center-next-gen
    method: GET
    data_selector: packages
    params: {}
- name: cpc-gateway
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cpc-gateway
    method: GET
    data_selector: packages
    params: {}
- name: cp-kcat
  endpoint:
    path: https://hub.docker.com/r/confluentinc/cp-kcat
    method: GET
    data_selector: packages
    params: {}
- name: mqtt_proxy
  endpoint:
    path: /cp-kafka-mqtt
    method: GET
- name: replicator
  endpoint:
    path: /cp-enterprise-replicator
    method: GET
- name: replicator_executable
  endpoint:
    path: /cp-enterprise-replicator-executable
    method: GET
- name: confluent_operator
  endpoint:
    path: /confluent-operator
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Kafka Connect
  endpoint:
    path: /services/data/v8.1.0/sobjects/KafkaConnect
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: docker_image
  endpoint:
    path: /docker/image
    method: GET
    data_selector: records
- name: Docker Developer Guide
  endpoint:
    path: /docker/developer-guide
    method: GET
    data_selector: docs
- name: Confluent Hub Connectors
  endpoint:
    path: /confluent/hub/connectors
    method: GET
    data_selector: connectors
- name: query
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: kafka
  endpoint:
    path: /etc/kafka/broker.properties
    method: GET
- name: kafka-connect
  endpoint:
    path: /etc/kafka-connect/
    method: GET
- name: kafka-rest
  endpoint:
    path: /etc/kafka-rest/
    method: GET
- name: ksql
  endpoint:
    path: /etc/ksql/
    method: GET
- name: schema-registry
  endpoint:
    path: /etc/schema-registry/
    method: GET
- name: license_clients
  endpoint:
    path: /configure/license/clients
    method: POST
    data_selector: clients
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql/cluster/status
    method: GET
    data_selector: status
- name: info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
- name: repository
  endpoint:
    path: /rpm/8.1
    method: GET
- name: clients_repository
  endpoint:
    path: /clients/rpm/centos/$releasever/$basearch
    method: GET
- name: avro
  endpoint:
    path: /avro
    method: GET
    data_selector: records
    params: {}
- name: confluent-control-center
  endpoint:
    path: /confluent-control-center
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka
  endpoint:
    path: /confluent-kafka
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-connect-replicator
  endpoint:
    path: /confluent-kafka-connect-replicator
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-dotnet
  endpoint:
    path: /confluent-kafka-dotnet
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-javascript
  endpoint:
    path: /confluent-kafka-javascript
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-go
  endpoint:
    path: /confluent-kafka-go
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-python
  endpoint:
    path: /confluent-kafka-python
    method: GET
    data_selector: records
    params: {}
- name: confluent-kafka-rest
  endpoint:
    path: /confluent-kafka-rest
    method: GET
    data_selector: records
    params: {}
- name: confluent-ksqldb
  endpoint:
    path: /confluent-ksqldb
    method: GET
    data_selector: records
    params: {}
- name: confluent-rebalancer
  endpoint:
    path: /confluent-rebalancer
    method: GET
    data_selector: records
    params: {}
- name: confluent-schema-registry
  endpoint:
    path: /confluent-schema-registry
    method: GET
    data_selector: records
    params: {}
- name: confluent-security
  endpoint:
    path: /confluent-security
    method: GET
    data_selector: records
    params: {}
- name: confluent-server
  endpoint:
    path: /confluent-server
    method: GET
    data_selector: records
    params: {}
- name: librdkafka
  endpoint:
    path: /librdkafka
    method: GET
    data_selector: packages
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: migrate_confluent_kafka_to_confluent_server
  endpoint:
    path: /migrate/confluent-kafka/to/confluent-server
    method: GET
    data_selector: records
    params: {}
- name: migrate_confluent_server_to_confluent_kafka
  endpoint:
    path: /migrate/confluent-server/to/confluent-kafka
    method: GET
    data_selector: records
    params: {}
- name: kafka_metadata_recovery
  endpoint:
    path: /kafka-metadata-recovery
    method: POST
    data_selector: result
    params: {}
- name: migration_tool
  endpoint:
    path: /kafka-migration-check
    method: GET
    data_selector: status
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: Admin API
  endpoint:
    path: /platform/8.1/clients/api-docs/admin-api.html
    method: GET
- name: Control Center metrics integration
  endpoint:
    path: /platform/8.1/clients/api-docs/control-center-metrics.html
    method: GET
- name: Custom partitioner
  endpoint:
    path: /platform/8.1/clients/api-docs/custom-partitioner.html
    method: GET
- name: Exactly Once Semantics
  endpoint:
    path: /platform/8.1/clients/api-docs/exactly-once-semantics.html
    method: GET
- name: Idempotent Producer
  endpoint:
    path: /platform/8.1/clients/api-docs/idempotent-producer.html
    method: GET
- name: Kafka Streams
  endpoint:
    path: /platform/8.1/clients/api-docs/kafka-streams.html
    method: GET
- name: Record Headers
  endpoint:
    path: /platform/8.1/clients/api-docs/record-headers.html
    method: GET
- name: SASL Kerberos/GSSAPI
  endpoint:
    path: /platform/8.1/clients/api-docs/sasl-kerberos.html
    method: GET
- name: SASL PLAIN
  endpoint:
    path: /platform/8.1/clients/api-docs/sasl-plain.html
    method: GET
- name: SASL SCRAM
  endpoint:
    path: /platform/8.1/clients/api-docs/sasl-scram.html
    method: GET
- name: SASL OAUTHBEARER
  endpoint:
    path: /platform/8.1/clients/api-docs/sasl-oauthbearer.html
    method: GET
- name: Simplified installation
  endpoint:
    path: /platform/8.1/clients/api-docs/simplified-installation.html
    method: GET
- name: Schema Registry
  endpoint:
    path: /platform/8.1/clients/api-docs/schema-registry.html
    method: GET
- name: Topic Metadata API
  endpoint:
    path: /platform/8.1/clients/api-docs/topic-metadata-api.html
    method: GET
- name: consumer_group
  endpoint:
    path: /consumer_groups
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /topics/{topicName}
    method: POST
    data_selector: records
- name: topics_partitions
  endpoint:
    path: /topics/{topicName}/partitions/{partition}
    method: POST
    data_selector: records
- name: cluster_status
  endpoint:
    path: /api/v1/clusters/status
    method: GET
    data_selector: records
    params: {}
- name: info
  endpoint:
    path: /api/v1/info
    method: GET
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /produce
    method: POST
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /produce
    method: POST
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /consume
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Producer
  endpoint:
    path: /v1/producer
    method: POST
    data_selector: records
- name: Consumer
  endpoint:
    path: /v1/consumer
    method: POST
    data_selector: records
- name: consumer
  endpoint:
    path: /v1/consumer
    method: POST
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /v1/producer
    method: POST
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /producer
    method: POST
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /consumer
    method: GET
    data_selector: messages
    params: {}
- name: producer
  endpoint:
    path: /kafka/producer
    method: POST
    data_selector: records
- name: consumer
  endpoint:
    path: /kafka/consumer
    method: POST
    data_selector: records
- name: producer
  endpoint:
    path: /api/v1/producer
    method: POST
    data_selector: records
    params: {}
- name: consumer
  endpoint:
    path: /api/v1/consumer
    method: GET
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /producer
    method: POST
    data_selector: records
- name: consumer
  endpoint:
    path: /consumer
    method: GET
    data_selector: records
- name: consumer
  endpoint:
    path: /kafka/consumer
    method: GET
    data_selector: records
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
    params: {}
- name: topics
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: clusters
  endpoint:
    path: /api/v1/clusters
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: producer
  endpoint:
    path: /producer
    method: POST
- name: consumer
  endpoint:
    path: /consumer
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: producer
  endpoint:
    path: /clients/cloud/groovy/ProducerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: consumer
  endpoint:
    path: /clients/cloud/groovy/ConsumerExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: streams
  endpoint:
    path: /clients/cloud/groovy/StreamsExample
    method: POST
    data_selector: records
    params:
      configPath: $HOME/.confluent/java.config
      topic: test1
- name: cluster_status
  endpoint:
    path: /ksql/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
    params: {}
- name: datagen
  endpoint:
    path: /kafka-connect-datagen
    method: POST
    data_selector: records
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: test1
  endpoint:
    path: /kafka-connect-datagen
    method: POST
    data_selector: records
- name: test2
  endpoint:
    path: /kafka-connect-datagen
    method: POST
    data_selector: records
- name: subjects
  endpoint:
    path: /subjects
    method: GET
- name: schema_information
  endpoint:
    path: /subjects/test2-value/versions/1
    method: GET
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: kafka_topic
  endpoint:
    path: /kafka/topics
    method: POST
    data_selector: topics
    params:
      incremental: created_at
- name: test
  endpoint:
    path: /topics/test
    method: GET
    data_selector: messages
- name: producer_example
  endpoint:
    path: /clients/cloud/kotlin/ProducerExample
    method: POST
    data_selector: records
- name: consumer_example
  endpoint:
    path: /clients/cloud/kotlin/ConsumerExample
    method: POST
    data_selector: records
- name: streams_example
  endpoint:
    path: /clients/cloud/kotlin/StreamsExample
    method: POST
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: producer
  endpoint:
    path: /producer
    method: POST
    data_selector: records
    params: {}
- name: test1
  endpoint:
    path: /consumer/test1
    method: GET
- name: cluster_status
  endpoint:
    path: /status
    method: GET
    data_selector: status
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
- name: producer
  endpoint:
    path: /clients/cloud/rust/producer
    method: POST
    data_selector: records
    params:
      config: $HOME/.confluent/librdkafka.config
      topic: test1
- name: consumer
  endpoint:
    path: /clients/cloud/rust/consumer
    method: POST
    data_selector: records
    params:
      config: $HOME/.confluent/librdkafka.config
      topic: test1
- name: cluster_status
  endpoint:
    path: /api/v1/clusters/status
    method: GET
    data_selector: status
- name: info
  endpoint:
    path: /api/v1/info
    method: GET
    data_selector: info
- name: producer
  endpoint:
    path: /clients/cloud/scala/Producer
    method: POST
- name: consumer
  endpoint:
    path: /clients/cloud/scala/Consumer
    method: POST
- name: streams
  endpoint:
    path: /clients/cloud/scala/Streams
    method: POST
- name: cluster_status
  endpoint:
    path: /ksqlDB/rest/v1/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqlDB/rest/v1/info
    method: GET
    data_selector: info
    params: {}
- name: create_topics
  endpoint:
    path: create_topics
    method: POST
    data_selector: futures
- name: delete_topics
  endpoint:
    path: delete_topics
    method: POST
    data_selector: futures
- name: list_topics
  endpoint:
    path: list_topics
    method: GET
    data_selector: ClusterMetadata
- name: list_groups
  endpoint:
    path: list_groups
    method: GET
    data_selector: GroupMetadata
- name: create_partitions
  endpoint:
    path: create_partitions
    method: POST
    data_selector: futures
- name: describe_configs
  endpoint:
    path: describe_configs
    method: POST
    data_selector: futures
- name: alter_configs
  endpoint:
    path: alter_configs
    method: POST
    data_selector: futures
- name: incremental_alter_configs
  endpoint:
    path: incremental_alter_configs
    method: POST
    data_selector: futures
- name: create_acls
  endpoint:
    path: create_acls
    method: POST
    data_selector: futures
- name: describe_acls
  endpoint:
    path: describe_acls
    method: POST
    data_selector: futures
- name: delete_acls
  endpoint:
    path: delete_acls
    method: POST
    data_selector: futures
- name: list_consumer_groups
  endpoint:
    path: list_consumer_groups
    method: GET
    data_selector: ListConsumerGroupsResult
- name: describe_consumer_groups
  endpoint:
    path: describe_consumer_groups
    method: POST
    data_selector: futures
- name: describe_topics
  endpoint:
    path: describe_topics
    method: POST
    data_selector: futures
- name: describe_cluster
  endpoint:
    path: /describe_cluster
    method: GET
    data_selector: result
    params:
      include_authorized_operations: false
      request_timeout: socket.timeout.ms/1000.0
- name: delete_consumer_groups
  endpoint:
    path: /delete_consumer_groups
    method: DELETE
    data_selector: result
    params:
      group_ids: []
      request_timeout: socket.timeout.ms/1000.0
- name: list_consumer_group_offsets
  endpoint:
    path: /list_consumer_group_offsets
    method: GET
    data_selector: result
    params:
      list_consumer_group_offsets_request: []
      require_stable: false
      request_timeout: socket.timeout.ms/1000.0
- name: alter_consumer_group_offsets
  endpoint:
    path: /alter_consumer_group_offsets
    method: POST
    data_selector: result
    params:
      alter_consumer_group_offsets_request: []
      request_timeout: socket.timeout.ms/1000.0
- name: set_sasl_credentials
  endpoint:
    path: /set_sasl_credentials
    method: POST
    data_selector: result
    params:
      username: ''
      password: ''
- name: describe_user_scram_credentials
  endpoint:
    path: /describe_user_scram_credentials
    method: GET
    data_selector: result
    params:
      users: null
      request_timeout: socket.timeout.ms/1000.0
- name: alter_user_scram_credentials
  endpoint:
    path: /alter_user_scram_credentials
    method: POST
    data_selector: result
    params:
      alterations: []
      request_timeout: socket.timeout.ms/1000.0
- name: list_offsets
  endpoint:
    path: /list_offsets
    method: GET
    data_selector: result
    params:
      topic_partition_offsets: {}
      isolation_level: null
      request_timeout: socket.timeout.ms/1000.0
- name: delete_records
  endpoint:
    path: /delete_records
    method: DELETE
    data_selector: result
    params:
      topic_partition_offsets: []
      request_timeout: socket.timeout.ms/1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: elect_leaders
  endpoint:
    path: /elect_leaders
    method: POST
    data_selector: result
    params:
      election_type: ''
      partitions: null
      request_timeout: socket.timeout.ms*1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: NewTopic
  endpoint:
    path: /admin/topics
    method: POST
    data_selector: topics
    params: {}
- name: NewPartitions
  endpoint:
    path: /admin/partitions
    method: POST
    data_selector: partitions
    params: {}
- name: ConfigSource
  endpoint:
    path: /admin/config/source
    method: GET
    data_selector: config_source
    params: {}
- name: ConfigEntry
  endpoint:
    path: /admin/config/entry
    method: GET
    data_selector: config_entry
    params: {}
- name: ConfigResource
  endpoint:
    path: /admin/config/resource
    method: GET
    data_selector: config_resource
    params: {}
- name: ResourceType
  endpoint:
    path: /admin/resource/type
    method: GET
    data_selector: resource_type
    params: {}
- name: ResourcePatternType
  endpoint:
    path: /admin/resource/pattern/type
    method: GET
    data_selector: resource_pattern_type
    params: {}
- name: AlterConfigOpType
  endpoint:
    path: /admin/alter/config/operation
    method: GET
    data_selector: alter_config_operation
    params: {}
- name: AclOperation
  endpoint:
    path: /admin/acl/operation
    method: GET
    data_selector: acl_operation
    params: {}
- name: AclPermissionType
  endpoint:
    path: /admin/acl/permission/type
    method: GET
    data_selector: acl_permission_type
    params: {}
- name: AclBinding
  endpoint:
    path: /admin/acl/binding
    method: GET
    data_selector: acl_binding
    params: {}
- name: DeletedRecords
  endpoint:
    path: /deleted_records
    method: GET
    data_selector: low_watermark
    params: {}
- name: MemberAssignment
  endpoint:
    path: /member_assignment
    method: GET
    data_selector: topic_partitions
    params: {}
- name: MemberDescription
  endpoint:
    path: /member_description
    method: GET
    data_selector: member_info
    params: {}
- name: consumer
  endpoint:
    path: /consumer
    method: GET
    data_selector: consumer_data
- name: SerializingProducer
  endpoint:
    path: /confluent_kafka/SerializingProducer
    method: GET
    data_selector: configuration_properties
    params: {}
- name: Deserializer
  endpoint:
    path: /confluent_kafka/serialization/Deserializer
    method: GET
    data_selector: implementations
    params: {}
- name: DoubleDeserializer
  endpoint:
    path: /confluent_kafka/serialization/DoubleDeserializer
    method: GET
    data_selector: description
    params: {}
- name: IntegerDeserializer
  endpoint:
    path: /confluent_kafka/serialization/IntegerDeserializer
    method: GET
    data_selector: description
    params: {}
- name: StringDeserializer
  endpoint:
    path: /confluent_kafka/serialization/StringDeserializer
    method: GET
    data_selector: description
    params: {}
- name: StringSerializer
  endpoint:
    path: /confluent_kafka/serialization/StringSerializer
    method: GET
    data_selector: description
    params: {}
- name: Message
  endpoint:
    path: /confluent_kafka/Message
    method: GET
    data_selector: records
- name: TopicPartition
  endpoint:
    path: /confluent_kafka/TopicPartition
    method: GET
    data_selector: records
- name: ConsumerGroupState
  endpoint:
    path: /confluent_kafka/ConsumerGroupState
    method: GET
    data_selector: records
- name: AdminClient
  endpoint:
    path: /admin/client
    method: GET
    data_selector: topics
    params: {}
- name: ListConsumerGroupsResult
  endpoint:
    path: /admin/list_consumer_groups
    method: GET
    data_selector: valid
- name: ConsumerGroupDescription
  endpoint:
    path: /admin/describe_consumer_groups
    method: GET
    data_selector: members
- name: DeletedRecords
  endpoint:
    path: /admin/deleted_records
    method: GET
    data_selector: low_watermark
- name: MemberAssignment
  endpoint:
    path: /admin/member_assignment
    method: GET
    data_selector: topic_partitions
- name: MemberDescription
  endpoint:
    path: /admin/member_description
    method: GET
    data_selector: member_id
- name: Consumer
  endpoint:
    path: /admin/consumer
    method: GET
    data_selector: config
- name: AdminClient
  endpoint:
    path: /admin/client
    method: GET
    data_selector: admin_operations
    params: {}
- name: Producer
  endpoint:
    path: /producer
    method: POST
    data_selector: messages
    params: {}
- name: Consumer
  endpoint:
    path: /consumer
    method: POST
    data_selector: messages
    params: {}
- name: delete_records
  endpoint:
    path: /delete_records
    method: POST
    data_selector: futures
    params:
      request_timeout: socket.timeout.ms/1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: elect_leaders
  endpoint:
    path: /elect_leaders
    method: POST
    data_selector: futures
    params:
      request_timeout: socket.timeout.ms/1000.0
      operation_timeout: socket.timeout.ms/1000.0
- name: AclPermissionType
  endpoint:
    path: /aclpermissiontype
    method: GET
- name: AclBinding
  endpoint:
    path: /aclbinding
    method: GET
- name: AclBindingFilter
  endpoint:
    path: /aclbindingfilter
    method: GET
- name: ScramMechanism
  endpoint:
    path: /scrammechanism
    method: GET
- name: ScramCredentialInfo
  endpoint:
    path: /scramcredentialinfo
    method: GET
- name: UserScramCredentialsDescription
  endpoint:
    path: /userscramcredentialsdescription
    method: GET
- name: UserScramCredentialAlteration
  endpoint:
    path: /userscramcredentialalteration
    method: POST
- name: UserScramCredentialUpsertion
  endpoint:
    path: /userscramcredentialupsertion
    method: POST
- name: UserScramCredentialDeletion
  endpoint:
    path: /userscramcredentialdeletion
    method: DELETE
- name: OffsetSpec
  endpoint:
    path: /offsetspec
    method: GET
- name: ListOffsetsResultInfo
  endpoint:
    path: /listoffsetsresultinfo
    method: GET
- name: TopicDescription
  endpoint:
    path: /topicdescription
    method: GET
- name: DescribeClusterResult
  endpoint:
    path: /describeclusterresult
    method: GET
- name: BrokerMetadata
  endpoint:
    path: /brokermetadata
    method: GET
- name: ClusterMetadata
  endpoint:
    path: /clustermetadata
    method: GET
- name: GroupMember
  endpoint:
    path: /groupmember
    method: GET
- name: GroupMetadata
  endpoint:
    path: /groupmetadata
    method: GET
- name: PartitionMetadata
  endpoint:
    path: /partitionmetadata
    method: GET
- name: TopicMetadata
  endpoint:
    path: /topicmetadata
    method: GET
- name: ConsumerGroupListing
  endpoint:
    path: /consumergrouplisting
    method: GET
- name: ListConsumerGroupsResult
  endpoint:
    path: /admin/consumer_groups
    method: GET
    data_selector: valid
- name: ConsumerGroupDescription
  endpoint:
    path: /admin/describe_consumer_groups
    method: GET
    data_selector: members
- name: DeletedRecords
  endpoint:
    path: /admin/deleted_records
    method: GET
    data_selector: low_watermark
- name: MemberAssignment
  endpoint:
    path: /admin/member_assignment
    method: GET
    data_selector: topic_partitions
- name: MemberDescription
  endpoint:
    path: /admin/member_description
    method: GET
    data_selector: assignment
- name: classic_protocol
  endpoint:
    path: /protocols/classic
    method: GET
    data_selector: protocol_data
    params:
      group.protocol: classic
      partition.assignment.strategy: <range,roundrobin,sticky
      session.timeout.ms: 45000
      heartbeat.interval.ms: 15000
- name: next_gen_protocol
  endpoint:
    path: /protocols/next-gen
    method: GET
    data_selector: protocol_data
    params:
      group.protocol: consumer
      group.remote.assignor: <uniform,range
- name: consumer_group_description
  endpoint:
    path: /describe/consumer_groups
    method: GET
    data_selector: ConsumerGroupDescription
    params: {}
- name: consumer_group_listing
  endpoint:
    path: /list/consumer_groups
    method: GET
    data_selector: ConsumerGroupListing
    params: {}
- name: ElectLeadersRequest
  endpoint:
    path: /ElectLeadersRequest
    method: POST
    data_selector: results
- name: ElectLeadersResult
  endpoint:
    path: /ElectLeadersResult
    method: GET
    data_selector: TopicPartitions
- name: client_deprecation
  endpoint:
    path: /platform/current/clients/deprecate-how-to.html
    method: GET
    data_selector: clients
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
    params: {}
- name: streams
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: tables
  endpoint:
    path: /tables
    method: GET
    data_selector: results
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
    params: {}
- name: run_sql
  endpoint:
    path: /ksql/run
    method: POST
    data_selector: results
    params: {}
- name: query_stream
  endpoint:
    path: /ksql/query
    method: GET
    data_selector: query
    params: {}
- name: temperature
  endpoint:
    path: /temperature
    method: POST
    data_selector: records
    params: {}
- name: brightness
  endpoint:
    path: /brightness
    method: POST
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
- name: confluent.license
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: confluent.topic.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: confluent.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: confluent.topic.replication.factor
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: input_topic
  endpoint:
    path: /topics/input_topic
    method: POST
    data_selector: messages
- name: output_topic
  endpoint:
    path: /topics/output_topic
    method: GET
    data_selector: messages
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: records
- name: stream
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
- name: topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: records
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: records
    params: {}
- name: OrderDetailsService
  endpoint:
    path: /microservices/orders/OrderDetailsService
    method: GET
    data_selector: records
- name: EmailService
  endpoint:
    path: /microservices/orders/EmailService
    method: GET
    data_selector: records
- name: FraudService
  endpoint:
    path: /microservices/orders/FraudService
    method: GET
    data_selector: records
- name: ValidationsAggregatorService
  endpoint:
    path: /microservices/orders/ValidationsAggregatorService
    method: GET
    data_selector: records
- name: InventoryService
  endpoint:
    path: /microservices/orders/InventoryService
    method: GET
    data_selector: records
- name: streams
  endpoint:
    path: /v1/streams
    method: GET
    data_selector: data
    params: {}
- name: topics
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: data
    params: {}
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: order
    params: {}
- name: EmailService
  endpoint:
    path: /exercises/EmailService.java
    method: GET
- name: FraudService
  endpoint:
    path: /exercises/FraudService.java
    method: GET
- name: ValidationsAggregatorService
  endpoint:
    path: /exercises/ValidationsAggregatorService.java
    method: GET
- name: InventoryService
  endpoint:
    path: /exercises/InventoryService.java
    method: GET
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: event
    params: {}
- name: orders
  endpoint:
    path: /orders
    method: GET
    data_selector: records
- name: OrderDetailsService
  endpoint:
    path: /microservices/orders/OrderDetailsService
    method: GET
    data_selector: records
    params: {}
- name: EmailService
  endpoint:
    path: /microservices/orders/EmailService
    method: GET
    data_selector: records
    params: {}
- name: FraudService
  endpoint:
    path: /microservices/orders/FraudService
    method: GET
    data_selector: records
    params: {}
- name: ValidationsAggregatorService
  endpoint:
    path: /microservices/orders/ValidationsAggregatorService
    method: GET
    data_selector: records
    params: {}
- name: InventoryService
  endpoint:
    path: /microservices/orders/InventoryService
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params:
      incremental: updated_at
- name: connectors
  endpoint:
    path: /api/v1/connectors
    method: GET
    data_selector: data
    params: {}
- name: orders
  endpoint:
    path: /api/orders
    method: POST
    data_selector: records
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: records
    params: {}
- name: order-validations
  endpoint:
    path: /order-validations
    method: POST
    data_selector: records
    params: {}
- name: OrderDetailsService
  endpoint:
    path: /services/data/vXX.X/sobjects/OrderDetailsService
    method: GET
    data_selector: records
- name: EmailService
  endpoint:
    path: /services/data/vXX.X/sobjects/EmailService
    method: GET
    data_selector: records
- name: FraudService
  endpoint:
    path: /services/data/vXX.X/sobjects/FraudService
    method: GET
    data_selector: records
- name: ValidationsAggregatorService
  endpoint:
    path: /services/data/vXX.X/sobjects/ValidationsAggregatorService
    method: GET
    data_selector: records
- name: InventoryService
  endpoint:
    path: /services/data/vXX.X/sobjects/InventoryService
    method: GET
    data_selector: records
- name: orders
  endpoint:
    path: /orders
    method: POST
    data_selector: records
    params: {}
- name: payments
  endpoint:
    path: /payments_original
    method: GET
- name: orders
  endpoint:
    path: /ordersWithTotals
    method: GET
- name: customers
  endpoint:
    path: /customers
    method: GET
- name: inventory
  endpoint:
    path: /warehouse_inventory
    method: GET
- name: topics
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: data
    params: {}
- name: create_acls
  endpoint:
    path: /kafka-acls
    method: POST
- name: stream
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
    params: {}
- name: topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: processor_topology
  endpoint:
    path: /streams/topology
    method: GET
    data_selector: topologies
- name: state_store
  endpoint:
    path: /streams/state_store
    method: GET
    data_selector: stateStores
- name: Order Service
  endpoint:
    path: /orders
    method: POST
    data_selector: order
    params: {}
- name: Order Retrieval
  endpoint:
    path: /orders
    method: GET
    data_selector: orders
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: kafka-streams
  endpoint:
    path: /kafka-streams
    method: GET
    data_selector: records
    params: {}
- name: kafka-clients
  endpoint:
    path: /kafka-clients
    method: GET
    data_selector: records
    params: {}
- name: kafka-streams-scala
  endpoint:
    path: /kafka-streams-scala
    method: GET
    data_selector: records
    params: {}
- name: kafka-avro-serializer
  endpoint:
    path: /kafka-avro-serializer
    method: GET
    data_selector: records
    params: {}
- name: avro
  endpoint:
    path: /avro
    method: GET
    data_selector: records
    params: {}
- name: avro-maven-plugin
  endpoint:
    path: /avro-maven-plugin
    method: GET
    data_selector: records
    params: {}
- name: streams_application
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Cluster Status
  endpoint:
    path: /ksqlDB/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: Info
  endpoint:
    path: /ksqlDB/info
    method: GET
    data_selector: info
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: topology_test_driver
  endpoint:
    path: /test-utils/topology-test-driver
    method: GET
    data_selector: records
- name: test_input_topic
  endpoint:
    path: /test-utils/test-input-topic
    method: GET
    data_selector: records
- name: test_output_topic
  endpoint:
    path: /test-utils/test-output-topic
    method: GET
    data_selector: records
- name: kafka_topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: KStream
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#stream(java.lang.String)
    method: GET
- name: KTable
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#table-java.lang.String(java.lang.String)
    method: GET
- name: GlobalKTable
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/StreamsBuilder.html#globalTable-java.lang.String(java.lang.String)
    method: GET
- name: stateless_transformations
  endpoint:
    path: /streams/stateless_transformations
    method: GET
    data_selector: transformations
    params: {}
- name: KGroupedStream
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html
    method: GET
    data_selector: records
    params: {}
- name: CogroupedKStream
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/CogroupedKStream.html
    method: GET
    data_selector: records
    params: {}
- name: KGroupedTable
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KGroupedTable.html
    method: GET
    data_selector: records
    params: {}
- name: inner_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KStream.html#join-org.apache.kafka.streams.kstream.KStream-org.apache.kafka.streams.kstream.ValueJoiner-org.apache.kafka.streams.kstream.JoinWindows-
    method: GET
    data_selector: details
    params: {}
- name: left_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KStream.html#leftJoin-org.apache.kafka.streams.kstream.KStream-org.apache.kafka.streams.kstream.ValueJoiner-org.apache.kafka.streams.kstream.JoinWindows-
    method: GET
    data_selector: details
    params: {}
- name: outer_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KStream.html#outerJoin-org.apache.kafka.streams.kstream.KStream-org.apache.kafka.streams.kstream.ValueJoiner-org.apache.kafka.streams.kstream.JoinWindows-
    method: GET
    data_selector: details
    params: {}
- name: ktable_inner_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KTable.html#join-org.apache.kafka.streams.kstream.KTable-org.apache.kafka.streams.kstream.ValueJoiner-
    method: GET
    data_selector: details
    params: {}
- name: ktable_left_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KTable.html#leftJoin-org.apache.kafka.streams.kstream.KTable-org.apache.kafka.streams.kstream.ValueJoiner-
    method: GET
    data_selector: details
    params: {}
- name: ktable_outer_join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KTable.html#outerJoin-org.apache.kafka.streams.kstream.KTable-org.apache.kafka.streams.kstream.ValueJoiner-
    method: GET
    data_selector: details
    params: {}
- name: Inner Join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KStream.html#join-org.apache.kafka.streams.kstream.GlobalKTable-org.apache.kafka.streams.kstream.KeyValueMapper-org.apache.kafka.streams.kstream.ValueJoiner-
    method: GET
- name: Left Join
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/KStream.html#leftJoin-org.apache.kafka.streams.kstream.GlobalKTable-org.apache.kafka.streams.kstream.KeyValueMapper-org.apache.kafka.streams.kstream.ValueJoiner-
    method: GET
- name: Windowing
  endpoint:
    path: /platform/8.1/streams/developer-guide/dsl-windowing
    method: GET
- name: Tumbling time window
  endpoint:
    path: /platform/8.1/streams/developer-guide/dsl-tumbling
    method: GET
- name: Hopping time window
  endpoint:
    path: /platform/8.1/streams/developer-guide/dsl-hopping
    method: GET
- name: Sliding time window
  endpoint:
    path: /platform/8.1/streams/developer-guide/dsl-sliding
    method: GET
- name: Session Windows
  endpoint:
    path: /platform/8.1/streams/developer-guide/dsl-session-windows
    method: GET
- name: Categorize logs by severity
  endpoint:
    path: /streams/logs/categorize
    method: POST
    data_selector: records
    params: {}
- name: Cumulative discounts for a loyalty program
  endpoint:
    path: /streams/discounts/cumulative
    method: POST
    data_selector: records
    params: {}
- name: Replace slang in text messages
  endpoint:
    path: /streams/messages/replace-slang
    method: POST
    data_selector: records
    params: {}
- name: Traffic radar monitoring car count
  endpoint:
    path: /streams/cars/count
    method: POST
    data_selector: records
    params: {}
- name: userClicksStream
  endpoint:
    path: /user/clicks
    method: GET
    data_selector: records
- name: userRegionsTable
  endpoint:
    path: /user/regions
    method: GET
    data_selector: records
- name: clicksPerRegion
  endpoint:
    path: /clicks/per/region
    method: GET
    data_selector: records
- name: Customer_transactions_input_topic
  endpoint:
    path: input
    method: stream
    data_selector: records
- name: Mapped_transactions_output_topic
  endpoint:
    path: output
    method: to
    data_selector: records
- name: Purchase_count_store
  endpoint:
    path: output
    method: count
    data_selector: records
- name: topics
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: data
    params: {}
- name: clusters
  endpoint:
    path: /v1/clusters
    method: GET
    data_selector: data
    params: {}
- name: performance_guidelines
  endpoint:
    path: /ksqldb/operate-and-deploy/performance-guidelines.html
    method: GET
    data_selector: records
    params: {}
- name: schema_inference_with_id
  endpoint:
    path: /ksqldb/operate-and-deploy/schema-inference-with-id.html
    method: GET
    data_selector: records
    params: {}
- name: schema_inference
  endpoint:
    path: /ksqldb/operate-and-deploy/schema-registry-integration.html
    method: GET
    data_selector: records
    params: {}
- name: stream_processor
  endpoint:
    path: /streams/processors
    method: POST
    data_selector: records
- name: state_store
  endpoint:
    path: /streams/state-stores
    method: GET
    data_selector: stores
- name: versioned_key_value_state_stores
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/state/VersionedBytesStoreSupplier.html
    method: GET
    data_selector: records
    params: {}
- name: fault_tolerant_state_stores
  endpoint:
    path: /platform/8.1/streams/javadocs/javadoc/org/apache/kafka/streams/state/Stores.html#persistentKeyValueStore(java.lang.String).html
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: PrimitiveAvroSerde
  endpoint:
    path: /kafka-streams/primitive-avro-serde
    method: GET
    data_selector: records
- name: ReflectionAvroSerde
  endpoint:
    path: /kafka-streams/reflection-avro-serde
    method: GET
    data_selector: records
- name: JsonSchemaSerde
  endpoint:
    path: /kafka-streams/json-schema-serde
    method: GET
    data_selector: records
- name: ProtobufSerde
  endpoint:
    path: /kafka-streams/protobuf-serde
    method: GET
    data_selector: records
- name: implementing_custom_serdes
  endpoint:
    path: /implementing/custom/serdes
    method: GET
- name: kafka_streams_dsl_for_scala_implicit_serdes
  endpoint:
    path: /kafka/streams/dsl/scala/implicit/serdes
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: local_state
  endpoint:
    path: /local/state
    method: GET
- name: remote_state
  endpoint:
    path: /remote/state
    method: GET
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: internal_topics
  endpoint:
    path: /internal/topics
    method: GET
    data_selector: topics
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: data
    params: {}
- name: application_reset_tool
  endpoint:
    path: /bin/kafka-streams-application-reset
    method: GET
    data_selector: actions
    params: {}
- name: streams
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: locations
  endpoint:
    path: /jdbc/source
    method: GET
    data_selector: records
    params:
      incremental: id
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
- name: stream_query
  endpoint:
    path: /v1/streams/query
    method: POST
    data_selector: results
- name: Cluster Status
  endpoint:
    path: /ksql/api/v1/cluster/status
    method: GET
    data_selector: status
- name: Run SQL statements
  endpoint:
    path: /ksql/api/v1/ksql
    method: POST
    data_selector: statement
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: state
  endpoint:
    path: /kafkastreams/state
    method: GET
    data_selector: state
- name: threadMetadata
  endpoint:
    path: /kafkastreams/localThreadsMetadata
    method: GET
    data_selector: threadMetadata
- name: producer metrics
  endpoint:
    path: ../kafka/monitoring.html#kafka-monitoring-metrics-producer
    method: GET
- name: consumer metrics
  endpoint:
    path: ../kafka/monitoring.html#kafka-monitoring-metrics-consumer
    method: GET
- name: Cluster Status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: Info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: riderLocations
  endpoint:
    path: /riderLocations
    method: POST
    data_selector: records
    params: {}
- name: currentLocation
  endpoint:
    path: /currentLocation
    method: GET
    data_selector: records
    params: {}
- name: ridersNearMountainView
  endpoint:
    path: /ridersNearMountainView
    method: GET
    data_selector: records
    params: {}
- name: Cluster Status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: Info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: streams
  endpoint:
    path: /api/streams
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streaming_application
  endpoint:
    path: /api/streaming/application
    method: GET
    data_selector: records
- name: records
  endpoint:
    path: /records
    method: GET
    data_selector: records
- name: topics
  endpoint:
    path: /topics
    method: GET
    data_selector: records
- name: partitions
  endpoint:
    path: /partitions
    method: GET
    data_selector: records
- name: producers
  endpoint:
    path: /producers
    method: GET
    data_selector: records
- name: consumers
  endpoint:
    path: /consumers
    method: GET
    data_selector: records
- name: brokers
  endpoint:
    path: /brokers
    method: GET
    data_selector: records
- name: serializers
  endpoint:
    path: /serializers
    method: GET
    data_selector: records
- name: schemas
  endpoint:
    path: /schemas
    method: GET
    data_selector: records
- name: consumer_groups
  endpoint:
    path: /consumer_groups
    method: GET
    data_selector: records
- name: streams
  endpoint:
    path: /streams
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /api/v1/streams
    method: GET
    data_selector: records
- name: materialized_view
  endpoint:
    path: /materialized_views
    method: GET
    data_selector: views
    params: {}
- name: persistent_query
  endpoint:
    path: /queries/persistent
    method: POST
    data_selector: results
- name: push_query
  endpoint:
    path: /queries/push
    method: POST
    data_selector: results
- name: pull_query
  endpoint:
    path: /queries/pull
    method: POST
    data_selector: results
- name: cluster_status
  endpoint:
    path: /rest/v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /rest/v1/info
    method: GET
    data_selector: info
    params: {}
- name: rock_songs
  endpoint:
    path: /create_stream/rock_songs
    method: POST
    data_selector: records
    params: {}
- name: title_cased_songs
  endpoint:
    path: /create_stream/title_cased_songs
    method: POST
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /create_table/products
    method: POST
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /create_stream/orders
    method: POST
    data_selector: records
    params: {}
- name: order_metrics
  endpoint:
    path: /create_table/order_metrics
    method: POST
    data_selector: records
    params: {}
- name: page_view_metrics
  endpoint:
    path: /create_table/page_view_metrics
    method: POST
    data_selector: records
    params: {}
- name: page_view_metrics_mountain_view
  endpoint:
    path: /create_table/page_view_metrics_mountain_view
    method: POST
    data_selector: records
    params: {}
- name: impressions
  endpoint:
    path: /create_stream/impressions
    method: POST
    data_selector: records
    params: {}
- name: clicks
  endpoint:
    path: /create_stream/clicks
    method: POST
    data_selector: records
    params: {}
- name: clicked_impressions
  endpoint:
    path: /create_stream/clicked_impressions
    method: POST
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /streams/data
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /streams
    method: GET
    data_selector: records
- name: latest_view
  endpoint:
    path: /CREATE SOURCE TABLE/latest_view
    method: CREATE
    data_selector: records
    params:
      kafka_topic: changelog
      partitions: 1
      value_format: JSON
- name: changelog_stream
  endpoint:
    path: /changelog_stream
    method: SELECT
    data_selector: '*'
    params: {}
- name: latest_events
  endpoint:
    path: /latest_events
    method: SELECT
    data_selector: '*'
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: formula
  endpoint:
    path: /functions/formula
    method: POST
    data_selector: results
- name: index_seq
  endpoint:
    path: /functions/index_seq
    method: POST
    data_selector: results
- name: rolling_sum
  endpoint:
    path: /functions/rolling_sum
    method: POST
    data_selector: results
- name: stats
  endpoint:
    path: /functions/stats
    method: POST
    data_selector: results
    params: {}
- name: formula
  endpoint:
    path: /functions/formula
    method: POST
    data_selector: results
    params: {}
- name: index_seq
  endpoint:
    path: /functions/index_seq
    method: POST
    data_selector: results
    params: {}
- name: rolling_sum
  endpoint:
    path: /functions/rolling_sum
    method: POST
    data_selector: results
    params: {}
- name: source_connector
  endpoint:
    path: /connectors
    method: CREATE
    data_selector: connector.class
    params:
      connector.class: io.mdrogalis.voluble.VolubleSourceConnector
      global.throttle.ms: '500'
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: people
  endpoint:
    path: /topics/people
    method: PRINT
    data_selector: value
    params: {}
- name: s2
  endpoint:
    path: /CREATE/STREAM/s2
    method: POST
    data_selector: records
- name: s3
  endpoint:
    path: /CREATE/STREAM/s3
    method: POST
    data_selector: records
- name: s4
  endpoint:
    path: /CREATE/STREAM/s4
    method: POST
    data_selector: records
- name: s4
  endpoint:
    path: /s4
    method: CREATE
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: valid_purchases
  endpoint:
    path: /valid_purchases
    method: CREATE OR REPLACE
    data_selector: '*'
    params: {}
- name: purchase_stats
  endpoint:
    path: /purchase_stats
    method: CREATE OR REPLACE
    data_selector: '*'
    params: {}
- name: stream1
  endpoint:
    path: /create/stream1
    method: CREATE
    data_selector: records
    params: {}
- name: kafka_topic
  endpoint:
    path: /v3/clusters/{cluster_id}/topics
    method: GET
    data_selector: topics
    params: {}
- name: s1
  endpoint:
    path: /CREATE_STREAM_s1
    method: POST
    data_selector: records
    params:
      kafka_topic: s1
      partitions: 1
      value_format: avro
      timestamp: ts
      timestamp_format: yyyy-MM-dd HH:mm:ss
- name: s2
  endpoint:
    path: /CREATE_STREAM_s2
    method: POST
    data_selector: records
    params:
      timestamp: ts
      timestamp_format: yyyy-MM-dd HH:mm:ss
- name: s3
  endpoint:
    path: /CREATE_STREAM_s3
    method: POST
    data_selector: records
    params:
      kafka_topic: s3
      partitions: 1
      value_format: avro
      timestamp: ts
      timestamp_format: yyyy-MM-dd HH:mm:ss
- name: s4
  endpoint:
    path: /CREATE_STREAM_s4
    method: POST
    data_selector: records
    params:
      kafka_topic: s4
      partitions: 1
      value_format: avro
      timestamp: ts
- name: s5
  endpoint:
    path: /create/stream/s5
    method: CREATE
    data_selector: records
    params:
      kafka_topic: s5
      partitions: 1
      value_format: avro
      timestamp: ts
- name: stream1
  endpoint:
    path: /create_stream/stream1
    method: POST
    data_selector: records
    params: {}
- name: output
  endpoint:
    path: /create_stream/output
    method: POST
    data_selector: records
    params: {}
- name: stream2
  endpoint:
    path: /create_stream/stream2
    method: POST
    data_selector: records
    params: {}
- name: output2
  endpoint:
    path: /create_stream/output2
    method: POST
    data_selector: records
    params: {}
- name: stream3
  endpoint:
    path: /create_stream/stream3
    method: POST
    data_selector: records
    params: {}
- name: output3
  endpoint:
    path: /create_stream/output3
    method: POST
    data_selector: records
    params: {}
- name: stream4
  endpoint:
    path: /create/stream4
    method: POST
    data_selector: stream
    params: {}
- name: output4
  endpoint:
    path: /create/output4
    method: POST
    data_selector: output
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksql/info
    method: GET
    data_selector: info
    params: {}
- name: Cluster Status
  endpoint:
    path: /ksqldb/rest/v1/cluster/status
    method: GET
- name: Info
  endpoint:
    path: /ksqldb/rest/v1/info
    method: GET
- name: Run SQL statements
  endpoint:
    path: /ksqldb/rest/v1/ksql
    method: POST
- name: Cluster Status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: Info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: Run SQL statements
  endpoint:
    path: /query
    method: POST
    data_selector: queryResults
    params: {}
- name: stream
  endpoint:
    path: /services/data/vXX.X/streams
    method: GET
    data_selector: records
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: stream
  endpoint:
    path: /streams
    method: GET
    data_selector: streams
    params: {}
- name: aggregate_functions
  endpoint:
    path: /aggregate_functions
    method: GET
    data_selector: functions
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /services/data/vXX.X/sobjects/Stream
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: jdbc-connector
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: stream
  endpoint:
    path: /create_stream_as_select
    method: POST
    data_selector: stream_properties
    params: {}
- name: filtered_stream
  endpoint:
    path: /create_stream/filtered
    method: POST
    data_selector: CREATE STREAM filtered AS SELECT a, few, columns FROM source_stream
      EMIT CHANGES
- name: enriched_stream
  endpoint:
    path: /create_stream/enriched
    method: POST
    data_selector: CREATE STREAM enriched AS SELECT cs.*, u.name, u.classification,
      u.level FROM clickstream cs JOIN users u ON u.id = cs.userId EMIT CHANGES
- name: enriched_stream_with_schema
  endpoint:
    path: /create_stream/enriched_with_schema
    method: POST
    data_selector: CREATE STREAM enriched WITH (VALUE_SCHEMA_ID = 1) AS SELECT cs.*,
      u.name, u.classification, u.level FROM clickstream cs JOIN users u ON u.id =
      cs.userId EMIT CHANGES
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: statements
    params: {}
- name: stream
  endpoint:
    path: /create/stream
    method: POST
    data_selector: stream_properties
- name: pageviews
  endpoint:
    path: /create-stream/pageviews
    method: CREATE
    data_selector: pageviews
    params:
      KAFKA_TOPIC: keyless-pageviews-topic
      VALUE_FORMAT: JSON
- name: keyed_pageviews
  endpoint:
    path: /create-stream/keyed_pageviews
    method: CREATE
    data_selector: keyed_pageviews
    params:
      KAFKA_TOPIC: keyed-pageviews-topic
      VALUE_FORMAT: JSON
- name: keyless_pageviews_schema_registry
  endpoint:
    path: /create-stream/keyless_pageviews_schema_registry
    method: CREATE
    data_selector: keyless_pageviews
    params:
      KAFKA_TOPIC: keyless-pageviews-topic
      VALUE_FORMAT: JSON_SR
- name: keyed_pageviews_schema_registry
  endpoint:
    path: /create-stream/keyed_pageviews_schema_registry
    method: CREATE
    data_selector: keyed_pageviews
    params:
      KAFKA_TOPIC: keyed-pageviews-topic
      VALUE_FORMAT: JSON_SR
- name: key_schema_id_pageviews
  endpoint:
    path: /create-stream/key_schema_id_pageviews
    method: CREATE
    data_selector: key_schema_id_pageviews
    params:
      KAFKA_TOPIC: keyless-pageviews-topic
      KEY_FORMAT: AVRO
      KEY_SCHEMA_ID: 1
      VALUE_FORMAT: JSON_SR
- name: value_schema_id_pageviews
  endpoint:
    path: /create-stream/value_schema_id_pageviews
    method: CREATE
    data_selector: value_schema_id_pageviews
    params:
      KAFKA_TOPIC: keyed-pageviews-topic
      VALUE_FORMAT: JSON_SR
      VALUE_SCHEMA_ID: 2
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: derived
  endpoint:
    path: /create_table_as_select/derived
    method: POST
    data_selector: output
    params:
      VALUE_SCHEMA_ID: 1
- name: weeklyMusicCharts
  endpoint:
    path: /create_table_as_select/weeklyMusicCharts
    method: POST
    data_selector: output
    params: {}
- name: pageviews_per_region
  endpoint:
    path: /create_table_as_select/pageviews_per_region
    method: POST
    data_selector: output
    params: {}
- name: top_orders
  endpoint:
    path: /create_table_as_select/top_orders
    method: POST
    data_selector: output
    params: {}
- name: CREATE_TABLE
  endpoint:
    path: /create_table
    method: POST
    data_selector: table
    params: {}
- name: users
  endpoint:
    path: /create_table
    method: CREATE
    data_selector: columns
    params:
      KAFKA_TOPIC: my-users-topic
      VALUE_FORMAT: JSON
- name: users
  endpoint:
    path: /create_table
    method: CREATE
    data_selector: columns
    params:
      KAFKA_TOPIC: my-users-topic
      VALUE_FORMAT: JSON_SR
- name: users
  endpoint:
    path: /create_table
    method: CREATE
    data_selector: columns
    params:
      KAFKA_TOPIC: my-users-topic
      VALUE_FORMAT: JSON_SR
      VALUE_SCHEMA_ID: 2
- name: users
  endpoint:
    path: /create_table
    method: CREATE
    data_selector: columns
    params:
      KAFKA_TOPIC: my-users-topic
      KEY_FORMAT: AVRO
      KEY_SCHEMA_ID: 1
      VALUE_FORMAT: JSON_SR
- name: ADDRESS
  endpoint:
    path: /create_type/address
    method: POST
    data_selector: records
- name: PERSON
  endpoint:
    path: /create_type/person
    method: POST
    data_selector: records
- name: replicas
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: format
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: topicName
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: topics
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: schemas
  endpoint:
    path: /v1/schemas
    method: GET
    data_selector: schemas
    params: {}
- name: function_description
  endpoint:
    path: /describe/function
    method: GET
    data_selector: description
    params: {}
- name: topic
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: data
    params: {}
- name: consumer_group
  endpoint:
    path: /v1/consumer-groups
    method: GET
    data_selector: data
    params: {}
- name: describe_table
  endpoint:
    path: /describe
    method: GET
    data_selector: records
    params: {}
- name: describe_tables
  endpoint:
    path: /describe/tables
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /ksql
    method: POST
    data_selector: response
    params: {}
- name: queries
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: drop_connector
  endpoint:
    path: /drop-connector
    method: POST
    data_selector: response
    params:
      if_exists: true
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: stream
  endpoint:
    path: /streams
    method: GET
    data_selector: records
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: explain_statement
  endpoint:
    path: /EXPLAIN
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: operators
  endpoint:
    path: /operators
    method: GET
    data_selector: operators
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: print_topic
  endpoint:
    path: /print
    method: POST
    data_selector: results
- name: topic
  endpoint:
    path: /api/v1/topics
    method: GET
    data_selector: data
    params: {}
- name: right_join
  endpoint:
    path: /right_join
    method: GET
    data_selector: records
- name: run_script
  endpoint:
    path: /run_script
    method: GET
    data_selector: records
- name: select_pull_query
  endpoint:
    path: /select_pull_query
    method: GET
    data_selector: records
- name: select_push_query
  endpoint:
    path: /select_push_query
    method: GET
    data_selector: records
- name: session
  endpoint:
    path: /session
    method: GET
    data_selector: records
- name: set_property
  endpoint:
    path: /set_property
    method: GET
    data_selector: records
- name: show_connectors
  endpoint:
    path: /show_connectors
    method: GET
    data_selector: records
- name: show_functions
  endpoint:
    path: /show_functions
    method: GET
    data_selector: records
- name: show_properties
  endpoint:
    path: /show_properties
    method: GET
    data_selector: records
- name: show_queries
  endpoint:
    path: /show_queries
    method: GET
    data_selector: records
- name: show_streams
  endpoint:
    path: /show_streams
    method: GET
    data_selector: records
- name: show_tables
  endpoint:
    path: /show_tables
    method: GET
    data_selector: records
- name: show_topics
  endpoint:
    path: /show_topics
    method: GET
    data_selector: records
- name: show_types
  endpoint:
    path: /show_types
    method: GET
    data_selector: records
- name: show_variables
  endpoint:
    path: /show_variables
    method: GET
    data_selector: records
- name: size
  endpoint:
    path: /size
    method: GET
    data_selector: records
- name: spool
  endpoint:
    path: /spool
    method: GET
    data_selector: records
- name: terminate
  endpoint:
    path: /terminate
    method: GET
    data_selector: records
- name: tumbling
  endpoint:
    path: /tumbling
    method: GET
    data_selector: records
- name: undefine
  endpoint:
    path: /undefine
    method: GET
    data_selector: records
- name: unset_property
  endpoint:
    path: /unset_property
    method: GET
    data_selector: records
- name: where
  endpoint:
    path: /where
    method: GET
    data_selector: records
- name: window
  endpoint:
    path: /window
    method: GET
    data_selector: records
- name: resume_query
  endpoint:
    path: /resume
    method: POST
    data_selector: query_id
    params: {}
- name: run_script
  endpoint:
    path: /run_script
    method: POST
    data_selector: result
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ABS
  endpoint:
    path: /scalar-functions/ABS
    method: GET
    data_selector: records
    params: {}
- name: ACOS
  endpoint:
    path: /scalar-functions/ACOS
    method: GET
    data_selector: records
    params: {}
- name: ASIN
  endpoint:
    path: /scalar-functions/ASIN
    method: GET
    data_selector: records
    params: {}
- name: AS_VALUE
  endpoint:
    path: /scalar-functions/AS_VALUE
    method: GET
    data_selector: records
    params: {}
- name: ATAN
  endpoint:
    path: /scalar-functions/ATAN
    method: GET
    data_selector: records
    params: {}
- name: ATAN2
  endpoint:
    path: /scalar-functions/ATAN2
    method: GET
    data_selector: records
    params: {}
- name: CAST
  endpoint:
    path: /scalar-functions/CAST
    method: GET
    data_selector: records
    params: {}
- name: CBRT
  endpoint:
    path: /scalar-functions/CBRT
    method: GET
    data_selector: records
    params: {}
- name: CEIL
  endpoint:
    path: /scalar-functions/CEIL
    method: GET
    data_selector: records
    params: {}
- name: COS
  endpoint:
    path: /scalar-functions/COS
    method: GET
    data_selector: records
    params: {}
- name: COSH
  endpoint:
    path: /scalar-functions/COSH
    method: GET
    data_selector: records
    params: {}
- name: COT
  endpoint:
    path: /scalar-functions/COT
    method: GET
    data_selector: records
    params: {}
- name: DEGREES
  endpoint:
    path: /scalar-functions/DEGREES
    method: GET
    data_selector: records
    params: {}
- name: ENTRIES
  endpoint:
    path: /scalar-functions/ENTRIES
    method: GET
    data_selector: records
    params: {}
- name: EXP
  endpoint:
    path: /scalar-functions/EXP
    method: GET
    data_selector: records
    params: {}
- name: FLOOR
  endpoint:
    path: /scalar-functions/FLOOR
    method: GET
    data_selector: records
    params: {}
- name: GENERATE_SERIES
  endpoint:
    path: /scalar-functions/GENERATE_SERIES
    method: GET
    data_selector: records
    params: {}
- name: GEO_DISTANCE
  endpoint:
    path: /scalar-functions/GEO_DISTANCE
    method: GET
    data_selector: records
    params: {}
- name: GREATEST
  endpoint:
    path: /scalar-functions/GREATEST
    method: GET
    data_selector: records
    params: {}
- name: LEAST
  endpoint:
    path: /scalar-functions/LEAST
    method: GET
    data_selector: records
    params: {}
- name: LN
  endpoint:
    path: /scalar-functions/LN
    method: GET
    data_selector: records
    params: {}
- name: LOG
  endpoint:
    path: /scalar-functions/LOG
    method: GET
    data_selector: records
    params: {}
- name: PI
  endpoint:
    path: /scalar-functions/PI
    method: GET
    data_selector: records
    params: {}
- name: POWER
  endpoint:
    path: /scalar-functions/POWER
    method: GET
    data_selector: records
    params: {}
- name: RADIANS
  endpoint:
    path: /scalar-functions/RADIANS
    method: GET
    data_selector: records
    params: {}
- name: RANDOM
  endpoint:
    path: /scalar-functions/RANDOM
    method: GET
    data_selector: records
    params: {}
- name: ROUND
  endpoint:
    path: /scalar-functions/ROUND
    method: GET
    data_selector: records
    params: {}
- name: SIGN
  endpoint:
    path: /scalar-functions/SIGN
    method: GET
    data_selector: records
    params: {}
- name: SIN
  endpoint:
    path: /scalar-functions/SIN
    method: GET
    data_selector: records
    params: {}
- name: SINH
  endpoint:
    path: /scalar-functions/SINH
    method: GET
    data_selector: records
    params: {}
- name: SQRT
  endpoint:
    path: /scalar-functions/SQRT
    method: GET
    data_selector: records
    params: {}
- name: TAN
  endpoint:
    path: /scalar-functions/TAN
    method: GET
    data_selector: records
    params: {}
- name: TANH
  endpoint:
    path: /scalar-functions/TANH
    method: GET
    data_selector: records
    params: {}
- name: TRUNC
  endpoint:
    path: /scalar-functions/TRUNC
    method: GET
    data_selector: records
    params: {}
- name: ARRAY
  endpoint:
    path: /collections/ARRAY
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_CONCAT
  endpoint:
    path: /collections/ARRAY_CONCAT
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_CONTAINS
  endpoint:
    path: /collections/ARRAY_CONTAINS
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_DISTINCT
  endpoint:
    path: /collections/ARRAY_DISTINCT
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_EXCEPT
  endpoint:
    path: /collections/ARRAY_EXCEPT
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_INTERSECT
  endpoint:
    path: /collections/ARRAY_INTERSECT
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_JOIN
  endpoint:
    path: /collections/ARRAY_JOIN
    method: GET
    data_selector: records
    params: {}
- name: ARRAY_LENGTH
  endpoint:
    path: /collections/ARRAY_LENGTH
    method: GET
    data_selector: records
    params: {}
- name: JSON_RECORDS
  endpoint:
    path: /json_records
    method: GET
    data_selector: records
- name: JSON_ITEMS
  endpoint:
    path: /json_items
    method: GET
    data_selector: records
- name: TO_JSON_STRING
  endpoint:
    path: /to_json_string
    method: GET
    data_selector: records
- name: INITCAP
  endpoint:
    path: /initcap
    method: GET
    data_selector: records
- name: INSTR
  endpoint:
    path: /instr
    method: GET
    data_selector: records
- name: LCASE
  endpoint:
    path: /lcase
    method: GET
    data_selector: records
- name: LEN
  endpoint:
    path: /len
    method: GET
    data_selector: records
- name: LPAD
  endpoint:
    path: /lpad
    method: GET
    data_selector: records
- name: MASK
  endpoint:
    path: /mask
    method: GET
    data_selector: records
- name: MASK_KEEP_LEFT
  endpoint:
    path: /mask_keep_left
    method: GET
    data_selector: records
- name: MASK_KEEP_RIGHT
  endpoint:
    path: /mask_keep_right
    method: GET
    data_selector: records
- name: MASK_LEFT
  endpoint:
    path: /mask_left
    method: GET
    data_selector: records
- name: MASK_RIGHT
  endpoint:
    path: /mask_right
    method: GET
    data_selector: records
- name: MD5
  endpoint:
    path: /md5
    method: GET
    data_selector: records
- name: REPLACE
  endpoint:
    path: /replace
    method: GET
    data_selector: records
- name: REGEXP_EXTRACT
  endpoint:
    path: /regexp_extract
    method: GET
    data_selector: records
- name: REGEXP_EXTRACT_ALL
  endpoint:
    path: /regexp_extract_all
    method: GET
    data_selector: records
- name: REGEXP_REPLACE
  endpoint:
    path: /regexp_replace
    method: GET
    data_selector: records
- name: REGEXP_SPLIT_TO_ARRAY
  endpoint:
    path: /regexp_split_to_array
    method: GET
    data_selector: records
- name: RPAD
  endpoint:
    path: /rpad
    method: GET
    data_selector: records
- name: SPLIT
  endpoint:
    path: /split
    method: GET
    data_selector: records
- name: SPLIT_TO_MAP
  endpoint:
    path: /split_to_map
    method: GET
    data_selector: records
- name: SUBSTRING
  endpoint:
    path: /substring
    method: GET
    data_selector: records
- name: TO_BYTES
  endpoint:
    path: /to_bytes
    method: GET
    data_selector: records
- name: TRIM
  endpoint:
    path: /trim
    method: GET
    data_selector: records
- name: UCASE
  endpoint:
    path: /ucase
    method: GET
    data_selector: records
- name: UUID
  endpoint:
    path: /uuid
    method: GET
    data_selector: records
- name: IFNULL
  endpoint:
    path: /scalar-functions/IFNULL
    method: GET
    data_selector: function_description
    params: {}
- name: NULLIF
  endpoint:
    path: /scalar-functions/NULLIF
    method: GET
    data_selector: function_description
    params: {}
- name: CONVERT_TZ
  endpoint:
    path: /scalar-functions/CONVERT_TZ
    method: GET
    data_selector: function_description
    params: {}
- name: DATEADD
  endpoint:
    path: /scalar-functions/DATEADD
    method: GET
    data_selector: function_description
    params: {}
- name: DATESUB
  endpoint:
    path: /scalar-functions/DATESUB
    method: GET
    data_selector: function_description
    params: {}
- name: FORMAT_DATE
  endpoint:
    path: /scalar-functions/FORMAT_DATE
    method: GET
    data_selector: function_description
    params: {}
- name: FORMAT_TIME
  endpoint:
    path: /scalar-functions/FORMAT_TIME
    method: GET
    data_selector: function_description
    params: {}
- name: FORMAT_TIMESTAMP
  endpoint:
    path: /scalar-functions/FORMAT_TIMESTAMP
    method: GET
    data_selector: function_description
    params: {}
- name: FROM_DAYS
  endpoint:
    path: /scalar-functions/FROM_DAYS
    method: GET
    data_selector: function_description
    params: {}
- name: FROM_UNIXTIME
  endpoint:
    path: /scalar-functions/FROM_UNIXTIME
    method: GET
    data_selector: function_description
    params: {}
- name: PARSE_DATE
  endpoint:
    path: /scalar-functions/PARSE_DATE
    method: GET
    data_selector: function_description
    params: {}
- name: PARSE_TIME
  endpoint:
    path: /scalar-functions/PARSE_TIME
    method: GET
    data_selector: function_description
    params: {}
- name: PARSE_TIMESTAMP
  endpoint:
    path: /scalar-functions/PARSE_TIMESTAMP
    method: GET
    data_selector: function_description
    params: {}
- name: TIMEADD
  endpoint:
    path: /scalar-functions/TIMEADD
    method: GET
    data_selector: function_description
    params: {}
- name: TIMESUB
  endpoint:
    path: /scalar-functions/TIMESUB
    method: GET
    data_selector: function_description
    params: {}
- name: TIMESTAMPADD
  endpoint:
    path: /scalar-functions/TIMESTAMPADD
    method: GET
    data_selector: function_description
    params: {}
- name: TIMESTAMPSUB
  endpoint:
    path: /scalar-functions/TIMESTAMPSUB
    method: GET
    data_selector: function_description
    params: {}
- name: UNIX_DATE
  endpoint:
    path: /scalar-functions/UNIX_DATE
    method: GET
    data_selector: function_description
    params: {}
- name: UNIX_TIMESTAMP
  endpoint:
    path: /scalar-functions/UNIX_TIMESTAMP
    method: GET
    data_selector: function_description
    params: {}
- name: URL_DECODE_PARAM
  endpoint:
    path: /scalar-functions/URL_DECODE_PARAM
    method: GET
    data_selector: function_description
    params: {}
- name: URL_ENCODE_PARAM
  endpoint:
    path: /scalar-functions/URL_ENCODE_PARAM
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_FRAGMENT
  endpoint:
    path: /scalar-functions/URL_EXTRACT_FRAGMENT
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_HOST
  endpoint:
    path: /scalar-functions/URL_EXTRACT_HOST
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_PARAMETER
  endpoint:
    path: /scalar-functions/URL_EXTRACT_PARAMETER
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_PATH
  endpoint:
    path: /scalar-functions/URL_EXTRACT_PATH
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_PORT
  endpoint:
    path: /scalar-functions/URL_EXTRACT_PORT
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_PROTOCOL
  endpoint:
    path: /scalar-functions/URL_EXTRACT_PROTOCOL
    method: GET
    data_selector: function_description
    params: {}
- name: URL_EXTRACT_QUERY
  endpoint:
    path: /scalar-functions/URL_EXTRACT_QUERY
    method: GET
    data_selector: function_description
    params: {}
- name: pull_query
  endpoint:
    path: /ksql
    method: POST
    data_selector: response
    params: {}
- name: users
  endpoint:
    path: /users
    method: GET
    data_selector: records
    params: {}
- name: pageviews
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: functions
  endpoint:
    path: /functions
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: properties
  endpoint:
    path: /SHOW_PROPERTIES
    method: GET
    data_selector: properties
    params: {}
- name: queries
  endpoint:
    path: /show/queries
    method: GET
    data_selector: queries
    params: {}
- name: ksqlDB
  endpoint:
    path: /services/data/vXX.X/ksqlDB
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /show-streams
    method: GET
    data_selector: streams
- name: Kafka
  endpoint:
    path: /kafka
    method: GET
- name: Schema Registry
  endpoint:
    path: /schemas
    method: GET
- name: tables
  endpoint:
    path: /show/tables
    method: GET
    data_selector: tables
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /SHOW TOPICS
    method: GET
    data_selector: available_topics
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: types
  endpoint:
    path: /types
    method: GET
    data_selector: types
    params: {}
- name: variables
  endpoint:
    path: /SHOW VARIABLES
    method: GET
    data_selector: Variable Name | Value
    params: {}
- name: stream
  endpoint:
    path: /streams
    method: GET
    data_selector: records
    params: {}
- name: Kafka REST API
  endpoint:
    path: /kafka/v3/clusters
    method: GET
    data_selector: clusters
    params: {}
- name: spool
  endpoint:
    path: /spool
    method: GET
    data_selector: commands
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksqldb
    method: GET
    data_selector: results
- name: exploded_stream
  endpoint:
    path: CREATE STREAM exploded_stream AS
    method: POST
    data_selector: output
    params: {}
- name: country_customers
  endpoint:
    path: CREATE STREAM country_customers AS
    method: POST
    data_selector: output
    params: {}
- name: event
  endpoint:
    path: /services/data/vXX.X/sobjects/Event
    method: GET
    data_selector: records
- name: undefine_variable
  endpoint:
    path: /ksqldb/undefine
    method: POST
    data_selector: undefined_variable
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: clusterStatus
  endpoint:
    path: /clusterStatus
    method: GET
- name: info
  endpoint:
    path: /info
    method: GET
- name: is_valid_property
  endpoint:
    path: /is_valid_property
    method: GET
- name: ksql
  endpoint:
    path: /ksql
    method: POST
- name: query-stream
  endpoint:
    path: /query-stream
    method: POST
- name: status
  endpoint:
    path: /status
    method: GET
- name: terminate
  endpoint:
    path: /terminate
    method: POST
- name: cluster_status
  endpoint:
    path: /clusterStatus
    method: GET
- name: info
  endpoint:
    path: /info
    method: GET
- name: is_valid_property
  endpoint:
    path: /is_valid_property
    method: GET
- name: ksql
  endpoint:
    path: /ksql
    method: POST
- name: query_stream
  endpoint:
    path: /query-stream
    method: POST
- name: status
  endpoint:
    path: /status
    method: GET
- name: terminate
  endpoint:
    path: /terminate
    method: POST
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: execute_statement
  endpoint:
    path: /ksql
    method: POST
    data_selector: ''
    params: {}
- name: run_query
  endpoint:
    path: /query
    method: POST
    data_selector: ''
    params: {}
- name: run_push_pull_queries
  endpoint:
    path: /query-stream
    method: POST
    data_selector: ''
    params: {}
- name: terminate_cluster
  endpoint:
    path: /ksql/terminate
    method: POST
    data_selector: ''
    params: {}
- name: introspect_query_status
  endpoint:
    path: /status
    method: GET
    data_selector: ''
    params: {}
- name: introspect_server_status
  endpoint:
    path: /info
    method: GET
    data_selector: ''
    params: {}
- name: introspect_cluster_status
  endpoint:
    path: /clusterStatus
    method: GET
    data_selector: ''
    params: {}
- name: get_validity_of_property
  endpoint:
    path: /is_valid_property
    method: GET
    data_selector: ''
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: clusterStatus
  endpoint:
    path: /clusterStatus
    method: GET
    data_selector: clusterStatus
- name: Cluster Status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: Info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: .
- name: healthcheck
  endpoint:
    path: /healthcheck
    method: GET
    data_selector: .
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ksql
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: ksql_statement
  endpoint:
    path: /ksql
    method: POST
    data_selector: statementText
    params: {}
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: row
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: row
    params: {}
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: status
  endpoint:
    path: /status/(string:commandId)
    method: GET
    data_selector: status
    params: {}
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: query_stream
  endpoint:
    path: /query-stream
    method: POST
    data_selector: results
- name: close_query
  endpoint:
    path: /close-query
    method: POST
    data_selector: acknowledgment
- name: inserts_stream
  endpoint:
    path: /inserts-stream
    method: POST
    data_selector: acknowledgment
- name: cluster_status
  endpoint:
    path: /cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: terminate_cluster
  endpoint:
    path: /ksql/terminate
    method: POST
    data_selector: ''
    params: {}
- name: query_stream
  endpoint:
    path: /query-stream
    method: POST
- name: inserts_stream
  endpoint:
    path: /inserts-stream
    method: POST
- name: ksql
  endpoint:
    path: /ksql
    method: POST
- name: streamInserts
  endpoint:
    path: /streamInserts
    method: POST
    data_selector: rows
    params: {}
- name: executeStatement
  endpoint:
    path: /executeStatement
    method: POST
    data_selector: result
    params: {}
- name: listStreams
  endpoint:
    path: /listStreams
    method: GET
    data_selector: streams
    params: {}
- name: describeSource
  endpoint:
    path: /describeSource
    method: GET
    data_selector: description
    params: {}
- name: serverInfo
  endpoint:
    path: /serverInfo
    method: GET
    data_selector: server
    params: {}
- name: createConnector
  endpoint:
    path: /createConnector
    method: POST
    data_selector: connector
    params: {}
- name: listConnectors
  endpoint:
    path: /listConnectors
    method: GET
    data_selector: connectors
    params: {}
- name: assertTopic
  endpoint:
    path: /assertTopic
    method: POST
    data_selector: topic
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: server_info
  endpoint:
    path: /info
    method: GET
- name: health_check
  endpoint:
    path: /healthcheck
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: server_info
  endpoint:
    path: /info
    method: GET
- name: health_check
  endpoint:
    path: /healthcheck
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB Server
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params:
      bootstrap.servers: localhost:9092
      listeners: http://0.0.0.0:8088
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB Server
  endpoint:
    path: /ksqldb
    method: GET
    data_selector: status
    params:
      bootstrap.servers: localhost:9092
      listeners: http://0.0.0.0:8088
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ksqlDB Quickstart stack
  endpoint:
    path: /quickstart
    method: GET
- name: Full ksqlDB event processing application
  endpoint:
    path: /cloud/current/cp-component/ksql-cloud-config.html
    method: GET
- name: server_mode
  endpoint:
    path: /ksqldb/server
    method: GET
- name: cli_mode
  endpoint:
    path: /ksqldb/cli
    method: GET
- name: cluster_status
  endpoint:
    path: /clusters/status
    method: GET
    data_selector: status
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
- name: ksqlDB Server
  endpoint:
    path: /ksqldb/server
    method: GET
    data_selector: records
- name: ksqlDB CLI
  endpoint:
    path: /ksqldb/cli
    method: GET
    data_selector: records
- name: ksql_service_2_
  endpoint:
    path: /ksql
    method: POST
    data_selector: queryResults
    params:
      bootstrap.servers: localhost:9092
      listeners: http://0.0.0.0:8088/
      service.id: ksql_service_2_
- name: ksql_standalone_1_
  endpoint:
    path: /ksql
    method: POST
    data_selector: queryResults
    params:
      bootstrap.servers: localhost:9092
      queries.file: /path/in/container/queries.sql
      service.id: ksql_standalone_1_
- name: ksql_server
  endpoint:
    path: /ksql
    method: POST
    data_selector: queries
    params: {}
- name: ksqlDB Server
  endpoint:
    path: /ksqlDB
    method: GET
    data_selector: records
- name: ksqlDB CLI
  endpoint:
    path: /ksqldb-cli
    method: GET
    data_selector: records
- name: server_info
  endpoint:
    path: /info
    method: GET
    data_selector: server_info
    params: {}
- name: healthcheck
  endpoint:
    path: /healthcheck
    method: GET
    data_selector: health_status
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: ksql-server
  endpoint:
    path: <path-to-confluent>/etc/ksqldb/ksql-server.properties
    method: GET
    data_selector: properties
    params: {}
- name: ksql_server
  endpoint:
    path: /ksql-server/start
    method: POST
    data_selector: server_config
    params: {}
- name: ksql-server
  endpoint:
    path: /services/ksql-server
    method: POST
    data_selector: results
- name: kafka-cluster
  endpoint:
    path: /kafka-cluster
    method: GET
    data_selector: resources
    params: {}
- name: input_topics
  endpoint:
    path: /input/topics
    method: GET
    data_selector: topics
    params: {}
- name: output_topics
  endpoint:
    path: /output/topics
    method: GET
    data_selector: topics
    params: {}
- name: consumer_groups
  endpoint:
    path: /consumer/groups
    method: GET
    data_selector: groups
    params: {}
- name: queries
  endpoint:
    path: /queries
    method: GET
    data_selector: queries
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: results
    params: {}
- name: stream_query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: authorization_attempts
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: possible_fraud
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: output_topics
  endpoint:
    path: /output/topics
    method: GET
    data_selector: topics
    params: {}
- name: internal_topics
  endpoint:
    path: /internal/topics
    method: GET
    data_selector: topics
    params: {}
- name: command_topics
  endpoint:
    path: /command/topics
    method: GET
    data_selector: topics
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: high_availability_configuration
  endpoint:
    path: /high-availability/configuration
    method: GET
    data_selector: configs
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: migrations_metadata
  endpoint:
    path: /migrations
    method: GET
    data_selector: migrations
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: pageviews
  endpoint:
    path: /ksql
    method: POST
    data_selector: records
    params:
      KAFKA_TOPIC: pageviews-avro-topic
      VALUE_FORMAT: AVRO
      VALUE_SCHEMA_ID: 1
- name: pageview_count
  endpoint:
    path: /ksql
    method: POST
    data_selector: records
    params:
      KAFKA_TOPIC: pageview-count
      KEY_FORMAT: KAFKA
      VALUE_FORMAT: AVRO
      VALUE_SCHEMA_ID: 2
- name: pageviews_new
  endpoint:
    path: /ksql
    method: POST
    data_selector: records
    params:
      VALUE_FORMAT: AVRO
      VALUE_SCHEMA_ID: 1
- name: pageviews
  endpoint:
    path: /services/data/vXX.X/sobjects/PageView
    method: CREATE
    data_selector: records
    params:
      KAFKA_TOPIC: pageviews-avro-topic
      KEY_FORMAT: KAFKA
      VALUE_FORMAT: AVRO
      VALUE_SCHEMA_ID: 1
      PARTITIONS: 1
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: pageviews
  endpoint:
    path: /create_stream/pageviews
    method: POST
    data_selector: streams
    params:
      KAFKA_TOPIC: pageviews-avro-topic
      VALUE_FORMAT: AVRO
- name: users
  endpoint:
    path: /create_table/users
    method: POST
    data_selector: tables
    params:
      KAFKA_TOPIC: users-avro-topic
      KEY_FORMAT: AVRO
      VALUE_FORMAT: AVRO
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: keywords
  endpoint:
    path: /keywords
    method: GET
    data_selector: keywords
    params: {}
- name: Cluster Status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streams
  endpoint:
    path: /streams
    method: POST
    data_selector: streams
    params: {}
- name: tables
  endpoint:
    path: /tables
    method: POST
    data_selector: tables
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streaming_application
  endpoint:
    path: /streaming/application
    method: GET
    data_selector: records
- name: time_units
  endpoint:
    path: /time_units
    method: GET
- name: timestamp_formats
  endpoint:
    path: /timestamp_formats
    method: GET
- name: stream
  endpoint:
    path: /ksql
    method: POST
    data_selector: streams
    params: {}
- name: persistent_queries
  endpoint:
    path: /metrics/persistent_queries
    method: GET
    data_selector: attributes
- name: pull_queries
  endpoint:
    path: /metrics/pull_queries
    method: GET
    data_selector: attributes
- name: user_defined_functions
  endpoint:
    path: /metrics/user_defined_functions
    method: GET
    data_selector: attributes
- name: command_runner
  endpoint:
    path: /metrics/command_runner
    method: GET
    data_selector: attributes
- name: rocksdb
  endpoint:
    path: /metrics/rocksdb
    method: GET
    data_selector: attributes
- name: block_cache_usage
  endpoint:
    path: /metrics/block-cache-usage
    method: GET
    data_selector: metrics
    params: {}
- name: cur_size_all_mem_tables
  endpoint:
    path: /metrics/cur-size-all-mem-tables
    method: GET
    data_selector: metrics
    params: {}
- name: migrations_stream
  endpoint:
    path: /ksql/migrations/stream
    method: GET
    data_selector: MIGRATION_EVENTS
- name: migrations_table
  endpoint:
    path: /ksql/migrations/table
    method: GET
    data_selector: MIGRATION_SCHEMA_VERSIONS
- name: ksql.migrations.dir.override
- name: processing_log
  endpoint:
    path: /processing/log
    method: GET
    data_selector: log_entries
- name: ORDERS
  endpoint:
    path: /create_table_orders
    method: CREATE
    data_selector: records
    params:
      FORMAT: JSON
      KEY_FORMAT: KAFKA
      VALUE_FORMAT: JSON
- name: KEYLESS_STREAM
  endpoint:
    path: /create_stream_keyless
    method: CREATE
    data_selector: records
    params:
      KEY_FORMAT: NONE
      VALUE_FORMAT: JSON
      KAFKA_TOPIC: foo
- name: delim_stream
  endpoint:
    path: /create_stream_delim
    method: CREATE
    data_selector: records
    params:
      FORMAT: DELIMITED
      VALUE_DELIMITER: 
- name: x
  endpoint:
    path: /create_stream_x
    method: CREATE
    data_selector: records
    params:
      VALUE_FORMAT: JSON
- name: y
  endpoint:
    path: /create_stream_y
    method: CREATE
    data_selector: records
    params:
      WRAP_SINGLE_VALUE: 'false'
- name: USERS
  endpoint:
    path: /create_stream_users
    method: CREATE
    data_selector: records
    params:
      VALUE_FORMAT: JSON
- name: ksql_logging_server_rate_limited_request_paths
  endpoint:
    path: /ksql/logging/rate-limited/request/paths
    method: GET
    data_selector: paths
    params: {}
- name: ksql_metrics_tags_custom
  endpoint:
    path: /ksql/metrics/tags/custom
    method: GET
    data_selector: tags
    params: {}
- name: schemas
  endpoint:
    path: /schemas
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: user_accounts
  endpoint:
    path: /v1/user-accounts
    method: GET
    data_selector: data
    params: {}
- name: service_quotas
  endpoint:
    path: /v1/service-quotas
    method: GET
    data_selector: data
    params: {}
- name: riderLocations
  endpoint:
    path: /services/data/vXX.X/sobjects/riderLocations
    method: GET
    data_selector: records
- name: currentLocation
  endpoint:
    path: /services/data/vXX.X/sobjects/currentLocation
    method: GET
    data_selector: records
- name: ridersNearMountainView
  endpoint:
    path: /services/data/vXX.X/sobjects/ridersNearMountainView
    method: GET
    data_selector: records
- name: topic
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: ksql_cluster
  endpoint:
    path: /ksql/terminate
    method: POST
    data_selector: response
- name: full_access_to_subject
  endpoint:
    path: /iam/rbac/role-binding
    method: CREATE
    data_selector: role-binding
    params:
      principal: User:$USER_NAME
      role: ResourceOwner
      resource: Subject:${SINK_TOPIC_NAME}-value
      kafka_id: $KAFKA_ID
      schema_registry_cluster: $SR_ID
- name: read_only_access_to_topic
  endpoint:
    path: /iam/rbac/role-binding
    method: CREATE
    data_selector: role-binding
    params:
      principal: User:$USER_NAME
      role: ResourceOwner
      resource: Topic:$TOPIC_NAME
      kafka_id: $KAFKA_ID
- name: read_only_access_to_subject
  endpoint:
    path: /iam/rbac/role-binding
    method: CREATE
    data_selector: role-binding
    params:
      principal: User:$USER_NAME
      role: DeveloperRead
      resource: Subject:${TOPIC_NAME}-value
      kafka_id: $KAFKA_ID
      schema_registry_cluster: $SR_ID
- name: grant_write_access_to_topic
  endpoint:
    path: /iam/rbac/role-binding
    method: CREATE
    data_selector: role-binding
    params:
      principal: User:$USER_NAME
      role: DeveloperWrite
      resource: Topic:$TOPIC_NAME
      kafka_id: $KAFKA_ID
- name: delete_access_to_topic
  endpoint:
    path: /iam/rbac/role-binding
    method: CREATE
    data_selector: role-binding
    params:
      principal: User:$USER_NAME
      role: DeveloperManage
      resource: Topic:$TOPIC_NAME
      kafka_id: $KAFKA_ID
- name: kafka_topic
  endpoint:
    path: /topics
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: kafka_topic
  endpoint:
    path: /v1/topics
    method: GET
    data_selector: topics
    params: {}
- name: consumer_group
  endpoint:
    path: /v1/consumer-groups
    method: GET
    data_selector: consumerGroups
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: records
    params: {}
- name: broker
  endpoint:
    path: /broker
    method: GET
    data_selector: records
    params: {}
- name: schema-registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: records
    params: {}
- name: ksqldb-server
  endpoint:
    path: /ksqldb-server
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: mysql
  endpoint:
    path: /etc/mysql/conf.d/custom-config.cnf
    method: GET
    data_selector: configuration
    params: {}
- name: docker-compose
  endpoint:
    path: docker-compose.yml
    method: GET
    data_selector: services
    params: {}
- name: calls
  endpoint:
    path: /call-center-db.call-center.calls
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /customers.public.customers
    method: CREATE
    data_selector: ''
    params: {}
- name: orders
  endpoint:
    path: /my-replica-set.logistics.orders
    method: CREATE
    data_selector: ''
    params: {}
- name: shipments
  endpoint:
    path: /my-replica-set.logistics.shipments
    method: CREATE
    data_selector: ''
    params: {}
- name: enriched_orders
  endpoint:
    path: /shipped_orders
    method: GET
    data_selector: _source
    params: {}
- name: broker
  endpoint:
    path: /broker
    method: GET
- name: schema-registry
  endpoint:
    path: /schema-registry
    method: GET
- name: ksqldb-server
  endpoint:
    path: /ksqldb-server
    method: GET
- name: broker
  endpoint:
    path: /v1/cluster
    method: GET
    data_selector: cluster
- name: schema-registry
  endpoint:
    path: /subjects
    method: GET
    data_selector: subjects
- name: ksqldb-server
  endpoint:
    path: /info
    method: GET
    data_selector: info
- name: possible_anomalies
  endpoint:
    path: /possible_anomalies
    method: GET
    data_selector: records
- name: transactions
  endpoint:
    path: /transactions
    method: INSERT
    data_selector: records
- name: transactions
  endpoint:
    path: /transactions
    method: POST
    data_selector: records
    params: {}
- name: possible_anomalies
  endpoint:
    path: /possible_anomalies
    method: GET
    data_selector: records
    params: {}
- name: possible_anomalies
  endpoint:
    path: /services/data/vXX.X/sobjects/PossibleAnomaly
    method: GET
    data_selector: records
- name: transactions
  endpoint:
    path: /api/v1/transactions
    method: GET
    data_selector: records
- name: streaming_application
  endpoint:
    path: /streaming/applications
    method: GET
    data_selector: records
    params: {}
- name: clickstream
  endpoint:
    path: /clickstream
    method: GET
    data_selector: records
- name: clickstream_codes
  endpoint:
    path: /clickstream_codes
    method: GET
    data_selector: records
- name: clickstream_users
  endpoint:
    path: /clickstream_users
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: leader_metrics
  endpoint:
    path: /kafka.server:type=IntelligentReplication
    method: GET
    data_selector: metrics
    params: {}
- name: follower_metrics
  endpoint:
    path: /kafka.server:type=IntelligentReplication
    method: GET
    data_selector: metrics
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: default-configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
- name: delete-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: DELETE
- name: unregister-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: DELETE
- name: broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
- name: links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
- name: link-name
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: GET
- name: link-configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs
    method: GET
- name: link-config-name
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: GET
- name: alter-link-configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs:alter
    method: POST
- name: link-mirrors
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors
    method: GET
- name: mirrors
  endpoint:
    path: /clusters/{cluster_id}/links/-/mirrors
    method: GET
- name: mirror-topic-name
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors/{mirror_topic_name}
    method: GET
- name: promote-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:promote
    method: POST
- name: failover-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:failover
    method: POST
- name: pause-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
- name: resume-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:resume
    method: POST
- name: balancer-status
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
- name: any-uneven-load
  endpoint:
    path: /clusters/{cluster_id}/balancer/any-uneven-load
    method: GET
- name: broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
- name: broker-id-tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks
    method: GET
- name: task-type
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks/{task_type}
    method: GET
- name: broker-id-task-type
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks/{task_type}
    method: GET
- name: broker-replica-exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
- name: broker-id-replica-exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions/{broker_id}
    method: GET
- name: create-replica-exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:create
    method: POST
- name: delete-replica-exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:delete
    method: DELETE
- name: remove-broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: POST
- name: broker-id-remove-task
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks/{broker_id}
    method: POST
- name: replica-status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
- name: topic-name-replica-status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/replica-status
    method: GET
- name: topic-name-partition-id-replica-status
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replica-status
    method: GET
- name: default-configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
- name: delete-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: DELETE
- name: unregister-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: DELETE
- name: broker-info
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
- name: cluster-links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
- name: balancer-status
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
- name: broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
- name: broker-replica-exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
- name: remove-broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: POST
- name: replica-status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
- name: json_messages
  endpoint:
    path: /topics/jsontest
    method: POST
    data_selector: records
- name: avro_messages
  endpoint:
    path: /topics/avrokeytest2
    method: POST
    data_selector: records
- name: binary_messages
  endpoint:
    path: /topics/binarytest
    method: POST
    data_selector: records
- name: protobuf_messages
  endpoint:
    path: /topics/jsonschematest
    method: POST
    data_selector: records
- name: json_schema_messages
  endpoint:
    path: /topics/jsonschematest
    method: POST
    data_selector: records
- name: topic_metadata
  endpoint:
    path: /topics
    method: GET
    data_selector: topics
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: default-configs
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/default-configs
    method: GET
- name: delete-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers:delete
    method: DELETE
- name: unregister-broker
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}:unregister
    method: DELETE
- name: broker-info
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}
    method: GET
- name: links
  endpoint:
    path: /clusters/{cluster_id}/links
    method: GET
- name: link-info
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}
    method: GET
- name: link-configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs
    method: GET
- name: link-config-info
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs/{config_name}
    method: GET
- name: alter-link-configs
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/configs:alter
    method: POST
- name: link-mirrors
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors
    method: GET
- name: link-mirror-info
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors/{mirror_topic_name}
    method: GET
- name: promote-link-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:promote
    method: POST
- name: failover-link-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:failover
    method: POST
- name: pause-link-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:pause
    method: POST
- name: resume-link-mirror
  endpoint:
    path: /clusters/{cluster_id}/links/{link_name}/mirrors:resume
    method: POST
- name: balancer-status
  endpoint:
    path: /clusters/{cluster_id}/balancer
    method: GET
- name: any-uneven-load
  endpoint:
    path: /clusters/{cluster_id}/balancer/any-uneven-load
    method: GET
- name: broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks
    method: GET
- name: broker-task-info
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks
    method: GET
- name: task-type
  endpoint:
    path: /clusters/{cluster_id}/brokers/-/tasks/{task_type}
    method: GET
- name: broker-task-type-info
  endpoint:
    path: /clusters/{cluster_id}/brokers/{broker_id}/tasks/{task_type}
    method: GET
- name: broker-replica-exclusions
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions
    method: GET
- name: broker-replica-exclusion-info
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions/{broker_id}
    method: GET
- name: create-broker-replica-exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:create
    method: POST
- name: delete-broker-replica-exclusion
  endpoint:
    path: /clusters/{cluster_id}/broker-replica-exclusions:delete
    method: DELETE
- name: remove-broker-tasks
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks
    method: POST
- name: remove-broker-task-info
  endpoint:
    path: /clusters/{cluster_id}/remove-broker-tasks/{broker_id}
    method: GET
- name: replica-status
  endpoint:
    path: /clusters/{cluster_id}/topics/-/partitions/-/replica-status
    method: GET
- name: replica-status-by-topic
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/-/replica-status
    method: GET
- name: replica-status-by-partition
  endpoint:
    path: /clusters/{cluster_id}/topics/{topic_name}/partitions/{partition_id}/replica-status
    method: GET
- name: ksqlDB
  endpoint:
    path: /ksql
    method: POST
    data_selector: results
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /v1/clusters/status
    method: GET
    data_selector: status
- name: info
  endpoint:
    path: /v1/info
    method: GET
    data_selector: info
- name: brokers.list
  endpoint:
    path: /brokers
    method: GET
- name: consumer.commit
  endpoint:
    path: /consumers/{group}/instances/{instance}/offsets
    method: POST
- name: consumer.create
  endpoint:
    path: /consumers/{group}
    method: POST
- name: consumer.delete
  endpoint:
    path: /consumers/{group}/instances/{instance}
    method: DELETE
- name: consumer.topic.read-avro
  endpoint:
    path: /consumers/{group}/instances/{instance}/topics/{topic}
    method: GET
    headers:
      Accept: application/vnd.kafka.avro.v1+json
- name: consumer.topic.read-binary
  endpoint:
    path: /consumers/{group}/instances/{instance}/topics/{topic}
    method: GET
    headers:
      Accept: application/vnd.kafka.binary.v1+json
- name: partition.get
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: GET
- name: partition.produce-avro
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.avro.v1+json
- name: partition.produce-json
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.json.v1+json
- name: partition.produce-binary
  endpoint:
    path: /topics/{topic}/partitions/{partition}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.binary.v1+json
- name: partitions.list
  endpoint:
    path: /topics/{topic}/partitions
    method: GET
- name: topic.get
  endpoint:
    path: /topics/{topic}
    method: GET
- name: topic.produce-avro
  endpoint:
    path: /topics/{topic}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.avro.v1+json
- name: topic.produce-binary
  endpoint:
    path: /topics/{topic}
    method: POST
    headers:
      Content-Type: application/vnd.kafka.binary.v1+json
- name: brokers.list
  endpoint:
    path: /topics
    method: GET
- name: client-REST Proxy connection
  endpoint:
    path: /client-REST-Proxy
    method: POST
- name: REST Proxy-broker connection
  endpoint:
    path: /REST-Proxy-broker
    method: POST
- name: metadata_service
  endpoint:
    path: /metadata
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: topics
  endpoint:
    path: /topics/test
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Confluent Platform
  endpoint:
    path: /versions
    method: GET
    data_selector: versions
- name: Apache Flink
  endpoint:
    path: /flink
    method: GET
    data_selector: flink_versions
- name: Kubernetes Operator
  endpoint:
    path: /kubernetes
    method: GET
    data_selector: kubernetes_versions
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cmf
  endpoint:
    path: /confluentinc/confluent-manager-for-apache-flink
    method: GET
    data_selector: records
- name: Kafka Cluster
  endpoint:
    path: /clusters
    method: GET
    data_selector: clusters
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: cluster_status
  endpoint:
    path: /ksql
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /info
    method: GET
    data_selector: info
    params: {}
- name: streams
  endpoint:
    path: /ksql/streams
    method: GET
    data_selector: streams
    params: {}
- name: tables
  endpoint:
    path: /ksql/tables
    method: GET
    data_selector: tables
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: encryption
  endpoint:
    path: /secrets
    method: POST
    data_selector: secrets
    params:
      kubernetesSecretName: cmf-encryption-key
      kubernetesSecretProperty: encryption-key
- name: cluster_status
  endpoint:
    path: /ksqlDB/rest/api/v1/cluster/status
    method: GET
    data_selector: status
    params: {}
- name: info
  endpoint:
    path: /ksqlDB/rest/api/v1/info
    method: GET
    data_selector: info
    params: {}
- name: encryption
  endpoint:
    path: /kubernetes/secrets
    method: POST
    data_selector: secrets
    params:
      kubernetesSecretName: cmf-encryption-key
      kubernetesSecretProperty: encryption-key
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: streaming_endpoint
  endpoint:
    path: /api/v1/stream
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: flink_application
  endpoint:
    path: /flink/application
    method: POST
    data_selector: spec
    params: {}
notes:
- Requires OAuth2 authentication setup
- Rate limits apply to API calls
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- This quick start gets you up and running with Confluent Cloud using a Basic Kafka
  cluster.
- You need to have a Stream Governance package to get started.
- Docker memory is allocated minimally at 6 GB on Mac.
- Confluent connectors will not support data systems or versions that are no longer
  supported by their vendors.
- Preview features are not currently supported and are not recommended for production
  use.
- Kafka Connect can ingest entire databases or collect metrics from all your application
  servers into Kafka topics.
- Kafka Connect provides a low barrier to entry and low operational overhead.
- Schema Registry is available on both Confluent Cloud and Confluent Platform.
- Fully-managed data streaming platform with a cloud-native Kafka engine (KORA) for
  elastic scaling, with enterprise security, stream processing, governance.
- Uses OAuth2 with refresh token  requires setup of connected app in Confluent Cloud
- Flink is fully managed on Confluent Cloud and autoscales up and down with your workloads.
- You pay only for what you use, not what you provision.
- You must have Confluent Platform installed to run the examples.
- The replication factor should be set to 3 for real-world scenarios.
- Control Center ships, installs, and runs independently of Confluent Platform as
  of version 8.0.
- The monitoring backend now uses Prometheus for metrics collection.
- Ensure that the KAFKA_CLUSTER_ID is available as an environment variable when starting
  brokers.
- Control Center depends on embedded REST Proxy to manage brokers.
- Control Center acts as an OTLP receiver, listening on localhost:9090 for incoming
  metrics.
- Use Kafka REST for producing and consuming messages
- Auto-generated message data can be sent to topics using kafka-producer-perf-test
- All data is in Avro format, uses Confluent Schema Registry
- This example showcases a secure Confluent Platform for educational purposes and
  is not meant to be complete best practices.
- In production, Confluent Control Center should be deployed with a valid license
  in reduced infrastructure mode.
- In production, you should not deploy all Confluent Platform services on a single
  host as shown in cp-demo.
- This option is not supported with Gitpod.
- Use curl to create and list topics with embedded REST Proxy.
- Confluent Cloud example uses real resources that may be billable
- The hosted monitoring endpoint is in preview and will eventually be renamed to /v2/metrics/health-plus/query
- If the start script does not complete successfully, go through the troubleshooting
  steps.
- Verify that the status of all the Docker containers show Up state.
- 'In the advanced Docker preferences settings, verify the following resources are
  allocated to Docker: Memory: at least 8 GB (default is 2 GB), CPU cores: at least
  2 cores.'
- If there are any errors that indicate issues with TLS communication, force TLS certificates
  to be regenerated by starting the script with CLEAN=true.
- Uses OAuth2 with refresh token  requires setup of connected app in Confluent
- Make sure to monitor API usage limits
- If you are using a MacBook with an M1 or later chip, you can use the Confluent CLI
  and the ARM64 version of the Confluent Platform Docker images for local install.
- The `confluent local` subcommands are currently not supported on the ARM64 environment.
- Confluent Platform supports both ARM64 and X86 hardware architecture.
- Identical hardware across all nodes is essential for high availability and reliable
  performance.
- Control Center requires a separate installation.
- Enterprise support is not provided unless specific packages are installed.
- Enterprise support is not provided unless the `confluent-security` package is installed
  on all nodes and a valid Enterprise license key is configured.
- You should enable compression to help mitigate the performance impacts while maintaining
  business continuity.
- You should disable vMotion and disk snapshotting for Confluent Platform as the features
  could cause a full cluster outage when used with Kafka.
- Some objects may return nulls in deeply nested fields
- You must minimally configure the following components.
- For Kafka in KRaft mode, configure a node to be a broker or a controller.
- You must minimally configure the process.roles, node.id, and controller.quorum.voters.
- Minimum of three brokers and three controllers recommended for production.
- For Kafka in KRaft mode, you must configure a node to be a broker or a controller.
- Typically in a production environment, you should have a minimum of three brokers
  and three controllers.
- You must minimally configure process.roles, node.id, and controller.quorum.voters
  for each node.
- This configuration is for a three node multi-node cluster.
- RHEL 7 support is deprecated in Confluent Platform 7.x and will be removed in Confluent
  Platform 8.x.
- Starting with Confluent Platform 8.0, the librdkafka, Avro, and libserdes C/C++
  client packages will only be available from the https://packages.confluent.io/clients
  location.
- Uses OAuth2 with refresh token.
- Uses OAuth2 with refresh token  requires setup of connected app in ksqlDB
- Some queries may return nulls in deeply nested fields
- 'Multi-node Environment: For more information, see Configure a Multi-Node Confluent
  Platform Environment with Docker.'
- 'Persistent Data (Mounted Volumes): When deploying Kafka images, you should always
  use Mount Docker External Volumes in Confluent Platform for the file systems those
  images use for their persistent data.'
- 'Bridge Networking vs. Host Networking: Bridge networking is currently only supported
  on a single host.'
- 'Adding Connectors to the Kafka Connect Image: Here are the methods to add new connectors
  to the Kafka Connect image.'
- 'Supported Java: The Confluent Docker images are tested and shipped with Temurin
  JDK.'
- 'Untested Features: The images are not currently tested on Docker Swarm.'
- KRaft mode is required as of Confluent Platform 8.0.
- Uses Docker to run ksqlDB Server.
- KRaft mode is required for metadata management.
- KRaft combined mode is for local experimentation only.
- Uses SASL_SSL for secure connections
- Default number of replicas for topics created by ksqlDB is 3
- Combined mode is for local experimentation and is not supported in production.
- Requires configuration of SASL mechanism and JAAS for authentication
- Multiple bootstrap servers can be used in the form host1:port1,host2:port2,host3:port3...
- You need to install Confluent for Kubernetes image using Helm.
- Using security with your Kafka clusters is optional, but recommended.
- When you enable security for the Confluent Platform, you must pass secrets such
  as credentials, certificates, keytabs, Kerberos configuration, and more to the container.
- Uses Docker for building and running Confluent Platform images.
- Docker memory is allocated minimally at 8 GB.
- If using Docker Desktop for Mac, change the default allocation to 8 GB.
- Older versions of the Oracle JDK are not recommended for production use as they
  no longer receive security patches.
- Ensure the log directory is world-writable.
- The systemd service unit files are optimized for staging and production deployments
  on Linux-based systems.
- Default component configuration file points to component-specific data directories
  /var/lib/<component>.
- CFK actively monitors the custom resources to ensure their state matches the desired
  state.
- CFK is the next generation of Confluent Operator.
- Developer licenses never expire.
- Some responses may include additional metadata
- Confluent Platform 8.1 does not support ZooKeeper for metadata management.
- If you are currently running ZooKeeper mode, you must upgrade to a version of Confluent
  Platform that supports migrating to KRaft, migrate to KRaft and then upgrade to
  Confluent Platform 8.1.
- While enabling Telemetry and Health+ is highly encouraged and beneficial to minimize
  downtime, these features are not mandatory.
- Enabling Telemetry and Health+ is highly encouraged and beneficial to minimize downtime,
  these features are not mandatory.
- Confluent does not provide support for issues encountered during the operating system
  upgrade itself. We recommend that you reach out to your OS provider for any OS-specific
  issues.
- Before upgrading to any Confluent Platform 8.x version, remove the message.format.version,
  message.downconversion.enable, and message.timestamp.difference.max.ms properties
  from your static configuration files.
- 'Command to start a broker: kafka-server-start -daemon /etc/kafka/broker.properties'
- You must migrate to Log4j 2 before upgrading to Confluent Platform 8.1.
- Starting with Confluent Platform 8.0, certain client packages will only be available
  from the specified location.
- You should not move from Kafka to Confluent Platform while also doing a migration
  from ZooKeeper to KRaft mode.
- An existing cluster can typically be upgraded easily by performing a rolling restart
  of Kafka nodes.
- Confluent Server requires that the confluent.license property be set to a valid
  license string in each broker properties file.
- This migration procedure applies to Confluent Platform versions 8.1.0 and later.
- Before proceeding, ensure you have a complete backup of your original metadata directory.
- ZooKeeper has been removed from Confluent Platform version 8.1 and later, so you
  must migrate to KRaft before you upgrade to Confluent Platform version 8.1 or later.
- Migration of production clusters is generally available in Confluent Platform version
  7.6.1; however, it is recommended that you migrate Confluent Platform version 7.7.0
  or later.
- Always use a supported operating system to avoid security vulnerabilities and compatibility
  issues.
- Upgrade to Confluent Platform 7.7.1 or later in ZooKeeper mode.
- Inter-node communication problems can cause cluster instability. Check for common
  issues like firewall-blocked ports, incorrect listener configurations, or DNS resolution
  problems.
- Some queries may return nulls in results
- Confluent recommends upgrading to the latest client since current fixes are generally
  not found in older clients.
- Consumers can fetch and consume messages from out-of-sync follower replicas if using
  a fetch-from-follower configuration.
- All members in the group must advertise a compatible, ordered list of assignors.
- Do not switch to listing only CooperativeStickyAssignor until all members run client
  versions that support it (2.4.0 or later); otherwise the group fails to find a common
  assignor.
- Static membership (group.instance.id) further reduces movement during restarts and
  pairs well with cooperative rebalancing.
- Share groups are a preview feature available to Java clients only.
- Confluent Cloud users cannot set broker configuration properties.
- Share groups are not enabled by default
- Broker administrators must enable share groups
- The producer sends a produce request to the leader of the partition.
- Messages written to the partition leader are not immediately readable by consumers.
- Requires setup of connected app in api
- Clients to Schema Registry track with Confluent Platform versions.
- Schema Registry clients 7.7.4 and later include support for auto retries for 429
  exceptions.
- Kafka clients need to handle errors with retries, or fail gracefully until a solution
  can be implemented to resolve the error.
- API supports REST calls for management operations.
- Ensure that the client ID and secret are securely stored.
- Migrate from mTLS to OAuth Authentication
- Uses API key and secret for authentication
- Client requires bootstrap.servers to be specified.
- Group ID is mandatory for consumers.
- Confluent tested FIPS compliance for the client using OpenSSL 3.0.
- Older versions of OpenSSL have not been verified (although they may work).
- The .NET client supports idempotent produce.
- Use the EnableIdempotence configuration property for exactly once delivery guarantees.
- To view a list of all supported configuration properties, see librdkafka Configurations.
- The default partitioner in Java producer uses the murmur2 hash function while the
  default partitioner in librdkafka uses crc32.
- The poll timeout is hard-coded to 500 milliseconds.
- The consumer intentionally avoids a specific threading model.
- It is not safe for multi-threaded access.
- Some API calls may have rate limits
- The easiest way to follow this tutorial is with Confluent Cloud because you dont
  have to run a local Kafka cluster.
- You can alternatively use the supported CLI or REST API, or the community-supported
  ccloud-stack utility for Confluent Cloud.
- If you dont want to use Confluent Cloud, you can also use this tutorial with a
  Kafka cluster running on your local host or any other remote server.
- Uses OAuth2 with refresh token  requires setup in Confluent Cloud
- Required connection configs for Kafka producer, consumer, and admin
- Best practice for higher availability in Apache Kafka clients prior to 3.0
- Uses OAuth2 for authentication
- Ensure proper permissions are set for API access
- Uses OAuth2 with refresh token  requires setup of connected app in Confluent.
- Some queries may have performance limitations.
- Uses Confluent Cloud for simplicity
- 'Best practice for higher availability in Apache Kafka clients prior to 3.0: session.timeout.ms=45000'
- 'Best practice for Kafka producer to prevent data loss: acks=all'
- Requires setup of ksqlDB server
- Check for compatibility with Kafka version
- Uses Confluent Cloud for Kafka cluster setup.
- Uses Confluent Cloud for Kafka cluster setup
- Follow the instructions in the provided GitHub repository for configuration
- Best practice for higher availability in librdkafka clients prior to 1.7
- 'Required connection configs for Kafka producer, consumer, and admin: bootstrap.servers={{
  BROKER_ENDPOINT }}, security.protocol=SASL_SSL, sasl.mechanisms=PLAIN, sasl.username={{
  CLUSTER_API_KEY }}, sasl.password={{ CLUSTER_API_SECRET }}.'
- 'Best practice for higher availability in librdkafka clients prior to 1.7: session.timeout.ms=45000.'
- Check server health for operational status
- Use Confluent Cloud for easy setup without a local Kafka cluster
- Local Kafka cluster can be used as an alternative
- This class is experimental and likely to be removed, or subject to incompatible
  API changes in future versions of the library.
- This class is not user-instantiable.
- Timestamps require broker version 0.10.0.0 or later.
- The new consumer group protocol defined in KIP-848 is not enabled by default.
- 'On close() or unsubscribe with auto-commit enabled: Member retries committing offsets
  until a timeout expires.'
- Currently uses the default remote session timeout.
- Future KIP-1092 will allow custom commit timeouts.
- KIP-848 fences new member on duplicate group.instance.id
- KIP-848 retries until remote timeout
- KIP-848 does not return error on subscription if topic missing
- 'The overall request timeout in seconds, including broker lookup, request transmission,
  operation time on broker, and response. Default: socket.timeout.ms/1000.0.'
- '''message'' and ''offsets'' are mutually exclusive.'
- Stored offsets will be committed according to auto.commit.interval.ms or manual
  offset-less commit().
- enable.auto.offset.store must be set to False when using this API.
- Optional; default is 'classic'
- KIP-848 supports upgrade/downgrade from/to classic and consumer protocols
- Session & heartbeat now controlled by broker
- The stored offsets will be committed according to auto.commit.interval.ms or manual
  offset-less commit().
- Note that enable.auto.offset.store must be set to False when using this API.
- Deprecated since version 2.0.2.
- This class will be removed in a future version of the library.
- A group made up entirely of classic consumers runs under the classic protocol.
- The group is upgraded to the consumer protocol as soon as at least one consumer
  protocol member joins.
- The group is downgraded back to the classic protocol if the last consumer protocol
  member leaves while classic members remain.
- Both rolling upgrade (classic  consumer) and rolling downgrade (consumer  classic)
  are supported.
- Pre-built binary support will be dropped after the EOL of the node version or the
  OS.
- The Confluent Kafka Go client is safe for concurrent use.
- Topic creation is non-atomic and may succeed for some topics but fail for others,
  make sure to check the result for topic-specific errors.
- Due to the buffering nature of channels, the use of the events channel risks receiving
  outdated events and messages.
- Needs to be explicitly enabled by setting the `enable.partition.eof` configuration
  property to true.
- Build version go1.22.2.
- The 3.7 release marked these older, client protocol API versions deprecated with
  a notice that they will be completely dropped by 4.0.
- Any pre-8.0 Confluent Platform version will maintain compatibility with the older
  client protocol API versions.
- With the release of Kafka 4.0 in 2025, Confluent sets the new baseline for client
  protocol API versions to Kafka 2.1.0.
- Many Kafka clients may not report their official software library or version in
  their requests.
- Inspect a clients `clientId` to identify the client type.
- Some client identifiers have custom formats making it hard to determine their version
  or library.
- Some API responses may vary based on user permissions
- Uses OAuth2 for authentication.
- Confluent Cloud connections require reauthenticating after four hours, and you will
  be prompted to reauthenticate.
- Preview links for non-default organizations work only after switching to the non-default
  organization in the Confluent Cloud Console in your browser.
- OAuth2 is required for authentication.
- MQTT Proxy is deprecated in Confluent Platform version 7.9 and will be removed in
  a future version.
- 'Kafka: 6.0.0-ccs'
- MQTT Clients supporting the MQTT 3.1.1 protocol
- The MQTT Proxy is deprecated in Confluent Platform version 7.9 and will be removed
  in a future version.
- This website includes content developed at the Apache Software Foundation under
  the terms of the Apache License v2.
- No processing cluster required
- No external dependencies other than Kafka
- This example showcases an entire end-to-end streaming ETL deployment, built around
  the microservices.
- Requires API key and secret for authentication
- Some endpoints may have rate limits
- This example illustrates a microservices ecosystem using Kafka.
- This is an example code, not a production system.
- Certain elements are left for further work.
- Assumes existence of ksqlDB stream 'orders' and table 'customers_table'.
- This example showcases an entire end-to-end streaming ETL deployment, built around
  the microservices described above.
- All the services are client applications written in Java, and they use the Kafka
  Streams API.
- Ensure proper configuration for OAuth clients.
- Save the API key and secret. You require this information to configure your client
  applications.
- Ensure proper setup of the OAuth2 authentication.
- Uses at-least-once and exactly-once processing guarantees.
- Kafka does not provide any guarantee about timestamp order, so records in a topic
  arent ordered by their timestamp and can be 'out-of-order'.
- Records that arrive after a window is closed are dropped and not processed.
- Kafka Streams builds on the Apache Kafka producer and consumer APIs.
- State stores can be RocksDB, in-memory hash map, or another structure.
- Kafka Streams does not use a backpressure mechanism because it does not need one.
- The SHUTDOWN_APPLICATION option is best-effort only and doesnt guarantee that all
  application instances will be stopped.
- Each stream processing application must have a unique ID.
- This extractor logs a warn message and returns the invalid timestamp to Kafka Streams,
  which will not process but silently drop the record.
- If you cant extract a valid timestamp, you can either throw an exception, return
  a negative timestamp, or estimate a timestamp.
- Returning a negative timestamp results in data loss, as the corresponding record
  isnt processed, but instead, its dropped silently.
- Increase the replication factor to 3 to ensure that the internal Kafka Streams topic
  can tolerate up to 2 broker failures.
- Kafka Streams uses different default values for some of the underlying client configurations.
- Kafka Streams makes your stream processing applications elastic and scalable.
- You can add and remove processing capacity dynamically during application runtime
  without any downtime or data loss.
- Data should be equally distributed across topic partitions.
- Processing workload should be equally distributed across topic partitions.
- Some API endpoints may have rate limits
- State stores are fault-tolerant.
- The initializer and aggregator must be stateless.
- Typically, you should only disable record caches for testing or debugging purposes.
  Under normal circumstances, its better to leave record caches enabled.
- Data must be co-partitioned
- Causes data re-partitioning of a stream if and only if the stream was marked for
  re-partitioning
- Suppression emits nothing for a window until it closes, then emits the final result.
- Buffer can be configured with no upper bound in simple examples.
- Changing window duration in a join operation invalidates state store segments.
- Erasing RocksDB allows the rebuilding of internal topics without resetting after
  changing window duration.
- Custom time windows can be created by extending the Java Windows abstract class.
- Several deprecated methods in the Kafka Streams API have been removed and replaced
  with the Processor API.
- Uses a custom processor as a stream sink is not a recommended pattern.
- All Serdes specified in the config will be ignored by the Scala APIs.
- Naming processors in topology enhances readability and context.
- It is a good practice to name your processing nodes when using the DSL.
- If you have an existing topology and you havent named your state stores, you should
  do so as a best practice.
- Optimizations are disabled by default, so if you dont want to enable optimizations,
  you dont have to take any action.
- If you have an existing application, a rolling upgrade is not possible.
- Some API responses may include pagination
- Versioned stores dont support caching or interactive queries.
- You cant version window stores or global tables.
- You must set default Serdes via a Properties instance or specify explicit Serdes
  when calling API methods.
- Querying state stores is always read-only to guarantee that the underlying state
  stores will never be mutated out-of-band.
- To enable remote state store discovery in a distributed Kafka Streams application,
  you must set the application.server property in the config properties instance.
- You can specify the total memory (RAM) size used for internal caching and compacting
  of records.
- The Kafka Streams record cache and the RocksDB cache are not mutually exclusive.
- By default, both are enabled in a Kafka Streams app.
- The default RocksDB block-cache size is 50 MB per store, but the default size of
  the Kafka Streams record cache is 10 MB for caching for the entire instance.
- Iterators should be closed explicitly to release resources
- User topics must be created and manually managed ahead of time.
- If security is enabled on the Kafka brokers, you must grant the underlying clients
  admin permissions to create internal topics.
- The internal repartition and changelog topics must be created with the correct number
  of partitions.
- Changelog topics must be created with log compaction enabled.
- Monitor your Kafka Streams application log files for such error messages to spot
  any misconfigured applications quickly.
- Requires setup of OAuth2 connected app in Confluent
- Rate limits may apply based on usage
- The application reset tool handles input, intermediate, and internal topics differently.
- Some operations may have rate limits or specific requirements
- Some objects may have nested structures that require parsing.
- This tutorial runs Confluent Platform in Docker.
- one needs to provide the Serdes twice, (1) when calling `StreamsBuilder#stream()`
  and (2) when calling `KStream#groupByKey()`
- Confluent distribution provides packages for `GenericAvroSerde` and `SpecificAvroSerde`
- adds APIs to be able to embed Kafka Connect into client applications
- one cannot use the `--key-serializer` argument in `kafka-console-producer` to serialize
  the key as a `Long`. As a result, in this example the key is serialized as a `String`.
  As a workaround, you could write your own kafka.common.MessageReader (e.g. check
  out the default implementation of LineMessageReader) and then you can specify `--line-reader`
  argument in `kafka-console-producer`.
- allows the connector to set the namespace in the schema.
- Some responses may contain nested structures
- Make sure to handle token expiration properly.
- Load balancing is automatic across tasks with no user involvement.
- 'The store/RocksDB performance appears low: The workload might be IO bound. This
  happens especially when using a hard disk drive, instead of an SSD.'
- 'RocksDBs file sizes appear larger than expected: RocksDB tends to allocate sparse
  files, hence although the file sized might appear large, the actual amount of storage
  consumed might be low.'
- 'The apps memory utilization seems high: If you have many stores in your topology,
  there is a fixed per-store memory cost.'
- All Kafka Streams metrics can be accessed programmatically through KafkaStreams#metrics()
- 'Metrics can be recorded at different levels: info, debug, trace'
- Restore consumers of an application are displayed separately.
- The Streams API uses a dedicated 'restore' consumer for fault tolerance and state
  management.
- Upgrading from any earlier Kafka Streams version to Confluent Platform 8.1.0 is
  supported.
- Enabling the protocol requires that brokers and clients are running Confluent Platform
  8.1 (Kafka 4.1).
- Kafka Streams depends on a RocksDB version that requires MacOS 10.14 or higher.
- Downgrading from Confluent Platform 7.5.x (Kafka Streams 3.5.x) or later to Confluent
  Platform 7.4.x or earlier requires special attention.
- Starting with Confluent Platform 7.3.0, source and sink node metrics for consumed
  and produced throughput are available in Kafka Streams.
- For distributed Kafka Streams applications, each instance must be paused and resumed
  separately.
- Kafka Streams applications dont run inside the Kafka brokers.
- The configuration setting offsets.retention.minutes controls how long Kafka remembers
  offsets in the special topic.
- By default, Kafka Streams applications dont modify the resultant records timestamp
  from its original source topics.
- 'Be careful when returning null: null record values have special semantics for tables,
  where they are interpreted as tombstones that will cause the deletion of record
  keys from the table.'
- If you get an exception similar to the one shown below, there are multiple possible
  causes.
- This error means that the timestamp extractor of your Kafka Streams application
  failed to extract a valid timestamp from a record.
- If you are using the default `FailOnInvalidTimestamp` timestamp extractor, it is
  most likely that your records do not carry an embedded timestamp.
- It is recommended to use the Scala wrapper to avoid this issue.
- There is a known issue with RocksDB when running in environments with just one CPU
  core. In some scenarios, the symptom is that the applications performance might
  be very slow or unresponsive.
- The workaround is to set a particular RocksDB configuration using the RocksDB config
  setter.
- ksqlDB for Confluent Platform includes enterprise features, like role-based access
  control.
- ksqlDB runs as a server which clients connect to in order to issue queries.
- Use 'CREATE STREAM' to register a stream on a topic.
- You can deploy ksqlDB by using Docker containers.
- ksqlDB CLI does not support automatic failover to another ksqlDB server.
- The ksqlDB REST API supports a 'server info' request at http://<server>:8088/info
  and a basic server health check endpoint at http://<server>:8088/healthcheck.
- ksqlDB CLI doesnt connect to ksqlDB server may occur due to incorrect port or server
  not running.
- OAuth/OIDC integration for various Confluent components
- If the 'merge repartition topics' optimization is enabled in your configuration,
  which is the default, upgrading to ksqlDB 8.0.0 may be unsafe, due to a known bug
  in KStream.processValues().
- If you are migrating to Confluent Platform 8.0.x from earlier versions, you may
  want to use existing Log4j configs, but this is not recommended.
- For ksqlDB RBAC, the CREATE STREAM and CREATE TABLE statements now require the ResourceOwner
  role for source topics, instead of DeveloperRead.
- All DDL statements that require permissions on a source topic must have the ResourceOwner
  role.
- Ensure that the RocksDB version is consistent across your environment.
- You may need to update the existing data to ensure compatibility with the current
  RocksDB version.
- This may involve steps like exporting the data before the version change and importing
  it after the change.
- You may be able to delete the state store so ksqlDB recreates the state store from
  the changelog topic.
- Some responses may be paginated
- Secure REST Proxy with OAuth/OIDC
- Events come from Apache Kafka topics.
- Events are stored in a stream, which is a Kafka topic with a defined schema.
- ksqlDB offers a SQL-like interface for transforming streams.
- ksqlDB enables stream processing, which is a way to compute over events as they
  arrive, rather than in batches at a later time.
- Events are added to a stream as rows, which are essentially Kafka records with extra
  metadata.
- For fault tolerance, ksqlDB uses state snapshots and stream replay.
- For environments that need to share connect clusters and provide predictable workloads,
  running Connect externally is the recommended deployment option.
- ksqlDB represents events using a key/value model similar to Kafka's notion of a
  record.
- Events in ksqlDB carry information about the time at which the event was true.
- A materialized view evaluates a query on the changes only instead of the entire
  table.
- Queries against materialized views are highly efficient.
- Use OAuth with ksqlDB
- Records may be out-of-order within the stream.
- Dont mix streams or tables that have different time semantics.
- ksqlDB is based on the Unix epoch time in the UTC timezone, and this can affect
  time windows.
- The default grace period is 24 hours.
- Tune the update frequency by using the ksql.streams.cache.max.bytes.buffering and
  ksql.streams.commit.interval.ms configuration settings.
- The specified retention period should be larger than the sum of window size and
  any grace period.
- Specify EMIT FINAL in your SELECT statement to suppress intermediate results.
- ksqlDB uppercases all identifiers by default, you need to use backticks to preserve
  the desired casing.
- Use SET 'auto.offset.reset' = 'earliest' to start all queries from the earliest
  point
- Requires setup of UDFs in ksqlDB.
- UDFs are loaded only once as ksqlDB server starts up.
- If you want to change the code of a UDF, create a new uberjar and restart the server.
- 'ksqlDB can run connectors in two different modes: embedded or external.'
- In embedded mode, ksqlDB runs connectors directly on its servers.
- Uses a Voluble connector to source random events into a Kafka topic.
- Begin queries with SET 'auto.offset.reset' = 'earliest';
- Begin by telling ksqlDB to start all queries from the earliest point in each topic.
- ksqlDB ships with a command line tool to test KSQL statements automatically.
- It doesnt require an active Apache Kafka or ksqlDB cluster.
- ksqlDB maintains state in order to accomplish stateful tasks such as aggregations.
- To ensure that all intermediate state is compatible, ksqlDB ensures that the intermediate
  schema is identical when changing a filter.
- Variables are case-insensitive.
- Single-quotes are removed during variable substitution.
- Ensure the cluster_id is specified for topic retrieval.
- Uses event-time for processing events
- Timestamp can be set as a string or as milliseconds since the Unix epoch
- Timestamps require data to be in yyyy-mm-ddThh:mm:ss[.S] format
- Make sure to set auto.offset.reset = earliest
- Currently, the join expression must be a single column equal comparison. Non-equi
  joins, or joins on multiple columns are not supported.
- Only stream-stream joins are windowed.
- ksqlDB provides best-effort on time synchronization, but there are no guarantees,
  which can cause missing results or leftRecord-NULL results.
- The limitations and restrictions described in the previous sections to each intermediate
  step in N-way joins.
- Although ksqlDB supports N-way joins in general, its not possible to have a FULL
  OUTER N-way table join in a single query.
- When you use ksqlDB to join streaming data, you must ensure that your streams and
  tables are co-partitioned, which means that input records on both sides of the join
  have the same configuration settings for partitions.
- The input records for the join must have the same key schema.
- The input records must have the same number of partitions on both sides.
- Both sides of the join must have the same partitioning strategy.
- Kafka guarantees the relative order of any two messages from one source partition
  only if they are both in the same partition after the repartition.
- ksqlDB automatically repartitions a stream if a join requires it, but for stream-table
  and table-table joins, ksqlDB rejects a join on a (right) tables column that is
  not the primary key.
- If you are using the same sources in more than one join that requires the data to
  be repartitioned, you may prefer to repartition manually to avoid ksqlDB repartitioning
  multiple times.
- If the PARTITION BY expression evaluates to NULL, the resulting row is produced
  to a random partition.
- Uses REST API for interaction with ksqlDB.
- The default name of a synthetic key column is ROWKEY.
- If any sources used in the join already contain a column named ROWKEY, the synthetic
  key column is named ROWKEY_1, or ROWKEY_2 if there exists a source column called
  ROWKEY_1.
- In Confluent Cloud, the ksql.functions.collect_list.limit config is set to 1000
  and cant be changed.
- In Confluent Cloud, the ksql.functions.collect_set.limit config is set to 1000 and
  cant be changed.
- The HISTOGRAM function limits the number of distinct values which can be counted
  to 1000, beyond which any additional entries are ignored.
- Using ALTER SYSTEM results in the new value being applied immediately across the
  cluster and restarts the shared runtime to take effect for existing queries.
- ALTER SYSTEM is available only from the ksql command line and only when connected
  to a ksqlDB cluster in Confluent Cloud.
- Uses OAuth2 with refresh token  requires setup of connected app in api.
- Some objects may return nulls in deeply nested fields.
- The TIMEOUT clause specifies the amount of time to wait for the assertion to succeed
  before failing.
- If the TIMEOUT clause is not present, then ksqlDB will use the timeout specified
  by the server configuration ksql.assert.schema.default.timeout.ms, which is 1000
  ms by default.
- If the assertion fails, then an error will be returned.
- If the TIMEOUT clause is not present, then ksqlDB will use the timeout specified
  by the server configuration ksql.assert.topic.default.timeout.ms, which is 1000
  ms by default.
- Creating a stream from a query creates a new backing topic with the specified number
  of partitions, if the topic doesnt exist already.
- Once a stream is created, you cant change the number of partitions.
- A stream can store its data in KEY, VALUE, or HEADERS / HEADER(key) columns.
- If you provide the IF NOT EXISTS clause, the statement wont fail if a stream with
  the same name already exists.
- To use the Avro, Protobuf, or JSON_SR formats, you must enable Schema Registry and
  set ksql.schema.registry.url in the ksqlDB Server configuration file.
- The JSON format doesnt require Schema Registry to be enabled.
- OAuth2 authentication requires a connected app and proper scope configuration.
- Some objects may have rate limits and require handling of retries
- Requires OAuth2 setup on Confluent Cloud
- Response format may vary based on query type
- DELETE TOPIC will not necessarily work if your Kafka cluster is configured to create
  topics automatically with auto.create.topics.enable=true.
- If the IF EXISTS clause is present, the statement doesnt fail if the stream doesnt
  exist.
- If the IF EXISTS clause is present, the statement doesnt fail if the table doesnt
  exist.
- This statement doesnt fail if the type is in use in active queries or user-defined
  functions, because the DROP TYPE statement doesnt track whether queries are using
  the type.
- This means that you can drop a type any time, and old queries continue to work.
- Records written into the stream are not timestamp-ordered with respect to other
  queries.
- The schema and partitioning column(s) produced by the query must match the streams
  schema and key, respectively.
- Tombstones are not supported with INSERT INTO.
- Pseudo columns are supported on a case by case basis.
- The *topicName* is case sensitive. Quote the name if it contains invalid characters.
- The PRINT statement supports the properties FROM BEGINNING, INTERVAL, and LIMIT.
- Some API calls may require specific permissions
- Protect Data in Motion with TLS Encryption
- Protect Sensitive Data Using Client-side Field Level Encryption
- RUN SCRIPT command supports a subset of ksqlDB statements.
- RUN SCRIPT is only available from the ksqlDB command line.
- Deprecated since 0.17.0
- Pull queries run with predictably low latency because materialized views are incrementally
  updated as new events arrive.
- Currently, we do not support pull queries against tables created by using a CREATE
  TABLE statement.
- Push queries are not shared; each client computes independent results.
- EMIT output refinement defaults to CHANGES unless set to FINAL.
- You can use the WINDOW clause only if the `from_item` is a stream.
- 'Windowing adds two additional system columns to the data, which provide the window
  bounds: `WINDOWSTART` and `WINDOWEND`.'
- If you specify a GRACE PERIOD for left/outer joins, the grace period defines when
  the left/outer join result is emitted.
- If you dont specify a grace period explicitly, the default grace period is 24 hours.
- The SHOW and LIST statements dont distinguish connectors that are created by using
  the ksqlDB from connectors that are created independently by using the Connect API.
- SHOW QUERIES lists queries running in the cluster.
- SHOW QUERIES EXTENDED lists queries running in the cluster in more detail.
- SHOW TOPICS does not display hidden topics by default.
- SHOW ALL TOPICS lists all topics, including hidden topics.
- Requires integration with Schema Registry for proper functioning
- Table functions are supported only on stream sources.
- The current implementation of table functions only allows a single column to be
  returned.
- Numeric types can be coerced to a wider numeric type.
- ARRAY types can be coerced to any other ARRAY type.
- MAP types can be coerced to any other MAP type.
- STRUCT types can be coerced to any other STRUCT type.
- Requires setup of connected app in Confluent
- Some fields may return nulls
- Undefines a currently defined variable.
- ksqlDB servers in a cluster discover each other through persistent queries.
- If you have no persistent queries running, then the /clusterStatus endpoint contains
  info for the particular server that was queried, rather than all servers in the
  cluster.
- Some endpoints may return nulls in deeply nested fields
- This endpoint was proposed to be deprecated in favor of the newer /query-stream
  endpoint.
- These endpoints are only available when using HTTP 2.
- Client is compatible only with ksqlDB deployments that are on version 0.10.0 or
  later.
- You must specify configuration parameters like bootstrap.servers and listeners.
- You must specify configuration parameters including required parameters like bootstrap.servers
  and listeners.
- The default settings are bootstrap.servers=localhost:9092 and listeners=http://0.0.0.0:8088.
- Secure ksqlDB with RBAC
- How-to Guides available for various operations
- Without Schema Registry, your ksqlDB applications can use only JSON or delimited
  formats.
- You must install Confluent Platform to run this application.
- The Confluent Platform images are distinct from the images that are used in this
  topic.
- OAuth2 authentication is required for accessing the API.
- Default mode - starts KSQLDB server
- Uses Docker to run ksqlDB server in interactive and headless modes.
- 'KSQL_BOOTSTRAP_SERVERS: A list of hosts for establishing the initial connection
  to the Kafka cluster.'
- 'KSQL_KSQL_SERVICE_ID: The service ID of the ksqlDB Server, which is used as the
  prefix for the internal topics created by ksqlDB.'
- 'KSQL_LISTENERS: A list of URIs, including the protocol, that the broker listens
  on.'
- Run ksqlDB Server in a Docker container.
- Default mode runs ksqlDB server on port 8088
- ksqlDB CLI connects to a running ksqlDB Server instance
- This script pings the ksqlDB Server at <ksql-server-ip-address>:8088 every five
  seconds, until it receives an HTTP 200 response.
- The previous script doesnt work with headless deployments of ksqlDB Server, because
  headless deployments dont have a REST API server.
- Headless deployments do not have a REST API server.
- JMX indicates that the JVM is up and responsive.
- This test is similar to confirming if the ksqlDB process is running, but a successful
  response doesnt necessarily mean that the ksqlDB service is fully operational.
- Properties set with KSQL_OPTS take precedence over those specified in the ksqlDB
  configuration file.
- By default, the ksqlDB server configuration file is located at <path-to-confluent>/etc/ksqldb/ksql-server.properties.
- If youre using Basic authentication, you should configure ksqlDB to use HTTPS for
  secure communication.
- Configuring listener for SSL encryption requires creating SSL key and trust stores.
- Unlike interactive deployments, there are no exposed REST APIs, so security is greatly
  simplified.
- You can configure ksqlDB to connect to Schema Registry over HTTP by setting the
  ksql.schema.registry.url to the HTTPS endpoint of Schema Registry.
- Interactive ksqlDB clusters require open access to create, read, write, delete topics,
  and use any consumer group.
- The authenticated ksqlDB user always requires DESCRIBE_CONFIGS permission on the
  CLUSTER resource type.
- The authenticated ksqlDB user requires DESCRIBE and READ permissions for each input
  topic.
- The authenticated ksqlDB user requires DESCRIBE and WRITE permissions on each output
  topic.
- ksqlDB will attempt to create any output topics that do not exist.
- The authenticated ksqlDB user requires CREATE permissions on the CLUSTER resource
  type to allow topic creation.
- The authenticated ksqlDB user requires DESCRIBE, READ, and WRITE permissions for
  each changelog and repartition TOPIC.
- All changelog and repartition topics are prefixed with _confluent-ksql-<ksql.service.id>.
- Some queries may return empty results if no data is available.
- There is no automatic failover of your CLI session to another ksqlDB Server if the
  original server that the CLI is connected to becomes unavailable.
- Any persistent queries you executed will continue to run in the ksqlDB cluster.
- Dont use the SET statement in the ksqlDB CLI to configure the registry endpoint.
- In-place upgrades are supported from ksqlDB 0.14.0 (CP 6.1.x) to 0.29.0 (CP 7.7.x)
  and later.
- The GRACE PERIOD clause is now supported in stream-stream joins since 0.20.0.
- In-place upgrades are supported from ksqlDB 0.9.0 to 0.10.0.
- In-place upgrades from pre-0.7.0 versions to 0.10.0 are not supported, as ksqlDB
  0.7.0 is not backward compatible.
- ALL, WINDOWSTART and WINDOWEND are now reserved identifiers
- Configure Security
- Archived documentation is not updated.
- Uses OAuth with ksqlDB
- ksqlDB isnt a good fit for BI reports, ad-hoc querying, or queries with random
  access patterns, because its a continuous query system on data streams.
- Dont deploy multi-tenant ksqlDB Server instances.
- ksqlDB currently doesnt have a mechanism to guarantee resource utilization fairness
  between queries.
- Run ksqlDB with KsqlBoundedMemoryRocksDBConfigSetter to configure a bound on usage
  across all RocksDB instances.
- Reserving 25 percent of available memory for RocksDB is a good starting point.
- Project/Filter is stateless and therefore doesnt need to account for state store
  memory. 8 GB are recommended for the Java heap space for record processing.
- ksqlDB nodes are CPU-bound; for a query, each byte consumes 1/R CPU-seconds.
- Each ksqlDB Server should have at least about 12 GB of memory.
- At-least-once semantics is enabled by default in your ksqlDB configuration, with
  `processing.guarantee="at_least_once"`.
- To enable exactly-once semantics, set `processing.guarantee="exactly_once_v2"` in
  your ksqlDB configuration.
- Your Kafka broker version must be 2.5 or later.
- If youre using the Confluent Platform distribution of ksqlDB, you need Confluent
  Platform version 5.5 or later.
- Uses OAuth2 authentication
- High availability is turned off by default, but you can enable it with specific
  server configuration parameters.
- In Confluent Cloud, ksqlDB clusters with 8 or 12 CSUs are automatically configured
  for high availability. High availability cant be enabled for Confluent Cloud clusters
  with fewer than 8 CSUs.
- Confluent Cloud is configured with HA enabled by default on clusters 8 CSUs or more.
- ksqlDB is not backward compatible with previous versions of KSQL.
- Persistent streaming queries dont require the EMIT CHANGES modifier.
- ksql-migrations tool supports ksqlDB versions starting from 0.17
- Ensure that you provide valid credentials in the ksql-migrations.properties file.
- The ksql-migrations tool does not support performing simultaneous migrations.
- Port 1099 is exposed, which corresponds to the JMX port set in the KSQL_JMX_OPTS
  configuration.
- Combine as much work as possible into as few persistent queries as possible.
- Minimize the size of your state stores.
- Avoid unnecessary serialization and deserialization.
- Schemas referred to by KEY_SCHEMA_ID and VALUE_SCHEMA_ID must be registered in Schema
  Registry.
- You cant define key or value columns in a statement if a corresponding KEY_SCHEMA_ID
  or VALUE_SCHEMA_ID is supplied.
- ksqlDB doesnt check that null can be serialized in a physical schema that contains
  required fields.
- The schema must be registered in Schema Registry under the subject '<topic-name>-value'.
- ksqlDB ignores unsupported types in the physical schema and continues translating
  supported types to the logical schema. You should verify that the logical schema
  is translated as expected.
- During schema translation from a physical schema to a logical schema, struct type
  field names are used as column names in the logical schema. Field names are translated
  to uppercase.
- Streams are immutable, append-only collections.
- Tables model the current state and allow updates.
- ksqlDB doesnt support not-null constraint.
- The varchar type represents a string in UTF-16 format.
- Comparisons between varchar instances dont account for locale.
- Storing values outside of the supported range results in an error.
- The decimal type can be used to store fractional numbers with exact precision.
- ksqlDB goes beyond SQL-92, because the standard currently has no constructs for
  streaming queries.
- Pull query metrics must be enabled explicitly by setting the ksql.query.pull.metrics.enabled
  server configuration to true.
- UDF metrics must be enabled explicitly by setting the ksql.udf.collect.metrics server
  configuration to true.
- The ksql.server.url property is required.
- The processing log is not for server logging, but rather for per-record logging
  on ksqlDB applications.
- Processing log stream auto-creation is supported for interactive mode only.
- Enabling this setting in headless mode causes a warning to be printed to the server
  log.
- Protobuf handles null values differently than AVRO and JSON.
- Avoid unwrapped single-field schemas if the field can have a null value.
- ksqlDB Server configuration settings take precedence over those set in the ksqlDB
  CLI.
- The default prefix for automatically created topic names is an empty string.
- The default value for the KEY_FORMAT property is KAFKA.
- The default value for the VALUE_FORMAT property is null.
- The default value for ksql.persistence.wrap.single.values is null.
- The default value for ksql.streams.replication.factor is -1.
- The default value for ksql.runtime.feature.shared.enabled is false.
- The default value for ksql.service.id is default_.
- The default value for ksql.source.table.materialization.enabled is true.
- The default value for ksql.headers.columns.enabled is true.
- The default value for ksql.streams.auto.offset.reset is latest.
- The default value for ksql.streams.bootstrap.servers is localhost:9092.
- The default value for ksql.streams.buffered.records.per.partition is 1000.
- The default value for ksql.streams.cache.max.bytes.buffering is 10000000.
- The default value for ksql.streams.commit.interval.ms is 2000.
- The default value for ksql.streams.max.task.idle.ms is null.
- The default value for ksql.streams.num.standby.replicas is 0.
- The default value for ksql.streams.num.stream.threads is 4.
- The default value for ksql.streams.processing.guarantee is at_least_once.
- The default value for ksql.streams.producer.compression.type is snappy.
- The default value for ksql.streams.state.dir is /tmp/kafka-streams.
- The default value for ksql.streams.task.timeout.ms is 300000.
- The default value for ksql.queries.file is null.
- The default for Confluent Platform is 2147483647. The default for Confluent Cloud
  is 20.
- The default value is false.
- The default is 9223372036854775807.
- The default is true.
- The default for Confluent Platform is 2147483647. The default for Confluent Cloud
  is 1000000.
- The default is 2147483647.
- The default timeout is 24 hours.
- The default listeners is http://0.0.0.0:8088.
- The default number of replicas for the topics created by ksqlDB is one. This property
  has been deprecated.
- User-defined functions enable you to extend ksqlDBs suite of built-in functions
  using Java hooks.
- In some deployment environments, it may be necessary to restrict the classes that
  UDFs have access to.
- Metric collection can be enabled by setting the config ksql.udf.collect.metrics
  to true.
- This defaults to false and is generally not recommended for production usage, as
  metrics are collected on each invocation and introduce some overhead to processing
  time.
- Ensure proper OAuth configuration for API access.
- Rate limits apply to API calls.
- ksqlDB automatically creates the underlying Kafka topic.
- Push queries never return until they are terminated explicitly.
- Some API endpoints may require specific permissions.
- Uses API key and secret for authentication.
- Ensure to save the API key and secret as they are not retrievable later.
- Both ksqlDB and Control Center must be running on either HTTP or HTTPS.
- ksqlDB supports RBAC to provide authorization to ksqlDB clusters when running in
  interactive mode.
- RBAC for users isnt used in ksqlDB clusters that are deployed in headless mode.
- When you grant roles to a user, remember to grant the same roles to the ksqlDB service
  principal, which is named 'ksql' by default.
- Ensure proper access controls are set for users accessing the processing log.
- ksqlDB applications run with the roles granted to a privileged ksqlDB user service
  principal.
- Grant delete access for a user to a subject.
- ksqlDB supports exactly-once processing.
- ksqlDB supports DELIMITED, JSON, and Avro message values.
- ksqlDB doesnt support structured keys, so you cant create a stream from a windowed
  aggregate.
- Make sure that your Kafka cluster is configured with delete.topic.enable=true.
- Some API responses may have nested structures
- MySQL requires custom configuration for Debezium
- Debezium documentation provides details for MySQL configuration
- MySQL requires custom configuration to play well with Debezium.
- The ksqlDB server image mounts the confluent-hub-components directory.
- Uses Debezium to stream MySQL's changelog into Kafka
- Set 'auto.offset.reset' = 'earliest' in ksqlDB
- Debezium captures changes from Postgres and MongoDB databases.
- Uses Docker Compose to set up the environment.
- Matches the broker port specified in the Docker Compose file.
- Matches the Schema Registry port specified in the Docker Compose file.
- Matches the topic name specified in the ksqlDB CREATE TABLE statement.
- Uses Avro format for the value part of Kafka records
- The stream is configured with a custom timestamp to signal event-time usage
- This microservice is configured to checkpoint its progress every 100 milliseconds
  through the ENABLE_AUTO_COMMIT_CONFIG configuration.
- ksqlDB emits a new event every time a tumbling window changes.
- The clickstream demo simulates user sessions with a script.
- Session windows monitor user behavior.
- Confluent Gateway abstracts the complexity of Kafka connectivity.
- Multiple authentication methods, Identity Pass-through, and Authentication Swapping
  are supported.
- Requires setup of connected app in Confluent Cloud
- Observers are not compatible with Intelligent Replication.
- This feature is only available in Confluent Private Cloud.
- Observers are not compatible with Intelligent Replication. If a cluster includes
  Observers, enabling Intelligent Replication has no effect on Observer nodes.
- Monitor PushManagerMemoryBytesUsed to prevent memory pressure.
- Increasing PushEventProcessingFailure values indicate system health issues.
- Uses OAuth2 with refresh token  requires setup of connected app in ksqldb
- Investigate high ingress rates, slow followers, or network/storage issues.
- Monitor the metric over time to ensure it returns to normal levels.
- Monitor reason tags - frequent REQUEST_NON_RETRIABLE_ERROR or LEADER_REPLICATION_ERROR
  may indicate issues requiring push replication disabling.
- If error rates are high, consider disabling intelligent replication until underlying
  issues are resolved.
- Check network connectivity between leaders and followers.
- Use this metric to confirm push replication is enabled and functioning.
- Monitor network connectivity and follower broker health.
- Ensure that `confluent.intelligent.replication.enable=true` is properly set and
  that brokers have been restarted.
- If issues persist, consider temporarily disabling Intelligent Replication by setting
  `confluent.intelligent.replication.enable=false` and restarting brokers.
- By default, the server starts bound to port 8082 and does not specify a unique instance
  ID.
- Default configuration for local testing setup.
- If a listener uses HTTPS, the appropriate TLS configuration parameters must also
  be set.
- If the interbroker listener of the broker that the REST Proxy is running on has
  security enabled and there is an authorizer.class.name configured, you must manually
  configure the Java clients in the REST Proxy so that they can securely communicate
  with Kafka.
- If confluent.metadata.server.listeners is used instead of confluent.http.server.listeners,
  then the Metadata Service will also be enabled on the same listener.
- If the interbroker listener of the broker that the REST Proxy is running on has
  security enabled, you must manually configure the Java clients in the REST Proxy
  so that they can securely communicate with Kafka.
- Without principal propagation, authentication terminates at the REST Proxy.
- RBAC-enabled Kafka and Schema Registry clusters are required for authorization.
- The REST Proxy does not require any coordination between instances, so you can easily
  scale your deployment up or down.
- The only requirement for multiple instances is that you set a unique id for each
  instance.
- The REST Proxy does not store any state on disk.
- A fast and reliable network will likely have the biggest impact on the REST Proxys
  performance.
- License configurations are only required if you are using principal propagation.
- Without the license key, you can use Confluent security plugins for a 30-day trial
  period.
- The REST Proxy reports a variety of metrics through JMX.
- Global metrics help you monitor the overall health of the service.
- Per-endpoint metrics monitor each API endpoint request method.
- No authorization actually occurs on REST Proxy, set broker ACLs to enforce restrictions.
- HTTPS is recommended, but not required.
- RBAC-enabled Kafka and Schema Registry clusters are prerequisites.
- If you have high latency from Schema Registry to MDS for requests, increase the
  timeout value of the optional setting confluent.metadata.http.request.timeout.ms
  to account for the extra latency.
- Do not use API v1. API v1 has a ZooKeeper dependency that does not work in Confluent
  Cloud.
- Standard support means any support level below Platinum support.
- The end of support date applies to minor versions and any maintenance versions that
  come after the minor version.
- CMF will create a PersistentVolumeClaim (PVC) in Kubernetes.
- If the PVC remains in status Pending, check your Kubernetes cluster configuration
  and make sure a Container Storage Interface (CSI) driver is installed and configured
  correctly.
- CMF requires its own service principal to authorize user principals.
- Choose the role with the least privileges to meet your needs.
- 'CMF encrypts the following sensitive data: Secret data: The data field provided
  in the Secrets is stored encrypted.'
- When CMF runs in production mode, encryption is automatically enabled and required.
- This deployment is NOT suitable for production use.
- A CMF database that has been initialized as a non-production mode deployment can
  not be turned into a production mode later on.
- Key Rotation is Not Supported.
- Requires API key for authentication.
- 'CMF encrypts the following sensitive data: Secret data, the data field provided
  in the Secrets is stored encrypted.'
- In development mode, encryption can be disabled. Sensitive data is stored in plain
  text and is NOT suitable for production use.
- This is a NON-PRODUCTION deployment of Confluent Manager for Apache Flink.
- 'You are not allowed to switch between productions mode, i.e you cannot have the
  same CMF instance started with `cmf.sql.production: true` and move to `cmf.sql.production:
  false` or vice versa.'
- Key Rotation is Not Supported
- Once you set an encryption key, you cannot change it. The same key must be used
  for the entire lifetime of your CMF database.
- 'CMF encrypts the following sensitive data: Secret data.'
- CMF uses AES-GCM (Galois/Counter Mode) encryption.
- For development and testing, encryption can be disabled.
- To deploy in production mode, set cmf.sql.production=true in your values.yaml and
  create an encryption key, according to the documentation.
- You are not allowed to switch between productions mode.
- Requires setup of connected app in API
- Ensure correct configuration for ksqlDB server
- Enable production mode (required for encryption)
- CMF automatically encrypts Secrets stored in the database.
- In the production disabled mode, even when you provide the `encryption.key.kubernetesSecretName`
  and `encryption.key.kubernetesSecretProperty`, it is not used.
- Key Rotation is Not Supported. Once you set an encryption key, you cannot change
  it.
- If the CMF database is file system based, using `helm uninstall cmf` will lead to
  the loss of the database because the Persistent Volume Claim (PVC) is managed by
  CMF.
- The cmf.sql.production=false setting initializing the CMF database without encryption.
  You cannot enable encryption later on.
- The cmf.sql.production=false setting initializes the CMF database without encryption.
errors:
- '401 Unauthorized: Check your authentication credentials'
- '404 Not Found: Verify the endpoint path'
- '429 Too Many Requests: Rate limit exceeded'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Kafka Authentication Error'
- '403 Forbidden: Kafka Authorization Error'
- '404 Not Found: Topic not found'
- '422 Unprocessable Entity: The request payload is either improperly formatted or
  contains semantic errors'
- '500 Internal Server Error: Kafka error'
- '400 Bad Request: Serialization error'
- '422 Unprocessable Entity: Invalid schema'
- '404 Not Found: Partition not found'
- '422 Unprocessable Entity: Request includes keys and uses a format that requires
  schemas, but does not include the key_schema or key_schema_id fields'
- '409 Conflict: Consumer instance with the specified name already exists.'
- '422 Unprocessable Entity: Invalid consumer configuration.'
- '404 Not Found: Consumer instance not found'
- '406 Not Acceptable: Consumer format does not match the embedded format requested
  by the Accept header.'
- '400 Bad Request: Indicates a bad request error.'
- '401 Unauthorized: Authentication failed.'
- '403 Forbidden: Request is not authorized.'
- '429 Too Many Requests: Indicates that a rate limit threshold has been reached.'
- '500 Internal Server Error: Internal Server Error'
- '401 Unauthorized: Indicates a client authentication error.'
- '403 Forbidden: Indicates a client authorization error.'
- '500 Internal Server Error: A server-side problem.'
- '400 Bad Request: Indicates a bad request error. It could be caused by an unexpected
  request body format or other forms of request validation failure.'
- '429 Too Many Requests: Indicates that a rate limit threshold has been reached,
  and the client should retry again later.'
- '500 Internal Server Error: A server-side problem that might not be addressable
  from the client side.'
- '500 Internal Server Error: Indicates a server-side problem.'
- '429 Too Many Requests: Rate limit threshold has been reached.'
- '401 Unauthorized: Indicates a client authentication error. Kafka authentication
  failures will contain error code 40101 in the response body.'
- '403 Forbidden: Indicates a client authorization error. Kafka authorization failures
  will contain error code 40301 in the response body.'
- '404 Not Found: Indicates attempted access to an unreachable or non-existing resource.'
- '404 Not Found: Indicates attempted access to an unreachable or non-existing resource
  like e.g. an unknown topic or partition.'
- '5XX: A server-side problem that might not be addressable from the client side.'
- '404 Not Found: Broker not found.'
- '400 Bad Request: Indicates a bad request error'
- '401 Unauthorized: Authentication failed'
- '403 Forbidden: Request is not authorized'
- '429 Too Many Requests: Indicates that a rate limit threshold has been reached'
- '404 Not Found: Broker not found'
- '5XX: A server-side problem that might not be addressable from the client side.
  Retriable Kafka errors will contain error code 50003 in the response body.'
- '413 Request Entity Too Large: This implies the client is sending a request payload
  that is larger than the maximum message size the server can accept.'
- '415 Unsupported Media Type: This implies the client is sending the request payload
  format in an unsupported format.'
- '422 Unprocessable Entity: Indicates a bad request error. It could be caused by
  an unexpected request body format or other forms of request validation failure.'
- '500 Internal Server Error: Internal Server Error.'
- '404 Not Found: The requested resource could not be found.'
- '404 Not Found: The given broker ID was not registered in the cluster.'
- '401 Unauthorized: Check your API key and secret.'
- Ensure Docker is running before starting Confluent Platform.
- '40301: User is denied operation Write on Subject: users-value'
- '40301: Not authorized to access topics: [users]'
- '40301: Not authorized to access group: my_avro_consumer'
- '401 Unauthorized: User does not exist in LDAP'
- '401 Unauthorized: Check credentials used for basic authentication.'
- '401 Unauthorized: Recheck API key or secret'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- 'Invalid configuration: Check property names and values.'
- 'Connection refused: Ensure Kafka is running and accessible.'
- '401 Unauthorized: Recheck license key for enterprise features.'
- 'Failed to start: Control Center, Replicator, Connectors, JMS client, ADB, MQTT
  Proxy, Security plugins'
- Confluent Server starts but generates frequent errors until a new license is provided.
- 'unmet dependencies: occurs when trying to install a specific older Confluent package
  when a newer patch version is available.'
- '429: Throttle API calls or reduce frequency'
- '401 Unauthorized: Verify client credentials and permissions.'
- '404 Not Found: Check the endpoint URL.'
- 'KafkaException: If the message could not be enqueued due to local produce queue
  being full'
- 'KafkaError._PARTITION_EOF: End of partition event.'
- 'KafkaException: If the message could not be enqueued.'
- 'KafkaException: Indicates an error occurred during Kafka operations.'
- 'Local_State: Indicates an erroneous state during offset commit.'
- 'UNKNOWN_TOPIC_OR_PART: The requested topic or partition does not exist.'
- 'TOPIC_AUTHORIZATION_FAILED: The user does not have access to the topic.'
- Failed to lookup hostname
- Failed to create new producer
- Failed to create new consumer
- Failed to start consuming topics
- Failed to close consumer
- 'WakeupException: Raised when the consumer''s wakeup method is called.'
- 'CommitFailedException: The commit failed with an unrecoverable error.'
- '400 Bad Request: Check the request parameters'
- '403 Forbidden: Ensure the correct permissions are set'
- '500 Internal Server Error: Try again later'
- '401 Unauthorized: Verify OAuth credentials and token expiration'
- 'KafkaException: Operation failed locally or on broker.'
- 'TypeError: Invalid input type.'
- 'ValueError: Invalid input value.'
- 'ValueError: if configuration validation fails'
- 'KafkaError: Use exc.args[0].retriable() to check if the operation may be retried.'
- '_BAD_MSG: Local: Bad message format'
- '_BAD_COMPRESSION: Local: Invalid compressed data'
- '_DESTROY: Local: Broker handle destroyed for termination'
- '_FAIL: Local: Communication failure with broker'
- '_TRANSPORT: Local: Broker transport failure'
- '_CRIT_SYS_RESOURCE: Local: Critical system resource failure'
- '_RESOLVE: Local: Host resolution failure'
- '_MSG_TIMED_OUT: Local: Message timed out'
- '_PARTITION_EOF: Broker: No more messages'
- '_UNKNOWN_PARTITION: Local: Unknown partition'
- '_FS: Local: File or filesystem error'
- '_UNKNOWN_TOPIC: Local: Unknown topic'
- '_ALL_BROKERS_DOWN: Local: All broker connections are down'
- '_INVALID_ARG: Local: Invalid argument or configuration'
- '_TIMED_OUT: Local: Timed out'
- '_QUEUE_FULL: Local: Queue full'
- '_ISR_INSUFF: Local: ISR count insufficient'
- '_NODE_UPDATE: Local: Broker node update'
- '_SSL: Local: SSL error'
- '_WAIT_COORD: Local: Waiting for coordinator'
- '_UNKNOWN_GROUP: Local: Unknown group'
- '_IN_PROGRESS: Local: Operation in progress'
- '_PREV_IN_PROGRESS: Local: Previous operation in progress'
- '_EXISTING_SUBSCRIPTION: Local: Existing subscription'
- '_ASSIGN_PARTITIONS: Local: Assign partitions'
- '_REVOKE_PARTITIONS: Local: Revoke partitions'
- '_CONFLICT: Local: Conflicting use'
- '_STATE: Local: Erroneous state'
- '_UNKNOWN_PROTOCOL: Local: Unknown protocol'
- '_NOT_IMPLEMENTED: Local: Not implemented'
- '_AUTHENTICATION: Local: Authentication failure'
- '_NO_OFFSET: Local: No offset stored'
- '_OUTDATED: Local: Outdated'
- '_TIMED_OUT_QUEUE: Local: Timed out in queue'
- '_UNSUPPORTED_FEATURE: Local: Required feature not supported by broker'
- '_WAIT_CACHE: Local: Awaiting cache update'
- '_INTR: Local: Operation interrupted'
- '_KEY_SERIALIZATION: Local: Key serialization error'
- '_VALUE_SERIALIZATION: Local: Value serialization error'
- '_KEY_DESERIALIZATION: Local: Key deserialization error'
- '_VALUE_DESERIALIZATION: Local: Value deserialization error'
- '_PARTIAL: Local: Partial response'
- '_READ_ONLY: Local: Read-only object'
- '_NOENT: Local: No such entry'
- '_UNDERFLOW: Local: Read underflow'
- '_INVALID_TYPE: Local: Invalid type'
- '_RETRY: Local: Retry operation'
- '_PURGE_QUEUE: Local: Purged in queue'
- '_PURGE_INFLIGHT: Local: Purged in flight'
- '_FATAL: Local: Fatal error'
- '_INCONSISTENT: Local: Inconsistent state'
- '_GAPLESS_GUARANTEE: Local: Gap-less ordering would not be guaranteed if proceeding'
- '_MAX_POLL_EXCEEDED: Local: Maximum application poll interval (max.poll.interval.ms)
  exceeded'
- '_UNKNOWN_BROKER: Local: Unknown broker'
- '_NOT_CONFIGURED: Local: Functionality not configured'
- '_FENCED: Local: This instance has been fenced by a newer instance'
- '_APPLICATION: Local: Application generated error'
- '_ASSIGNMENT_LOST: Local: Group partition assignment lost'
- '_NOOP: Local: No operation performed'
- '_AUTO_OFFSET_RESET: Local: No offset to automatically reset to'
- '_LOG_TRUNCATION: Local: Partition log truncation detected'
- '_INVALID_DIFFERENT_RECORD: Local: an invalid record in the same batch caused the
  failure of this message too'
- '_DESTROY_BROKER: Local: Broker handle destroyed without termination'
- 'UNKNOWN: Unknown broker error'
- 'NO_ERROR: Success'
- 'OFFSET_OUT_OF_RANGE: Broker: Offset out of range'
- 'INVALID_MSG: Broker: Invalid message'
- 'UNKNOWN_TOPIC_OR_PART: Broker: Unknown topic or partition'
- 'INVALID_MSG_SIZE: Broker: Invalid message size'
- 'LEADER_NOT_AVAILABLE: Broker: Leader not available'
- 'NOT_LEADER_FOR_PARTITION: Broker: Not leader for partition'
- 'REQUEST_TIMED_OUT: Broker: Request timed out'
- 'BROKER_NOT_AVAILABLE: Broker: Broker not available'
- 'REPLICA_NOT_AVAILABLE: Broker: Replica not available'
- 'MSG_SIZE_TOO_LARGE: Broker: Message size too large'
- 'STALE_CTRL_EPOCH: Broker: StaleControllerEpochCode'
- 'OFFSET_METADATA_TOO_LARGE: Broker: Offset metadata string too large'
- 'NETWORK_EXCEPTION: Broker: Broker disconnected before response received'
- 'COORDINATOR_LOAD_IN_PROGRESS: Broker: Coordinator load in progress'
- 'COORDINATOR_NOT_AVAILABLE: Broker: Coordinator not available'
- 'NOT_COORDINATOR: Broker: Not coordinator'
- 'TOPIC_EXCEPTION: Broker: Invalid topic'
- 'RECORD_LIST_TOO_LARGE: Broker: Message batch larger than configured server segment
  size'
- 'NOT_ENOUGH_REPLICAS: Broker: Not enough in-sync replicas'
- 'NOT_ENOUGH_REPLICAS_AFTER_APPEND: Broker: Message(s) written to insufficient number
  of in-sync replicas'
- 'INVALID_REQUIRED_ACKS: Broker: Invalid required acks value'
- 'ILLEGAL_GENERATION: Broker: Specified group generation id is not valid'
- 'INCONSISTENT_GROUP_PROTOCOL: Broker: Inconsistent group protocol'
- 'INVALID_GROUP_ID: Broker: Invalid group.id'
- 'UNKNOWN_MEMBER_ID: Broker: Unknown member'
- 'INVALID_SESSION_TIMEOUT: Broker: Invalid session timeout'
- 'REBALANCE_IN_PROGRESS: Broker: Group rebalance in progress'
- 'INVALID_COMMIT_OFFSET_SIZE: Broker: Commit offset data size is not valid'
- 'TOPIC_AUTHORIZATION_FAILED: Broker: Topic authorization failed'
- 'GROUP_AUTHORIZATION_FAILED: Broker: Group authorization failed'
- 'CLUSTER_AUTHORIZATION_FAILED: Broker: Cluster authorization failed'
- 'INVALID_TIMESTAMP: Broker: Invalid timestamp'
- 'UNSUPPORTED_SASL_MECHANISM: Broker: Unsupported SASL mechanism'
- 'ILLEGAL_SASL_STATE: Broker: Request not valid in current SASL state'
- 'UNSUPPORTED_VERSION: Broker: API version not supported'
- 'TOPIC_ALREADY_EXISTS: Broker: Topic already exists'
- 'INVALID_PARTITIONS: Broker: Invalid number of partitions'
- 'INVALID_REPLICATION_FACTOR: Broker: Invalid replication factor'
- 'INVALID_REPLICA_ASSIGNMENT: Broker: Invalid replica assignment'
- 'INVALID_CONFIG: Broker: Configuration is invalid'
- 'NOT_CONTROLLER: Broker: Not controller for cluster'
- 'INVALID_REQUEST: Broker: Invalid request'
- 'UNSUPPORTED_FOR_MESSAGE_FORMAT: Broker: Message format on broker does not support
  request'
- 'POLICY_VIOLATION: Broker: Policy violation'
- 'OUT_OF_ORDER_SEQUENCE_NUMBER: Broker: Broker received an out of order sequence
  number'
- 'DUPLICATE_SEQUENCE_NUMBER: Broker: Broker received a duplicate sequence number'
- 'INVALID_PRODUCER_EPOCH: Broker: Producer attempted to perform an operation with
  an old epoch'
- 'INVALID_TXN_STATE: Broker: Producer attempted a transactional operation in an invalid
  state'
- 'INVALID_PRODUCER_ID_MAPPING: Broker: Producer attempted to use a producer id which
  is not currently assigned to its transactional'
- 'INVALID_TRANSACTION_TIMEOUT: Broker: Transaction timeout is larger than the maximum
  value allowed by the brokers max.transaction'
- 'CONCURRENT_TRANSACTIONS: Broker: Producer attempted to update a transaction while
  another concurrent operation on the same transaction is in progress'
- 'TRANSACTION_COORDINATOR_FENCED: Broker: Indicates that the transaction coordinator
  sending a WriteTxnMarker is no longer the current'
- 'TRANSACTIONAL_ID_AUTHORIZATION_FAILED: Broker: Transactional Id authorization failed'
- 'SECURITY_DISABLED: Broker: Security features are disabled'
- 'OPERATION_NOT_ATTEMPTED: Broker: Operation not attempted'
- 'KAFKA_STORAGE_ERROR: Broker: Disk error when trying to access log file on disk'
- 'LOG_DIR_NOT_FOUND: Broker: The user-specified log directory is not found in the
  broker config'
- 'SASL_AUTHENTICATION_FAILED: Broker: SASL Authentication failed'
- 'UNKNOWN_PRODUCER_ID: Broker: Unknown Producer Id'
- 'REASSIGNMENT_IN_PROGRESS: Broker: Partition reassignment is in progress'
- 'DELEGATION_TOKEN_AUTH_DISABLED: Broker: Delegation Token feature is not enabled'
- 'DELEGATION_TOKEN_NOT_FOUND: Broker: Delegation Token is not found on server'
- 'DELEGATION_TOKEN_OWNER_MISMATCH: Broker: Specified Principal is not valid Owner/Renewer'
- 'DELEGATION_TOKEN_REQUEST_NOT_ALLOWED: Broker: Delegation Token requests are not
  allowed on this connection'
- 'DELEGATION_TOKEN_AUTHORIZATION_FAILED: Broker: Delegation Token authorization failed'
- 'DELEGATION_TOKEN_EXPIRED: Broker: Delegation Token is expired'
- 'INVALID_PRINCIPAL_TYPE: Broker: Supplied principalType is not supported'
- 'NON_EMPTY_GROUP: Broker: The group is not empty'
- 'GROUP_ID_NOT_FOUND: Broker: The group id does not exist'
- 'FETCH_SESSION_ID_NOT_FOUND: Broker: The fetch session ID was not found'
- 'INVALID_FETCH_SESSION_EPOCH: Broker: The fetch session epoch is invalid'
- 'LISTENER_NOT_FOUND: Broker: No matching listener'
- 'TOPIC_DELETION_DISABLED: Broker: Topic deletion is disabled'
- 'FENCED_LEADER_EPOCH: Broker: Leader epoch is older than broker epoch'
- 'UNKNOWN_LEADER_EPOCH: Broker: Leader epoch is newer than broker epoch'
- 'UNSUPPORTED_COMPRESSION_TYPE: Broker: Unsupported compression type'
- 'STALE_BROKER_EPOCH: Broker: Broker epoch has changed'
- 'OFFSET_NOT_AVAILABLE: Broker: Leader high watermark is not caught up'
- 'MEMBER_ID_REQUIRED: Broker: Group member needs a valid member ID'
- 'PREFERRED_LEADER_NOT_AVAILABLE: Broker: Preferred leader was not available'
- 'GROUP_MAX_SIZE_REACHED: Broker: Consumer group has reached maximum size'
- 'FENCED_INSTANCE_ID: Broker: Static consumer fenced by other consumer with same
  group.instance.id'
- 'ELIGIBLE_LEADERS_NOT_AVAILABLE: Broker: Eligible partition leaders are not available'
- 'ELECTION_NOT_NEEDED: Broker: Leader election not needed for topic partition'
- 'NO_REASSIGNMENT_IN_PROGRESS: Broker: No partition reassignment is in progress'
- 'GROUP_SUBSCRIBED_TO_TOPIC: Broker: Deleting offsets of a topic while the consumer
  group is subscribed to it'
- 'INVALID_RECORD: Broker: Broker failed to validate record'
- 'UNSTABLE_OFFSET_COMMIT: Broker: There are unstable offsets that need to be cleared'
- 'THROTTLING_QUOTA_EXCEEDED: Broker: Throttling quota has been exceeded'
- 'PRODUCER_FENCED: Broker: There is a newer producer with the same transactionalId
  which fences the current one'
- 'RESOURCE_NOT_FOUND: Broker: Request illegally referred to resource that does not
  exist'
- 'DUPLICATE_RESOURCE: Broker: Request illegally referred to the same resource twice'
- 'UNACCEPTABLE_CREDENTIAL: Broker: Requested credential would not meet criteria for
  acceptability'
- 'INCONSISTENT_VOTER_SET: Broker: Indicates that the either the sender or recipient
  of a voter-only request is not one of the configured voters'
- 'INVALID_UPDATE_VERSION: Broker: Invalid update version'
- 'FEATURE_UPDATE_FAILED: Broker: Unable to update finalized features due to server
  error'
- 'PRINCIPAL_DESERIALIZATION_FAILURE: Broker: Request principal deserialization failed
  during forwarding'
- 'UNKNOWN_TOPIC_ID: Broker: Unknown topic id'
- 'FENCED_MEMBER_EPOCH: Broker: The member epoch is fenced by the group coordinator'
- 'UNRELEASED_INSTANCE_ID: Broker: The instance ID is still used by another member
  in the consumer group'
- 'UNSUPPORTED_ASSIGNOR: Broker: The assignor or its version range is not supported
  by the consumer group'
- 'STALE_MEMBER_EPOCH: Broker: The member epoch is stale'
- 'UNKNOWN_SUBSCRIPTION_ID: Broker: Client sent a push telemetry request with an invalid
  or outdated subscription ID'
- 'TELEMETRY_TOO_LARGE: Broker: Client sent a push telemetry request larger than the
  maximum size the broker will accept'
- 'REBOOTSTRAP_REQUIRED: Broker: Client metadata is stale, client should rebootstrap
  to obtain new metadata'
- 'UNKNOWN_TOPIC_OR_PART: No longer returned if a topic is missing in the local cache
  when subscribing; the subscription proceeds.'
- 'TOPIC_AUTHORIZATION_FAILED: Reported once per heartbeat or subscription change,
  even if only one topic is unauthorized.'
- 'TypeException: Invalid input.'
- 'ValueException: Invalid input.'
- KafkaException
- RuntimeError if called on a closed consumer
- 'BufferError: if the internal producer message queue is full.'
- 'KeySerializationError: If an error occurs during key serialization.'
- 'ValueSerializationError: If an error occurs during value serialization.'
- 'KafkaException: For all other errors.'
- 'INVALID_PRODUCER_EPOCH: Broker: Producer attempted to operate with an old epoch'
- 'INVALID_TXN_STATE: Broker: Producer attempted to perform a transactional operation
  in an invalid state'
- 'INCONSISTENT_VOTER_SET: Broker: Indicates that the either the sender or recipient
  of a voter-only request is not one of the authorized voters'
- _BAD_MSG
- _BAD_COMPRESSION
- _DESTROY
- _FAIL
- _TRANSPORT
- _CRIT_SYS_RESOURCE
- _RESOLVE
- _MSG_TIMED_OUT
- _PARTITION_EOF
- _UNKNOWN_PARTITION
- _FS
- _UNKNOWN_TOPIC
- _ALL_BROKERS_DOWN
- _INVALID_ARG
- _TIMED_OUT
- _QUEUE_FULL
- _ISR_INSUFF
- _NODE_UPDATE
- _SSL
- _WAIT_COORD
- _UNKNOWN_GROUP
- _IN_PROGRESS
- _PREV_IN_PROGRESS
- _EXISTING_SUBSCRIPTION
- _ASSIGN_PARTITIONS
- _REVOKE_PARTITIONS
- _CONFLICT
- _STATE
- _UNKNOWN_PROTOCOL
- _NOT_IMPLEMENTED
- _AUTHENTICATION
- _NO_OFFSET
- _OUTDATED
- _TIMED_OUT_QUEUE
- _UNSUPPORTED_FEATURE
- _WAIT_CACHE
- _INTR
- _KEY_SERIALIZATION
- _VALUE_SERIALIZATION
- _KEY_DESERIALIZATION
- _VALUE_DESERIALIZATION
- _PARTIAL
- _READ_ONLY
- _NOENT
- _UNDERFLOW
- _INVALID_TYPE
- _RETRY
- _PURGE_QUEUE
- _PURGE_INFLIGHT
- _FATAL
- _INCONSISTENT
- _GAPLESS_GUARANTEE
- _MAX_POLL_EXCEEDED
- _UNKNOWN_BROKER
- _NOT_CONFIGURED
- _FENCED
- _APPLICATION
- _ASSIGNMENT_LOST
- _NOOP
- _AUTO_OFFSET_RESET
- _LOG_TRUNCATION
- _INVALID_DIFFERENT_RECORD
- _DESTROY_BROKER
- UNKNOWN
- NO_ERROR
- OFFSET_OUT_OF_RANGE
- INVALID_MSG
- UNKNOWN_TOPIC_OR_PART
- INVALID_MSG_SIZE
- LEADER_NOT_AVAILABLE
- NOT_LEADER_FOR_PARTITION
- REQUEST_TIMED_OUT
- BROKER_NOT_AVAILABLE
- REPLICA_NOT_AVAILABLE
- MSG_SIZE_TOO_LARGE
- STALE_CTRL_EPOCH
- OFFSET_METADATA_TOO_LARGE
- NETWORK_EXCEPTION
- COORDINATOR_LOAD_IN_PROGRESS
- COORDINATOR_NOT_AVAILABLE
- NOT_COORDINATOR
- TOPIC_EXCEPTION
- RECORD_LIST_TOO_LARGE
- NOT_ENOUGH_REPLICAS
- NOT_ENOUGH_REPLICAS_AFTER_APPEND
- INVALID_REQUIRED_ACKS
- ILLEGAL_GENERATION
- INCONSISTENT_GROUP_PROTOCOL
- INVALID_GROUP_ID
- UNKNOWN_MEMBER_ID
- INVALID_SESSION_TIMEOUT
- REBALANCE_IN_PROGRESS
- INVALID_COMMIT_OFFSET_SIZE
- TOPIC_AUTHORIZATION_FAILED
- GROUP_AUTHORIZATION_FAILED
- CLUSTER_AUTHORIZATION_FAILED
- INVALID_TIMESTAMP
- UNSUPPORTED_SASL_MECHANISM
- ILLEGAL_SASL_STATE
- UNSUPPORTED_VERSION
- TOPIC_ALREADY_EXISTS
- INVALID_PARTITIONS
- INVALID_REPLICATION_FACTOR
- INVALID_REPLICA_ASSIGNMENT
- INVALID_CONFIG
- NOT_CONTROLLER
- INVALID_REQUEST
- UNSUPPORTED_FOR_MESSAGE_FORMAT
- POLICY_VIOLATION
- OUT_OF_ORDER_SEQUENCE_NUMBER
- DUPLICATE_SEQUENCE_NUMBER
- INVALID_PRODUCER_EPOCH
- INVALID_TXN_STATE
- INVALID_PRODUCER_ID_MAPPING
- INVALID_TRANSACTION_TIMEOUT
- CONCURRENT_TRANSACTIONS
- TRANSACTION_COORDINATOR_FENCED
- TRANSACTIONAL_ID_AUTHORIZATION_FAILED
- SECURITY_DISABLED
- OPERATION_NOT_ATTEMPTED
- KAFKA_STORAGE_ERROR
- LOG_DIR_NOT_FOUND
- SASL_AUTHENTICATION_FAILED
- UNKNOWN_PRODUCER_ID
- REASSIGNMENT_IN_PROGRESS
- DELEGATION_TOKEN_AUTH_DISABLED
- DELEGATION_TOKEN_NOT_FOUND
- DELEGATION_TOKEN_OWNER_MISMATCH
- DELEGATION_TOKEN_REQUEST_NOT_ALLOWED
- DELEGATION_TOKEN_AUTHORIZATION_FAILED
- DELEGATION_TOKEN_EXPIRED
- INVALID_PRINCIPAL_TYPE
- NON_EMPTY_GROUP
- GROUP_ID_NOT_FOUND
- FETCH_SESSION_ID_NOT_FOUND
- INVALID_FETCH_SESSION_EPOCH
- LISTENER_NOT_FOUND
- TOPIC_DELETION_DISABLED
- FENCED_LEADER_EPOCH
- UNKNOWN_LEADER_EPOCH
- UNSUPPORTED_COMPRESSION_TYPE
- STALE_BROKER_EPOCH
- OFFSET_NOT_AVAILABLE
- MEMBER_ID_REQUIRED
- PREFERRED_LEADER_NOT_AVAILABLE
- GROUP_MAX_SIZE_REACHED
- FENCED_INSTANCE_ID
- ELIGIBLE_LEADERS_NOT_AVAILABLE
- ELECTION_NOT_NEEDED
- NO_REASSIGNMENT_IN_PROGRESS
- GROUP_SUBSCRIBED_TO_TOPIC
- INVALID_RECORD
- UNSTABLE_OFFSET_COMMIT
- THROTTLING_QUOTA_EXCEEDED
- PRODUCER_FENCED
- RESOURCE_NOT_FOUND
- DUPLICATE_RESOURCE
- UNACCEPTABLE_CREDENTIAL
- INCONSISTENT_VOTER_SET
- INVALID_UPDATE_VERSION
- FEATURE_UPDATE_FAILED
- PRINCIPAL_DESERIALIZATION_FAILURE
- UNKNOWN_TOPIC_ID
- FENCED_MEMBER_EPOCH
- UNRELEASED_INSTANCE_ID
- UNSUPPORTED_ASSIGNOR
- STALE_MEMBER_EPOCH
- UNKNOWN_SUBSCRIPTION_ID
- TELEMETRY_TOO_LARGE
- REBOOTSTRAP_REQUIRED
- Session timeout on client; KIP-848 enforced on broker
- 'CLIENT_LEVEL_ERROR: Check client-level errors'
- 'Error: Timeout occurred during the operation'
- 'Error: Non-recoverable idempotent producer error'
- 'Error: Operation timed out locally'
- ErrorCode is the integer representation of local and broker error codes
- ErrBadMsg
- ErrBadCompression
- ErrDestroy
- ErrFail
- ErrTransport
- ErrCritSysResource
- ErrResolve
- ErrMsgTimedOut
- ErrPartitionEOF
- ErrUnknownPartition
- ErrFs
- ErrUnknownTopic
- ErrAllBrokersDown
- ErrInvalidArg
- ErrTimedOut
- ErrQueueFull
- ErrIsrInsuff
- ErrNodeUpdate
- ErrSsl
- ErrWaitCoord
- ErrUnknownGroup
- ErrInProgress
- ErrPrevInProgress
- ErrExistingSubscription
- ErrAssignPartitions
- ErrRevokePartitions
- ErrConflict
- ErrState
- ErrUnknownProtocol
- ErrNotImplemented
- ErrAuthentication
- ErrNoOffset
- ErrOutdated
- ErrTimedOutQueue
- ErrUnsupportedFeature
- ErrWaitCache
- ErrIntr
- ErrKeySerialization
- ErrValueSerialization
- ErrKeyDeserialization
- ErrValueDeserialization
- ErrPartial
- ErrReadOnly
- ErrNoent
- ErrUnderflow
- ErrInvalidType
- ErrRetry
- ErrPurgeQueue
- ErrPurgeInflight
- ErrFatal
- ErrInconsistent
- ErrGaplessGuarantee
- ErrMaxPollExceeded
- ErrUnknownBroker
- ErrNotConfigured
- ErrFenced
- ErrApplication
- ErrAssignmentLost
- ErrNoop
- ErrAutoOffsetReset
- ErrLogTruncation
- ErrInvalidDifferentRecord
- ErrDestroyBroker
- ErrUnknown
- ErrNoError
- ErrOffsetOutOfRange
- ErrInvalidMsg
- ErrUnknownTopicOrPart
- ErrInvalidMsgSize
- ErrLeaderNotAvailable
- ErrNotLeaderForPartition
- ErrRequestTimedOut
- ErrBrokerNotAvailable
- ErrReplicaNotAvailable
- ErrMsgSizeTooLarge
- ErrStaleCtrlEpoch
- ErrOffsetMetadataTooLarge
- ErrNetworkException
- ErrCoordinatorLoadInProgress
- ErrCoordinatorNotAvailable
- ErrNotCoordinator
- ErrTopicException
- ErrRecordListTooLarge
- ErrNotEnoughReplicas
- ErrNotEnoughReplicasAfterAppend
- ErrInvalidRequiredAcks
- ErrIllegalGeneration
- ErrInconsistentGroupProtocol
- ErrInvalidGroupID
- ErrUnknownMemberID
- ErrInvalidSessionTimeout
- ErrRebalanceInProgress
- ErrInvalidCommitOffsetSize
- ErrTopicAuthorizationFailed
- ErrGroupAuthorizationFailed
- ErrClusterAuthorizationFailed
- ErrInvalidTimestamp
- ErrUnsupportedSaslMechanism
- ErrIllegalSaslState
- ErrUnsupportedVersion
- ErrTopicAlreadyExists
- ErrInvalidPartitions
- ErrInvalidReplicationFactor
- ErrInvalidReplicaAssignment
- ErrInvalidConfig
- ErrNotController
- ErrInvalidRequest
- ErrUnsupportedForMessageFormat
- ErrPolicyViolation
- ErrOutOfOrderSequenceNumber
- ErrDuplicateSequenceNumber
- ErrInvalidProducerEpoch
- ErrInvalidTxnState
- ErrInvalidProducerIDMapping
- ErrInvalidTransactionTimeout
- ErrConcurrentTransactions
- ErrTransactionCoordinatorFenced
- ErrTransactionalIDAuthorizationFailed
- ErrSecurityDisabled
- ErrOperationNotAttempted
- ErrKafkaStorageError
- ErrLogDirNotFound
- ErrSaslAuthenticationFailed
- ErrUnknownProducerID
- ErrReassignmentInProgress
- ErrDelegationTokenAuthDisabled
- ErrDelegationTokenNotFound
- ErrDelegationTokenOwnerMismatch
- ErrDelegationTokenRequestNotAllowed
- ErrDelegationTokenAuthorizationFailed
- ErrDelegationTokenExpired
- C.RD_KAFKA_RESP_ERR_INVALID_PRINCIPAL_TYPE
- C.RD_KAFKA_RESP_ERR_NON_EMPTY_GROUP
- C.RD_KAFKA_RESP_ERR_GROUP_ID_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_FETCH_SESSION_ID_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_INVALID_FETCH_SESSION_EPOCH
- C.RD_KAFKA_RESP_ERR_LISTENER_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_TOPIC_DELETION_DISABLED
- C.RD_KAFKA_RESP_ERR_FENCED_LEADER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNKNOWN_LEADER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNSUPPORTED_COMPRESSION_TYPE
- C.RD_KAFKA_RESP_ERR_STALE_BROKER_EPOCH
- C.RD_KAFKA_RESP_ERR_OFFSET_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_MEMBER_ID_REQUIRED
- C.RD_KAFKA_RESP_ERR_PREFERRED_LEADER_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_GROUP_MAX_SIZE_REACHED
- C.RD_KAFKA_RESP_ERR_FENCED_INSTANCE_ID
- C.RD_KAFKA_RESP_ERR_ELIGIBLE_LEADERS_NOT_AVAILABLE
- C.RD_KAFKA_RESP_ERR_ELECTION_NOT_NEEDED
- C.RD_KAFKA_RESP_ERR_NO_REASSIGNMENT_IN_PROGRESS
- C.RD_KAFKA_RESP_ERR_GROUP_SUBSCRIBED_TO_TOPIC
- C.RD_KAFKA_RESP_ERR_INVALID_RECORD
- C.RD_KAFKA_RESP_ERR_UNSTABLE_OFFSET_COMMIT
- C.RD_KAFKA_RESP_ERR_THROTTLING_QUOTA_EXCEEDED
- C.RD_KAFKA_RESP_ERR_PRODUCER_FENCED
- C.RD_KAFKA_RESP_ERR_RESOURCE_NOT_FOUND
- C.RD_KAFKA_RESP_ERR_DUPLICATE_RESOURCE
- C.RD_KAFKA_RESP_ERR_UNACCEPTABLE_CREDENTIAL
- C.RD_KAFKA_RESP_ERR_INCONSISTENT_VOTER_SET
- C.RD_KAFKA_RESP_ERR_INVALID_UPDATE_VERSION
- C.RD_KAFKA_RESP_ERR_FEATURE_UPDATE_FAILED
- C.RD_KAFKA_RESP_ERR_PRINCIPAL_DESERIALIZATION_FAILURE
- C.RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_ID
- C.RD_KAFKA_RESP_ERR_FENCED_MEMBER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNRELEASED_INSTANCE_ID
- C.RD_KAFKA_RESP_ERR_UNSUPPORTED_ASSIGNOR
- C.RD_KAFKA_RESP_ERR_STALE_MEMBER_EPOCH
- C.RD_KAFKA_RESP_ERR_UNKNOWN_SUBSCRIPTION_ID
- C.RD_KAFKA_RESP_ERR_TELEMETRY_TOO_LARGE
- C.RD_KAFKA_RESP_ERR_REBOOTSTRAP_REQUIRED
- 'UNSUPPORTED_VERSION: Kafka brokers that return an error when they receive a request
  with any of the removed API versions.'
- 'UnsupportedVersionException: Java clients that throw this when interacting with
  brokers that do not support the minimum protocol API versions.'
- '400 Bad Request: Check your SQL syntax.'
- '401 Unauthorized: Verify your credentials.'
- '404 Not Found: Resource does not exist.'
- '401 Unauthorized: Check your API key and token'
- '401 Unauthorized: Check client ID or secret'
- '403 Forbidden: Insufficient permissions'
- '404 Not Found: Resource does not exist'
- '401 Unauthorized: Check your credentials and OAuth2 token.'
- '429 Too Many Requests: Reduce the frequency of API calls'
- '401 Unauthorized: Check your OAuth token'
- '404 Not Found: Check the endpoint path'
- Incorrect configuration will cause runtime failures.
- 'Failed to construct kafka producer: Keystore was tampered with, or password was
  incorrect'
- Password verification failed
- '403 Forbidden: Check API key permissions'
- '404 Not Found: Verify endpoint and resource availability'
- '500 Internal Server Error: Retry the request later'
- 'Invalid application.id: Check for typos in application.id.'
- 'Consumer group is still active: Ensure all instances are stopped.'
- '400 Bad Request: Check the syntax of your query.'
- '401 Unauthorized: Ensure the token is valid and has the correct scopes.'
- '500 Internal Server Error: Retry after a few moments.'
- '400 Bad Request: Check the SQL statement syntax.'
- '401 Unauthorized: Verify your API key and secret.'
- 'IllegalStateException: This should not happen as topic() should only be called
  while a record is processed.'
- 'Query terminated: Indicates that a running query was stopped.'
- 'Exiting ksqlDB: Indicates that the CLI session has ended.'
- 'java.lang.NullPointerException: at index 2: caused by a mismatch between the ksqlDB
  CLI version and the ksqlDB server version.'
- 'java.lang.NullPointerException: at index 2'
- 'KAFKA-19668: known bug in KStream.processValues()'
- Upgrading to version 6.2 and later versions may fail if a DROP statement exists
  in the command topic for an object that is referenced by other objects.
- Unknown Footer version. Maybe this file was created with newer version of RocksDB?
- 'Function not found: Ensure the function is correctly defined.'
- 'WARN Failed to create UDAF: Must specify ''aggregateSchema'' for STRUCT parameter.'
- '1: Indicates that a test has failed'
- '0: Indicates that all tests passed'
- '403 Forbidden: Check permissions for accessing topics.'
- '404 Not Found: Verify that the topic exists.'
- '400 Bad Request: Verify the syntax of the SQL statement.'
- '404 Not Found: Ensure the endpoint is correct and the resource exists.'
- '500 Internal Server Error: Check the server status.'
- Key missing from projection.
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- 'REQUEST_LIMIT_EXCEEDED: Reduce the frequency of API calls.'
- '401 Unauthorized: Check OAuth token validity.'
- '400 Bad Request: Check the syntax of your KSQL statements'
- '401 Unauthorized: Verify your authentication details'
- '500 Internal Server Error: Check the server logs for more details'
- '401 Unauthorized: Check OAuth credentials'
- '404 Not Found: Verify resource path'
- Deadlock may occur if you run multiple pull queries concurrently.
- 'RATE_LIMIT_EXCEEDED: Reduce the frequency of requests'
- '401 Unauthorized: Verify OAuth credentials and scopes'
- '40000: One or more properties overrides set locally are prohibited by the KSQL
  server (use UNSET to reset their default value)'
- '40001 (BAD_STATEMENT): The request contained an invalid SQL statement.'
- '40002 (QUERY_ENDPOINT): The request contained a statement that should be issued
  to the /query endpoint.'
- '200 OK: Stream created and running'
- '406 Not Acceptable: If you specify an unsupported content type in the Accept header.'
- 'Docker run command failed: Check Docker configuration'
- 'Connection refused: Ensure ksqlDB Server is running'
- 'Invalid configuration: Ensure properties are set correctly in ksql-server.properties'
- '401 Unauthorized: Recheck user credentials or roles'
- '404 Not Found: Check the endpoint path.'
- '401 Unauthorized: Verify the authentication credentials.'
- 'Failed to insert values into ''PAGEVIEWS''. Could not serialize value: [ null |
  null ]. Error serializing message to topic: pageviews-avro-topic. Invalid value:
  null used for required field: "page_name", schema type: STRING'
- 'INVALID_REQUEST: Check the syntax of your SQL statement.'
- 'TOPIC_NOT_FOUND: Ensure the Kafka topic exists before creating a stream or table.'
- '403 Forbidden: Check user permissions.'
- '404 Not Found: Verify endpoint path.'
- '429 Too Many Requests: Rate limit exceeded.'
- '403 FORBIDDEN: Check API permissions'
- '404 NOT FOUND: Verify the endpoint URL'
- '401 Unauthorized: Check API key and secret.'
- 'Permission denied: Check user roles and permissions'
- 'Invalid principal: Ensure the principal is correctly specified'
- 'RATE_LIMIT_EXCEEDED: Reduce the number of requests'
- 'UNAUTHORIZED: Check OAuth credentials'
- 'Invalid topic: Verify the topic name and ensure it exists'
- 'Schema not found: Ensure the schema is registered in Schema Registry'
- 'QUERY_TIMEOUT: Break down queries or add selectivity'
- Monitor reason tags for PushSessionEndCount to identify issues.
- 'consumer.instance.timeout.ms: default is equivalent to 5 minutes (300,000ms)'
- '403 Forbidden: User does not have the requisite role or ACL permission for the
  requested resource.'
- If you do not specify a license, CMF will generate a trial license.
- Throttle API calls or reduce frequency
- Break down filters or add selectivity
- Recheck OAuth scopes or token expiration
- 'Key Rotation is Not Supported: Once you set an encryption key, you cannot change
  it.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls'
- '401 Unauthorized: Recheck OAuth token'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - User:appSA
  - User:badapp
  - PropertyFileLoginModule
  - service account
  - API key
  - API secret
  - OAuthToken
  - KsqlDBApi
  - KsqlDBClient
  - KsqlServer-Props
  - jaas_config.file
client:
  base_url: https://docs.confluent.io/private-cloud-gateway/current
  auth:
    type: oauth2
source_metadata: null
