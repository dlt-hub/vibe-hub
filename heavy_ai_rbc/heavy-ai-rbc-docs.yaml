resources:
- name: NYC Taxi Rides
  endpoint:
    path: /path/to/nyc-taxi-rides
    method: GET
    data_selector: data
    params: {}
- name: values
  endpoint:
    path: /values.yml
    method: GET
    data_selector: values
    params: {}
- name: example-heavyai-pvc
  endpoint:
    path: /example-heavyai-pvc.yml
    method: GET
    data_selector: pvc
    params: {}
- name: example-heavyai-pv
  endpoint:
    path: /example-heavyai-pv.yml
    method: GET
    data_selector: pv
    params: {}
- name: heavyai-pvc
  endpoint:
    path: /api/v1/persistentvolumeclaims/heavyai-pvc
    method: GET
- name: heavyai-pv
  endpoint:
    path: /api/v1/persistentvolumes/heavyai-pv
    method: GET
- name: heavydb
  endpoint:
    path: /systemd/heavydb
    method: GET
- name: heavy_web_server
  endpoint:
    path: /systemd/heavy_web_server
    method: GET
- name: HeavyDB
  endpoint:
    path: /api/heavydb
    method: GET
    data_selector: records
- name: heavy.conf
  endpoint:
    path: /heavy.conf
    method: GET
    data_selector: configurations
- name: heavydb_backend
  endpoint:
    path: /heavydb/backend
    method: GET
- name: dashboard
  endpoint:
    path: /dashboard
    method: REVOKE
    data_selector: privileges
    params: {}
- name: database
  endpoint:
    path: /database
    method: REVOKE
    data_selector: privileges
    params: {}
- name: server
  endpoint:
    path: /server
    method: REVOKE
    data_selector: privileges
    params: {}
- name: users
  endpoint:
    path: /users
    method: GET
    data_selector: records
- name: views
  endpoint:
    path: /views
    method: GET
    data_selector: records
- name: SAML Integration
  endpoint:
    path: /saml-post
    method: POST
    data_selector: response
    params: {}
- name: pki_configuration
  endpoint:
    params:
      ssl-cert: /tls_certs/self_signed_server.example.com_self_signed/self_signed_server.example.com.pem
      ssl-private-key: /tls_certs/self_signed_server.example.com_self_signed/private/self_signed_server.example.com_key.pem
      ssl-trust-store: /tls_certs/self_signed_server.example.com_self_signed/trust_store_self_signed_server.example.com.jks
      ssl-trust-password: truststore_password
      ssl-keystore: /tls_certs/self_signed_server.example.com_self_signed/key_store_self_signed_server.example.com.jks
      ssl-keystore-password: keystore_password
      ssl-trust-ca: /tls_certs/self_signed_server.example.com_self_signed/self_signed_server.example.com.pem
      ssl-trust-ca-server: /tls_certs/ca_primary/ca_primary_cert.pem
- name: view_users_limited
  endpoint:
    path: /create/view/view_users_limited
    method: CREATE
    data_selector: view_users_limited
    params: {}
- name: view_users_full
  endpoint:
    path: /create/view/view_users_full
    method: CREATE
    data_selector: view_users_full
    params: {}
- name: readonly_users
  endpoint:
    path: /create/user
    method: CREATE
    data_selector: readonly_users
    params: {}
- name: roles
  endpoint:
    path: /create/role
    method: CREATE
    data_selector: roles
    params: {}
- name: heavyai_server
  endpoint:
    path: /start/heavyai_server
    method: POST
    params:
      pki-db-client-auth: true
      ssl-cert: /tls_certs/self_signed_server.example.com_self_signed/self_signed_server.example.com.pem
      ssl-private-key: /tls_certs/self_signed_server.example.com_self_signed/private/self_signed_server.example.com_key.pem
      ssl-trust-store: /tls_certs/self_signed_server.example.com_self_signed/trust_store_self_signed_server.example.com.jks
      ssl-trust-password: truststore_password
      ssl-keystore: /tls_certs/self_signed_server.example.com_self_signed/key_store_self_signed_server.example.com.jks
      ssl-keystore-password: keystore_password
      ssl-trust-ca: /tls_certs/self_signed_server.example.com_self_signed/self_signed_server.example.com.pem
      ssl-trust-ca-server: /tls_certs/ca_primary/ca_primary_cert.pem
- name: stream1
  endpoint:
    path: /stream1
    method: POST
    data_selector: rows
    params:
      delimiter: ','
- name: geo
  endpoint:
    path: /load/geo
    method: COPY
    data_selector: records
    params:
      geo: 'true'
- name: legacy_geo
  endpoint:
    path: /load/legacy_geo
    method: COPY
    data_selector: records
    params:
      lonlat: 'false'
- name: esri_file_geodatabase
  endpoint:
    path: /load/esri_gdb
    method: COPY
    data_selector: records
    params:
      geo: 'true'
- name: cluster_configuration
  endpoint:
    path: /path/to/cluster/configuration
    method: GET
    data_selector: nodes
    params: {}
- name: heavy_conf_file
  endpoint:
    path: /path/to/heavy.conf
    method: GET
    data_selector: configuration
    params: {}
- name: heavy_agg_conf_file
  endpoint:
    path: /path/to/heavy-agg.conf
    method: GET
    data_selector: configuration
    params: {}
- name: tweets
  endpoint:
    path: /COPY
    method: COPY
    data_selector: tweets
    params:
      nulls: NA
- name: trips
  endpoint:
    path: /COPY
    method: COPY
    data_selector: trips
    params:
      source_type: parquet_file
- name: KafkaImporter
  endpoint:
    path: /KafkaImporter
    method: GET
    data_selector: data
    params: {}
- name: StreamImporter
  endpoint:
    path: /StreamImporter
    method: GET
    data_selector: data
    params: {}
- name: geo
  endpoint:
    path: /sql/data-definition-ddl/tables#create-table
    method: CREATE
    data_selector: geo
    params: {}
- name: tweets
  endpoint:
    path: /tmp/tweets.csv
    method: COPY
    data_selector: records
    params:
      nulls: '''NA'''
- name: trips
  endpoint:
    path: /mnt/trip/trip.parquet/part-00000-0284f745-1595-4743-b5c4-3aa0262e4de3-c000.snappy.parquet
    method: COPY
    data_selector: records
    params:
      parquet: '''true'''
- name: create_user
  endpoint:
    path: /create_user
    method: POST
    data_selector: records
    params: {}
- name: drop_user
  endpoint:
    path: /drop_user
    method: DELETE
    data_selector: records
    params: {}
- name: alter_user
  endpoint:
    path: /alter_user
    method: PUT
    data_selector: records
    params: {}
- name: create_database
  endpoint:
    path: /create_database
    method: POST
    data_selector: records
    params: {}
- name: drop_database
  endpoint:
    path: /drop_database
    method: DELETE
    data_selector: records
    params: {}
- name: alter_database
  endpoint:
    path: /alter_database
    method: PUT
    data_selector: records
    params: {}
- name: reassign_owned
  endpoint:
    path: /reassign_owned
    method: POST
    data_selector: records
    params: {}
- name: tweets
  endpoint:
    path: /tweets
    method: POST
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /CREATE TABLE customers( accountId TEXT, name TEXT, timeCreated TIMESTAMP)
    method: CREATE
    data_selector: customers
    params: {}
- name: tweets
  endpoint:
    path: /CREATE TABLE IF NOT EXISTS tweets ( tweet_id BIGINT NOT NULL, tweet_time
      TIMESTAMP NOT NULL ENCODING FIXED(32), lat FLOAT, lon FLOAT, sender_id BIGINT
      NOT NULL, sender_name TEXT NOT NULL ENCODING DICT, location TEXT ENCODING  DICT,
      source TEXT ENCODING DICT, reply_to_user_id BIGINT, reply_to_tweet_id BIGINT,
      lang TEXT ENCODING  DICT, followers INT, followees INT, tweet_count INT, join_time
      TIMESTAMP ENCODING  FIXED(32), tweet_text TEXT, state TEXT ENCODING  DICT, county
      TEXT ENCODING DICT, place_name TEXT, state_abbr TEXT ENCODING DICT, county_state
      TEXT ENCODING DICT, origin TEXT ENCODING DICT, phone_numbers bigint)
    method: CREATE
    data_selector: tweets
    params: {}
- name: delta
  endpoint:
    path: /CREATE TABLE delta ( id INTEGER NOT NULL, name TEXT NOT NULL, city TEXT
      NOT NULL DEFAULT 'San Francisco' ENCODING DICT(16))
    method: CREATE
    data_selector: delta
    params: {}
- name: validate_cluster
  endpoint:
    path: /validate/cluster
    method: POST
    data_selector: result
    params: {}
- name: users
  endpoint:
    path: /information_schema/users
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /information_schema/databases
    method: GET
    data_selector: records
- name: permissions
  endpoint:
    path: /information_schema/permissions
    method: GET
    data_selector: records
- name: roles
  endpoint:
    path: /information_schema/roles
    method: GET
    data_selector: records
- name: tables
  endpoint:
    path: /information_schema/tables
    method: GET
    data_selector: records
- name: dashboards
  endpoint:
    path: /information_schema/dashboards
    method: GET
    data_selector: records
- name: role_assignments
  endpoint:
    path: /information_schema/role_assignments
    method: GET
    data_selector: records
- name: memory_summary
  endpoint:
    path: /information_schema/memory_summary
    method: GET
    data_selector: records
- name: memory_details
  endpoint:
    path: /information_schema/memory_details
    method: GET
    data_selector: records
- name: storage_details
  endpoint:
    path: /information_schema/storage_details
    method: GET
    data_selector: records
- name: request_logs
  endpoint:
    path: /information_schema/request_logs
    method: GET
    data_selector: records
- name: server_logs
  endpoint:
    path: /information_schema/server_logs
    method: GET
    data_selector: records
- name: web_server_logs
  endpoint:
    path: /information_schema/web_server_logs
    method: GET
    data_selector: records
- name: web_server_access_logs
  endpoint:
    path: /information_schema/web_server_access_logs
    method: GET
    data_selector: records
- name: create_user
  endpoint:
    path: /create_user
    method: POST
    data_selector: user_creation
    params: {}
- name: drop_user
  endpoint:
    path: /drop_user
    method: POST
    data_selector: user_deletion
    params: {}
- name: alter_user
  endpoint:
    path: /alter_user
    method: POST
    data_selector: user_alteration
    params: {}
- name: create_database
  endpoint:
    path: /create_database
    method: POST
    data_selector: database_creation
    params: {}
- name: drop_database
  endpoint:
    path: /drop_database
    method: POST
    data_selector: database_deletion
    params: {}
- name: alter_database
  endpoint:
    path: /alter_database
    method: POST
    data_selector: database_alteration
    params: {}
- name: validate_cluster
  endpoint:
    path: /validate_cluster
    method: GET
    data_selector: Result
    params: {}
- name: restore_table
  endpoint:
    path: /restore_table
    method: POST
    data_selector: Result
    params: {}
- name: truncate_table
  endpoint:
    path: /truncate_table
    method: POST
    data_selector: Result
    params: {}
- name: optimize_table
  endpoint:
    path: /optimize_table
    method: POST
    data_selector: Result
    params: {}
- name: validate
  endpoint:
    path: /validate
    method: GET
    data_selector: Result
    params: {}
- name: users
  endpoint:
    path: /information_schema/users
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /information_schema/databases
    method: GET
    data_selector: records
- name: permissions
  endpoint:
    path: /information_schema/permissions
    method: GET
    data_selector: records
- name: roles
  endpoint:
    path: /information_schema/roles
    method: GET
    data_selector: records
- name: tables
  endpoint:
    path: /information_schema/tables
    method: GET
    data_selector: records
- name: dashboards
  endpoint:
    path: /information_schema/dashboards
    method: GET
    data_selector: records
- name: role_assignments
  endpoint:
    path: /information_schema/role_assignments
    method: GET
    data_selector: records
- name: memory_summary
  endpoint:
    path: /information_schema/memory_summary
    method: GET
    data_selector: records
- name: memory_details
  endpoint:
    path: /information_schema/memory_details
    method: GET
    data_selector: records
- name: storage_details
  endpoint:
    path: /information_schema/storage_details
    method: GET
    data_selector: records
- name: request_logs
  endpoint:
    path: /information_schema/request_logs
    method: GET
    data_selector: records
- name: server_logs
  endpoint:
    path: /information_schema/server_logs
    method: GET
    data_selector: records
- name: web_server_logs
  endpoint:
    path: /information_schema/web_server_logs
    method: GET
    data_selector: records
- name: web_server_access_logs
  endpoint:
    path: /information_schema/web_server_access_logs
    method: GET
    data_selector: records
- name: mathematical_operators
  endpoint:
    path: /functions/operators/mathematical
    method: GET
    data_selector: operators
    params: {}
- name: comparison_operators
  endpoint:
    path: /functions/operators/comparison
    method: GET
    data_selector: operators
    params: {}
- name: mathematical_functions
  endpoint:
    path: /functions/operators/mathematical_functions
    method: GET
    data_selector: functions
    params: {}
- name: trigonometric_functions
  endpoint:
    path: /functions/operators/trigonometric_functions
    method: GET
    data_selector: functions
    params: {}
- name: geometric_functions
  endpoint:
    path: /functions/operators/geometric_functions
    method: GET
    data_selector: functions
    params: {}
- name: string_functions
  endpoint:
    path: /functions/operators/string_functions
    method: GET
    data_selector: functions
    params: {}
- name: pattern_matching_functions
  endpoint:
    path: /functions/operators/pattern_matching_functions
    method: GET
    data_selector: functions
    params: {}
- name: geospatial_datatypes
  endpoint:
    path: /geospatial/datatype
    method: GET
    data_selector: types
    params: {}
- name: geospatial_functions
  endpoint:
    path: /geospatial/functions
    method: GET
    data_selector: functions
    params: {}
- name: generate_random_strings
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_random_strings
    method: GET
- name: generate_series (Integers)
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_series#generate_series-integers
    method: GET
- name: generate_series (Timestamps)
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_series#generate_series-timestamps
    method: GET
- name: tf_compute_dwell_times
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_compute_dwell_times
    method: GET
- name: tf_feature_self_similarity
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_feature_self_similarity
    method: GET
- name: tf_feature_similarity
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_feature_similarity
    method: GET
- name: tf_geo_rasterize
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_geo_rasterize
    method: GET
- name: tf_geo_rasterize_slope
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_geo_rasterize_slope
    method: GET
- name: tf_graph_shortest_path
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_graph_shortest_path
    method: GET
- name: tf_graph_shortest_paths_distances
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_graph_shortest_paths_distances
    method: GET
- name: tf_load_point_cloud
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_load_point_cloud
    method: GET
- name: tf_point_cloud_metadata
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_point_cloud_metadata
    method: GET
- name: Basic Mathematical Operators
  endpoint:
    path: /functions/operators/mathematical
    method: GET
    data_selector: operators
    params: {}
- name: Comparison Operators
  endpoint:
    path: /functions/operators/comparison
    method: GET
    data_selector: operators
    params: {}
- name: Mathematical Functions
  endpoint:
    path: /functions/operators/mathematical-functions
    method: GET
    data_selector: functions
    params: {}
- name: Trigonometric Functions
  endpoint:
    path: /functions/operators/trigonometric-functions
    method: GET
    data_selector: functions
    params: {}
- name: Geometric Functions
  endpoint:
    path: /functions/operators/geometric-functions
    method: GET
    data_selector: functions
    params: {}
- name: String Functions
  endpoint:
    path: /functions/operators/string-functions
    method: GET
    data_selector: functions
    params: {}
- name: Pattern-Matching Functions
  endpoint:
    path: /functions/operators/pattern-matching-functions
    method: GET
    data_selector: functions
    params: {}
- name: generate_random_strings
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_random_strings
    method: GET
- name: generate_series (Integers)
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_series#generate_series-integers
    method: GET
- name: generate_series (Timestamps)
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/generate_series#generate_series-timestamps
    method: GET
- name: tf_compute_dwell_times
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_compute_dwell_times
    method: GET
- name: tf_feature_self_similarity
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_feature_self_similarity
    method: GET
- name: tf_feature_similarity
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_feature_similarity
    method: GET
- name: tf_geo_rasterize
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_geo_rasterize
    method: GET
- name: tf_geo_rasterize_slope
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_geo_rasterize_slope
    method: GET
- name: tf_graph_shortest_path
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_graph_shortest_path
    method: GET
- name: tf_graph_shortest_paths_distances
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_graph_shortest_paths_distances
    method: GET
- name: tf_load_point_cloud
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_load_point_cloud
    method: GET
- name: tf_point_cloud_metadata
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_point_cloud_metadata
    method: GET
- name: tf_raster_contour_lines
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_raster_contour_lines-tf_raster_contour_polygons
    method: GET
- name: tf_raster_contour_polygons
  endpoint:
    path: /sql/data-manipulation-dml/system-table-functions/tf_raster_contour_lines-tf_raster_contour_polygons
    method: GET
- name: tf_rf_prop
  endpoint:
    path: /heavyrf/heavyrf-table-functions#tf_rf_prop
    method: GET
- name: tf_rf_prop_max_signal (Directional Antennas)
  endpoint:
    path: /heavyrf/heavyrf-table-functions#tf_rf_prop_max_signal-directional-antennas-and-terrain-attenuation
    method: GET
- name: tf_rf_prop_max_signal (Isotropic Antennas)
  endpoint:
    path: /heavyrf/heavyrf-table-functions#tf_rf_prop_max_signal-isotropic-antennas
    method: GET
- name: measures
  endpoint:
    path: /immerse/measures-and-dimensions#measures
    method: GET
    data_selector: measures
    params: {}
- name: dimensions
  endpoint:
    path: /immerse/measures-and-dimensions#dimensions
    method: GET
    data_selector: dimensions
    params: {}
- name: customizing_formats
  endpoint:
    path: /immerse/measures-and-dimensions#customize-formats
    method: GET
    data_selector: formats
    params: {}
- name: Carrier 1
  endpoint:
    params:
      carrier_name: American Airlines
- name: Carrier 2
  endpoint:
    params:
      carrier_name: American Airlines
- name: Group by dimension
  endpoint:
    params:
      plane_model: plane_model
- name: Request Logs and Monitoring
  endpoint:
    path: /request_logs
    method: GET
    data_selector: charts
    params: {}
- name: System Resources
  endpoint:
    path: /system_resources
    method: GET
    data_selector: charts
    params: {}
- name: User Roles and Permissions
  endpoint:
    path: /user_roles_permissions
    method: GET
    data_selector: charts
    params: {}
- name: supported_charts
  endpoint:
    path: /supported_charts
    method: GET
    data_selector: charts
    params: {}
- name: crossfilter_replay
  endpoint:
    path: /immerse/customization#feature-flags
    method: GET
    data_selector: feature_flags
    params:
      ui/enable_crossfilter_replay: 'true'
      ui/enable_chart_addons: 'true'
      performance/query_cache_duration: '240000'
      performance/query_cache_size: '50'
      performance/vega_cache_duration: '240000'
      performance/vega_cache_size: '40'
- name: feature_flags
  endpoint:
    path: /immerse/admin-portal#feature-flags
    method: GET
    data_selector: featureFlags
    params: {}
- name: system_resources
  endpoint:
    path: /immerse/admin-portal#system-resources-dashboard
    method: GET
    data_selector: systemResources
    params: {}
- name: request_logs
  endpoint:
    path: /immerse/admin-portal#request-logs-and-monitoring
    method: GET
    data_selector: requestLogs
    params: {}
- name: user_roles
  endpoint:
    path: /immerse/admin-portal#user-roles-and-permissions-dashboard
    method: GET
    data_selector: userRoles
    params: {}
- name: access_logs
  endpoint:
    path: /immerse/admin-portal#web-server-logs
    method: GET
    data_selector: accessLogs
    params: {}
- name: all_logs
  endpoint:
    path: /immerse/admin-portal#web-server-logs
    method: GET
    data_selector: allLogs
    params: {}
- name: error_logs
  endpoint:
    path: /immerse/admin-portal#database-logs
    method: GET
    data_selector: errorLogs
    params: {}
- name: info_logs
  endpoint:
    path: /immerse/admin-portal#database-logs
    method: GET
    data_selector: infoLogs
    params: {}
- name: warning_logs
  endpoint:
    path: /immerse/admin-portal#database-logs
    method: GET
    data_selector: warningLogs
    params: {}
- name: servers
  endpoint:
    path: /servers.json
    method: GET
    data_selector: servers
    params: {}
- name: BoxPlot
  endpoint:
    path: /charts/boxplot
    method: GET
    data_selector: data
    params: {}
- name: ViolinPlot
  endpoint:
    path: /charts/violinplot
    method: GET
    data_selector: data
    params: {}
- name: records_by_arr_timestamp_carrier_1
  endpoint:
    path: /path/to/records_by_arr_timestamp_carrier_1
    method: GET
    data_selector: records
    params:
      carrier_name: American Airlines
      group_by: plane_model
- name: records_by_arr_timestamp_carrier_2
  endpoint:
    path: /path/to/records_by_arr_timestamp_carrier_2
    method: GET
    data_selector: records
    params:
      carrier_name: American Airlines
      group_by: plane_model
- name: filter_set
  endpoint:
    path: /filter/set
    method: GET
    data_selector: filterSets
    params: {}
- name: chart
  endpoint:
    path: /charts
    method: GET
    data_selector: charts
    params: {}
- name: crossCountry
  endpoint:
    path: /import/data
    method: POST
    data_selector: data
    params: {}
- name: number_chart
  endpoint:
    path: /immerse/number-chart
    method: GET
    data_selector: quantity
- name: servers
  endpoint:
    path: /servers.json
    method: GET
    data_selector: servers
- name: Size Domain
  endpoint:
    data_selector: bounds
    params:
      minimum: size_min
      maximum: size_max
- name: Size Range
  endpoint:
    data_selector: sizes
    params:
      min_size: 1
      max_size: 20
- name: Mark Shape
  endpoint:
    data_selector: shapes
    params: {}
- name: Density Gradient
  endpoint:
    data_selector: density
    params: {}
- name: Box Plot
  endpoint:
    path: /immerse/box-plot
    method: GET
- name: Violin Plot
  endpoint:
    path: /immerse/violin-plot
    method: GET
- name: stacked_bar_chart
  endpoint:
    path: /immerse/stacked_bar_chart
    method: GET
    data_selector: records
- name: crossCountry
  endpoint:
    path: /path/to/crossCountry
    method: POST
    data_selector: records
    params: {}
- name: pointmap
  endpoint:
    path: /ufo-sightings
    method: GET
- name: tf_rf_prop_max_signal
  endpoint:
    path: /heavyrf/heavyrf-table-functions/tf_rf_prop_max_signal
    method: GET
    data_selector: records
    params: {}
- name: tf_rf_prop_max_signal
  endpoint:
    path: /TABLE(tf_rf_prop)
    method: SELECT
    data_selector: '*'
    params: {}
- name: tf_rf_prop
  endpoint:
    path: /tf_rf_prop
    method: SELECT
    data_selector: records
- name: foreign_data_wrapper
  endpoint:
    path: /foreign_data_wrapper
    method: GET
    data_selector: data
    params: {}
- name: foreign_server
  endpoint:
    path: /foreign_server
    method: GET
    data_selector: data
    params: {}
- name: foreign_table
  endpoint:
    path: /foreign_table
    method: GET
    data_selector: data
    params: {}
- name: user_mapping
  endpoint:
    path: /user_mapping
    method: GET
    data_selector: data
    params: {}
- name: example_year_2020
  endpoint:
    path: /2020
    method: CREATE
    data_selector: records
    params:
      REFRESH_TIMING_TYPE: SCHEDULED
      REFRESH_UPDATE_TYPE: APPEND
      REFRESH_START_DATE_TIME: 2020-01-31 22:30
      REFRESH_INTERVAL: 31D
- name: example_month_march
  endpoint:
    path: /Mar
    method: CREATE
    data_selector: records
    params:
      REFRESH_TIMING_TYPE: SCHEDULED
      REFRESH_UPDATE_TYPE: APPEND
      REFRESH_START_DATE_TIME: 2020-03-01 22:30
      REFRESH_INTERVAL: 7D
- name: example_day_march_01
  endpoint:
    path: /Mar/Mar_01_2020.parquet
    method: CREATE
    data_selector: records
    params:
      REFRESH_TIMING_TYPE: SCHEDULED
      REFRESH_UPDATE_TYPE: ALL
      REFRESH_START_DATE_TIME: 2020-03-01 23:30
      REFRESH_INTERVAL: 1D
- name: example_yoy_january
  endpoint:
    path: /yoy_january
    method: CREATE
    data_selector: records
    params:
      REFRESH_TIMING_TYPE: manual
      REFRESH_UPDATE_TYPE: ALL
- name: access_logs
  endpoint:
    path: /logs/sample.log
    method: CREATE
    data_selector: records
    params:
      line_regex: ^\d+\.\d+\.\d+\.\d+\s+-\s+(\w+)\s+\[([^\]]+)\]\s+"(\w+)\s+([^\s]+)\s+HTTP\/1\.1"\s+(\d+)\s+(\d+)$
- name: stacked_bar_chart
  endpoint:
    path: /immerse/stacked_bar_chart
    method: GET
    data_selector: records
- name: example_table
  endpoint:
    path: /remote_postgres_table
    method: SELECT
    data_selector: '*'
    params:
      sql_select: SELECT * FROM remote_postgres_table WHERE event_timestamp > $$2020-01-01$$
      sql_order_by: event_timestamp
- name: my_csv_server
  endpoint:
    path: /home/my_user/data/csv/
    method: CREATE
    data_selector: ''
    params:
      storage_type: LOCAL_FILE
      base_path: /home/my_user/data/csv/
- name: my_aws_server
  endpoint:
    path: ''
    method: CREATE
    data_selector: ''
    params:
      storage_type: AWS_S3
      s3_bucket: fsi-s3-bucket
      aws_region: us-west-1
- name: my_postgres_server
  endpoint:
    path: ''
    method: CREATE
    data_selector: ''
    params:
      data_source_name: postgres_db_1
- name: my_postgres_server
  endpoint:
    path: ''
    method: CREATE
    data_selector: ''
    params:
      connection_string: Driver=PostgreSQL;Database=my_postgres_db;Servername=my_postgres.example.com;Port=1234
- name: create_user_mapping
  endpoint:
    path: /create_user_mapping
    method: CREATE
    data_selector: user_mapping
    params: {}
- name: delete_user_mapping
  endpoint:
    path: /delete_user_mapping
    method: DELETE
    data_selector: user_mapping
    params: {}
- name: raster_table
  endpoint:
    path: /create_foreign_table
    method: CREATE
    data_selector: records
    params: {}
- name: terrain_data
  endpoint:
    path: /heavyrf/terrain
    method: GET
    data_selector: records
- name: antenna_data
  endpoint:
    path: /heavyrf/antenna
    method: GET
    data_selector: records
- name: KMeans
  endpoint:
    path: /kmeans
    method: SELECT
    data_selector: id, cluster_id
    params:
      num_clusters: <number of clusters>
      num_iterations: <number of iterations>
      init_type: <initialization type>
- name: DBScan
  endpoint:
    path: /dbscan
    method: SELECT
    data_selector: id, cluster_id
    params:
      epsilon: <neighborhood radius around each point>
      min_observations: <minimum number of observations (rows) per cluster>
- name: tf_rf_prop_max_signal
  endpoint:
    path: /heavyrf/heavyrf-table-functions#tf_rf_prop_max_signal-directional-antennas
    method: SELECT
    data_selector: records
    params: {}
- name: tf_rf_prop_max_signal
  endpoint:
    path: /api/tf_rf_prop_max_signal
    method: POST
    data_selector: results
    params: {}
- name: models
  endpoint:
    path: /SHOW MODELS
    method: GET
    data_selector: model_name
    params: {}
- name: model_details
  endpoint:
    path: /SHOW MODEL DETAILS
    method: GET
    data_selector: model_name
    params: {}
- name: model_feature_details
  endpoint:
    path: /SHOW MODEL FEATURE DETAILS
    method: GET
    data_selector: feature_id
    params: {}
- name: ml_models
  endpoint:
    path: /information_schema/ml_models
    method: GET
    data_selector: '*'
    params: {}
- name: tf_rf_prop
  endpoint:
    path: /tf_rf_prop
    method: SELECT
    data_selector: TABLE
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: random_forest_regression
  endpoint:
    path: /random_forest_regression
    method: CREATE
    data_selector: model
    params:
      num_trees: 10
      obs_per_tree_fraction: 1
      max_tree_depth: 0
      features_per_node: square root of the total number of features in the dataset
      impurity_threshold: 0
      bootstrap: true
      min_obs_per_leaf_node: 5
      min_obs_per_split_node: 2
      min_weight_fraction_in_leaf_node: 0.0
      min_impurity_decrease_in_split_node: 0.0
      max_leaf_nodes: 0
      use_histogram: true
      var_importance_metric: MDI
- name: fl_parcel_price_dt
  endpoint:
    path: /path/to/decision_tree_regression
    method: CREATE
    data_selector: model
    params: {}
- name: access_logs
  endpoint:
    path: /logs/sample.log
    method: GET
    data_selector: records
    params: {}
- name: example_year_2020
  endpoint:
    path: /services/data/vXX.X/sobjects/ExampleYear2020
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: example_month_march
  endpoint:
    path: /services/data/vXX.X/sobjects/ExampleMonthMarch
    method: GET
    data_selector: records
    params: {}
- name: example_day_march_01
  endpoint:
    path: /services/data/vXX.X/sobjects/ExampleDayMarch01
    method: GET
    data_selector: records
    params: {}
- name: example_table
  endpoint:
    path: /remote_postgres_table
    method: SELECT
    data_selector: '*'
    params:
      sql_select: SELECT * FROM remote_postgres_table WHERE event_timestamp > $$2020-01-01$$
      sql_order_by: event_timestamp
- name: pca_model_creation
  endpoint:
    path: /heavyml/pca/model
    method: CREATE
    data_selector: model_creation
    params: {}
- name: pca_model_details
  endpoint:
    path: /heavyml/pca/model/details
    method: SHOW
    data_selector: model_details
    params: {}
- name: pca_projection
  endpoint:
    path: /heavyml/pca/project
    method: SELECT
    data_selector: pca_projection
    params: {}
- name: my_csv_server
  endpoint:
    path: /home/my_user/data/csv/
    method: CREATE
    data_selector: ''
    params:
      storage_type: LOCAL_FILE
      base_path: /home/my_user/data/csv/
- name: my_aws_server
  endpoint:
    path: /home/my_user/data/csv/
    method: CREATE
    data_selector: ''
    params:
      storage_type: AWS_S3
      s3_bucket: fsi-s3-bucket
      aws_region: us-west-1
- name: my_postgres_server
  endpoint:
    path: ''
    method: CREATE
    data_selector: ''
    params:
      data_source_name: postgres_db_1
- name: my_postgres_server
  endpoint:
    path: ''
    method: CREATE
    data_selector: ''
    params:
      connection_string: Driver=PostgreSQL;Database=my_postgres_db;Servername=my_postgres.example.com;Port=1234
- name: flights
  endpoint:
    path: /flights
    method: GET
    data_selector: results
- name: user_mapping
  endpoint:
    path: /CREATE_USER_MAPPING
    method: POST
    data_selector: user_mapping
    params: {}
- name: delete_user_mapping
  endpoint:
    path: /DELETE_USER_MAPPING
    method: POST
    data_selector: delete_user_mapping
    params: {}
- name: servers
  endpoint:
    path: /heavyai-storage/servers.json
    method: GET
    data_selector: servers
    params: {}
- name: heavyaiserver
  endpoint:
    path: /heavyai-storage
    method: GET
    data_selector: servers.json
- name: raster_table
  endpoint:
    path: /raster_table
    method: CREATE
    data_selector: table
    params:
      file_path: <path>
- name: jhub_heavyai_dropbox
  endpoint:
    path: /var/lib/heavyai/data/heavyai_import/jhub_heavyai_dropbox
    method: POST
    data_selector: records
- name: jupyter_user_volume
  endpoint:
    path: /var/lib/heavyai/jupyter/
    method: POST
    data_selector: records
- name: heavydb
  endpoint:
    path: /heavydb
    method: POST
    data_selector: results
- name: fahrenheit2celsius
  endpoint:
    path: /register/function
    method: POST
    data_selector: function
    params: {}
- name: kmeans
  endpoint:
    path: /kmeans
    method: POST
    data_selector: id, cluster_id
    params: {}
- name: dbscan
  endpoint:
    path: /dbscan
    method: POST
    data_selector: id, cluster_id
    params: {}
- name: fahrenheit2celsius
  endpoint:
    path: /udtf/fahrenheit2celsius
    method: POST
    data_selector: output
    params: {}
- name: models
  endpoint:
    path: /SHOW_MODELS
    method: GET
    data_selector: model_name
- name: model_details
  endpoint:
    path: /SHOW_MODEL_DETAILS
    method: GET
    data_selector: model_name|model_type|predicted|predictors|training_query|num_logical_features|num_physical_features|num_categorical_features|num_numeric_features|eval_fraction
- name: model_feature_details
  endpoint:
    path: /SHOW_MODEL_FEATURE_DETAILS
    method: GET
    data_selector: feature_id|feature|sub_feature_id|sub_feature|coefficient
- name: ml_models
  endpoint:
    path: /information_schema/ml_models
    method: GET
    data_selector: model_name|model_type|predicted|predictors|training_query|num_logical_features|num_physical_features|num_categorical_features|num_numeric_features|eval_fraction
- name: github
  endpoint:
    path: /github
    method: GET
    data_selector: records
- name: flights_donotmodify
  endpoint:
    path: /flights_donotmodify
    method: GET
    data_selector: records
- name: flights_donotmodify
  endpoint:
    path: /flights_donotmodify
    method: GET
    data_selector: records
    params: {}
- name: model_creation
  endpoint:
    path: /create_model
    method: POST
    data_selector: model_info
    params: {}
- name: make_prediction
  endpoint:
    path: /predict
    method: POST
    data_selector: prediction_result
    params: {}
- name: jdbc
  endpoint:
    path: jdbc:heavyai:<host>:[<keyvalue1>:<keyvalue2>:<keyvalue3>...][?<key1=value1>&<key2=value2>&<key3=value3>...]
    method: CONNECT
    data_selector: connection
    params:
      max_rows: '10000'
- name: model_creation
  endpoint:
    path: /model_creation
    method: POST
    data_selector: model
    params:
      model_name: fl_parcel_price_gbt
      type: GBT_REG
      select_statement: SELECT SALEPRC1, PARUSEDESC, CNTYNAME, ACRES, TOTLVGAREA,
        EFFYRBLT, SALEYR1 FROM fl_res_parcels_2018
      options:
        CAT_TOP_K: 70
        EVAL_FRACTION: 0.2
- name: model_prediction
  endpoint:
    path: /model_prediction
    method: POST
    data_selector: prediction
    params:
      model_name: fl_parcel_price_gbt
      predictors:
      - PARUSEDESC
      - CNTYNAME
      - ACRES
      - TOTLVGAREA
      - EFFYRBLT
      - SALEYR1
- name: flights
  endpoint:
    path: /flights
    method: GET
    data_selector: result
    params: {}
- name: enmap_hyperspectral_pca
  endpoint:
    path: /create_model
    method: POST
    data_selector: model
    params: {}
- name: PCA_PROJECT
  endpoint:
    path: /pca_project
    method: POST
    data_selector: pca_values
    params: {}
- name: Heavy.AI
  endpoint:
    path: /odbc/heavyai
    method: GET
    data_selector: records
    params: {}
- name: flights
  endpoint:
    path: /flights
    method: GET
    data_selector: records
- name: heavyaiserver
  endpoint:
    path: /heavyai-storage/heavyai.conf
    method: GET
    data_selector: servers.json
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: vega_specification
  endpoint:
    path: /apis-and-interfaces/vega/vega-reference-overview
    method: GET
    data_selector: ''
    params: {}
- name: jhub_heavyai_dropbox
  endpoint:
    path: /var/lib/heavyai/data/heavyai_import/jhub_heavyai_dropbox
    method: POST
    data_selector: records
- name: heavyai_jupyter
  endpoint:
    path: /heavyai-storage/servers.json
    method: GET
    data_selector: enableJupyter
- name: heavy_data
  endpoint:
    path: /heavy-data-endpoint
    method: GET
    data_selector: data
    params: {}
- name: HEAVY.AI
  endpoint:
    path: jdbc:heavyai:machine:9091:dbname
    method: GET
    data_selector: records
- name: zipcodes_2017
  endpoint:
    path: /table/zipcodes_2017
    method: GET
- name: flights_donotmodify
  endpoint:
    path: /flights_donotmodify
    method: GET
- name: tweets
  endpoint:
    path: /tweets_nov_feb
    method: GET
    data_selector: results
    params:
      country: CO
- name: flights
  endpoint:
    path: /flights_donotmodify
    method: GET
    data_selector: results
    params: {}
- name: flights
  endpoint:
    path: /flights_donotmodify
    method: GET
    data_selector: records
    params: {}
- name: flavors
  endpoint:
    path: /flavors
    method: POST
    data_selector: value
    params: {}
- name: movies
  endpoint:
    path: /movies
    method: GET
    data_selector: records
    params: {}
- name: jdbc_connection
  endpoint:
    path: jdbc:heavyai:<host>:[<keyvalue1>:<keyvalue2>:<keyvalue3>...][?<key1=value1>&<key2=value2>&<key3=value3>...]
    method: GET
    data_selector: records
    params: {}
- name: ODBC Data Source
  endpoint:
    path: /path/to/odbc/source
    method: GET
    data_selector: records
    params:
      incremental: connection_timeout
- name: polygon
  endpoint:
    path: /api/v1/polygon
    method: GET
    data_selector: data
    params: {}
- name: multilinestring
  endpoint:
    path: /api/v1/multilinestring
    method: GET
    data_selector: data
    params: {}
- name: HEAVY.AI Driver
  endpoint:
    path: /drivers
    method: POST
- name: HEAVY.AI Alias
  endpoint:
    path: /aliases
    method: POST
- name: tweets
  endpoint:
    path: /tweets_nov_feb
    method: GET
    data_selector: results
    params:
      country: CO
- name: flights
  endpoint:
    path: /flights_donotmodify
    method: GET
    data_selector: results
    params: {}
- name: database
  endpoint:
    path: /api/v1/databases
    method: GET
    data_selector: databases
    params: {}
- name: flavors
  endpoint:
    path: /flavors
    method: INSERT
    data_selector: flavor
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: OmniSciDB
  endpoint:
    path: /services/data/vXX.X/sobjects/OmniSciDB
    method: GET
    data_selector: records
    params: {}
- name: flights
  endpoint:
    path: /flights
    method: GET
    data_selector: results
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: user_details
  endpoint:
    path: /services/data/v5.6/sobjects/UserDetails
    method: GET
    data_selector: records
- name: geospatial_data
  endpoint:
    path: /services/data/v5.6/sobjects/GeospatialData
    method: GET
    data_selector: records
- name: view_queries
  endpoint:
    path: /services/data/vXX.X/sobjects/ViewQueries
    method: GET
    data_selector: records
- name: user_roles
  endpoint:
    path: /services/data/vXX.X/sobjects/UserRoles
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /login_panel_databases
    method: GET
    data_selector: databases
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: flights
  endpoint:
    path: /flights
    method: GET
    data_selector: records
    params:
      limit: 100
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- In HeavyDB Release 8.x.x, the system catalog is automatically migrated to support
  the new Column Level Security feature.
- Uses SSDs for the Storage Layer to minimize latency.
- Optimal GPUs include NVIDIA Tesla A100, V100 v2, and V100 v1.
- The recommended NVIDIA driver for all supported HEAVY.AI versions is 535.
- SELECT privileges are now required in order to execute UPDATE or DELETE commands.
- Adds a new "columns" system table that contains information about all table columns
  across all databases.
- The Host ID can be displayed using the heavysql “\status” command.
- Changing fundamental hardware components of the machine can result in a change in
  the Host ID.
- Uses SSD storage for optimal performance.
- Avoid mixing card types in the same system.
- 'Ubuntu 22.04 - EOS: June 2027 <https://wiki.ubuntu.com/Releases>'
- 'Ubuntu 20.04 - EOS: April 2025 <https://wiki.ubuntu.com/Releases>'
- Ubuntu 18.04 - EOS - June 2023 <https://wiki.ubuntu.com/Releases>
- CentOS 8 EOL - December 31, 2021 <https://www.redhat.com/en/events/webinar/centos-linux-reaching-its-end-life-now-what>
- Node locked licenses restrict the use of the HEAVY.AI platform to a single machine.
- Floating licenses allow for the deployment of the HEAVY.AI platform on multiple
  machines on the same network.
- Requires a security key pair to launch HEAVY.AI instance on AWS.
- Connect to Heavy Immerse using public IP and specified port.
- Azure-specific configuration is complete.
- Version of heavyai to install in the format 'v7.0.0' or 'latest' for the latest
  version released.
- Persistant volume claim name to use with heavyai.
- Namespace to install heavyai in.
- Number or GPU's to assign to heavyai or 0 to run the CPU version of heavyai.
- NodeName to install heavyai on, if you wish to let Kubernetes schedule a host, leave
  it blank.
- Immerse port redirect of 6273.
- TCP port redirect of 6274.
- HTTP port redirect of 6278.
- Uses SSH with private key for connection
- Default user is centos or ubuntu depending on the version
- Record the Admin password (Temporary)
- Copy your license key from the registration email message
- HEAVY.AI Installation files are located at /opt/heavyai
- Configuration files and storage are located at /var/lib/heavyai
- Use a GPU-enabled instance for HEAVY.AI on Microsoft Azure.
- Allow inbound port 6273 for Heavy Immerse access.
- HEAVY.AI recommends heavyai as the install directory.
- Must be dedicated to HEAVY.AI.
- initdb initializes the data directory and creates subdirectories for catalogs, data,
  logs, and disk cache.
- The -f flag forces initdb to overwrite existing data and catalogs.
- The --skip-geo flag prevents loading sample geospatial data.
- PVC name is provided to the helm install command.
- PVC and PV space defined for the HEAVY.AI instance is not removed on uninstall.
- Some objects like Contact may return nulls in deeply nested fields
- The $HEAVYAI_PATH defaults to '/opt/heavyai'
- The $HEAVYAI_BASE directory defaults to '/var/lib/heavyai'
- HEAVY.AI requires that all configuration flags used at startup match a flag on the
  HEAVY.AI server.
- The heavy.conf file is stored in the $HEAVYAI_BASE directory.
- For encrypted backend connections, if you do not use a configuration file to start
  the database, Calcite expects passwords to be supplied through the command line.
- Allowed file paths are enforced by default. The default export path (<data directory>/heavyai_export)
  is allowed by default, and all child paths of that path are allowed.
- The -f flag forces initdb to overwrite existing data and catalogs in the specified
  directory.
- By default, initdb adds a sample table of geospatial data.
- 'Required for running SAML. An identity provider (like Okta) supplies a metadata
  file. From this file, HEAVY.AI uses: 1. Public key of the identity provider to verify
  that the SAML response comes from it and not from somewhere else. 2. URL of the
  SSO login page used to obtain a SAML token.'
- Required for running SAML. Used to verify that a SAML token was issued for HEAVY.AI
  and not for some other service.
- ERM is enabled by default.
- If HeavyDB is compiled to run on GPUs and if GPUs are available, query steps/kernels
  will execute on GPU unless certain conditions are met.
- Strings must be enclosed in quotes in the configuration file.
- Allowing any origin is a less secure mode than what heavy_web_server requires by
  default.
- Use to provide an HEAVY.AI front end that can run on both the HTTP protocol and
  on the primary HTTPS protocol.
- Uses OAuth2 — requires setup of connected app in Heavy.ai
- Maximum size of the hashtable that the hashtable recycler can store is 2147483648
  (2GB).
- Sets the maximum number of GPUs that each query can use is 0.
- Maximum duration of the active session is 43200 minutes (30 days).
- A value that determines how the result of the Number of Distinct Values (NDV) group
  estimator is scaled is 1.5.
- Allows processing to complete when the dataset would cause a divide by zero error
  is FALSE.
- Number of GPUs to use is -1.
- Number of reader threads to use is 0.
- HeavyDB server port is 6274.
- Frequency with which to check the interrupt status of pending queries is 1000 milliseconds
  (1 sec).
- Attempt authentication of users through a PKI certificate is FALSE.
- Enable read-only mode is FALSE.
- Specifies the size of a per-GPU buffer that render query results are written to
  is 500000000 bytes.
- Enable or disable backend rendering is TRUE.
- Reserved memory for GPU is 134217728 bytes.
- The maximum number of rows in the inner table of a loop join considered to be trivially
  small is 1000.
- Use the CPU memory buffer pool for output buffers is TRUE.
- Set to TRUE to enable the hashtable recycler is TRUE.
- Specify the percentage of deleted rows in a fragment at which to perform automatic
  vacuuming is 0.1.
- The number of strings that can be casted using the ENCODED_TEXT string operator
  is 1000000.
- Tree fan out of the aggregation tree is used to compute aggregation over the window
  frame is 8.
- Requires setup of connected app in HeavyDB
- Some configurations like SAML require additional metadata files
- Roles can be created and dropped at any time.
- Object privileges and roles can be granted or revoked at any time, and the action
  takes effect immediately.
- User readonly1 sees no tables, only the specific view granted, and only the three
  specific columns returned in the view.
- User readonly2 sees no tables, only the specific view granted, and all seven columns
  returned in the view.
- Uses SAML for authentication.
- Automatically falls back to LDAP if SAML login fails.
- Privileges granted on a database-type object are inherited by all tables of that
  database.
- Passwords for the SSL truststore and keystore can be enclosed in single (') or double
  (" ) quotes.
- Revoke all possible access privileges on <dashboardId> for <entityList>.
- Revoke CREATE privilege for <entityList>.
- Revoke DELETE privilege on <dashboardId> for <entityList>.
- Revoke EDIT privilege on <dashboardId> for <entityList>.
- Revoke VIEW privilege on <dashboardId> for <entityList>.
- Set the file path of the encryption key file to the encryption-key-file-path web
  server parameter in heavyai.conf
- If SAML login fails, HEAVY.AI automatically falls back to log in using LDAP if it
  is configured.
- If both SAML and LDAP authentication fail, you are authenticated against a locally
  stored password, but only if the `allow-local-auth-fallback` flag is set.
- When installing a distributed cluster, you must run `initdb --skip-geo` to avoid
  the automatic creation of the sample geospatial data table.
- The parameter `string-servers` identifies the file containing the cluster configuration.
- If your aggregator node is sharing a machine with a leaf node, there might be a
  conflict on the `calcite-port`.
- Start pki authentication requires configuration in heavyai.conf for encrypted connection.
- Uses Apache Kafka for distributed streaming
- Generate a 128- or 256-bit encryption key and save it to a file.
- Set the file path of the encryption key file to the encryption-key-file-path web
  server parameter in heavyai.conf.
- HEAVY.AI supports only delimiter-separated formats such as CSV and TSV.
- HEAVY.AI supports Latin-1 ASCII format and UTF-8.
- LDAP mapping is available only in HEAVY.AI Enterprise edition.
- If you do not import the .prj file, the coordinate system will be incorrect.
- You can import geo files or archives directly from an Amazon S3 bucket.
- Each leaf node requires a heavy.conf configuration file.
- The aggregator node requires a heavy-agg.conf configuration file.
- Uses Kafka for data streaming and ingestion into HeavyDB
- 'Shapefiles include four mandatory files: .shp, .shx, .dbf, and .prj.'
- If you upload shapefiles separately, Heavy Immerse issues an error message.
- HEAVY.AI accepts data with any SRID, or with no SRID.
- If two geometries are used in one operation, the SRID values need to match.
- Rendering of geo LINESTRING, MULTILINESTRING, POLYGON and MULTIPOLYGON is possible
  only with data stored in the default lon/lat WGS84 (SRID 4326) format.
- HEAVY.AI does not use file extensions to determine the delimiter.
- HEAVY.AI does not support geometry arrays.
- Timestamp values are always stored in 8 bytes. The greater the precision, the lower
  the fidelity.
- WGS84 (SRID 4326) coordinates are compressed to 32 bits by default. This sacrifices
  some precision but reduces storage requirements by half.
- You can explicitly disable compression.
- Fixed-length arrays require less storage space than variable-length arrays.
- Understanding your schema and the scope of potential values in each field helps
  you to apply fixed encoding types and save significant storage space.
- COPY FROM appends data from the source into the target table. It does not truncate
  the table or overwrite existing data.
- Uses AWS S3 for file import
- HEAVY.AI recommends using a service account with read-only permissions
- KafkaImporter requires a functioning Kafka cluster
- Data format must be a record-level format supported by HEAVY.AI
- Spaces and special characters other than underscore (_) cannot be used in Heavy
  Immerse.
- Column and table names enclosed in double quotes cannot be used in Heavy Immerse.
- Perform checks and report discovered issues on a running HEAVY.AI cluster.
- WGS84 (SRID 4326) coordinates are compressed to 32 bits by default.
- Some of the INTEGER options overlap.
- Log-based system tables are considered beta functionality in Release 6.1.0 and are
  disabled by default.
- User names can be enclosed in optional double quotation marks.
- Database names cannot include quotes, spaces, or special characters.
- Database names are case insensitive in Release 6.3.0 and later.
- 'View object names must use the NAME format, described in regex notation as: [A-Za-z_][A-Za-z0-9\$_]*'
- Admin rights are required to create or drop RLS policies.
- RESTORE command is not supported on distributed configurations.
- Currently comments are not supported with the CREATE TABLE command, and COMMENT
  ON is the canonical means to set or unset comments.
- Currently COMMENT ON is supported only on tables and and columns of that table.
  Other objects such as VIEW are not currently supported.
- HEAVY.AI supports SRID 4326 (WGS 84) and 900913 (Google Web Mercator), and 32601-32660,
  32701-32760 (Universal Transverse Mercator (UTM) Zones).
- CREATE TABLE AS SELECT is not currently supported for geo data types in distributed
  mode.
- GROUP BY is not supported for geo types (POINT, MULTIPOINT, LINESTRING, MULTILINESTRING,
  POLYGON, or MULTIPOLYGON).
- Admin rights are required to create or drop RLS policies
- COUNT(DISTINCT x), especially when used in conjunction with GROUP BY, can require
  a very large amount of memory to keep track of all distinct values in large tables
  with large cardinalities.
- APPROX_COUNT_DISTINCT(x, e) gives an approximate count of the value x, based on
  an expected error rate defined in e.
- The accuracy of APPROX_MEDIAN (x) depends on the distribution of data.
- To use window framing, you may need an ORDER BY clause in the window definition.
- Currently, all window functions including aggregation over window frame are computed
  via CPU-mode.
- For window frame bound expressions, only non-negative integer literals are supported.
- GROUPING mode and EXCLUDING are not currently supported.
- COUNT(DISTINCT x), especially when used in conjunction with GROUP BY, can require
  a very large amount of memory to keep track of all distinct values in large tables
  with large cardinalities. To avoid this large overhead, use APPROX_COUNT_DISTINCT.
- APPROX_COUNT_DISTINCT(x, e) gives an approximate count of the value x, based on
  an expected error rate defined in e, where e is an integer value from 1 to 100.
  The lower the value of e, the higher the precision, and the higher the memory cost.
  Select a value for e based on the level of precision required. On large tables with
  large cardinalities, consider using APPROX_COUNT_DISTINCT when possible to preserve
  memory. When data cardinalities permit, OmniSci uses the precise implementation
  of COUNT(DISTINCT x) for APPROX_COUNT_DISTINCT. Set the default error rate using
  the -hll-precision-bits configuration parameter.
- 'The accuracy of APPROX_MEDIAN (x) upon the distribution of data. For example: For
  100,000,000 integers (1, 2, 3, ... 100M) in random order, APPROX_MEDIAN can provide
  a highly accurate answer 5+ significant digits. For 100,000,001 integers, where
  50,000,000 have value of 0 and 50,000,001 have value of 1, APPROX_MEDIAN returns
  a value close to 0.5, even though the median is 1.'
- Currently, OmniSci does not support grouping by non-dictionary-encoded strings.
  However, with the SAMPLE aggregate function, you can select non-dictionary-encoded
  strings that are presumed to be unique in a group.
- LLM_TRANSFORM operator requires HeavyIQ to be configured, otherwise the operator
  will error.
- By default, LLM_TRANSFORM is restricted to run on up to 1,000 unique string inputs
  per operator call.
- The database will only run LLM_TRANSFORM once per unique input to allow for performant
  use of the operator on large tables with low cardinality columns.
- The database will try to push down any filters below an LLM_TRANSFORM call.
- Currently embedded pipe characters for output choice constraints, or leading or
  trailing forward slashes for output regex constraints are not allowed.
- The Request Logs and Monitoring dashboard does not appear in the Admin portal by
  default.
- To turn on the dashboard, set the enable-logs-system-tables parameter to TRUE in
  heavy.conf and restart the database.
- The Control Panel is considered beta functionality.
- Currently, you cannot add, delete, or edit roles or users in the Control Panel.
- Feature flags cannot be modified through the Control Panel.
- The theme setting is saved on the browser you are currently using. The setting does
  not affect other machines or other browsers you might use
- When you create a map, the theme defaults to Current Immerse Theme. This is necessary
  to automatically update a map when you toggle themes. You can still explicitly choose
  Light Mode or Dark Mode, but if you do so the map stays in that theme, regardless
  of the Immerse theme. Existing maps have a hard coded theme and are currently going
  to stay that way. HEAVY.AI recommends that you use Current Immerse Theme, unless
  you want the map to always display in Light Mode or Dark Mode.
- You can also configure the theme in the servers.json/servers.local.json file. Add
  a new key for theme and set it to dark. If you set no key, the default is light.
- The system theme setting only takes effect if you have never switched themes. Once
  you choose your own preferred theme, it always overrides the system setting.
- Users that have been assigned the restricted_sharing role cannot share a dashboard,
  and the Share icon is unavailable.
- Measures can be aggregated using SUM, AVERAGE, MIN, MAX, etc.
- Dimensions group measures and can be numerical or categorical.
- To make the chart easier to read, apply the format %m/%d/%y to dimension date formatting
  and SI to measure number formatting
- To simplify the percentage format to one decimal place, apply ,.1% to the Custom
  Profit Ratio
- In the following Bar chart, the average profit by state is expressed in whole dollars,
  both positive and negative, by using the ($0.f format in the Avg Profit input box
- The LLM_TRANSFORM operator requires HeavyIQ to be configured, otherwise the operator
  will error.
- By default, LLM_TRANSFORM is restricted to run on up to 1,000 unique string inputs
  per operator call. This limit can be changed at HeavyDB startup via the configuration
  flag --llm-transform-max-num-unique-value.
- The database will try to push down any filters below an LLM_TRANSFORM call, so that
  LLM_TRANSFORM needs to be called on the minimal set of inputs possible.
- Currently embedded pipe characters (|) for output choice constraints, or leading
  or trailing forward slashes (/) for output regex contraints are not allowed as they
  cannot be escaped, but this is planned to be addressed in a future release.
- If a parameter values has spaces, replace the spaces with + or %20.
- Uses Zoom tools for filtering Pointmap and Scatter Plot charts
- Some objects may return nulls in deeply nested fields
- The Request Logs and Monitoring dashboard does not appear by default.
- To enable the dashboard, set the `enable-logs-system-tables` parameter to TRUE in
  heavy.conf and restart the database.
- Hash coloring brings significant benefits but may introduce hash collisions.
- Disabling 'Color All Values' is required if you're using Custom SQL with a CASE
  statement for a chart's color measure.
- Chart animation is available only for New Combo charts with time dimensions.
- Due to required client-side caching, crossfilter replay is inherently not compatible
  with live streaming data.
- Currently, feature flags can only be viewed in Immerse; they cannot be set or removed.
- Enable Custom Basemap Styles for Geo Charts
- Entering a source incorrectly can result in unexpected behavior in the geo charts,
  or cause Immerse to not load.
- The theme setting is saved on the browser you are currently using. The setting does
  not affect other machines or other browsers you might use.
- When you create a map, the theme defaults to Current Immerse Theme. This is necessary
  to automatically update a map when you toggle themes.
- 'For optimal performance across very large data sets, #UNIQUE uses APPROX_COUNT_DISTINCT.'
- COUNT(DISTINCT x) can require a very large amount of memory to hold all distinct
  values in large tables with large cardinalities.
- When a dimension is set to a numerical column, Immerse presents information grouped
  by each number.
- You can manually disable automatic binning to force the display of all numbers in
  the dimension.
- To make the chart easier to read, apply the format %m/%d/%y to dimension date formatting
  and SI to measure number formatting.
- To simplify the percentage format to one decimal place, apply ,.1% to the Custom
  Profit Ratio.
- In the following Bar chart, the average profit by state is expressed in whole dollars,
  both positive and negative, by using the ($0.f format in the Avg Profit input box.
- Outlier points are disabled by default. This can be changed by setting the `ui/enable_box_plot_outliers_default`
  feature flag to ON.
- Displaying outliers can impact performance - we recommend enabling outliers only
  when you specifically need to investigate these extreme values; otherwise, keeping
  them off will ensure faster loading and a focused view of your core data.
- Parameters enable all tabs to reference the same data.
- Any hidden parameters would be unhidden after query parameters are overwritten with
  the value defined in the URL.
- Any existing parameters are unaffected if not defined in the URL.
- Default Filter Set is available for every dashboard.
- Filters panel is hidden by default when creating or opening a dashboard.
- Uses hash function for color consistency across different data sources
- Disabling 'Color All Values' is required if using Custom SQL with a CASE statement
  for a chart's color measure
- Due to required client-side caching, crossfilter replay is inherently not compatible
  with live streaming data. It should be used only on a table that does not change
  during the animation.
- In releases before Release 6.1.0, custom styles with overlapping names would be
  appended to the list.
- If a theme is deleted, then any maps referencing that theme revert to the default.
- Dimensions are optional.
- Longitude and latitude (or POINT defined by longitude and latitude) are required.
- Dimensions are optional for Scatter Plot charts.
- When POINT AUTOSIZE is turned on, when you zoom in to focus on an area of the map,
  the points become smaller, and can be difficult to see.
- By default, Box and Violin plots will auto-scale to highlight the core spread of
  your data and prevent extreme outliers from compressing the view.
- Outlier points can be toggled on by clicking the 'Show outliers' option in the Settings
  panel on the right.
- Not all outliers will be shown on the chart - only the 10 highest and 10 lowest
  outliers per group will be plotted.
- Due to high query complexity, outlier points are disabled by default.
- You can change the `ui/enable_box_plot_outliers_default` feature flag to ON to show
  outliers.
- 'Box Plots will sort by # Records, so that the group with the most values is displayed
  first.'
- 'The default # of Groups is set to 50.'
- The current default for Violin Distribution Precision is set to 60, and the max
  is set to 200.
- The optional multi-series capability of the Line chart can break out values by an
  additional dimension.
- The entire volume must be rectilinear.
- The lat/lon plane must be regular; that is, equal spacing in both dimensions.
- There must be one unique sample point for every voxel in the volume.
- Queries will be automatically capped at 5,000 rows for most visualizations.
- For pointmap visualizations that limit is 5,000,000.
- Requires setup of connected app for access
- Maximum of one measure.
- If you use custom SQL for your measure, it must use an aggregate function.
- Use LiDAR-based terrain data, which provides the best accuracy.
- Terrain data may require additional ETL processes prior to importing/linking.
- Size Domain sets the minimum and maximum bounds for the size measure.
- Size Range represents sizes of the smallest and largest points on the map, measured
  in pixels.
- When POINT AUTOSIZE is turned on, points become smaller when zoomed in.
- HeavyRF is a separately licensed module addition to the HEAVY.AI Enterprise platform
  Release 6.3.0 and higher.
- Set `ui/enable_custom_source_manager` to `true` in Heavy Immerse to enable creation
  of custom data sources.
- Dimensions are optional for Scatter Plot charts. When you add a dimension, Immerse
  represents binned values by averaging X and Y coordinates.
- While you normally would show as many points as possible, you have the option of
  decreasing the number of points displayed if that makes the most important values
  stand out more prominently.
- Setting the size range is a balance between making it easy to spot large values
  while still displaying all significant information.
- This version of the RF function can be used in optimizing antenna placements use
  cases or to minimize interference.
- By allowing overlaps in the initial simulation, you avoid introducing coverage gaps
  when dropping out antennas.
- If you omit the Color dimension, you create a single-color vertical bar chart.
- When you use the Color dimension, Immerse selects the top five categories for the
  dimension and measure.
- 'In some cases, if you disable All Others from the color menu, the # of Groups does
  not match what is shown on the x axis.'
- ODBC HeavyConnect and import is currently a beta feature.
- The PostgreSQL unicode ODBC driver is currently not supported.
- Use of a special parameter ${Crossfilter.tablename} allows access to any crossfilter
  in SQL.
- File 'globbing' is only supported locally in HEAVY.AI release 6.1.0 and later.
- KMeans can be significantly more performant than DBScan.
- KMeans is sensitive to differences in scales between different variables.
- Uses ray tracing technique to simulate RF propagation
- Uses line-of-sight 2.5D RF signal propagation for max signal calculation.
- Filter push-down enabled by default for outputs rf_source_id, x, y, and elevation_amsl_meters.
- CREATE MODEL statement supports multiple variations for model creation.
- EVAL_FRACTION must be >= 0.0 and < 1.0 with a default value of 0.0.
- TRAIN_FRACTION must be >= 0.0 and <= 1.0.
- If you are not sure that your data meet these requirements, you may want to first
  do some exploratory visualizations and transformations using scatter plots to visualize
  pairwise relationships.
- The relatively low R2 scores obtained for the linear regression model are not atypical
  for complex multi-variate relations. As noted above, in such cases, it will likely
  be worth trying a random forest or Gradient-Boosted Tree (GBT) regression model,
  as the accuracy of these models can be dramatically higher in many cases.
- HeavyConnect introduces four new types of database objects to HeavyDB.
- A foreign data wrapper represents an adapter object that connects HeavyDB to an
  external data source and accesses required data.
- A foreign server object represents a named external datasource, along with configuration
  information required to access the datasource.
- A foreign table acts as a proxy to an external data source for which access details
  are provided by a server object.
- A user mapping is an object that contains access credentials to an external datasource/server.
- This model trains in approximately 240ms.
- Produces a single tree model with r2 of 85%.
- Gradient boosted regression models may perform better than random forests regression
  in domains with high complexity, nonlinearity, and noise.
- Uses scheduled refresh with different intervals depending on the table.
- PCA algorithm does not support NULL data values — records with NULLs must be deleted
  or imputed.
- HEAVY.AI supports GPU-accelerated data science and machine learning.
- The API allows configurable output to GPU-based dataframes.
- Ensure Docker is configured with NVIDIA runtime
- Validate NVIDIA Docker installation using nvidia-smi
- HEAVY.AI uses container paths instead of operating system paths
- Requires installation of NVIDIA drivers and nvidia-container-runtime
- Ensure that all paths, such as cert and key, are accessible by Docker.
- You must locate your docker-compose.yml file in a location that is reachable from
  Docker.
- HeavyML is currently in beta and features will continue to be added at a fast pace.
- UDFs and UDTFs are enabled with the flags --enable-runtime-udfs and --enable-table-functions
- A function can be made available on both the CPU and GPU by using device=['cpu',
  'gpu'].
- Clustering algorithms can be sensitive to differences in scales between different
  variables.
- By default, a UDTF that has a variable number of rows in the output table is not
  thread-safe.
- If the configuration flag --restrict-ml-model-metadata-to-superusers is set to false
  (the default), any user can execute SHOW MODELS and SHOW MODEL DETAILS commands.
- For models with categorical predictors, CAT_TOP_K default value is 10 and CAT_MIN_FRACTION
  default value is 0.01.
- The relatively low R2 scores obtained for the linear regression model are not atypical
  for complex multi-variate relations.
- You can use Altair directly with pandas, without using Ibis.
- Every data source is an Ibis expression that generates SQL queries to a HEAVY.AI
  backend.
- The HEAVY.AI server has a default one hour timeout on individual HTTP requests
- If the connection to the server is lost, heavysql automatically attempts to reconnect.
- Unsupported features include transaction statements, cursors, multiple result sets,
  domains, rules, database procedures, indexes, keys, and constraints.
- Uses JDBC for database connection
- PCA algorithm does not support NULL data values — records with NULLs need to be
  deleted or imputed.
- Provide a time interval (in seconds) used to terminate the failed connection attempts.
- Requires connection to HEAVY.AI instance
- Create the file `/var/lib/heavyai/heavyai.conf`, and configure the `jupyter-url`
  setting under the `[web]` section to point to the Jupyter service.
- Create the following `/var/lib/heavyai/servers.json` entry to enable Jupyter features
  in Immerse.
- Width and height are required properties in the Vega specification.
- A function can also be made available on both the CPU and GPU by using device=['cpu',
  'gpu'].
- This example connects to a public HEAVY.AI server, but you can use any HEAVY.AI
  server you have access to.
- Supports parallel execution of forecasts using Dask or Ray.
- Each click generates 1,000 records.
- Loading large flat files of 100 MB or larger is the most efficient way to import
  data to HEAVY.AI.
- Consider increasing the block sizes of StreamInserter or SQLImporter to reduce the
  overhead of records loaded or streamed.
- HeavyDB supports only Web Mercator projection.
- Variable length types are not supported when performing columnar conversion.
- Do not set BLOSC_* environment variables.
- Possible integer overflow on select count(*) for tables with more than 2^32 rows.
- HEAVY.AI does not currently support UPDATE from a subquery.
- UPDATE is not currently supported on variable-length data types.
- On NVIDIA DGX systems, you might get a CUDA error if no fabric manager is installed.
- When evaluating sizing points by meters at large zoom levels, some errors are introduced.
- You can import higher-precision timestamps via the data manager, but you cannot
  use them as part of actual queries or filters for a chart.
- Dashboards can be shared only as 'read-only'.
- Dashboard sharing does not currently work in HEAVY.AI Cloud.
- If your HEAVY.AI instance is set up to autoload specific dashboards on login, you
  need to update the entry to use the dashboard ID instead.
- Logs rotate when the instance restarts.
- Logs also rotate once they reach 10MB in size.
- HeavyDB keeps a maximum of 100 historical log files.
- On a connection error, you can view the error message to determine the cause of
  the error.
- To determine the cause of Vega specification errors, catch and handle the renderVega()
  exception.
- Uses OAuth2
- Width and height are required in the Vega specification.
- IMPORTANT - In HeavyDB Release 7.0, the 'render groups' mechanism, part of the previous
  implementation of polygon rendering, has been removed. When you upgrade to HeavyDB
  Release 7.0, all existing tables that have a POLYGON or MULTIPOLYGON geo column
  are automatically migrated to remove a hidden column containing 'render groups'
  metadata.
- As always, HEAVY.AI strongly recommends that all databases be backed up, or at the
  very least, dumps are made of tables with POLYGON or MULTIPOLYGON columns using
  the existing HeavyDB version, before upgrading to HeavyDB Release 7.0.
- The default for the header option of `COPY TO` to a CSV/TSV file has been changed
  from `'false'` to `'true'.
- New default information_schema database contains 10 new system tables that provide
  information regarding CPU/GPU memory utilization, storage space utilization, database
  objects, and database object permissions.
- New Vulkan backend renderer will replace the current OpenGL renderer.
- This API supports TLS; disable certificate verification only in development.
- Ideally, each batch should be fairly substantial in size, minimally 1,000 rows or
  more.
- OmniSciDB now supports Domain-based Authentication, allowing for multiple login
  modes (SAML, LDAP, and local) depending on user ID.
- HeavyDB supports only Web Mercator projection
- Variable length types are not supported when performing columnar conversion
- Do not set BLOSC_* environment variables
- Possible integer overflow on select count(*) for tables with more than 2^32 rows
- HEAVY.AI does not currently support UPDATE from a subquery
- UPDATE is not currently supported on variable-length data types
- On NVIDIA DGX systems, you might get a CUDA error if no fabric manager is installed
- When evaluating the new convert_meters_to_pixel_width and convert_meters_to_pixel_height
  extension functions for accuracy, some errors are introduced by the extension functions
  at large zoom levels
- You can import higher-precision timestamps via the data manager, but you cannot
  use them as part of actual queries or filters for a chart
- Dashboards can be shared only as 'read-only'
- Dashboard sharing does not automatically provide permissions on underlying tables/views
- Dashboard sharing does not currently work in HEAVY.AI Cloud
- If your HEAVY.AI instance is set up to autoload specific dashboards on login by
  specifying the name in servers.json, you need to update the entry to use the dashboard
  ID instead
- The default username is now admin.
- The default database has changed from mapd to omnisci.
- Users set up via Okta cannot be superusers.
- This is a backward incompatible change. OmniSci recommends that you back up your
  existing database before installing OmniSci 4.2.
- HEAVY.AI writes to system logs and to HEAVY.AI-specific logs.
- Logs rotate once they reach 10MB in size.
- The HEAVY.AI web server can show current log files through a web browser. Only super
  users who are logged in can access the log files.
- This version introduces a change in the Thrift API which breaks compatibility between
  3.0 servers/clients and older servers/clients.
- The deprecated v.1 Immerse is no longer available as of this release.
- Requires setup of HEAVY.AI server
- Some queries may return large datasets
- IMPORTANT - In HeavyDB Release 7.0, the 'render groups' mechanism, part of the previous
  implementation of polygon rendering, has been removed.
- The default for the header option of COPY TO to a CSV/TSV file has been changed
  from 'false' to 'true'.
- Improved error messaging when attempting to save a dashboard that uses a duplicate
  dashboard name.
- Parallel executors now on by default (with default `--num-executors=2`)
- Various bug fixes and performance improvements for runtime query interrupt.
- Improved performance for multi-column GROUP BY queries
- Support for LINESTRING, POLYGON, and MULTIPOLYGON in user-defined functions
- Enhanced OmniSciDB log files to include inbound client IP address and protocol for
  all endpoints.
- JDBC authentication using a PKI certificate is supported.
- There is a known limitation with users set up via Okta, where such users cannot
  be superusers.
- Backward incompatible change. OmniSci recommends that you back up your existing
  database before installing OmniSci 4.2.
- Before installing MapD 4.0, MapD recommends that you back up your database. Security
  features are now enabled by default. Changes in the database to support security
  features do not allow reversion to an earlier release.
- All tables that include polygon information must be re-imported for version 4.0.
- The minimum supported NVIDIA driver version for MapD 4.0 is 384.81. Use the `nvidia-smi`
  utility included with the drivers to see the current active driver version. If you
  need assistance upgrading the driver, contact MapD support at support@omnisci.com.
- Functions are not persisted on the database and need to be registered if the server
  is restarted.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Authentication failed: Check license information or SAML configuration.'
- 1 Rows Inserted, 0 rows skipped
- Query couldn’t keep the entire working set of columns in GPU Memory.
- 'AccessDenied: Access Denied'
- 'AuthorizationHeaderMalformed: The authorization header is malformed'
- Negative epoch value found for table.
- Epoch values for table are inconsistent.
- Negative epoch value found for table
- Epoch values are inconsistent
- 'NLtoSQLException: Language model failed to generate a valid SQL query after 2 tries'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- 'Naming collision in CREATE MODEL without qualifiers: throws an error.'
- 'EVAL_FRACTION not specified: requires specific evaluation query.'
- '401 Unauthorized: Recheck OAuth scopes or token expiration.'
- 'LIMIT_EXCEEDED: Reduce the amount of data requested'
- 'UNAUTHORIZED: Check user credentials'
- 'Naming collision in CREATE MODEL: The statement will throw an error if a model
  already exists by the same name.'
- EVALUATE MODEL requires the model to have a specified EVAL_FRACTION.
- Invalid non-mandatory options--for example, those that have misspellings--are ignored
  and do not generate errors.
- 'E1001: Exception: fopen(/tmp/25882.csv): No such file or directory'
- 'E1001: Incorrect Row (expected 58 columns, has 57): [MOBILE, EVDOA, Access...]'
- 'Ran out of slots in the query output buffer: exception that could occur when using
  stale cached cardinality values.'
- Fix a device lost error that could occur with complex polygon renders
- 'error while loading shared libraries: libvulkan.so.1: cannot open shared object
  file: No such file or directory'
- '401 Unauthorized: Check your credentials'
- '404 Not Found: Ensure the endpoint exists'
- '500 Internal Server Error: Contact support if this persists'
- '404 Not Found: Endpoint does not exist'
- '500 Internal Server Error: Server encountered an error'
- Fixes an issue where the root user could be deleted in certain cases.
- Fixes an issue where staging directories for S3 import could remain when imports
  failed.
- Fixed SAML-configured login that made regular access to other utilities awkward
  or restrictive.
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - SAML
  - LDAP
  - local
  - local OmniSci authorization
client:
  base_url: http://jupyterhub:8000
  headers:
    Content-Type: application/json
source_metadata: null
