resources:
- name: Request
  endpoint:
    path: /topics/request-response.html#scrapy.http.Request
    method: GET
- name: Response
  endpoint:
    path: /topics/request-response.html#scrapy.http.Response
    method: GET
- name: Request
  endpoint:
    path: /topics/request-response.html#scrapy.http.Request
    method: GET
- name: Response
  endpoint:
    path: /topics/request-response.html#scrapy.http.Response
    method: GET
- name: Request
  endpoint:
    path: /topics/request-response.html#request-objects
    method: GET
- name: Response
  endpoint:
    path: /topics/request-response.html#response-objects
    method: GET
- name: Request
  endpoint:
    path: /scrapy/http/Request
    method: GET
    data_selector: Request
    params: {}
- name: FormRequest
  endpoint:
    path: /scrapy/http/FormRequest
    method: GET
    data_selector: FormRequest
    params: {}
- name: login
  endpoint:
    path: /users/login.php
    method: POST
    data_selector: response
    params:
      formdata:
        username: john
        password: secret
- name: quotes
  endpoint:
    path: https://quotes.toscrape.com/tag/humor/
    method: GET
    data_selector: quotes
    params: {}
- name: SETTINGS_PRIORITIES
  endpoint:
    path: /settings/priorities
    method: GET
    data_selector: SETTINGS_PRIORITIES
    params: {}
- name: spider
  endpoint:
    path: /spider
    method: GET
    data_selector: spiders
    params: {}
- name: Response
  endpoint:
    path: /scrapy/http/response
    method: GET
    data_selector: Response
- name: errback_example
  endpoint:
    path: http://www.httpbin.org/
    method: GET
    data_selector: response
    params: {}
- name: Response
  endpoint:
    path: /scrapy/http/response
    method: GET
- name: login
  endpoint:
    path: /users/login.php
    method: POST
    data_selector: response
    params: {}
- name: Request
  endpoint:
    path: /topics/request-response.html#request-objects
    method: GET
- name: Response
  endpoint:
    path: /topics/request-response.html#response-objects
    method: GET
- name: request
  endpoint:
    path: /scrapy/request
    method: GET
- name: errback_example
  endpoint:
    path: /errback_example
    method: GET
    data_selector: response
    params: {}
- name: FormRequest
  endpoint:
    path: /topics/request-response.html#formrequest-objects
    method: GET
    data_selector: records
    params: {}
- name: Request
  endpoint:
    path: /topics/request-response.html#request-usage-examples
    method: GET
    data_selector: records
    params: {}
- name: response
  endpoint:
    path: /response
    method: GET
    data_selector: attributes
- name: media_pipelines
  endpoint:
    path: /media/pipelines
    method: GET
- name: item_exporters
  endpoint:
    path: /item/exporters
    method: GET
- name: feed_exports
  endpoint:
    path: /feed/exports
    method: GET
- name: contributors
  endpoint:
    path: /en/latest/news.html
    method: GET
    data_selector: contributors
    params: {}
- name: scrapy_service
  endpoint:
    path: /scrapyd
    method: GET
    data_selector: service_status
notes:
- 'Caution: Using from_curl() from Request subclasses, such as JSONRequest, or XmlRpcRequest,
  may modify the Request object.'
- The XmlResponse class is a subclass of TextResponse which adds encoding auto-discovering
  support by looking into the XML declaration line.
- Uses feed exports to generate the JSON file
- Using from_curl() from Request subclasses, such as JsonRequest, or XmlRpcRequest,
  may modify the Request object.
- 'Added in version 2.0: The flags parameter.'
- 'Added in version 2.0.0: The certificate parameter.'
- 'Added in version 2.1.0: The ip_address parameter.'
- 'Added in version 2.5.0: The protocol parameter.'
- Request.cb_kwargs was introduced in version 1.7.
- The `certificate` parameter is populated for `https` responses, `None` otherwise.
- The `ip_address` parameter is currently only populated by the HTTP 1.1 download
  handler.
- Caching saves CPU by ensuring that fingerprints are calculated only once per request
- Using WeakKeyDictionary saves memory by ensuring that request objects do not stay
  in memory forever
- Uses errbacks to catch exceptions in request processing
- Request.cb_kwargs was introduced in version 1.7
- Uses scrapy framework for web scraping and request management.
- TextResponse objects support encoding capabilities
- HtmlResponse adds encoding auto-discovering support
- XmlResponse adds encoding auto-discovering support
- The asyncio reactor is now enabled by default
- Replaced start_requests() (sync) with start() (async) and changed how it is iterated.
- Added the allow_offsite request meta key
- Spider middlewares that don’t support asynchronous spider output are deprecated
- Added a base class for universal spider middlewares
- The start_requests() method of Spider is deprecated, use start() instead.
- Dropped support for Python 3.8, added support for Python 3.13
- scrapy.Spider.start_requests() can now yield items
- Added JsonResponse
- Added CLOSESPIDER_PAGECOUNT_NO_ITEM
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- brotlipy is deprecated, both in Scrapy and upstream. Use brotli instead if you can.
- Per-domain download settings.
- Compatibility with new cryptography and new parsel.
- JMESPath selectors from the new parsel.
- This is a maintenance release, with minor features, bug fixes, and cleanups.
- Added Python 3.11 support, dropped Python 3.6 support
- Improved support for asynchronous callbacks
- Asyncio support is enabled by default on new projects
- Security fixes for cookie handling
- Python 3.10 support
- asyncio support is no longer considered experimental, and works out-of-the-box on
  Windows regardless of your Python version
- Official Python 3.9 support
- Experimental HTTP/2 support
- Python 3.5 support has been dropped.
- The file_path method of media pipelines can now access the source item.
- The base implementation of item loaders has been moved into a separate library,
  itemloaders.
- New FEEDS setting to export to multiple feeds
- New Response.ip_address attribute
- The Authorization header is now dropped on redirects to a different domain.
- Improvements for crawls targeting multiple domains
- A cleaner way to pass arguments to callbacks
- A new class for JSON requests
- Improvements for rule-based spiders
- New features for feed exports
- better Windows support
- Python 3.7 compatibility
- big documentation improvements, including a switch from .extract_first() + .extract()
  API to .get() + .getall() API
- Some highlights include better error messages for responses over DOWNLOAD_WARNSIZE
  and DOWNLOAD_MAXSIZE limits.
- Scrapy now supports anonymous FTP sessions with customizable user and password via
  the new FTP_USER and FTP_PASSWORD settings.
- Link extractors do not canonicalize URLs by default anymore.
- Scrapy 1.1 has beta Python 3 support (requires Twisted >= 15.5).
- Scrapy does not work on Windows with Python 3
- Sending emails is not supported
- FTP download handler is not supported
- Telnet console is not supported
- Scrapyd now uses one process per spider
- A minimal web ui was added, available at http://localhost:6800 by default
- First release of Scrapy.
errors:
- 'HttpError: Log all failures'
- 'DNSLookupError: Log DNS errors'
- 'TimeoutError: Log timeout issues'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'RuntimeError: raised if called before open_spider()'
- 'TypeError: raised if the API of the configured scheduler does not meet expectations'
- AssertionError exceptions triggered by assert statements have been replaced by new
  exception types
- TypeError exception from None values in allowed_domains
- '429: now part of the RETRY_HTTP_CODES setting by default'
- Default Scrapy User-Agent now uses https link to scrapy.org.
auth_info:
  mentioned_objects: []
client:
  base_url: https://docs.scrapy.org/en/latest/topics/request-response.html
  headers:
    Accept: application/json
source_metadata: null
