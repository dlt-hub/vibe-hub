resources:
- name: clusters
  endpoint:
    path: /v1beta1/cks/clusters
    method: GET
    data_selector: items
- name: clusters
  endpoint:
    path: /v1beta1/cks/clusters/{id}
    method: PATCH
    data_selector: ''
    params: {}
- name: Node Pool
  endpoint:
    path: /docs/products/cks/nodes/nodes-and-node-pools
    method: GET
    data_selector: nodes
    params: {}
- name: Node Pool
  endpoint:
    path: /nodepool
    method: POST
    data_selector: nodepool
    params: {}
- name: node_pool
  endpoint:
    path: /api/v1/nodepools
    method: GET
    data_selector: nodePools
- name: NodePool
  endpoint:
    path: /v1alpha1/NodePool
    method: POST
    data_selector: spec
    params:
      autoscaling: 'true'
      maxNodes: 4
      minNodes: 2
- name: NodePool
  endpoint:
    path: /v1alpha1/nodes
    method: POST
    data_selector: nodes
    params:
      autoscaling: 'true'
      maxNodes: 4
      minNodes: 2
- name: devpod
  endpoint:
    path: /devpod
    method: POST
    data_selector: environment
    params:
      namespace: devpod
      image: pytorch/pytorch:2.4.1-cuda12.4-cudnn9-runtime
      resources: requests.cpu=16,requests.memory=32Gi,limits.nvidia.com/gpu=8
      disk_size: 100Gi
      kubernetes_pull_secrets_enabled: 'false'
      strict_security: 'true'
- name: devpod
  endpoint:
    path: /devpod
    method: POST
    data_selector: pods
    params: {}
- name: devpod-gpu-sidecar
  endpoint:
    path: /devpod-gpu-sidecar
    method: GET
    data_selector: devpod
    params: {}
- name: devpod
  endpoint:
    path: /devpod
    method: POST
    data_selector: pods
    params: {}
- name: pod_manifest_template
  endpoint:
    path: /pod_manifest_template.yaml
    method: GET
    data_selector: pod
- name: run_demo_gpu_cwsa
  endpoint:
    path: /run_demo_gpu_cwsa.sh
    method: GET
    data_selector: script
- name: devpod
  endpoint:
    path: /devpod
    method: GET
    data_selector: workspaces
    params: {}
- name: devpod
  endpoint:
    path: /devpod
    method: POST
    data_selector: data
    params: {}
- name: devpod
  endpoint:
    path: /devpod
    method: GET
    data_selector: records
- name: devpod
  endpoint:
    path: /devpod
    method: POST
    data_selector: environment
    params: {}
- name: devpod-gpu-sidecar
  endpoint:
    path: /devpod/gpu/sidecar
    method: GET
    data_selector: workspaces
    params: {}
- name: vllm_inference
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: data
    params: {}
- name: health_check
  endpoint:
    path: /health
    method: GET
    data_selector: status
    params: {}
- name: available_models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
    params: {}
- name: vLLM
  endpoint:
    path: /v1
    method: GET
    data_selector: metrics
    params: {}
- name: vLLM_inference_service
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: output
    params: {}
- name: cluster_components
  endpoint:
    path: /api/v1/cluster/components
    method: GET
    data_selector: components
    params: {}
- name: Node Pool
  endpoint:
    path: /nodepools
    method: GET
    data_selector: nodePools
- name: events
  endpoint:
    path: /events
    method: GET
- name: cilium
  endpoint:
    path: /cilium
    method: GET
    data_selector: version
    params: {}
- name: calico
  endpoint:
    path: /calico
    method: GET
    data_selector: version
    params: {}
- name: multus-cni
  endpoint:
    path: /multus-cni
    method: GET
    data_selector: version
    params: {}
- name: coredns
  endpoint:
    path: /coredns
    method: GET
    data_selector: version
    params: {}
- name: konnectivity-agent
  endpoint:
    path: /konnectivity-agent
    method: GET
    data_selector: version
    params: {}
- name: metallb
  endpoint:
    path: /metallb
    method: GET
    data_selector: version
    params: {}
- name: node-local-dns
  endpoint:
    path: /node-local-dns
    method: GET
    data_selector: version
    params: {}
- name: hpc-verification
  endpoint:
    path: /hpc-verification
    method: GET
    data_selector: version
    params: {}
- name: node-problem-detector
  endpoint:
    path: /node-problem-detector
    method: GET
    data_selector: version
    params: {}
- name: nfd
  endpoint:
    path: /nfd
    method: GET
    data_selector: version
    params: {}
- name: victoria-metrics
  endpoint:
    path: /victoria-metrics
    method: GET
    data_selector: version
    params: {}
- name: vmagent
  endpoint:
    path: /vmagent
    method: GET
    data_selector: version
    params: {}
- name: metrics-server
  endpoint:
    path: /metrics-server
    method: GET
    data_selector: version
    params: {}
- name: node-pci-exporter
  endpoint:
    path: /node-pci-exporter
    method: GET
    data_selector: version
    params: {}
- name: gpu-thermal-exporter
  endpoint:
    path: /gpu-thermal-exporter
    method: GET
    data_selector: version
    params: {}
- name: nvlink-exporter
  endpoint:
    path: /nvlink-exporter
    method: GET
    data_selector: version
    params: {}
- name: ping-exporter
  endpoint:
    path: /ping-exporter
    method: GET
    data_selector: version
    params: {}
- name: promtail
  endpoint:
    path: /promtail
    method: GET
    data_selector: version
    params: {}
- name: crowdstrike
  endpoint:
    path: /crowdstrike
    method: GET
    data_selector: version
    params: {}
- name: vast-csi
  endpoint:
    path: /vast-csi
    method: GET
    data_selector: version
    params: {}
- name: lota
  endpoint:
    path: /lota
    method: GET
    data_selector: version
    params: {}
- name: k8s-device-plugin
  endpoint:
    path: /k8s-device-plugin
    method: GET
    data_selector: version
    params: {}
- name: nvidia-imex
  endpoint:
    path: /nvidia-imex
    method: GET
    data_selector: version
    params: {}
- name: rdma-shared-dp
  endpoint:
    path: /rdma-shared-dp
    method: GET
    data_selector: version
    params: {}
- name: GPU_Instances
  endpoint:
    path: /gpu/instances
    method: GET
    data_selector: instances
    params: {}
- name: Node Pool
  endpoint:
    path: /nodepools
    method: GET
    data_selector: conditions
- name: RTX Pro 6000
  endpoint:
    path: /instances/rtx_pro_6000
    method: GET
    data_selector: specifications
    params: {}
- name: L40S
  endpoint:
    path: /instances/l40s
    method: GET
    data_selector: specifications
    params: {}
- name: L40
  endpoint:
    path: /instances/l40
    method: GET
    data_selector: specifications
    params: {}
- name: GH200
  endpoint:
    path: /instances/gh200
    method: GET
    data_selector: specifications
    params: {}
- name: A100
  endpoint:
    path: /instances/a100
    method: GET
    data_selector: specifications
    params: {}
- name: High Performance AMD Genoa
  endpoint:
    path: /instances/cpu-instances/high-performance-amd-genoa
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose AMD Genoa
  endpoint:
    path: /instances/cpu-instances/general-purpose-amd-genoa
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose AMD Genoa (High Storage)
  endpoint:
    path: /instances/cpu-instances/general-purpose-amd-genoa-high-storage
    method: GET
    data_selector: specifications
    params: {}
- name: High Core AMD Genoa
  endpoint:
    path: /instances/cpu-instances/high-core-amd-genoa
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose AMD Turin
  endpoint:
    path: /instances/cpu-instances/general-purpose-amd-turin
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose AMD Turin (High Storage)
  endpoint:
    path: /instances/cpu-instances/general-purpose-amd-turin-high-storage
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose Intel Emerald Rapids
  endpoint:
    path: /instances/cpu-instances/general-purpose-intel-emerald-rapids
    method: GET
    data_selector: specifications
    params: {}
- name: General Purpose Intel Ice Lake
  endpoint:
    path: /instances/cpu-instances/general-purpose-intel-ice-lake
    method: GET
    data_selector: specifications
    params: {}
- name: default_vpc
  endpoint:
    path: /vpc
    method: GET
    data_selector: vpcs
- name: custom_vpc
  endpoint:
    path: /vpc/custom
    method: POST
    data_selector: vpc
- name: CWDrainNode
  endpoint:
    path: /events/CWDrainNode
    method: GET
    data_selector: NodePool
- name: CWInstanceTypeNotInZone
  endpoint:
    path: /events/CWInstanceTypeNotInZone
    method: GET
    data_selector: NodePool
- name: CWInsufficientCapacity
  endpoint:
    path: /events/CWInsufficientCapacity
    method: GET
    data_selector: NodePool
- name: CWInvalidInstanceType
  endpoint:
    path: /events/CWInvalidInstanceType
    method: GET
    data_selector: NodePool
- name: CWNodeAssigned
  endpoint:
    path: /events/CWNodeAssigned
    method: GET
    data_selector: NodePool
- name: CWNodeDeleted
  endpoint:
    path: /events/CWNodeDeleted
    method: GET
    data_selector: NodePool
- name: CWNodeDeletionRequestSuccess
  endpoint:
    path: /events/CWNodeDeletionRequestSuccess
    method: GET
    data_selector: NodePool
- name: CWNodeDeliverFail
  endpoint:
    path: /events/CWNodeDeliverFail
    method: GET
    data_selector: NodePool
- name: CWNodePoolCreated
  endpoint:
    path: /events/CWNodePoolCreated
    method: GET
    data_selector: NodePool
- name: CWNodePoolDeleted
  endpoint:
    path: /events/CWNodePoolDeleted
    method: GET
    data_selector: NodePool
- name: CWNodePoolDisabled
  endpoint:
    path: /events/CWNodePoolDisabled
    method: GET
    data_selector: NodePool
- name: CWNodePoolMetadata
  endpoint:
    path: /events/CWNodePoolMetadata
    method: GET
    data_selector: NodePool
- name: CWNodePoolNodesRemoved
  endpoint:
    path: /events/CWNodePoolNodesRemoved
    method: GET
    data_selector: NodePool
- name: CWNodePoolNodesRemoveError
  endpoint:
    path: /events/CWNodePoolNodesRemoveError
    method: GET
    data_selector: NodePool
- name: CWNodePoolNodesRequestFailed
  endpoint:
    path: /events/CWNodePoolNodesRequestFailed
    method: GET
    data_selector: NodePool
- name: CWNodePoolQuotaCheckFailed
  endpoint:
    path: /events/CWNodePoolQuotaCheckFailed
    method: GET
    data_selector: NodePool
- name: CWNodePoolRemoveNodes
  endpoint:
    path: /events/CWNodePoolRemoveNodes
    method: GET
    data_selector: NodePool
- name: CWNodePoolScaledDown
  endpoint:
    path: /events/CWNodePoolScaledDown
    method: GET
    data_selector: NodePool
- name: CWNodePoolScaledUp
  endpoint:
    path: /events/CWNodePoolScaledUp
    method: GET
    data_selector: NodePool
- name: CWNodeRegistered
  endpoint:
    path: /events/CWNodeRegistered
    method: GET
    data_selector: Node, NodePool
- name: CWNodeRegistrationFailed
  endpoint:
    path: /events/CWNodeRegistrationFailed
    method: GET
    data_selector: Node
- name: CWNodeRequestQueued
  endpoint:
    path: /events/CWNodeRequestQueued
    method: GET
    data_selector: NodePool
- name: CWOverQuota
  endpoint:
    path: /events/CWOverQuota
    method: GET
    data_selector: NodePool
- name: CWNodeCordoned
  endpoint:
    path: /events/CWNodeCordoned
    method: GET
    data_selector: Node, NodePool
- name: CWNodeUncordoned
  endpoint:
    path: /events/CWNodeUncordoned
    method: GET
    data_selector: Node, NodePool
- name: CWNodeMarkedPrepareForTerminate
  endpoint:
    path: /events/CWNodeMarkedPrepareForTerminate
    method: GET
    data_selector: Node, NodePool
- name: CWNodeDraining
  endpoint:
    path: /events/CWNodeDraining
    method: GET
    data_selector: Node, NodePool
- name: CWNodeDrainingPDBViolation
  endpoint:
    path: /events/CWNodeDrainingPDBViolation
    method: GET
    data_selector: Node, NodePool
- name: CWNodeDrainingForceDelete
  endpoint:
    path: /events/CWNodeDrainingForceDelete
    method: GET
    data_selector: Node, NodePool
- name: CWNodePreparedForTerminate
  endpoint:
    path: /events/CWNodePreparedForTerminate
    method: GET
    data_selector: Node, NodePool
- name: CWNodeTainted
  endpoint:
    path: /events/CWNodeTainted
    method: GET
    data_selector: Node, NodePool
- name: CWNodeUntainted
  endpoint:
    path: /events/CWNodeUntainted
    method: GET
    data_selector: Node, NodePool
- name: CWNodePoolNodeConfigUpdated
  endpoint:
    path: /events/CWNodePoolNodeConfigUpdated
    method: GET
    data_selector: NodePool
- name: CWNodePoolNodeConfigUpdatePending
  endpoint:
    path: /events/CWNodePoolNodeConfigUpdatePending
    method: GET
    data_selector: NodePool
- name: CWNodePoolNodeConfigUpdateFailed
  endpoint:
    path: /events/CWNodePoolNodeConfigUpdateFailed
    method: GET
    data_selector: NodePool
- name: gpu_instances
  endpoint:
    path: /instances/gpu-instances
    method: GET
    data_selector: instances
    params: {}
- name: RTX Pro 6000
  endpoint:
    path: /instances/rtx_pro_6000
    method: GET
    data_selector: specifications
    params: {}
- name: L40S
  endpoint:
    path: /instances/l40s
    method: GET
    data_selector: specifications
    params: {}
- name: L40
  endpoint:
    path: /instances/l40
    method: GET
    data_selector: specifications
    params: {}
- name: GH200
  endpoint:
    path: /instances/gh200
    method: GET
    data_selector: specifications
    params: {}
- name: A100
  endpoint:
    path: /instances/a100
    method: GET
    data_selector: specifications
    params: {}
- name: Control Plane Logs
  endpoint:
    path: /d/cluster-health-logs/control-plane-logs
    method: GET
- name: Control Plane Metrics
  endpoint:
    path: /d/cluster-health/control-plane-metrics
    method: GET
- name: Kubernetes Audit Logs
  endpoint:
    path: /d/k8s-audit-logs/kubernetes-audit-logs
    method: GET
- name: Kueue Metrics Dashboard
  endpoint:
    path: /d/8c942c96-5a81-4355-8975-4623cee7abf1/kueue-metrics-dashboard
    method: GET
- name: Pod Inspector
  endpoint:
    path: /d/G8-j827Mk/pod-inspector
    method: GET
- name: Cabinet Visualizer
  endpoint:
    path: /d/de3qwk31ucpvkf/cabinet-visualizer
    method: GET
- name: Cabinet Wrangler
  endpoint:
    path: /d/ceif6c5udzytca/cabinet-wrangler
    method: GET
- name: Node Details
  endpoint:
    path: /d/ddbdicm9sw7c5x/node-details
    method: GET
- name: Node Wrangler
  endpoint:
    path: /d/FWuqvUH4z/node-wrangler
    method: GET
- name: Envoy Circuit Breaker
  endpoint:
    path: /d/adphtf60qpxj4c/envoy-circuit-breaker
    method: GET
- name: Global Revision
  endpoint:
    path: /d/DDMMMEYnz/global-revision-overview
    method: GET
- name: Kourier Gateway
  endpoint:
    path: /d/knative-kourier/kourier-gateway
    method: GET
- name: Namespace Revision
  endpoint:
    path: /d/knative-namespace-revision-overview/namespace-revision-overview
    method: GET
- name: Revision Overview
  endpoint:
    path: /d/knative-revision-overview/revision-overview
    method: GET
- name: Training Jobs
  endpoint:
    path: /d/k8s-training-jobs/kubernetes-training-jobs
    method: GET
- name: Internet Transit
  endpoint:
    path: /d/ce3pbkpbkhn9cd/internet-transit
    method: GET
- name: CAIOS LOTA
  endpoint:
    path: /d/eeff523hiaewwc/caios-lota
    method: GET
- name: CAIOS Usage
  endpoint:
    path: /d/bebi5t788t6v4c/caios-usage
    method: GET
- name: PVC Usage
  endpoint:
    path: /d/felwdhehb6ewwa/pvc-usage
    method: GET
- name: VAST Actors
  endpoint:
    path: /d/ddq5gg6gi5lvke/vast-actors
    method: GET
- name: WEKA
  endpoint:
    path: /d/feg6mkz6mg934d/weka
    method: GET
- name: Slurm Block Topology
  endpoint:
    path: /d/6d5afc99-124d-4ab4-9914-fde3bf8c581e/slurm-block-topology
    method: GET
- name: Slurm Cluster
  endpoint:
    path: /d/bX7jn6dZk/slurm-cluster
    method: GET
- name: Slurm Job Metrics
  endpoint:
    path: /d/slurm-job-metrics/slurm-job-metrics
    method: GET
- name: High Performance AMD Genoa
  endpoint:
    path: /docs/platform/instances/cpu-instances#high-performance-amd-genoa
    method: GET
    data_selector: records
    params: {}
- name: General Purpose AMD Genoa
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-amd-genoa
    method: GET
    data_selector: records
    params: {}
- name: General Purpose AMD Genoa (High Storage)
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-amd-genoa-high-storage
    method: GET
    data_selector: records
    params: {}
- name: High Core AMD Genoa
  endpoint:
    path: /docs/platform/instances/cpu-instances#high-core-amd-genoa
    method: GET
    data_selector: records
    params: {}
- name: General Purpose AMD Turin
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-amd-turin
    method: GET
    data_selector: records
    params: {}
- name: General Purpose AMD Turin (High Storage)
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-amd-turin-high-storage
    method: GET
    data_selector: records
    params: {}
- name: General Purpose Intel Emerald Rapids
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-intel-emerald-rapids
    method: GET
    data_selector: records
    params: {}
- name: General Purpose Intel Ice Lake
  endpoint:
    path: /docs/platform/instances/cpu-instances#general-purpose-intel-ice-lake
    method: GET
    data_selector: records
    params: {}
- name: default_vpcs
  endpoint:
    path: /docs/products/networking/vpc/default-vpcs
    method: GET
    data_selector: vpcs
    params: {}
- name: custom_vpc
  endpoint:
    path: /docs/products/networking/vpc/custom-vpc
    method: GET
    data_selector: vpc
    params: {}
- name: gpu_instances
  endpoint:
    path: /docs/pricing/pricing-instances/gpu-instances
    method: GET
    data_selector: instances
    params: {}
- name: cpu_instances
  endpoint:
    path: /docs/pricing/pricing-instances/cpu-instances
    method: GET
    data_selector: instances
    params: {}
notes:
- CKS does not require NVIDIA's GPU Operator. To avoid conflicts, customers should
  not attempt to install it.
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Learn how to manage your billing and invoices in the Cloud Console
- If you have admin or billing viewer permissions, you can find your invoices in the
  Cloud Console
- CoreWeave supports OIDC as an authentication method to CKS clusters.
- Regions provide redundancy and failover capabilities by allowing workloads to be
  distributed across multiple AZs.
- Regions are strategically placed to offer low latency, high-performance connectivity,
  and meet data residency requirements.
- Initial sign-up is only possible using an email address
- It is not possible to change the email address associated with a user's account
  after they have created a CoreWeave login
- Using resources, such as compute, incurs charges. Monitor your resource usage to
  avoid unexpected charges.
- CoreWeave is not responsible for the security of the Llama model provided by Hugging
  Face or the Open Web UI container image.
- CKS runs Kubernetes directly on bare metal GPU Nodes without any intermediary software.
- CKS provides the following benefits by eliminating most of the virtualized intermediary
  stages between customer and cluster.
- CKS currently only supports public clusters as a self-service option.
- All settings on this screen are optional. If you do not wish to enable any of these
  features, you may proceed with cluster creation by clicking the Next button, without
  selecting any options in this step.
- Clusters can only be deleted if they are in a Healthy state. Attempts to delete
  Unhealthy or Provisioning clusters will fail.
- For existing CKS clusters created on or before June 24, 2025, you need to rotate
  your existing Kubernetes Secrets once.
- Any new Secrets you create going forward will be encrypted automatically.
- Clusters can only be upgraded to the next Kubernetes minor version.
- All upgrades must be sequential. Downgrades are not supported once an upgrade has
  been performed.
- Node Pools are available in all General Access Regions.
- Node Pools are deployed as Kubernetes Custom Resources (CRs), which allocate the
  number, type, and regional placement of Nodes for use by a specified CKS cluster.
- If a quota's maximum is exceeded by the number set in targetNodes, the Node Pool
  creation will fail.
- Debug Pods are not privileged, so commands such as chroot /host that are restricted
  to superusers will fail.
- Removing Nodes from a Node Pool can take some time.
- Changes may take a moment to display on the Cloud Console.
- 'Avoid using Node conditions for automation: Node conditions are intended solely
  for internal CoreWeave use. Do not rely on them for custom automation or management.'
- 'Cordoning does not always indicate a serious or permanent Node issue: Nodes often
  recover automatically after temporary issues. If the issue is severe or persistent,
  CoreWeave proactively moves affected Nodes out of production into triage.'
- Node Pool autoscaling is currently in preview.
- 'No SUNK integration: SUNK does not currently support autoscaling Node Pools.'
- Scale up time takes between 20-30 minutes.
- 'Node Pool created with `minNodes:0`. When a Node Pool is created with `minNodes:
  0`, there are initially no Nodes present in the pool. The Autoscaler requires at
  least one Node (`targetNodes: 1`) so it can cache the "shape" (resource characteristics)
  of the Node.'
- CKS removes a Node, causing the Autoscaler to attempt to schedule a new Node with
  the wrong "shape". Occasionally, the Autoscaler may cache a Node that has been "tainted"
  (marked unschedulable) by CoreWeave automation.
- 'Cordoning does not always indicate a serious or permanent Node issue: Nodes often
  recover automatically after temporary issues.'
- Organizations refer to your entire CoreWeave Cloud Console account, as well as all
  users and resources within it.
- Node Pool autoscaling is currently in preview and has limitations.
- These sensitive values are never shown again after closing the modal. Be sure to
  record them in a secure location.
- 'Nodes don''t scale up. Node Pool created with `minNodes:0`. When a Node Pool is
  created with `minNodes: 0`, there are initially no Nodes present in the pool. The
  Autoscaler requires at least one Node (`targetNodes: 1`) so it can cache the "shape"
  (resource characteristics) of the Node.'
- Nodes don't scale down. Konnectivity Agent Replica Scheduling. CKS expects two replicas
  of the Konnectivity Agent to run for network connectivity. These agent Pods can
  block the Node Pool from scaling down to zero, or conversely, can trigger unexpected
  scaling up if there are resource needs in the pool being autoscaled.
- Users with admin permissions may deactivate user accounts, including other accounts
  with admin privileges.
- Managed Auth is the recommended path for handling user authorization in CKS.
- CoreWeave supports SAML/SSO as an organization-wide authentication method.
- All interactions with CKS filter users using their Org ID.
- The primary container uses the pytorch/pytorch:2.4.1-cuda12.4-cudnn9-runtime image.
- Pods require elevated privileges to function properly.
- Users with `admin` permissions may deactivate user accounts, including other accounts
  with `admin` privileges.
- Uses a Kubernetes namespace 'devpod' for organizing environments
- Pod manifest template defines tolerations for scheduling
- Uses Kubernetes as the provider
- Configures a Docker-in-Docker (DinD) sidecar container
- Pods require elevated privileges to function properly, but additional cluster-wide
  permissions are not needed.
- The pod is created in the devpod namespace to keep resources logically separated.
- The manifest specifies tolerations for both sunk.coreweave.com/nodes and is_cpu_compute.
- Ensure your Virtual Server has a browser installed and accessible
- Check that the DevPod workspace is running properly
- Use the full VS Code IDE for either setup path
- Uses Kubernetes for managing Pods.
- The namespace for DevPod environments is 'devpod'.
- Monitoring and observability require additional cluster resources.
- Ensure your CKS cluster has at least one CPU node for Prometheus and Grafana.
- Uses Kubernetes for managing development environments.
- The initial model download can take 10-30 minutes depending on the model size and
  network conditions.
- Ensure your CKS cluster has GPU Nodes with at least 16GB of GPU memory, required
  by the Llama 3.1 8B Instruct model used in this tutorial.
- For production deployments, consider using CoreWeave's managed monitoring stack
  instead of self-hosted Prometheus and Grafana for better reliability and maintenance.
- Monitoring stack requires additional cluster resources. Ensure your CKS cluster
  has at least one CPU node for Prometheus and Grafana to deploy to.
- Ensure cluster has access to new features, security patches, and stability improvements.
- Users can bring their own CNI if required.
- Node Pools use the API schema definitions outlined.
- Fired events related to Node Pool actions and states.
- Cilium, like CKS, is a managed product. We deploy Cilium with a minimal feature
  set.
- Customers who require a specific CNI can bring their own.
- Instances must be requested in multiples of 18 to leverage the full NVL72 rack architecture.
- CoreWeave is updating the Node Pool conditions to provide a better user experience
  and clearer semantics for understanding the state of Node Pools.
- Last updated on Aug 20, 2025
- Default VPCs are automatically created when the first CKS cluster is created.
- Managed Auth is the recommended path for handling user authentication for CKS Clusters.
- Grafana metrics are gated behind Managed Auth services.
- CoreWeave Grafana does not support the creation of new dashboards or the modification
  of existing ones.
- Our CPU instances are designed for workloads that require high CPU performance,
  such as high-performance computing (HPC), data analytics, and machine learning.
- Charges are based on the hourly rate of the instance multiplied by its usage duration.
- Host Prefixes cannot be modified or removed after they are added to a VPC.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- 'CWOverQuota: TargetNodes pushes org over quota by <OVERAGE>. Quota limit is <QUOTA>
  for instance type <INSTANCE_TYPE> in zone <ZONE>'
- 'CWNodePoolQuotaCheckFailed: Internal quota error'
- 'Nodes don''t scale up: Manually set `targetNodes` to `1`. This triggers a Node
  to be added, updating the cache (or clearing a bad cache entry) and causing a new
  Node to get scheduled.'
- 'Nodes don''t scale down: Follow the instructions in the [Scale-to-zero section](#scale-to-zero)
  for creating a Node Pool for the Konnectivity replicas to run on.'
- 'Insufficient resources: Cannot schedule Pods due to insufficient resources.'
- 'Quota exceeded: Must have available quota amount that meets or exceeds the specified
  maxNode field.'
- 'Model download failures: Ensure internet connectivity from worker nodes.'
- 'Pods stuck in pending state: Check GPU node availability.'
- '401 Unauthorized: Recheck API access tokens or authentication setup'
- 'InternalError: A system error occurred during validation.'
- 'Invalid: The Node Pool''s configuration is not considered valid.'
- 'OverTarget: The current Node count exceeds the target Node count.'
auth_info:
  mentioned_objects:
  - API Access Tokens
  - Kubeconfig Files
client:
  base_url: https://console.coreweave.com
source_metadata: null
