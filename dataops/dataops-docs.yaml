resources:
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: database
  endpoint:
    path: /dataops/snowflake/databases
    method: GET
    data_selector: records
- name: schema
  endpoint:
    path: /dataops/snowflake/schemas
    method: GET
    data_selector: records
- name: table
  endpoint:
    path: /dataops/snowflake/tables
    method: GET
    data_selector: records
- name: stage
  endpoint:
    path: /dataops/snowflake/stages
    method: GET
    data_selector: records
- name: warehouse_properties
  endpoint:
    path: /warehouse_properties
    method: GET
    data_selector: records
    params: {}
- name: warehouse_properties_override
  endpoint:
    path: /warehouse_properties_override
    method: GET
    data_selector: records
    params:
      warehouse_size: SMALL
- name: snowflake_objects
  endpoint:
    path: /services/data/vXX.X/sobjects/SnowflakeObjects
    method: GET
    data_selector: records
    params: {}
- name: warehouse_default_grants
  endpoint:
    path: /dataops/snowflake/components/warehouse.yml
    method: GET
    data_selector: grants
- name: INGESTION
  endpoint:
    path: /services/data/vXX.X/ingestion
    method: GET
    data_selector: records
- name: CSV_FORMAT
  endpoint:
    path: /services/data/vXX.X/csv_format
    method: GET
    data_selector: records
- name: models
  endpoint:
    path: /dataops/modelling
    method: GET
    data_selector: models
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: API Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/api-orchestrator/
    method: GET
- name: AWS Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/aws-orchestrator/
    method: GET
- name: Azure Data Factory Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/azure-data-factory-orchestrator/
    method: GET
- name: Azure Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/azure-orchestrator/
    method: GET
- name: Coalesce Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/coalesce-orchestrator/
    method: GET
- name: Collibra Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/collibra-orchestrator/
    method: GET
- name: Data Product Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/data-product-orchestrator/
    method: GET
- name: data.world catalog Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/datadotworld-orchestrator/
    method: GET
- name: Dataiku Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/dataiku-orchestrator/
    method: GET
- name: DataPrep Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/dataprep-orchestrator/
    method: GET
- name: dbt Cloud Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/dbt-cloud-orchestrator/
    method: GET
- name: Fivetran Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/fivetran-orchestrator/
    method: GET
- name: Git Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/git-orchestrator/
    method: GET
- name: Informatica Cloud Data Governance and Catalog (CDGC) Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/informatica-cloud-data-governance-and-catalog-orchestrator/
    method: GET
- name: Informatica Cloud Taskflow Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/informatica-cloud-taskflow-orchestrator/
    method: GET
- name: Java 8 Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/java8-orchestrator/
    method: GET
- name: Matillion Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/matillion-orchestrator/
    method: GET
- name: Montecarlo Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/montecarlo-orchestrator/
    method: GET
- name: Modeling and transformation Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/transform-orchestrator/
    method: GET
- name: Python Orchestrators
  endpoint:
    path: /docs/data-pipelines-orchestration/python3-orchestrator/
    method: GET
- name: R Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/r-orchestrator/
    method: GET
- name: Reporting Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/reporting-orchestrator/
    method: GET
- name: Secrets Manager Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/secretsmanager-orchestrator/
    method: GET
- name: Snowflake Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/snowflake-orchestrator/
    method: GET
- name: Snowpark (Python) Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/snowparkpython-orchestrator/
    method: GET
- name: SOLE (Snowflake Object Lifecycle Engine) Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/snowflakeobjectlifecycle-orchestrator/
    method: GET
- name: Stage Ingestion Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/stage-ingestion-orchestrator/
    method: GET
- name: Stitch Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/stitch-orchestrator/
    method: GET
- name: Talend (TAC) Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/talend-tac-orchestrator/
    method: GET
- name: Talend (TMC) Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/talend-tmc-orchestrator/
    method: GET
- name: Utils Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/utils-orchestrator/
    method: GET
- name: VaultSpeed Orchestrator
  endpoint:
    path: /docs/data-pipelines-orchestration/vaultspeed-orchestrator/
    method: GET
- name: users
  endpoint:
    path: /api/users
    method: POST
    data_selector: users
    params: {}
- name: assist_modes
  endpoint:
    path: /assist/modes
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: user
  endpoint:
    path: /api/v1/users
    method: GET
    data_selector: data
    params: {}
- name: project
  endpoint:
    path: /api/v1/projects
    method: GET
    data_selector: data
    params: {}
- name: personal_data
  endpoint:
    path: /services/data/v1/personal_data
    method: GET
    data_selector: records
    params: {}
- name: pipeline_schedules
  endpoint:
    path: /projects/:project_id/pipeline_schedules
    method: POST
    data_selector: null
    params:
      id: project_id
      description: description
      ref: ref
      cron: cron
      cron_timezone: cron_timezone
      active: active
- name: trigger_pipeline
  endpoint:
    path: /projects/$PROJECT_ID/trigger/pipeline
    method: POST
    data_selector: null
    params:
      Project ID: PROJECT_ID
      Trigger token: pipeline trigger token
      Git ref: git reference
      Pipeline configuration file path: pipeline file path
- name: get_pipelines
  endpoint:
    path: /projects/$PROJECT_ID/pipelines
    method: GET
    data_selector: null
    params:
      Project ID: PROJECT_ID
- name: get_pipeline_detail
  endpoint:
    path: /projects/$PROJECT_ID/pipelines/$PIPELINE_ID
    method: GET
    data_selector: null
    params:
      Project ID: PROJECT_ID
      Pipeline ID: PIPELINE_ID
- name: get_jobs
  endpoint:
    path: /projects/$PROJECT_ID/jobs
    method: GET
    data_selector: null
    params:
      Project ID: PROJECT_ID
- name: get_job_detail
  endpoint:
    path: /projects/$PROJECT_ID/jobs/$JOB_ID
    method: GET
    data_selector: null
    params:
      Project ID: PROJECT_ID
      Job ID: JOB_ID
- name: get_job_logs
  endpoint:
    path: /projects/$PROJECT_ID/jobs/$JOB_ID/trace
    method: GET
    data_selector: null
    params:
      Project ID: PROJECT_ID
      Job ID: JOB_ID
- name: report
  endpoint:
    path: /reporting/report
    method: GET
    data_selector: platform
- name: agent_tag
  endpoint:
    path: /pipelines/includes/config/agent_tag.yml
    method: GET
    data_selector: agent_tag
- name: stages
  endpoint:
    path: /pipelines/includes/config/stages.yml
    method: GET
    data_selector: stages
- name: variables
  endpoint:
    path: /pipelines/includes/config/variables.yml
    method: GET
    data_selector: variables
- name: pipeline_stages
  endpoint:
    path: /pipelines/includes/config/stages.yml
    method: GET
    data_selector: stages
- name: project_variables
  endpoint:
    path: /pipelines/includes/config/variables.yml
    method: GET
    data_selector: variables
- name: unit_test_calc_exchange_rates_macro
  endpoint:
    path: /dataops/modelling/macros/unit_tests/test_calc_exchange_rates_macro.sql
    method: GET
    data_selector: records
    params: {}
- name: multiply_two_numbers_data
  endpoint:
    path: /dataops/snowflake/database.template.yml
    method: POST
    data_selector: seeds
    params: {}
- name: expected_result
  endpoint:
    path: /dataops/snowflake/database.template.yml
    method: POST
    data_selector: models
    params: {}
- name: custom_job
  endpoint:
    path: /pipelines/includes/local_includes/my_jobs/custom_job.yml
    method: GET
    data_selector: artifacts
    params: {}
- name: my_report_job
  endpoint:
    path: /my-reporting-pipeline-ci.yml
    method: GET
    data_selector: artifacts
    params: {}
- name: DataOps Report
  endpoint:
    path: /pipelines/includes/local_includes/reports/my_report.yml
    method: GET
    data_selector: artifacts
    params: {}
- name: Tables row count
  endpoint:
    path: /pipelines/includes/local_includes/reporting_jobs/render-row-count-report.yml
    method: GET
    data_selector: artifacts
    params: {}
- name: git_commands
  endpoint:
    path: /git/commands
    method: GET
    data_selector: commands
    params: {}
- name: company_reference_project
  endpoint:
    path: /customer/reference-and-templates/company-reference-project
    method: GET
    data_selector: project_details
    params: {}
- name: my-custom-job
  endpoint:
    path: /pipelines/includes/default/my-custom-job.yml
    method: GET
- name: example_model
  endpoint:
    path: /dataops/modelling/models/example_model.yml
    method: POST
    data_selector: config
    params: {}
- name: share_macro
  endpoint:
    path: /dataops/modelling/macros/share_macro.sql
    method: POST
    data_selector: macro
    params: {}
- name: cancel_running_queries
  endpoint:
    path: /pipelines/includes/local_includes/cancel_queries.sql
    method: POST
    data_selector: results
    params: {}
- name: TABLE_DATA_TYPE_CHANGE
  endpoint:
    path: /tables/TABLE_DATA_TYPE_CHANGE
    method: GET
    data_selector: columns
    params: {}
- name: TABLE_DATA_TYPE_CHANGE_BACKUP
  endpoint:
    path: /tables/TABLE_DATA_TYPE_CHANGE_BACKUP
    method: GET
    data_selector: columns
    params: {}
- name: TABLE_DATA_TYPE_CHANGE_NEW
  endpoint:
    path: /tables/TABLE_DATA_TYPE_CHANGE_NEW
    method: GET
    data_selector: columns
    params: {}
- name: production-ci
  endpoint:
    path: /pipelines/includes/config/agent_tag_production.yml
    method: GET
- name: PIPE_1
  endpoint:
    path: /dataops/snowflake/database.template.yml
    method: GET
    data_selector: copy_statement
    params: {}
- name: source_table
  endpoint:
    path: dataops/modelling/sources/source_table.yml
    method: GET
    data_selector: tables
    params: {}
- name: history_inc
  endpoint:
    path: dataops/modelling/models/history_inc.sql
    method: GET
    data_selector: select
    params: {}
- name: custom_before_script
  endpoint:
    path: /pipelines/includes/config/variables.yml
    method: GET
    data_selector: variables
    params: {}
- name: dbt_package
  endpoint:
    path: /path/to/package-project.git
    method: GET
    data_selector: packages
    params: {}
- name: health_check_job
  endpoint:
    path: /pipelines/includes/default/healthcheck_job.yml
    method: GET
- name: databases
  endpoint:
    path: /databases
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: database_roles
  endpoint:
    path: /database/roles
    method: GET
    data_selector: roles
    params: {}
- name: account_roles
  endpoint:
    path: /account/roles
    method: GET
    data_selector: roles
    params: {}
- name: database_level_hooks
  endpoint:
    path: /dataops/snowflake/hooks.template.yml
    method: POST
    data_selector: commands
    params: {}
- name: grants_hooks
  endpoint:
    path: /dataops/snowflake/hooks.template.yml
    method: POST
    data_selector: commands
    params: {}
- name: roles
  endpoint:
    path: /dataops/snowflake/roles.template.yml
    method: POST
    data_selector: roles
    params: {}
- name: mart_orders
  endpoint:
    path: /dataops/modeling/models/mart/mart_orders.sql
    method: SELECT
    data_selector: results
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: pull_policy_retry
  endpoint:
    path: /runners/docker
    method: GET
    data_selector: pull_policy
    params: {}
- name: pull_policy_fallback
  endpoint:
    path: /runners/docker
    method: GET
    data_selector: pull_policy
    params: {}
- name: Iceberg Table Creation
  endpoint:
    path: /create/iceberg_table
    method: POST
    data_selector: result
    params: {}
- name: Database Creation
  endpoint:
    path: /create/database
    method: POST
    data_selector: result
    params:
      external_volume: <external_volume_name>
      catalog: <external_catalog_name>
- name: Database Alteration
  endpoint:
    path: /alter/database
    method: POST
    data_selector: result
    params:
      external_volume: <external_volume_name>
      catalog: <external_catalog_name>
- name: iceberg_table
  endpoint:
    path: /services/iceberg/tables
    method: POST
    data_selector: tables
    params: {}
- name: gitlab_sync
  endpoint:
    path: /api/v1/gitlab/sync
    method: POST
    data_selector: data
    params:
      scope: write_repository
- name: bitbucket_push
  endpoint:
    path: /api/v1/bitbucket/push
    method: POST
    data_selector: data
    params: {}
- name: runners
  endpoint:
    path: /runners
    method: GET
    data_selector: tags
- name: tags
  endpoint:
    path: /schemas/SAMPLE_SCHEMA/tags
    method: POST
    data_selector: tags
    params: {}
- name: tables
  endpoint:
    path: /schemas/SAMPLE_SCHEMA/tables
    method: POST
    data_selector: tables
    params: {}
notes:
- API requests are rate-limited per user. The rate limit for the DataOps.live API
  is 600 requests per minute.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- DataOps.live helps you create, test, and deploy data products and applications with
  simplicity and speed on Snowflake Data Cloud.
- It aims to bring the speed and agility of DevOps and CI/CD to data platforms.
- Learn how to navigate the platform and use it to its full potential.
- The typical development workflow you should leverage.
- SOC 2 compliant. We adhere to specific data security policies and procedures.
- The Dynamic Transformation requires External Access Integration (EAI) to enable
  seamless communication between DataOps runners, pipeline jobs, and Snowflake.
- While the default network rules cover essential connections, you can create new
  network rules for additional endpoints.
- The Dynamic Delivery requires External Access Integration (EAI) to enable seamless
  communication between DataOps runners, pipeline jobs, and Snowflake.
- Ensure that the default network rule that allows access to app.dataops.live is not
  removed when attaching new endpoints.
- Some objects like Contact may return nulls in deeply nested fields
- The template project assumes that you have access to the Snowflake sample data at
  sources/snowflake_sample_data/tpch_sf1. If this isn't enabled in your Snowflake
  environment, you may get an error.
- 'Mutually Exclusive Formats: The current configuration structure and SOLE for Data
  Products are mutually exclusive.'
- Using `rel()` to define relations allows SOLE to verify configuration more thoroughly
  before it is applied in Snowflake.
- If an object is defined as a relation, but its config is missing — either due to
  partial commit or due to accidental delete — SOLE will throw an error early on the
  stage.
- To enable SOLE for Data Products, add the file dataops_config.yml to the snowflake
  configuration directory.
- Warehouse for Ingestion operations
- Modeling and transforming source data into valuable data marts and data products
  by using the SQL-based DataOps Modelling and Transformation Engine (MATE)
- Orchestrating your dbt Cloud projects in pipelines ​for all environments
- Integrating enterprise systems required for building data products with 30+ out-of-box
  orchestrators
- The database name is automatically derived by DataOps.live.
- Operate pipelines include deployment and monitoring stages of the DataOps process.
- The data product platform is made up of many apps providing specialized capabilities
  ranging from data ingestion over data quality and data transformation to data observability
  and governance.
- Users can be invited with any email domain if using user and password authentication.
- All users have Assist enabled by default, enhancing their experience with seamless
  MR summaries.
- MCP servers handle authentication and access control, keeping your credentials and
  data secure.
- No setup is needed. Only a few clicks are required to create the data product project
  and feature branch.
- DataOps.live embeds data quality and accuracy directly into the manifest.
- Requires setup of connected app in DataOps.live
- The data product platform is the domain-specific orchestrator for data.
- DataOps.live offers support based on your needs and urgency.
- DataOps.live Create simplifies the creation process of data products through an
  intuitive user interface (UI).
- Running DataOps pipelines results in building data products, updating them, and
  refreshing their data to create different versions of the data products once there
  are any changes.
- DataOps.live takes every measure to commit to availability of 99.9%.
- Support is available Monday through Friday during GMT hours, excluding UK holidays.
- DataOps.live releases new capabilities and bug fixes regularly.
- For details about our maintenance window, read weekly maintenance window.
- Providing login to the Provider’s SaaS application
- Authorizing access to data pipelines
- Managing and operating such data pipelines
- We do not collect any special categories of personal data about you.
- We do not knowingly collect, maintain, or use personal information from children
  under 16 years of age, and no parts of our Services are directed to children.
- Uses cookies for authentication
- Currently, you must contact support@dataops.live to get an API key for your Organization.
- The reporting component of this API is opt-in. Please contact support@dataops.live
  to enable it for your organization.
- This endpoint is not suitable for security audit use-cases.
- You can manage most of the standard pipeline configuration and content from within
  the reference project.
- The bootstrap.yml file pulls in all config and default files and gets a project
  ready to run.
- The order of the stages is important as that defines the execution order of job
  execution.
- The SNOWFLAKE section is currently standardized and mandatory, along with the AWS.DEFAULT
  credentials section.
- Current system limitations require that the vault.yml file must exist on the runner.
- This is a simplified, idealized workflow based on the most typical implementations
  of DataOps.live.
- Set the variable DATAOPS_REMOVE_RENDERED_TEMPLATES in the variables.yml file in
  the project settings to remove the template once it has been processed.
- The output must not be generated by the same process that renders the report.
- The template rendering process can render output generated by other jobs/processes.
- Since our production pipelines are actually moving the data, and therefore using
  this functionality, and our qa branch is intended to mirror production these are
  the branches in which doing Data Quality Testing make sense.
- Unit testing should be considered within the overview of data testing approaches.
- 'When this runs, you get an output like this: Total Tests:8 Total Errors: 0 Unit
  tests passed.'
- If the function itself is changed incorrectly, all tests may fail.
- Data Types must be explicitly defined to avoid casting issues
- The artifacts keyword section is required to capture non-log output from jobs as
  part of a pipeline.
- If you want to disable artifact passing, define the job with empty dependencies.
- 'Setting expose_as: ''DataOpsReport'' is mandatory.'
- Ensure access to the reference project with CI_JOB_TOKEN for all users running pipelines.
- Release tag for reference project must match all ref values in includes within base_bootstrap.yml.
- Ensure you do so in each project. Otherwise, the local configuration will override
  that from your reference project, which may not be immediately apparent as it will
  probably be the same code right now.
- For training use only
- Should not be used to create real DataOps projects
- A primary key (PK) is a column in a database that uniquely identifies each row,
  and it cannot have null values.
- Having only one primary key column for each table or view in your database is essential.
- A primary key constraint is a rule or condition that ensures the uniqueness and
  integrity of a primary key column or a set of columns within a database table.
- You can define primary key constraints during the initial creation of a table or
  add them afterward using an ALTER TABLE statement.
- A table can have only one primary key constraint. However, a primary key can consist
  of multiple columns if necessary.
- One approach to adding primary keys to a table is using dbt post-hooks.
- You can also convert the ALTER statement into a dbt macro that works with all the
  materializations.
- Ensure to replace 'share_name1' and 'share_name2' with the names of the shares you
  want to add the tables or views to.
- Replace 'ROLE_OWNING_THE_SHARE' with the role that owns the share, and 'DATAOPS_WRITER'
  with the desired role to revert to after the sharing process is complete.
- Verifying that the tables or views have been successfully added to the existing
  share is essential.
- Using the MATE orchestrator enhances data-sharing capabilities and establishes a
  strong data access control framework.
- It is necessary to provide alternative credentials using a project access token
  against this project, using the api scope.
- Careful... The SNOWFLAKE_* parameters in the example job below may need adjusting
  depending on your project's configuration and secrets structure.
- Pipeline runs in a feature branch will only build/test models that have changed
  since the previous pipeline's commit.
- Ensure data can be converted without errors or loss of information.
- Hooks must be implemented for data load and unload.
- In a dev/feature pipeline, only the schemas you specify will be created and will
  be the only schemas in that branch's database.
- Every MATE model that builds in a dev/feature pipeline must have a schema that exists
  in that database.
- The end-to-end walkthrough in segregating environments supersedes this how-to guide
  now.
- While this guide still works, the new article is comprehensive.
- The select_statement parameter allows for customized data extraction based on specific
  query criteria.
- Defining where to put is_incremental() macro may significantly lighten your model
  and execution time, but one should take caution, e.g., when using window functions.
- Your new before_script is a standard shell script, so will need to have the execute
  bit set.
- DataOps orchestrator images are built as standard Docker-compatible images.
- Warning! As the DataOps images contain proprietary code, please do not publish custom
  images to any public registries or other publicly-accessible locations.
- Network rules are schema-level objects in Snowflake that store information about
  types of network traffic.
- When using a Snowflake Network Rule with an External Access Integration, only MODE
  as EGRESS and TYPE as HOST_PORT are supported.
- You won't notice any impact if you remove a network rule assigned to an external
  access integration.
- If you try to assign this external access integration to another object, like a
  procedure, you'll encounter an error stating that the network rule doesn't exist.
- Uses access token for authentication.
- Fallback jobs act as a retry mechanism.
- Incorporating fallback jobs enhances reliability and robustness.
- Users can no longer use their usernames & passwords with Git over HTTPS.
- They must now use a Personal Access Token (PAT)
- SSO users cannot use their SSO credentials to do Git operations over HTTPS.
- To use HTTPS, a Personal Access Token (PAT) is required.
- Create a deploy token for this project with read_repository access.
- Any pipeline job that uses this package will need to have the variable TRANSFORM_FORCE_DEPS
  set to 1.
- Once you no longer need the health check script, it is advised to remove this volume
  path.
- We do not recommend your logs exceed 50 MB.
- To maintain a good service level for all our customers, artifact support is subject
  to a fair use policy.
- Keep in mind that additional databases will require an extra layer of complexity
  in your project.
- If your use case works just as well using schemas, this may make things easier.
- Use LIFECYCLE_STATE_RESET!
- Ensure LIFECYCLE_STATE_RESET is set in the project to make sure objects do not get
  removed by future pipelines.
- Set the copy_grants parameter to true to retain access permissions from the original
  view when you recreate it.
- The schedule parameter of the task object supports a more structured format.
- Must apply changes to YAML configuration files to ensure compatibility with the
  latest version of SOLE.
- A backup file will be created automatically before conversion.
- Right now, database cloning is not supported. They can be imported into the mentioned
  resource, but any difference in behavior from standard database won't be handled
  (and can result in errors)
- To resolve this compatibility issue, reset the lifecycle state by setting the LIFECYCLE_STATE_RESET
  environment variable to 1.
- Database roles can control access to schema-level objects without cluttering the
  account role space.
- Account level role resides outside any database, whereas database role resides within
  a particular designated database.
- Roles are not automatically revoked if the hook is removed. The grant will still
  exist.
- By default, DataOps runs pipelines in parallel.
- It is possible to configure subsequent pipeline instances to auto-abort if another
  is already running.
- Recommended values for resource group names include using the name of the job or
  pipeline to limit concurrency.
- Be careful not to overuse the `retry` keyword. If your pipeline fails often, it
  is best to debug and rethink its logic rather than prolonged runs with 1 or 2 retries.
- The DataOps team does not have access to clients' runners for security reasons.
- One handy tip is performing a regular `docker system prune` command with a CRON
  job.
- The persistent_cache cannot be cleared from the data product platform, and it's
  unadvisable to clear it automatically.
- The proper way to resolve server certificate verification failed is to ensure you
  add the untrusted root certificate to your client system.
- In the case of the DataOps app, the untrusted root certificate is a flavor of the
  Let's Encrypt 'ISRG Root X1' root certificate.
- Setting up pre-commit hooks for a DataOps.live project is crucial in ensuring the
  quality and consistency of your code and data.
- There is a potential security risk with using the if-not-present policy.
- You can assign an external volume and catalog directly during Iceberg table creation
  or at the database level.
- For read and write Iceberg table catalog value will remain SNOWFLAKE and does not
  have to be specified.
- Ensure a hooks file path is specified in the dataops_config.yml file in the dataops/snowflake
  directory while working with Iceberg tables for SOLE for Data Products.
- If the external volume and catalog are set at the database level, you don't need
  to specify them again when creating the Iceberg table.
- DataOps project should be considered read-only from within DataOps - all changes
  should be made via GitLab.
- Pipeline run tags can be used as part of the synchronized information.
- Tags on runners serve as metadata attributes that help filter and select runners
  when configuring job executions in a pipeline.
- 'On weekdays, it is rendered as: - name: ADDRESS_ID description: ''ID of the address''
  tests: - weekday_test'
- 'However, on weekends, the same model is rendered as: - name: ADDRESS_ID description:
  ''ID of the address'' tests: - weekend_test'
- Tags are schema-level objects within SOLE.
- Use the `with_tags` key in the object definition to apply tags.
- DataOps uses the SSH protocol to communicate with Git securely.
- When you use SSH keys to authenticate to the DataOps.live remote server, you don't
  need to supply your username and password each time.
errors:
- '401 Unauthorized: Recheck authentication information'
- '429 Too Many Requests: Rate limit exceeded'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Error: SOLE will throw an error if current configuration is found while SOLE for
  Data Products is enabled.'
- '400 Bad Request: Check request parameters'
- '400 Bad Request: Check the request parameters and formatting'
- '401 Unauthorized: Validate authentication credentials'
- '404 Not Found: Ensure the endpoint exists'
- You have the right to make a complaint at any time to the Information Commissioner's
  Office (ICO), the UK regulator for data protection issues.
- Unit tests failed due to 1 error(s)
- Unit tests failed due to 8 error(s)
- If you run two jobs concurrently in a single pipeline stage, the job that finishes
  last creates the artifact files.
- 'Unauthorized: Check user access and permissions.'
- 'Not Found: Verify the project URL and path.'
- 'Invalid data type conversion: Ensure data type can be converted.'
- 'COPY INTO command fails: Check file format and table structure.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Spendview Owner
  - Spendview User
  - Observability User
client:
  base_url: https://app.dataops.live
  auth:
    type: oauth2
    flow: refresh_token
source_metadata: null
