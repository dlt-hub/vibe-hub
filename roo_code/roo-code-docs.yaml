resources:
- name: Code Mode
  endpoint:
    path: /using-modes/code
    method: GET
- name: Ask Mode
  endpoint:
    path: /using-modes/ask
    method: GET
- name: Architect Mode
  endpoint:
    path: /using-modes/architect
    method: GET
- name: Debug Mode
  endpoint:
    path: /using-modes/debug
    method: GET
- name: Orchestrator Mode
  endpoint:
    path: /using-modes/orchestrator
    method: GET
- name: browser_actions
  endpoint:
    path: /api/browser/actions
    method: POST
    data_selector: actions
    params: {}
- name: codebase_search
  endpoint:
    path: /advanced-usage/available-tools/codebase-search
    method: GET
    data_selector: results
- name: custom_modes
  endpoint:
    path: /api/custom_modes
    method: GET
    data_selector: modes
    params: {}
- name: custom_modes
  endpoint:
    path: /custom_modes
    method: GET
    data_selector: customModes
- name: custom_modes
  endpoint:
    path: /features/custom-modes
    method: GET
    data_selector: modes
    params: {}
- name: custom_modes
  endpoint:
    path: /custom_modes
    method: GET
    data_selector: customModes
- name: slash_commands
  endpoint:
    path: /slash-commands
    method: GET
    data_selector: commands
    params: {}
- name: shell_integration
  endpoint:
    path: /features/shell-integration
    method: GET
    data_selector: shell_integration_details
- name: new_task
  endpoint:
    path: /advanced-usage/available-tools/new-task
    method: POST
    data_selector: ''
    params: {}
- name: model_info
  endpoint:
    path: /v1/model/info
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: Default Model
  endpoint:
    path: /models/claude-sonnet-4@20250514
    method: GET
notes:
- API keys are stored securely in VSCode's Secret Storage and are never exposed in
  plain text.
- Analytics lets you track tokens, tasks, inference cost, and Cloud Agent credits
  usage in aggregate or according to a range of filters, defined by you.
- Tasks are private by default—nothing is shared until you create a link.
- You choose Organization vs Public visibility per link.
- One-way monitoring only; no cloud commands are sent to your IDE
- For live updates, your IDE must be open and connected
- Internet connection required for live updates
- Roomote Control is available in our paid plans
- Only one task can be active per workspace at a time – but you can have as many workspaces
  open as you want (or your computer can handle)
- Cloud Agents require an active paid plan.
- Model inference is billed by your provider (bring your own API keys). We don't mark
  inference costs up and are provider-agnostic.
- Requires API key from an AI model provider to function.
- Auto-approve settings bypass confirmation prompts, giving Roo direct access to your
  system.
- Browser Use requires models that support images.
- Each browser session must start with `launch` and end with `close`.
- Checkpoints are enabled by default.
- Git must be installed for checkpoints to function.
- Uses Google Gemini for embeddings — currently free
- Uses `@problems` for context
- Focus on fixing errors before warnings
- YAML is the preferred format for defining custom modes due to several advantages
  over JSON.
- Manual conversion is required for existing JSON .roomodes files.
- Roo Code can read .roomodes files in either YAML or JSON format.
- Fast Edits is enabled by default in Roo Code.
- Enabling editing through diffs provides significant benefits.
- Prompt enhancement is an experimental feature. The quality of the enhanced prompt
  may vary depending on the complexity of your request and the capabilities of the
  underlying model.
- The exported JSON file contains all your configured API Provider Profiles and Global
  Settings. Crucially, this includes API keys in plaintext.
- Intelligent Context Condensing is enabled by default
- Roo Code uses a default temperature of 0.0 for most models, optimizing for maximum
  determinism and precision in code generation.
- Some models use higher default temperatures - DeepSeek R1 models and certain reasoning-focused
  models default to 0.6, providing a balance between determinism and creative exploration.
- Models with thinking capabilities require a fixed temperature of 1.0.
- Enabled by default for a faster, more streamlined workflow.
- Configurable limit from 1 to 100 files.
- Roo actively monitors the .rooignore file. Any changes you make are reloaded automatically,
  ensuring Roo always uses the most current rules.
- The .rooignore file itself is always implicitly ignored, so Roo cannot change its
  own access rules.
- .rooignore rules apply only to files and directories within the current VS Code
  workspace root.
- Command names are automatically processed to lowercase and special characters removed
- Shell integration is built into Roo Code and works automatically in most cases.
- If you encounter issues, review troubleshooting steps provided in the documentation.
- Shell integration is automatically enabled in Roo Code and connects directly to
  your terminal's command execution lifecycle without requiring any setup from you.
- If you see 'Shell Integration Unavailable' messages or experience issues with command
  execution, try updating VSCode or ensuring a compatible shell is selected.
- If you have 32-bit Cygwin installed, use "C:\cygwin\bin\bash.exe" for the path.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Context poisoning is a persistent issue within a given session.
- Once a chat session's context is compromised, treat that session as disposable.
- Requires user approval before creating each new task
- Should not be used until previous tool uses are confirmed successful (guideline,
  not enforced)
- Limited to a single command for result demonstration
- Cannot present multiple command options
- Commands require user approval before execution
- Limited to demonstrating results that can be shown via CLI commands
- Cannot be used for partial task completion or progress updates
- Result formatting strips XML closing tags through internal processing
- Understanding and managing API usage is crucial for a smooth and cost-effective
  experience with Roo Code.
- Rate limits, which default to 0 (disabled) and typically don't need adjustment,
  are now configured per profile.
- Most AI providers charge based on the number of tokens used.
- Pricing varies depending on the provider and the specific model.
- The cost calculation is an estimate. The actual cost may vary slightly depending
  on the provider's billing practices.
- Some providers may offer free tiers or credits.
- Some providers offer prompt caching which greatly lowers cost.
- Running large language models locally can be resource-intensive.
- Once you've downloaded a model, you can use Roo Code offline with that model.
- 'Prompt Caching: Claude models support prompt caching, which can significantly reduce
  costs and latency for repeated prompts.'
- 'Context Window: Claude models have large context windows (200,000 tokens), allowing
  you to include a significant amount of code and context in your prompts.'
- Uses your existing Claude CLI authentication.
- Ensure your IAM user or role has the necessary permissions to invoke Bedrock models.
  The `bedrock:InvokeModel` permission is required.
- Using cross-region inference may result in higher latency.
- Cerebras specializes in extremely fast inference speeds, making it ideal for real-time
  coding assistance.
- The qwen-3-coder-480b-free model provides access to high-performance inference at
  no cost with rate limits.
- DeepInfra offers low latency with automatic load balancing across global locations.
- Competitive pricing with prompt caching to reduce costs for repeated contexts.
- Access to the latest open-source models including specialized coding models.
- Models support context windows up to 256K tokens for large codebases.
- Pay-per-use model with no minimums.
- Doubao uses the base URL https://ark.cn-beijing.volces.com/api/v3 and servers are
  located in Beijing, China.
- All models are currently free with no usage costs, making Featherless ideal for
  experimentation and development.
- No image support or prompt caching available on any model.
- 'Cost-Effective: Fireworks AI offers significantly lower pricing than proprietary
  models while maintaining competitive performance.'
- 'Large Context Windows: Most models support 128K-256K tokens, suitable for processing
  large documents and maintaining extended conversations.'
- 'OpenAI Compatibility: The provider uses an OpenAI-compatible API format with streaming
  support and usage tracking.'
- 'Text-Only: All models are text-only without image support or prompt caching capabilities.'
- 'Default Temperature: Uses 0.5 temperature by default for balanced creativity and
  consistency.'
- Gemini API usage is priced based on input and output tokens.
- URL context and search grounding may incur additional costs.
- Glama operates on a pay-per-use basis. Pricing varies depending on the model you
  choose.
- Glama supports prompt caching, which can significantly reduce costs and improve
  performance for repeated prompts.
- Requires constant copy-pasting between VS Code and your browser.
- The back-and-forth process is significantly slower than direct API integration.
- Manual copying and pasting can introduce errors or omissions.
- Get your API key from the IO Intelligence website
- Most people won't need to adjust the base URL.
- This is crucial.
- OpenRouter charges based on the underlying model's pricing.
- If you use your own key for the underlying service, OpenRouter charges 5% of what
  it normally would.
- 'OAuth 2.0: Secure authentication with automatic token refresh'
- Tokens refresh transparently with 30-second buffer
- 'Free Tier: 2,000 requests/day and 60 requests/minute with no token limits, available
  during a promotional period.'
- Requires Roo Code Cloud account
- Internet connection required
- Prompts and completions are logged for model improvement
- Available during promotional period only
- Unbound emphasizes security features for enterprise use.
- The default model is anthropic/claude-sonnet-4 if no model is selected.
- The default temperature is 0.7 and is configurable per model.
- Ensure your Google Cloud account has the necessary permissions to access Vertex
  AI and the specific models you want to use.
- This integration is highly experimental and may not work as expected.
- The VS Code Language Model API is still under development. Expect changes and potential
  instability.
- This feature relies entirely on other extensions providing models.
- grok-code-fast-1 is available for free through the Roo Code Cloud provider during
  the promotional period.
- Region setting determines both the API endpoint and available models.
- 'OpenAI Compatibility: Z AI uses an OpenAI-compatible API, providing streaming responses
  and usage reporting.'
- 'API Key Required: A valid API key is required for all requests.'
errors:
- 'Browser session not started: Ensure to use `launch` before any actions.'
- 'Cannot perform actions: Only one browser action can be used per message.'
- Connection to Qdrant failed
- Invalid API Key
- Model Not Found
- Access to [file_path] is blocked by the .rooignore file settings. You must try to
  continue in the task without using this file, or ask the user to update the .rooignore
  file.
- 'Shell Integration Unavailable: Check VSCode version and shell compatibility.'
- 'Execution Policy Error: Set appropriate execution policy in PowerShell.'
- 'Invalid API Key: Double-check that you''ve entered the API key correctly.'
- 'Model Not Found: Make sure you''re using a valid model ID for your chosen provider.'
- 'Connection Errors: Verify the Base URL is correct and that your provider''s API
  is accessible.'
- 'Unexpected Results: If you''re getting unexpected results, try a different model.'
- '401 Unauthorized: Provider should auto-refresh (check logs)'
auth_info:
  mentioned_objects: []
client:
  base_url: https://roocode.com
  headers:
    Accept: application/json
source_metadata: null
