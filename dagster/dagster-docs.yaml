resources:
- name: goodreads_dataset
  endpoint:
    path: /goodreads_books.json
    method: GET
- name: github_data
  endpoint:
    path: /graphql
    method: POST
- name: processed_data
  endpoint:
    path: src/dagster_quickstart/defs/data/sample_data.csv
    method: GET
    data_selector: processed_data
- name: data
  endpoint:
    path: /data
    method: GET
    data_selector: results
- name: customers
  endpoint:
    path: https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_customers.csv
    method: GET
- name: orders
  endpoint:
    path: https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_orders.csv
    method: GET
- name: payments
  endpoint:
    path: https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_payments.csv
    method: GET
- name: asset
  endpoint:
    path: /api/dagster/assets
    method: GET
- name: asset_check
  endpoint:
    path: /api/dagster/asset-checks
    method: GET
- name: jobs
  endpoint:
    path: /api/dagster/jobs
    method: GET
- name: schedules
  endpoint:
    path: /api/dagster/schedules-sensors
    method: GET
- name: sensors
  endpoint:
    path: /api/dagster/schedules-sensors
    method: GET
- name: resources
  endpoint:
    path: /api/dagster/resources
    method: GET
- name: io_managers
  endpoint:
    path: /api/dagster/io-managers
    method: GET
- name: ops
  endpoint:
    path: /api/dagster/ops
    method: GET
- name: graphs
  endpoint:
    path: /api/dagster/graphs
    method: GET
- name: partitions
  endpoint:
    path: /api/dagster/partitions
    method: GET
- name: config
  endpoint:
    path: /api/dagster/config
    method: GET
- name: definitions
  endpoint:
    path: /api/dagster/definitions
    method: GET
- name: daily_refresh
  endpoint:
    method: GET
- name: orders_aggregation_check
  endpoint:
    path: /assets/orders_aggregation/checks
    method: GET
- name: schedules
  endpoint:
    path: /guides/automate/schedules
- name: declarative_automation
  endpoint:
    path: /guides/automate/declarative-automation
- name: sensors
  endpoint:
    path: /guides/automate/sensors
- name: asset_sensors
  endpoint:
    path: /guides/automate/asset-sensors
- name: automation_conditions
  endpoint:
    path: /api/dagster/assets
    method: GET
- name: executor
  endpoint:
    path: /api/dagster/internals#dagster.executor
    method: GET
- name: ExecutorDefinition
  endpoint:
    path: /api/dagster/internals#dagster.ExecutorDefinition
    method: GET
- name: oss-telemetry
  endpoint:
    path: /oss-telemetry.js
    method: GET
- name: run_retries
  endpoint:
    method: GET
    params:
      max_retries: 3
      retry_strategy: FROM_FAILURE
      retry_on_asset_or_op_failure: true
- name: dagster_webserver
  endpoint:
    method: GET
- name: dagster_daemon
  endpoint:
    method: GET
- name: code_location_server
  endpoint:
    method: GET
- name: automation
  endpoint:
    path: /Automation
    method: GET
- name: assets
  endpoint:
    path: /Assets
    method: GET
- name: insights_metrics
  endpoint:
    method: GraphQL
- name: storage
  endpoint:
    method: GET
- name: run_launcher
  endpoint:
    method: GET
- name: run_coordinator
  endpoint:
    method: GET
- name: compute_logs
  endpoint:
    method: GET
- name: local_artifact_storage
  endpoint:
    method: GET
- name: sensors
  endpoint:
    method: GET
- name: schedules
  endpoint:
    method: GET
- name: backfills
  endpoint:
    method: GET
- name: auto_materialize
  endpoint:
    method: GET
- name: storage
  endpoint:
    path: /dagster.yaml
    method: GET
- name: run_launcher
  endpoint:
    path: /dagster.yaml
    method: GET
- name: run_coordinator
  endpoint:
    path: /dagster.yaml
    method: GET
- name: compute_logs
  endpoint:
    path: /dagster.yaml
    method: GET
- name: local_artifact_storage
  endpoint:
    path: /dagster.yaml
    method: GET
- name: telemetry
  endpoint:
    path: /dagster.yaml
    method: GET
- name: code_servers
  endpoint:
    path: /dagster.yaml
    method: GET
- name: data_retention
  endpoint:
    path: /dagster.yaml
    method: GET
- name: sensors
  endpoint:
    path: /dagster.yaml
    method: GET
- name: schedules
  endpoint:
    path: /dagster.yaml
    method: GET
- name: auto_materialize
  endpoint:
    path: /dagster.yaml
    method: GET
- name: backfills
  endpoint:
    path: /dagster.yaml
    method: GET
- name: orders
  endpoint:
    method: GET
- name: ConfigurableResource
  endpoint:
    path: /api/dagster/resources#dagster.ConfigurableResource
- name: ResourceDefinition
  endpoint:
    path: /api/dagster/resources#dagster.ResourceDefinition
- name: InitResourceContext
  endpoint:
    path: /api/dagster/resources#dagster.InitResourceContext
- name: build_init_resource_context
  endpoint:
    path: /api/dagster/resources#dagster.build_init_resource_context
- name: build_resources
  endpoint:
    path: /api/dagster/resources#dagster.build_resources
- name: with_resources
  endpoint:
    path: /api/dagster/resources#dagster.with_resources
- name: definitions
  endpoint:
    path: definitions.py
    method: GET
    data_selector: defs
- name: multi_asset_check
  endpoint:
    method: decorator
    params:
      specs: AssetCheckSpec list
      can_subset: true
- name: multi_asset
  endpoint:
    method: decorator
    params:
      specs: AssetSpec list
      check_specs: AssetCheckSpec list
      can_subset: true
      skippable: true
- name: python_module
  endpoint:
    path: python_module
    method: LOAD
    params:
      module_name: hello_world_module.definitions
- name: python_file
  endpoint:
    path: python_file
    method: LOAD
    params:
      relative_path: hello_world_repository.py
      attribute: hello_world_repository
      working_directory: my_working_directory/
      executable_path: venvs/path/to/python
      location_name: my_location
- name: grpc_server
  endpoint:
    path: grpc_server
    method: LOAD
    params:
      host: 0.0.0.0
      port: 4266
- name: python_file
  endpoint:
    path: ''
    method: GET
    params:
      relative_path: ''
      attribute: ''
      working_directory: ''
      executable_path: ''
      location_name: ''
- name: python_module
  endpoint:
    path: ''
    method: GET
    params:
      module_name: ''
- name: grpc_server
  endpoint:
    path: ''
    method: GET
    params:
      host: ''
      port: ''
      location_name: ''
- name: external_code
  endpoint:
    method: GET
- name: SnowflakePandasIOManager
  endpoint:
    params:
      database: '{{ dlt.secrets[''SNOWFLAKE_DATABASE''] }}'
      account: '{{ dlt.secrets[''SNOWFLAKE_ACCOUNT''] }}'
      user: '{{ dlt.secrets[''SNOWFLAKE_USER''] }}'
      password: '{{ dlt.secrets[''SNOWFLAKE_PASSWORD''] }}'
- name: locations
  endpoint:
    path: locations
    method: GET
- name: locations
  endpoint:
    path: /dagster_cloud.yaml
    method: GET
- name: ops
  endpoint:
    path: '@dg.op'
    method: decorator
- name: scheduler_daemon
  endpoint:
    method: RUN
    params:
      enabled_by: DagsterDaemonScheduler
- name: run_queue_daemon
  endpoint:
    method: RUN
    params:
      enabled_by: QueuedRunCoordinator
- name: sensor_daemon
  endpoint:
    method: RUN
    params:
      enabled_by: always
- name: run_monitoring_daemon
  endpoint:
    method: RUN
    params:
      enabled_by: run_monitoring_field
- name: DefaultRunCoordinator
  endpoint:
    path: /api/dagster/internals#dagster._core.run_coordinator.DefaultRunCoordinator
    method: GET
- name: QueuedRunCoordinator
  endpoint:
    path: /api/dagster/internals#dagster._core.run_coordinator.QueuedRunCoordinator
    method: GET
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data.runs
- name: assets
  endpoint:
    path: /graphql
    method: POST
    data_selector: data.assets
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data.jobs
- name: schedules
  endpoint:
    path: /graphql
    method: POST
    data_selector: data.schedules
- name: sensors
  endpoint:
    path: /graphql
    method: POST
    data_selector: data.sensors
- name: asset_materialization
  endpoint:
    path: /report_asset_materialization
    method: POST
- name: asset_observation
  endpoint:
    path: /report_asset_observation
    method: POST
- name: asset_check
  endpoint:
    path: /report_asset_check
    method: POST
- name: assets
  endpoint:
    path: /concepts/assets
    method: GET
- name: jobs
  endpoint:
    path: /concepts/ops-jobs-graphs
    method: GET
- name: partitions
  endpoint:
    path: /concepts/partitions-schedules-sensors/partitions
    method: GET
- name: assets
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /runs
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: partitions
  endpoint:
    method: GET
- name: assets
  endpoint:
    path: /concepts/assets/software-defined-assets
    method: GET
- name: jobs
  endpoint:
    path: /concepts/ops-jobs-graphs/jobs
    method: GET
- name: ops
  endpoint:
    path: /concepts/ops-jobs-graphs/ops
    method: GET
- name: graphs
  endpoint:
    path: /concepts/ops-jobs-graphs/graphs
    method: GET
- name: jobs
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
    data_selector: records
- name: pipeline_run_status
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: repository_location
  endpoint:
    path: /shutdown_repository_location
    method: POST
- name: launchRun
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: assetsOrError
  endpoint:
    method: GET
    params:
      assetKeys: []
- name: pipeline_runs
  endpoint:
    method: GET
- name: schedules
  endpoint:
    method: GET
- name: sensors
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: asset_materialization
  endpoint:
    path: /report_asset_materialization
    method: POST
- name: asset_observation
  endpoint:
    path: /report_asset_observation
    method: POST
- name: asset_check
  endpoint:
    path: /report_asset_check
    method: POST
- name: assets
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: asset_specs
  endpoint:
    method: GET
- name: backfills
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: sensors
  endpoint:
    method: GET
- name: schedules
  endpoint:
    method: GET
- name: asset_materialization
  endpoint:
    path: /dagster/asset/materialize
    method: POST
- name: backfill
  endpoint:
    path: /dagster/backfill
    method: POST
- name: sensorsOrError
  endpoint:
    method: POST
    data_selector: data
- name: assets
  endpoint:
    path: /graphql
    method: POST
- name: assets
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: automations
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: assets
  endpoint:
    path: /assets
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
- name: partitions
  endpoint:
    path: /partitions
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
- name: PipesSubprocessClient
  endpoint:
    method: GET
- name: PipesDocketClient
  endpoint:
    method: GET
- name: PipesK8sClient
  endpoint:
    method: GET
- name: PipesDatabricksClient
  endpoint:
    method: GET
- name: partitionsByAssets
  endpoint:
    path: /graphql
    method: POST
    data_selector: backfillParams
- name: assets
  endpoint:
    path: /concepts/assets
    method: GET
- name: jobs
  endpoint:
    path: /concepts/ops-jobs-graphs
    method: GET
- name: partitions
  endpoint:
    path: /concepts/partitions-schedules-sensors/partitions
    method: GET
- name: runs
  endpoint:
    method: GET
- name: assetMaterializations
  endpoint:
    method: GET
- name: asset
  endpoint:
    path: /graphql
    method: POST
- name: job
  endpoint:
    path: /graphql
    method: POST
- name: run
  endpoint:
    path: /graphql
    method: POST
- name: ecr_public_resource
  endpoint:
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
    data_selector: runs
- name: assets
  endpoint:
    path: /assets
    method: GET
    data_selector: assets
- name: jobs
  endpoint:
    path: /jobs
    method: GET
    data_selector: jobs
- name: runs
  endpoint:
    method: POST
    data_selector: data
- name: runs
  endpoint:
    method: POST
- name: assets
  endpoint:
    method: GET
- name: pipeline_runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: schedules
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: pipeline
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: campaign_member
  endpoint:
    method: GET
- name: contact
  endpoint:
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
- name: instance
  endpoint:
    path: /instance
    method: GET
- name: asset_materialization
  endpoint:
    path: /report_asset_materialization
    method: POST
- name: asset_observation
  endpoint:
    path: /report_asset_observation
    method: POST
- name: asset_check
  endpoint:
    path: /report_asset_check
    method: POST
- name: assets
  endpoint:
    method: POST
- name: runs
  endpoint:
    method: POST
- name: jobs
  endpoint:
    method: POST
- name: schedules
  endpoint:
    method: POST
- name: sensors
  endpoint:
    method: POST
- name: sensorsOrError
  endpoint:
    method: POST
    data_selector: data
    params:
      statuses: filter
- name: asset_records
  endpoint:
    method: GET
    params:
      before_storage_id: storage_id
      after_storage_id: storage_id
- name: asset_checks
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: sensors
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: backfills
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: schedules
  endpoint:
    method: GET
- name: sensors
  endpoint:
    method: GET
- name: backfills
  endpoint:
    method: GET
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    method: GET
- name: backfills
  endpoint:
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
    data_selector: assets
- name: runs
  endpoint:
    path: /runs
    method: GET
    data_selector: runs
- name: partitionsByAssets
  endpoint:
    path: /backfillParams
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
    data_selector: runs
- name: assets
  endpoint:
    path: /assets
    method: GET
    data_selector: assets
- name: asset_materializations
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: schedules
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: sensors
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: backfills
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: assets
  endpoint:
    method: GET
- name: runs
  endpoint:
    path: /runs
    method: GET
- name: locations
  endpoint:
    path: /locations
    method: GET
- name: assets
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: data
- name: asset
  endpoint:
    method: GET
- name: run
  endpoint:
    method: GET
- name: job
  endpoint:
    method: GET
- name: asset
  endpoint:
    path: /graphql
    method: POST
- name: items
  endpoint:
    path: /maxitem.json
    method: GET
- name: item
  endpoint:
    path: /item/{item_id}.json
    method: GET
- name: items
  endpoint:
    path: /item/{item_id}.json
    method: GET
    params: {}
- name: max_item
  endpoint:
    path: /maxitem.json
    method: GET
    params: {}
- name: items
  endpoint:
    path: /item/{item_id}.json
    method: GET
- name: maxitem
  endpoint:
    path: /maxitem.json
    method: GET
- name: items
  endpoint:
    path: /maxitem.json
    method: GET
- name: item
  endpoint:
    path: /item/{item_id}.json
    method: GET
- name: items
  endpoint:
    path: /item/{item_id}.json
    method: GET
- name: max_item
  endpoint:
    path: /maxitem.json
    method: GET
- name: DefaultRunLauncher
  endpoint:
    method: Python class instantiation
- name: K8sRunLauncher
  endpoint:
    method: Python class instantiation
- name: EcsRunLauncher
  endpoint:
    method: Python class instantiation
- name: DockerRunLauncher
  endpoint:
    method: Python class instantiation
- name: CeleryK8sRunLauncher
  endpoint:
    method: Python class instantiation
- name: runs
  endpoint:
    path: /graphql
    method: POST
    data_selector: runsOrError.results
    params:
      cursor: null
      limit: 10
- name: repositories
  endpoint:
    path: /graphql
    method: POST
    data_selector: repositoriesOrError.nodes
- name: jobs
  endpoint:
    path: /graphql
    method: POST
    data_selector: repositoryOrError.jobs
notes:
- Tutorial for building ETL pipeline with Dagster
- Integrates with DuckDB, dbt, and Evidence
- Requires Python 3.9+ and uv or pip
- Uses Dagster webserver on http://127.0.0.1:3000
- Public Goodreads JSON dataset used for LLM fine-tuning pipeline
- Data is loaded into DuckDB for processing
- Pipeline includes feature engineering and OpenAI fine-tuning job creation
- Schedules work with Assets, Ops, Graphs
- Declarative automation works with Assets only
- Sensors work with Assets, Ops, Graphs
- Asset sensors work with Assets only
- GraphQL triggers work with Assets, Ops, Jobs
- Requires enabling the default automation condition sensor in the UI
- Navigate to Automation, find desired code location, and toggle on the default_automation_condition_sensor
- For time-partitioned assets, conditions only consider the latest time partition
- For static and dynamic-partitioned assets, conditions consider all partitions
- Default executor is multi_or_in_process_executor which executes each step in its
  own process
- Executing a job via JobDefinition overrides the job's executor and uses in_process_executor
  instead
- Custom executors are possible but not well-documented with internal APIs in flux
- Telemetry data is collected from both frontend and backend
- Frontend telemetry collected from JavaScript bundle hosted unminified at https://dagster.io/oss-telemetry.js
- Backend telemetry collection logged at $DAGSTER_HOME/logs/ if $DAGSTER_HOME is set
  or ~/.dagster/logs/ if not set
- Does not collect any data processed by Dagster pipelines
- Does not collect any identifiable information about Dagster definitions including
  names of assets, ops, or jobs
- 'Can opt out by adding telemetry: enabled: false to $DAGSTER_HOME/dagster.yaml'
- Dagster+ is a managed orchestration platform built on top of Dagster's open source
  engine
- 'Comes in two flavors: Serverless and Hybrid offerings'
- Manages Dagster's web UI, metadata stores, and backend services for orchestration
- HIPAA compliant, SOC 2 Type II certified, and meets GDPR requirements
- Maximum of 100 GB of bandwidth per day
- Maximum of 4500 step-minutes per day
- Runs receive 4 vCPU cores, 16 GB of RAM, and 128 GB of ephemeral disk
- Code locations receive 0.25 vCPU cores and 1 GB of RAM
- All Serverless jobs run in the United States
- Infrastructure cannot be customized or extended, such as using additional containers
- Works best with workloads that primarily orchestrate other services or perform light
  computation
- Dagster+ imposes rate limits of 40,000 user log events per minute and 100MB of events
  per minute, with automatic retries for requests that exceed limits
- Run monitoring requires running the Dagster Daemon
- In Dagster+, run monitoring is always enabled and can be configured in deployment
  settings
- Detecting run worker crashes only works when using a run launcher other than the
  DefaultRunLauncher
- Resuming runs after run worker crashes is currently only supported when using K8sRunLauncher
  with k8s_job_executor or DockerRunLauncher with docker_executor
- Run retries kick off a new run whenever a run fails for any reason
- Maximum retry limit applies to the whole run instead of each individual op
- FROM_FAILURE requires an I/O manager that can access outputs from other runs
- Op retry count will reset for each retried run when both op and run retries are
  enabled
- Setting retry_on_asset_or_op_failure to false only changes retry behavior for runs
  on Dagster version 1.6.7 or greater
- Uses Python warnings module to filter API lifecycle warnings
- Dagster uses built-in DeprecationWarning for deprecated APIs
- Supports structured event logs and raw compute logs
- Logs stream back to the UI in real time
- Windows / Azure users may need to enable the environment variable PYTHONLEGACYWINDOWSSTDIO
  in order for compute logs to be displayed in the Dagster UI
- Built-in loggers support all levels of Python logs such as INFO, DEBUG, ERROR
- Can be configured to capture only specified levels such as ERROR
- Can include manually-defined messages produced inside certain Dagster definitions
  like assets, ops, and sensors
- Feature is only available in Dagster+
- Metrics are updated on a daily basis
- 'Data retention: Dagster+ Pro - 120 days, All other plans - 30 days'
- 'Built-in metrics include: Dagster credits, Compute duration, Materializations,
  Observations, Step failures, Step retries, Asset check warnings, Asset check errors,
  Retry compute'
- Pro plan supports integration of additional metrics like asset materialization metadata,
  Google BigQuery usage, and Snowflake usage
- This feature is only available in Dagster+
- Requires configuration of an alert notification service before creating alert policies
- Uses dagster.yaml file for configuration
- Environment variables can override values in dagster.yaml
- Default location specified by DAGSTER_HOME environment variable
- SQLite storage requires base_dir configuration
- PostgreSQL storage requires dagster-postgres library
- MySQL storage requires dagster-mysql library
- 'Auto-materialization default uses sensors: false, use_threads: false'
- Default op concurrency limit can be configured globally
- Uses dagster.yaml configuration file for instance settings
- Environment variable DAGSTER_HOME defines configuration location
- If DAGSTER_HOME not set, uses temporary directory cleaned up on exit
- All processes and services should share single instance config file
- Some configuration like execution parallelism is set per-job rather than instance
  level
- Telemetry is enabled by default
- Default retention for skipped sensor ticks is 7 days, other ticks indefinite
- Asset checks are tests that verify specific properties of your data assets
- Each asset check should test only a single asset property to keep tests uncomplicated,
  reusable, and easy to track over time
- By default, all jobs targeting an asset will also run associated checks
- By default, if a parent's asset check fails during a run, the run will continue
  and downstream assets will be materialized
- To prevent downstream materialization on check failure, set the blocking argument
  to True in the @asset_check decorator
- Code locations are loaded in a different process and communicate with Dagster system
  processes over an RPC mechanism
- Definitions within a code location have a common namespace and must have unique
  names
- Only one Definitions object may be in a single code location
- Uses @dg.multi_asset_check decorator with specs and can_subset arguments to execute
  subset of checks
- AssetCheckExecutionContext.selected_asset_check_keys identifies which computations
  to run
- Setting can_subset parameter to True allows executing subset of asset checks
- For multi-assets, Dagster assumes all checks should execute after materialization
- AssetSpec skippable parameter must be True to allow asset to be skipped
- Use AssetExecutionContext.selected_asset_keys to identify which computations to
  run
- Each code location is loaded in its own process
- Dagster command-line tools look for workspace files in the current directory when
  invoked
- If a code location can't be loaded due to syntax error or other unrecoverable error,
  a warning message will display in the Dagster UI
- If a code location is renamed or its configuration is modified, you must stop and
  restart any running schedules or sensors in that code location
- Dagster Pipes provides mechanism for invoking code outside of Dagster while providing
  scheduling, reporting, and observability benefits
- External code is not required to know anything about Dagster and can be a process
  running different language on remote machine
- Only requirement is that external code can be triggered from Python
- Python client available as part of dagster-pipes package for sending logs and metadata
  back to Dagster
- dagster_cloud.yaml is used to define multiple code locations for Dagster+
- Similar to workspace.yaml in Dagster open source
- For Hybrid deployments, can be used to manage environment variables and secrets
- File should be placed in the root of your Dagster project
- If repository contains multiple Dagster projects in subdirectories, add the dagster_cloud.yaml
  file to the root of where the Dagster projects are stored
- If you are just getting started with Dagster, we strongly recommend you use assets
  rather than ops to build your data pipelines
- The computational core of an asset definition is an op
- Ops are sealed units of logic independent of execution strategy
- Ops can be written against abstract resources with resource definitions later bound
  at the job level
- Several Dagster features require a long-running dagster-daemon process
- Each daemon runs on a regular interval in its own threads
- Daemon process reads from Dagster instance file to determine which daemons should
  be included
- If daemon is configured to use workspace file, they will periodically reload the
  file without restart
- Uses concurrency pools to limit the number of in progress op executions across all
  runs
- Pool granularity can be set to 'run' or 'op' to control enforcement level
- Default limit for op execution within a run depends on which executor is used
- multiprocess_executor by default limits ops to multiprocessing.cpu_count() value
- DefaultRunCoordinator calls launch_run on the instance's run launcher immediately
  in the same process, without applying any limits or prioritization rules
- QueuedRunCoordinator sends runs to the Dagster daemon via a run queue and enables
  instance-level limits on run concurrency as well as custom run prioritization rules
- Uses GraphQL API for most operations
- Supports real-time updates via websockets
- Asset materializations can be tracked incrementally
- Partitioned assets support time-based incremental loading
- Uses OAuth2 with refresh token - requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Fixed an issue where `EnvVar`s used in Sling source and target configuration would
  not work properly in some circumstances
- Fixed an issue where default config values set to `EnvVar` did not work properly
- Fixed an issue where resources which implemented `IAttachDifferentObjectToOpContext`
  would pass the incorrect object to schedules and sensors
- Fixed a bug that caused auto-materialize failures when using the `materialize_on_cron`
  rule with dynamically partitioned assets
- Fixed an issue where sensor ticks would sporadically fail with a StopIteration exception
- Uses OAuth2 with refresh token - requires setup of connected app in dagster
- Supports Pydantic 1.10 and Pydantic 2 models for Config and Resources
- Auto-materialize logic skips materializing when backfill is in progress or parent
  partitions are required but nonexistent
- Dramatically improved performance of partition-mapping for basic hourly and daily
  partitions definitions
- Global op concurrency slots are released between retries of op execution failures
- Uses OAuth2 with refresh token - requires setup of connected app
- Asset reconciliation sensor is now 100x more performant
- Dynamic partitions definitions allow partitioning assets dynamically
- Freshness policies can now be set on time-partitioned assets
- New dagster dev command runs both Dagit and Dagster daemon in same process
- Dagit has been renamed to Dagster UI
- Support for Python 3.7 has been dropped
- Default config values now automatically populate in the launchpad
- Asset backfills can now be canceled
- Auto-materialize policies provide automatic asset execution
- Uses OAuth2 with refresh token flow for authentication
- GraphQL requests may timeout with 504 errors
- Asset backfills can be canceled via UI button
- Supports streaming upload of compute logs to Dagster Cloud
- Long-running runs can now be terminated after going over a set runtime
- Performance improvement to partition status caching for multi-partitioned assets
  containing a time dimension
- Resources defined on Definitions are now automatically bound to jobs by default
- Default kubernetes namespace for run pods when using Dagster Helm chart with CeleryK8sRunLauncher
  is now the same namespace as the Helm chart
- SnowflakePandasIOManager now ensures timestamp data has timezone and stores as timestamps
  rather than strings
- MultiPartitionsDefinition is no longer marked experimental
- Improved performance of asset reconciliation sensor when non-partitioned asset depends
  on partitioned asset
- The minimum grpcio version supported by Dagster has been increased to 1.44.0 so
  that Dagster can support both protobuf 3 and protobuf 4
- The minimum protobuf version supported by Dagster has been increased to 3.20.0
- Prior to 0.9.19, asset keys were serialized in a legacy format. This release removes
  support for querying asset events serialized with this legacy format
- The run queue is now enabled by default in the Dagster Helm chart
- Run queue parallelism - by default, up to 4 runs can now be pulled off of the queue
  at a time
- Run retries are now enabled by default - runs will retry if they have the dagster/max_retries
  tag set
- Schedule and sensor parallelism - by default, the daemon will now run up to 4 sensors
  and up to 4 schedules in parallel
- Run monitoring is enabled by default - Dagster will detect hanging runs and move
  them into a FAILURE state
- The Databricks token that is passed to the databricks_client resource must now begin
  with https://
- Database I/O managers (Snowflake, DuckDB) now support static partitions, multi-partitions,
  and dynamic partitions
- Assets with time-window PartitionsDefinitions may now have a FreshnessPolicy
- Made performance improvements for job run queries which can be applied by running
  dagster instance migrate
- System tags (code + logical versions) are now shown in the asset sidebar and on
  the asset details page
- Source assets that have never been observed are presented more clearly on the asset
  graph
- The number of materialized and missing partitions are shown on the asset graph and
  in the asset catalog for partitioned assets
- Databricks-backed assets are now shown on the asset graph with a small Databricks
  logo
- Software-defined assets are now marked fully stable and ready for prime time
- Assets can be organized into groups by providing a group_name
- Asset definitions can be created from graphs via AssetsDefinition.from_graph
- Ops that return Outputs and DynamicOutputs now work well with Python type annotations
- Automatic re-execution of runs from failure is now supported
- Partitions and backfills pages have been redesigned for better performance
- Context was made an optional argument on the function decorated by @solid in 0.11.9
- Dagster now has a BoolMetadataValue representing boolean-type metadata
- Tags on schedules can now be expressed as nested JSON dictionaries
- FileCache is marked for deprecation in 0.15.0
- Calls to instance.get_event_records without an event type filter is deprecated and
  will generate a warning
- Assets can be materialized to storage conditionally by setting output_required=False
- Import time for the dagster module has been reduced by approximately 50%
- DAGSTER_GRPC_TIMEOUT_SECONDS environment variable allows overriding default timeout
- x-frame-options response header removed from Dagit allowing UI to be rendered in
  iframe
- As of 1.0.0, Dagster no longer guarantees support for python 3.6
- Python GraphQL client includes shutdown_repository_location API call
- Dagster libraries are pinned to same version as core dagster package
- Can be run in --read-only mode to disable mutations
- Supports websocket connections with graceful degradation
- The runConfigData argument to the launchRun GraphQL mutation can now be either a
  JSON-serialized string or a JSON object
- When running your own gRPC server, jobs that launch in a container will now default
  to using dagster as the entry point
- The scheduler will now check every 5 seconds for new schedules to run, instead of
  every 30 seconds
- Components are now stable and recommended for new Dagster projects
- dg CLI provides scaffolding, local iteration, execution, and static analysis
- create-dagster supersedes dagster project scaffold with modern src/ + defs/ layout
- Retry from asset failure with multi-assets allows rerunning only failed assets
- FreshnessPolicy API is under active development and will supersede freshness checks
- Unified asset selection syntax combines lineage traversal, attribute filters, and
  boolean logic
- Uses gRPC communication with compression and configurable message limits
- DAGSTER_GRPC_MAX_RX_BYTES environment variable can override message receive limit
- Pipeline runs executed in ephemeral Kubernetes jobs - logs not retrievable after
  cleanup
- ECS eventual consistency model requires exponential backoff for requests
- Memoization now available from all execution entrypoints
- Default byte size limit for gRPC requests and responses is 100MB, adjustable via
  DAGSTER_GRPC_MAX_RX_BYTES and DAGSTER_GRPC_MAX_SEND_BYTES environment variables
- Runs launched from UI get dagster/from_ui = true tag for filtering
- PipesK8sClient reconnects to log stream every 3600 seconds by default, configurable
  via DAGSTER_PIPES_K8S_CONSUME_POD_LOGS_REQUEST_TIMEOUT
- PipesK8sClient retries up to 5 times on log streaming errors, configurable via DAGSTER_PIPES_K8S_CONSUME_POD_LOGS_RETRIES
- BackfillPolicy is now marked as generally available (GA)
- Uses OAuth2 with refresh token for Dagster Plus deployments
- Supports both REST and GraphQL endpoints depending on the operation
- Database table used by DbIOManager is configurable via table output metadata key
- Global op concurrency is now enabled on the default SQLite storage
- Deployments that have not been migrated since 1.6.0 may need to run dagster instance
  migrate
- Manual and automatic retries of runs launched by backfills are incorporated into
  backfill status
- Invalid cron strings like '0 0 30 2 *' now raise exceptions when loaded
- dagster no longer supports Python 3.8, which hit EOL on 2024-10-07
- dagster now requires pydantic>=2
- AssetExecutionContext is no longer a subclass of OpExecutionContext
- AssetKey objects can no longer be iterated over or indexed
- The dagster/relation_identifier metadata key has been renamed to dagster/table_name
- Uses OAuth2 with refresh token for some integrations
- Some objects may return nulls in deeply nested fields
- Multi-asset sensor definition updated to be less likely to timeout queries against
  asset history storage
- When different assets have different PartitionsDefinitions, there will be one implicit
  asset job with all assets instead of separate jobs
- Run monitoring will detect runs stuck in CANCELING state and move them to CANCELED
- Performance improvements for time-partitioned run requests
- Zero-downtime agent updates can be configured for ECS agent
- Performance improvements when rendering the asset graph while runs are in progress
- Asset search results now display compute and storage kind icons
- Custom asset tags can be applied to Sling assets via the DagsterSlingTranslator
- dlt assets now automatically have dagster/storage_kind tags attached
- Dagster will now display a storage kind tag on assets in the UI, similar to the
  existing compute kind
- experimental integration for building and integration dlt ingestion pipelines with
  Dagster
- Dagster uses GraphQL API for data access
- Authentication may be required for Dagster Plus deployments
- API structure depends on specific Dagster instance configuration
- Auto-materialize history tracks why assets were or were not materialized according
  to their AutoMaterializePolicy
- Op concurrency feature allows limiting the number of concurrently executing ops
  across runs
- Default config values are automatically populated in the launchpad
- Python 3.10 is now used in Docker images instead of 3.7
- Pandas 2.x is now supported in all dagster packages
- Uses GraphQL Python client with submit_job_execution accepting RunConfig objects
- Asset backfills can be canceled via UI button to terminate unfinished runs
- Compatible with croniter>=1.4.0, users of earlier versions should pin croniter<1.4
- Dagster pipes is a new library that implements a protocol for launching compute
  into external execution environments and consuming streaming logs and Dagster metadata
  from those environments
- Out-the-box integrations include clients for local subprocess, Docker containers,
  Kubernetes, and Databricks
- Transport options include Unix pipes, Filesystem, s3, dbfs
- Currently supports Python language
- Dagster pipes is composable with existing launching infrastructure via open_pipes_session
- One can augment existing invocations rather than replacing them wholesale
- GraphQL Python client includes a default timeout of 300 seconds for each query
- Auto-materialize evaluation history stored for 1 month instead of 1 week
- Minimum grpcio version supported is 1.44.0 for protobuf 3 and 4 compatibility
- Minimum protobuf version supported is 3.20.0
- Legacy asset keys serialized before 0.9.19 are no longer supported
- Several Kubernetes Helm chart features now enabled by default including run queue,
  run retries, and run monitoring
- When using load_assets_from_dbt_project or load_assets_from_dbt_manifest with dbt-core>=1.4,
  AssetMaterialization events will be emitted as the dbt command executes
- The dagster-airflow library has been moved to 1.x.x to denote the stability of its
  apis going forward
- Large asset graphs can now be materialized in Dagit without needing to first enter
  an asset subset
- Added a pin of the sqlalchemy package to <2.0.0 due to a breaking change
- Added a pin of the dbt-core package to <1.4.0 due to breaking changes
- Added a pin of the jupyter-client package to <8.0 due to hangs while running dagstermill
  ops
- Jobs defined via define_asset_job now auto-infer their partitions definitions if
  not explicitly defined
- Observable source assets can now be run as part of a job via define_asset_job
- Dynamic partitions definitions can now exist as dimensions of multi-partitions definitions
- Asset partitions can now depend on earlier time partitions of the same asset
- Dagster now allows backfills that target assets with different partitions
- Assets can now be materialized to storage conditionally by setting output_required=False
- The environment variable DAGSTER_GRPC_TIMEOUT_SECONDS allows for overriding the
  default timeout for communications
- Dagster no longer guarantees support for python 3.6 as of 1.0.0
- Integration libraries remain on pre-1.0 (0.16.x) versioning track for API maturity
- MySQL storage implementations for Dagster storage is no longer marked as experimental
- dagster-snowflake has dropped support for python 3.6
- Daemon heartbeats are now processed in a batch request to the database
- Non-standard vixie-style cron strings supported like @hourly, @daily, @weekly, and
  @monthly
- dagster-k8s, dagster-celery-k8s, and dagster-docker now name step workers dagster-step-...
  rather than dagster-job-...
- EcsRunLauncher now attempts to reuse task definitions instead of registering a new
  task definition for every run
- dagster, dagit, and all extension libraries now contain py.typed files for static
  type checking
- Repositories can now contain asset definitions and source assets for the same asset
  key
- Asset graphs are now rendered asynchronously to prevent Dagit freezing on large
  graphs
- Default IO Managers now support asset partitions
- When an exception is wrapped by another exception and raised within an op, Dagit
  will now display the full chain of exceptions
- Made performance improvements to the loading of the partitions and backfill pages
- The Global Asset Graph is back by popular demand, and can be reached via a new "View
  global asset lineage" link on asset group and asset catalog pages
- Simplified the Content Security Policy and removed frame-ancestors restriction
- When you supply a runtime_metadata_fn when loading dbt assets, this metadata is
  added to the default metadata that dagster-dbt generates, rather than replacing
  it entirely
- When you load dbt assets with use_build_command=True, seeds and snapshots will now
  be represented as Dagster assets
- Uses asset_selection parameter on @sensor and SensorDefinition to target assets
  instead of jobs
- materialize and materialize_to_memory accept raise_on_error argument
- Multi-dimensional asset partitions supported through MultiPartitionsDefinition (experimental)
- Asset partition ranges can be launched using dagster/asset_partition_range_start
  and dagster/asset_partition_range_end tags
- Integration logos shown in Dagit for assets backed by notebooks, DBT, Airbyte
- DuckDB integration improvements with PySpark DataFrame support
- Fivetran load_assets_from_fivetran_instance helper (experimental)
- Asset reconciliation sensor behavior changed to focus on reconciliation
- NotebookMetadataValue can report executed jupyter notebook locations
- UPathIOManager provides filesystem-agnostic IOManager base class
- Threadpool option available for scheduler daemon
- define_dagstermill_asset loads notebooks as assets
- make_dagster_job_from_airflow_dag supports Airflow 2
- Asset inputs resolve to upstream assets in same group with same name
- Executors compress step worker arguments to avoid CLI length limits
- Run timeline shows future schedule ticks for visible time window
- Asset graph views refresh as materialization events arrive
- Python Log Level configuration applies to system logs during runs
- Database indexes added for performance improvements
- Tags can be provided to asset reconciliation sensor
- Output table schema metadata generated at asset definition time for Airbyte
- EcsRunLauncher allows dictionary configuration for task definition
- AssetMaterialization has metadata property for accessing metadata as dictionary
- DagsterInstance has get_latest_materialization_event method
- InMemoryIOManager added to public API
- TableSchema has static constructor for building from column dictionary
- dagster run migrate-repository CLI command for preserving run history
- DagsterCloudOperator and DagsterOperator support Airflow 2
- DuckDB integration for storing outputs in in-process database
- Default local code import timeout increased from 60 to 180 seconds
- Uses OAuth2 with refresh token — requires setup of connected app in api
- All assets where a group_name is not provided are now part of a group called default
- The group_name parameter value for @asset is now restricted to only allow letters,
  numbers and underscore
- FileCache is now marked for deprecation in 0.15.0
- fs_asset_io_manager has been removed in favor of merging its functionality with
  fs_io_manager
- Dagstermill is now compatible with current versions of papermill (2.x)
- The network key on DockerRunLauncher config can now be sourced from an environment
  variable
- Dagit may now be run in --read-only mode
- Failed backfills may now be resumed in Dagit
- Databricks step launcher includes max_completion_wait_time_seconds configuration
  option
- Run status sensors can now be invoked in unit tests
- Fixed issue with default_value on inputs
- Fixed pyspark_step_launcher log record re-emitting issue
- Asset catalog now displays materialized assets when subset of repositories selected
- Fixed databricks step launcher invariant check for local_dagster_job_package_path
- Sensor tick timeline shows cursor values in tooltip
- Pinned markupsafe dependency for Jinja2 compatibility
- Sensors with default status can now be manually started
- Dagster is a data orchestrator for machine learning, analytics, and ETL
- Version 0.13.x includes transition from pipeline/solid to job/op terminology
- Asset materialization events can be wiped/deleted
- Run page performance improvements in recent versions
- Op selection now supports selecting ops inside subgraphs using dot notation
- The dagster-daemon process creates fewer log entries when no actions are taken
- Run status/failure sensor invocation is not yet supported
- Uses OAuth2 with refresh token requires setup of connected app
- Queries and Mutations now use HTTP instead of a websocket-based connection
- dagster-graphql --remote now sends the query and variables as post body data, avoiding
  uri length limit issues
- Fixed limitation on gRPC message size when evaluating run requests for sensors,
  schedules, and backfills
- dagster-daemon process uses fewer resources and spins up fewer subprocesses to load
  pipeline information
- Tags applied to solid invocations using the tag method on solid invocations are
  now correctly propagated during execution
- MySQL is now supported as a backend for event log, run, & schedule storages
- BoolSource config type added (similar to StringSource type)
- QueuedRunCoordinator daemon is now more resilient to errors while dequeuing runs
- dagster-daemon process now runs faster when running multiple schedulers or sensors
  from the same repository
- Sensor minimum intervals now interact in a more compatible way with sensor daemon
  intervals
- fs_io_manager now defaults the base directory to base_dir via the Dagster instance's
  local_artifact_storage configuration
- Dagster is a data orchestration platform, not a traditional REST API
- Uses GraphQL for programmatic access via startPipelineExecution mutations
- Requires dagster instance migrate for version upgrades
- Historical execution plans viewable even if pipeline structure changes
- Uses GraphQL API for pipeline execution and management
- Scheduler configuration moved from @schedules decorator to DagsterInstance
- 'Config system renamed: Dict to Shape, List to Array, Optional to Noneable'
- All solids, types, and config functions must explicitly list required_resource_keys
- SQLite run and event log storages are more robust to concurrent use
- Performance improvements in Dagit when working with runs with large configurations
- DagsterGraphQLClient now takes optional transport argument for authentication
- AssetExecutionContext is now a subclass of OpExecutionContext
- Failed backfills may be resumed by putting them back into requested state
- Asset checks are experimental feature for defining and monitoring data quality
- Auto materialize customization available through AutoMaterializePolicies
- Uses GraphQL API for all operations
- EnvVars can be used for configuration values
- Supports OAuth2 authentication flows
- Asset materializations can be reported without runs
- Supports multiple partition types including dynamic partitions
- Asset checks can be defined with check_specs
- Auto-materialize policies support cron-based rules
- Uses OAuth2 with refresh token flow for connected apps
- GraphQL Explorer replaced with GraphiQL in version 1.8.1
- Supports filtering logs by metadata keys and values
- Timeline can be grouped by job or automation
- Asset catalog displays row count and relation identifier metadata
- Uses OAuth2 with refresh token – requires setup of connected app in api
- I/O managers are now optional for assets with None or MaterializeResult return type
  annotations
- Asset checks are now considered generally available
- Per-run Insights are now available on individual assets
- Asset metadata is now included in the insights metrics for jobs materializing those
  assets
- Python 3.12 officially supported
- Pendulum 3.0 support for Python 3.9+
- Maximum of 25K partitions per asset recommended
- dbt-core 1.4.* support removed due to end-of-life
- Asset lineage graph UI revamp with left-to-right flow and expandable/collapsible
  asset groups
- Dark mode support matching system theme with user settings override
- Report asset materializations from UI without executing code
- MaterializeResult, AssetSpec, and AssetDep APIs now stable
- Backfill previews show partitions for each asset
- Default op/asset concurrency limits configurable at deployment level
- Zero-value op/asset concurrency limits now supported
- Nothing-typed outputs no longer call I/O manager handle_output function
- Add the data load tool (dlt) integration for easily building and integration dlt
  ingestion pipelines with Dagster
- experimental integration for dlt
- Asset materializations can now be reported from the dropdown menu in the asset list
  view
- DbtProject is adopted and no longer experimental
- Asset backfills will now retry if there is an unexpected exception raised in the
  middle of the backfill
- Performance improvements to loading the asset overview tab
- Performance improvements for rendering gantt charts with 1000's of ops/steps
- The Dagster UI now loads events in batches of 1000 in the run log viewer, instead
  of batches of 10000
- Made performance improvements in both calculating and storing data version for assets,
  especially for assets with a large fan-in
- Standardized table row count metadata output by various integrations to dagster/row_count
- Fixed issue that could cause runs to fail if they targeted any assets which had
  a metadata value of type TableMetadataValue, TableSchemaMetadataValue, or TableColumnLineageMetadataValue
  defined
- Fixed an issue which could cause evaluations produced via the Auto-materialize system
  to not render the skip-type rules
- Backfills of asset jobs now correctly use the BackfillPolicy of the underlying assets
  in the job
- AutomationConditionSensorDefinitions will now emit backfills by default
- Deployments that have not been migrated since 1.6.0 may need to run dagster instance
  migrate to enable
- Invalid cron strings like '0 0 30 2 *' that represented invalid dates in February
  are no longer allowed
- Manual retries of runs launched by backfills are no longer considered part of the
  backfill if the backfill is complete when the retry is launched
- Supports OAuth2 with refresh token authentication
- Uses cursor-based pagination for data retrieval
- Includes automation condition evaluations and asset lineage navigation
- Supports multi-asset execution and retry policies
- The dagster-dlt library replaces the dlt module of dagster-embedded-elt
- Added pool support for dlt integrations
- Uses OAuth2 with refresh token for external integrations like Airbyte and dbt Cloud
- Multi-dimensional asset partitions now supported through MultiPartitionsDefinition
- Default LocalComputeLogManager captures compute logs by process instead of by step
- New Overview and Workspace pages enabled for all users with design updates and virtualized
  tables
- This appears to be changelog/release notes documentation for Dagster, not API configuration
  documentation
- Contains version history from 1.4.11 down to 1.4.0 with bugfixes, new features,
  and community contributions
- Includes experimental features like auto-materialize policies and asset checks
- 'Multiple library integrations mentioned: dagster-dbt, dagster-k8s, dagster-aws,
  dagster-azure, dagster-gcp, dagster-fivetran, dagster-airbyte, dagster-snowflake,
  dagster-wandb'
- Performance improvements for Asset Daemon with thousands of assets
- Auto-materialize policies replace the asset reconciliation sensor
- Asset backfill page - A new page in the UI for monitoring asset backfills
- Clearer labels for tracking changes to data and code - Instead of the opaque "stale"
  indicator, Dagster's UI now indicates whether code, upstream data, or dependencies
  have changed
- Auto-materialization and observable source assets - Assets downstream of an observable
  source asset now use the source asset observations to determine whether upstream
  data has changed
- Pythonic Config and Resources - The set of APIs introduced in 1.2 is no longer experimental
- Asset groups are now included in global search
- Assets in the asset catalog have richer status information that matches what is
  displayed on the asset graph
- Dagster code servers now wait to shut down until any calls that they are running
  have finished
- The dagster execute job cli now accepts --op-selection
- Option (Alt) + R now reloads all code locations (OSS only)
- New @partitioned_config decorator has been added for defined configuration for partitioned
  jobs
- ConfigurablePickledObjectS3IOManager has been renamed S3PickleIOManager for simplicity
- ConfigurablePickledObjectADLS2IOManager has been renamed ADLS2PickleIOManager for
  simplicity
- ConfigurablePickledObjectGCSIOManager has been renamed GCSPickleIOManager for simplicity
- DagsterGraphQLClient now includes a default timeout of 300 seconds for each query
- This is a changelog document for Dagster, not API documentation
- Contains version history and feature updates
- No specific API endpoints or authentication methods documented
- Multi-dimensional asset partitions support requires optional schema migration via
  'dagster instance migrate'
- Threadpool option available for scheduler daemon via dagster.yaml configuration
- Database indexes added to improve run view and asset query performance
- Launching a backfill of a non-subsettable multi-asset without including every asset
  will now raise a clear error at backfill submission time
- Passing an empty list to the assetKeys argument of the assetsOrError field in the
  GraphQL API would return every asset instead of an empty list of assets
- Large error stack traces from Dagster events will be automatically truncated if
  the message or stack trace exceeds 500kb
- Uses GraphQL subscriptions for LocationStateChange
- Environment variables from .env files are automatically loaded when running dagit
  or dagster-daemon locally
- Database migration required for multi-dimensional asset partitions and asset versioning
- 'URL path prefixes changed: /instance removed, /workspace changed to /locations'
- Uses OAuth2 with refresh token flow for external integrations
- Some integrations like databricks require compression for large event volumes
- Version 1.0 removes legacy pipeline and solid APIs
- Integration libraries remain on 0.16.x versioning track
- Added a new dagster dev command that can be used to run both Dagit and the Dagster
  daemon in the same process during local development
- The grpcio pin in Dagster to <1.48.1 has been restored for Python versions 3.10
  and 3.11, due to upstream issues in the grpcio package causing hangs in Dagster
- Previously, if a description was provided for an op that backed a multi-asset, the
  op's description would override the descriptions in Dagit for the individual assets.
  This has been fixed
- Sometimes, when applying an input_manager_key to an asset's input, incorrect resource
  config could be used when loading that input. This has been fixed
- Previously, the backfill page errored when partitions definitions changed for assets
  that had been backfilled. This has been fixed
- When displaying materialized partitions for multipartitioned assets, Dagit would
  error if a dimension had zero partitions. This has been fixed
- dagster-dlt library replaces the dlt module of dagster-embedded-elt
- DagsterDltTranslator.get_* methods have been superseded in favor of DagsterDltTranslator.get_asset_spec
- Pipes execution errors are no longer treated as framework errors
- Fixed an issue where the tick timeline wouldn't load for an automation condition
  sensor that emitted a backfill
- Fixed a bug with asset checks where additional_deps/additional_ins were not being
  threaded through properly in certain cases
- Fixed a bug where the UI will hit an unexpected error when loading details for a
  run containing a step retry before the step has started
- Fixed a bug with load_assets_from_x functions where we began erroring when a spec
  and AssetsDefinition had the same key in a given module
- Fixed a bug with load_assets_from_modules where AssetSpec objects were being given
  the key_prefix instead of the source_key_prefix
- Fixed a bug with the run queue criteria UI for branch deployments in Dagster Plus
- dagster-embedded-elt library is deprecated in favor of dagster-dlt and dagster-sling
- Added new function load_definitions_from_module which can load all the assets, checks,
  schedules, sensors, and job objects within a module scope
- Previously, asset backfills could only target selections of assets in which all
  assets had a BackfillPolicy, or none of them did. Mixed selections are now supported
- AssetSpecs may now contain a partitions_def
- Added the option to use a thread pool to process backfills in parallel
- Exceptions that are raised when a schedule or sensor is writing to logs will now
  write an error message to stdout instead of failing the tick
- Dagster officially supports Python 3.12
- Added support for the 3.0 release of the pendulum library for Python versions 3.9
  and higher
- Performance improvements when starting run worker processes or step worker processes
  for runs in code locations with a large number of jobs
- Support for dbt-core==1.4.* is now removed because the version has reached end-of-life
- Uses GraphQL API for asset management and metadata
- Support for dbt integration with BigQuery insights
- Asset health dashboard available
- Backfill daemon for asset materialization
- Hacker News API is 1-indexed, so adjust range by 1
- Community welcomes contributions to code and docs, issue and bug reports on GitHub
- Questions, feature requests, and discussion available in Slack
- Dagster core will forever be free and open source
- Commercial product Dagster+ operates as distinct layer on top of Dagster
- Core team will do best to respond to every message but can't guarantee response
  to everyone
- Don't treat community as personal customer support service
- This article only applies to Dagster Open Source (OSS) deployments
- Run launcher is configured as part of the Dagster instance
- DefaultRunLauncher spawns a new process per run on the same node as the job's code
  location
- Run launcher determines the behavior of the run worker, executor takes over management
  once execution starts
- Run queue operates as a first-in, first-out priority queue
- All runs have a priority of 0 by default
- Dagster launches runs with higher priority first
- Negative priorities are allowed and useful for de-prioritizing sets of runs like
  backfills
- Priority values must be integers specified as a string
- Run blocked by tag concurrency limits won't block runs submitted after it
- The GraphQL API is still evolving and is subject to breaking changes
- A large portion of the API is primarily for internal use by the Dagster webserver
- The webserver serves the GraphQL endpoint at the /graphql endpoint
- GraphQL playground available by navigating to the /graphql route in browser
errors:
- 'Cannot have more than one Definitions object defined at module scope: Only one
  Definitions object may be in a single code location'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'StopIteration exception: Sensor ticks sporadically failing'
- '504 Gateway Timeout: GraphQL requests may timeout'
- '409 Conflict: k8s_job_executor may fail when retrying pod creation'
- 'RuntimeError: dictionary changed size during iteration during code server reloads'
- 'REQUEST_LIMIT_EXCEEDED: Performance improvements available for runs that materialize
  large numbers of assets'
- 'GraphQL resolution error: Fixed when retrieving metadata for step failures in event
  log'
- 'JSON deserialization errors: Fixed when Op or Asset emitted JSON that doesn''t
  represent a DagsterEvent'
- 'REQUEST_LIMIT_EXCEEDED: Fixed an issue where pods launched by the k8s_job_executor
  would sometimes unexpectedly fail due to transient 401 errors in certain kubernetes
  clusters'
- 'QUERY_TIMEOUT: Fixed an issue with nth-weekday-of-the-month handling in cron schedules'
- GraphQL error when materializing graph-backed assets
- Duplicate definition found error when schedule targets partitioned asset job
- 'InvalidSubsetError: Launching an asset materialization run with source assets would
  error'
- 'DagsterExecutionStepNotFoundError: Re-executing an entire set of dynamic steps
  together with their upstream step'
- 'DagsterInstanceSchemaOutdated: Database operational exceptions were masked if instance
  storage was not up to date'
- 'RESOURCE_DOES_NOT_EXIST: Check if external step failed to start for root error'
- '500: GraphQL requests that encountered unexpected server error'
- 'REQUEST_LIMIT_EXCEEDED: Reduce API call frequency'
- 'QUERY_TIMEOUT: Break down queries or add selectivity'
- 'PIPELINE_INIT_FAILURE: Now produces PIPELINE_FAILURE event'
- 'gRPC message limit errors: Configure DAGSTER_GRPC_MAX_RX_BYTES environment variable'
- 'Config errors: Check executor config and ops vs solids config entries'
- 'REQUEST_LIMIT_EXCEEDED: ECS tasks may need retry with exponential backoff'
- 'Error: Got unexpected extra arguments: ECS Task Definition entrypoint conflicts
  with command override'
- 'gRPC error: May occur when loading several code locations at once'
- 'DagsterExecutionStepNotFoundError: occurs when trying to execute asset check step
  of run launched by backfill'
- 'InvalidSubsetError: race condition when adding new asset to graph with freshness
  check sensor'
- 'ThrottlingException: PipesCloudWatchMessageReader correctly identifies streams
  not ready'
- 'object has no attribute ''_base_path'': Issue with partitioned assets in Serverless'
- '409 Conflict: Job already created during previous attempt'
- '504 Gateway Timeout: GraphQL requests timing out'
- '504 Gateway Timeout: GraphQL requests timing out, toaster message shown in UI'
- '409 Conflict: k8s_job_executor may fail when retrying Kubernetes pod creation'
- 'RuntimeError: dictionary changed size during iteration - occurs during code server
  reloads'
- 'DagsterInvariantViolationError: When executing multi-asset with self-dependencies'
- 'RecursionError: During execution with parallel runs materializing same asset'
- 'REQUEST_LIMIT_EXCEEDED: API throttling - reduce call frequency'
- 'RESOURCE_DOES_NOT_EXIST: Check external step configuration'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency for Airbyte responses
  with 204 status code'
- 'QUERY_TIMEOUT: Break down filters or add selectivity for storage queries'
- 'sqlite3.OperationalError: Certain workspace or repo-scoped pages relied on versions
  of sqlite to be 3.25.0 or greater'
- 'sqlite3.OperationalError: affecting dagit instances using default SQLite schedule
  storage with SQLite version < 3.25.0'
- 'REQUEST_LIMIT_EXCEEDED: related to API call throttling'
- 'JSON deserialization error: when using Dagster resources that write to stdout with
  celery_docker_executor'
- 'WebWorker error: for users running Dagit with --path-prefix on large DAGs'
- Duplicate definition found error when a schedule definition targets a partitioned
  asset job
- DockerRunLauncher would sometimes use Dagit's Python environment as the entrypoint
  even if that environment did not exist in the container
- GraphQL error when graph inputs were type-annotated on graph-backed assets
- GraphQL error when attempting to materialize graph-backed assets
- Partitions could not be selected when materializing partitioned assets with associated
  resources
- GraphQL error when snapshot link fails to load
- 'StatusCode.RESOURCE_EXHAUSTED: Large number of run requests with large configs
  can cause gRPC errors'
- 'REQUEST_LIMIT_EXCEEDED: Performance improvements needed for large configurations'
- 'QUERY_TIMEOUT: Break down filters or add selectivity for large DAGs'
- 'GraphQL errors: Better handling of GraphQL errors and backend programming errors'
- 'REQUEST_LIMIT_EXCEEDED: API calls may be throttled'
- 'QUERY_TIMEOUT: Break down complex queries or add filters'
- 'StopIteration: Sensor ticks may fail sporadically'
- object has no attribute '_base_path'
- Asset must be part of at least one job
- DagsterInvalidSubsetError when trying to launch runs with multiple partitions definitions
  with same start date but differing end dates
- '413: Request Entity Too Large error when uploading heartbeat to Dagster Plus servers
  with many branch deployments'
- Duplicate check specs errors with singular tests ingested as asset checks
- 'DagsterInvariantViolationError: when executing a multi-asset where both assets
  have self-dependencies on earlier partitions'
- 'RecursionError: when processing asset graphs with long upstream dependency chains'
- 'Cannot specify resource requirements: when including a resource in both a schedule
  and a job'
- 'sqlite3.ProgrammingError: Error when creating ephemeral DagsterInstance'
- 'REQUEST_LIMIT_EXCEEDED: Performance issues with tag retrieval in run filter'
- 'QUERY_TIMEOUT: Issues with run queries on mysql versions < 8.0.31'
- 'RESOURCE_DOES_NOT_EXIST: Check databricks cluster configuration'
- 'DagsterDbtCliFatalRuntimeError: Check dbt logs for additional context'
- 'GraphQL errors: Repository loading issues with asset metadata'
- 'REQUEST_LIMIT_EXCEEDED: dagster-k8s Fixed an issue where setting runK8sConfig in
  the Dagster Helm chart would not pass configuration through to pods launched using
  the k8s_job_executor'
- 'DUPLICATE_NAMES: dagster-k8s Previously, using the execute_k8s_job op downstream
  of a dynamic output would result in k8s jobs with duplicate names being created.
  This has been fixed'
- 'SCHEMA_NOT_EXISTS: dagster-snowflake Previously, if the schema for storing outputs
  didn''t exist, the Snowflake I/O manager would fail. Now it creates the schema'
- gRPC error when loading several code locations at once
auth_info:
  mentioned_objects:
  - DuckDBResource
  - AutomationCondition
  - automation_condition_sensor
  - default_automation_condition_sensor
  - ExecutorDefinition
  - JobDefinition
  - GraphDefinition
  - Definitions
  - pyproject.toml
  - workspace.yaml
  - AssetCheckExecutionContext
  - AssetExecutionContext
  - AssetCheckKey
  - AssetKey
  - PipesContext
  - PipesClient
  - OauthToken
  - AuthProvider
  - NamedCredential
  - DagsterInstance
  - HookContext
  - OpExecutionContext
  - ScheduleEvaluationContext
  - DagsterGraphQLClient
  - RequestsHTTPTransport
  - DagsterRun
  - SensorReturnTypesUnion
  - AssetSpec
  - AssetsDefinition
  - ScheduleDefinition
  - SensorDefinition
  - secrets
  - client_id
  - client_secret
  - refresh_token
  - EnvVar
  - agent_token
  - service_account
  - AutoMaterializePolicy
  - AssetDefinition
  - SourceAsset
  - InitResourceContext
client:
  base_url: http://localhost:3000
source_metadata: null
