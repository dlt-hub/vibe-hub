resources:
- name: health_check
  endpoint:
    path: /dbos-healthz
    method: GET
    data_selector: null
    params: {}
- name: workflow_recovery
  endpoint:
    path: /dbos-workflow-recovery
    method: POST
    data_selector: null
    params: {}
- name: deactivate
  endpoint:
    path: /deactivate
    method: GET
    data_selector: null
    params: {}
- name: dbos_workflow
  endpoint:
    path: /
    method: GET
    data_selector: results
    params: {}
- name: send_message
  endpoint:
    path: /send
    method: POST
    data_selector: null
    params: {}
- name: receive_message
  endpoint:
    path: /recv
    method: GET
    data_selector: null
    params: {}
- name: checkout
  endpoint:
    path: /checkout/{idempotency_key}
    method: POST
    data_selector: payment_id
    params: {}
- name: checkout
  endpoint:
    path: /checkout/{idempotency_key}
    method: POST
    data_selector: payment_id
    params: {}
- name: list_workflows
  endpoint:
    path: /list_workflows
    method: GET
    data_selector: WorkflowStatus
    params: {}
- name: list_queued_workflows
  endpoint:
    path: /list_queued_workflows
    method: GET
    data_selector: WorkflowStatus
    params: {}
- name: list_workflow_steps
  endpoint:
    path: /list_workflow_steps/{workflow_id}
    method: GET
    data_selector: StepInfo
    params: {}
- name: checkout
  endpoint:
    path: /checkout/{idempotency_key}
    method: POST
    data_selector: payment_id
    params: {}
- name: cancel_workflow
  endpoint:
    path: /cancel_workflow
    method: POST
    data_selector: None
    params: {}
- name: resume_workflow
  endpoint:
    path: /resume_workflow
    method: POST
    data_selector: None
    params: {}
- name: fork_workflow
  endpoint:
    path: /fork_workflow
    method: POST
    data_selector: None
    params: {}
- name: workflow_status
  endpoint:
    path: /workflow_status
    method: GET
    data_selector: None
    params: {}
- name: workflow_status
  endpoint:
    path: /workflow/status
    method: GET
    data_selector: status
    params: {}
- name: workflow
  endpoint:
    path: /workflows
    method: GET
    data_selector: workflows
    params: {}
- name: example_workflow
  endpoint:
    path: /example/workflow
    method: GET
    data_selector: records
- name: taskWorkflow
  endpoint:
    path: /tasks
    method: POST
    data_selector: results
    params: {}
- name: queueWorkflow
  endpoint:
    path: /queues
    method: POST
    data_selector: results
    params: {}
- name: workflow
  endpoint:
    path: /
    method: GET
    data_selector: result
    params: {}
- name: queue
  endpoint:
    path: /
    method: GET
    data_selector: result
    params: {}
- name: product
  endpoint:
    path: /product
    method: GET
    data_selector: product
    params: {}
- name: orders
  endpoint:
    path: /orders
    method: GET
    data_selector: orders
    params: {}
- name: order
  endpoint:
    path: /order/:id
    method: GET
    data_selector: order
    params: {}
- name: restock
  endpoint:
    path: /restock
    method: POST
    data_selector: restock
    params: {}
- name: checkout
  endpoint:
    path: /checkout/:idempotency_key
    method: POST
    data_selector: checkout
    params: {}
- name: payment_webhook
  endpoint:
    path: /payment_webhook/:payment_id/:payment_status
    method: POST
    data_selector: payment
    params: {}
- name: crash_application
  endpoint:
    path: /crash_application
    method: POST
    data_selector: crash
    params: {}
- name: stories
  endpoint:
    path: /api/v1/search
    method: GET
    data_selector: hits
- name: comments
  endpoint:
    path: /api/v1/search
    method: GET
    data_selector: hits
- name: index
  endpoint:
    path: /index
    method: POST
    data_selector: ''
    params: {}
- name: chat
  endpoint:
    path: /chat
    method: POST
    data_selector: ''
    params: {}
- name: history
  endpoint:
    path: /history
    method: GET
    data_selector: ''
    params: {}
- name: frontend
  endpoint:
    path: /
    method: GET
    data_selector: ''
    params: {}
- name: checkout
  endpoint:
    path: /checkout/{idempotency_key}
    method: POST
    data_selector: payment_id
    params: {}
- name: payment
  endpoint:
    path: /payment_webhook/{payment_id}/{payment_status}
    method: POST
    data_selector: order_url
    params: {}
- name: order
  endpoint:
    path: /order/{order_id}
    method: GET
    data_selector: order_details
    params: {}
- name: product
  endpoint:
    path: /product
    method: GET
    data_selector: product_details
    params: {}
- name: orders
  endpoint:
    path: /orders
    method: GET
    data_selector: orders_list
    params: {}
- name: restock
  endpoint:
    path: /restock
    method: POST
    data_selector: restock_status
    params: {}
- name: frontend
  endpoint:
    path: /
    method: GET
    data_selector: html_response
    params: {}
- name: crash_application
  endpoint:
    path: /crash_application
    method: POST
    data_selector: crash_response
    params: {}
- name: chat
  endpoint:
    path: /chat
    method: POST
    data_selector: response_messages
- name: history
  endpoint:
    path: /history
    method: GET
    data_selector: message_list
- name: frontend
  endpoint:
    path: /
    method: GET
    data_selector: HTMLResponse
- name: start_transfer
  endpoint:
    path: /start_transfer
    method: POST
- name: cancel_transfer
  endpoint:
    path: /cancel/{transfer_id}
    method: POST
- name: transfer_status
  endpoint:
    path: /transfer_status/{transfer_id}
    method: GET
- name: crash_application
  endpoint:
    path: /crash_application
    method: POST
- name: email
  endpoint:
    path: /email
    method: POST
    data_selector: request
    params: {}
- name: frontend
  endpoint:
    path: /
    method: GET
    data_selector: html
    params: {}
- name: slack_message
  endpoint:
    path: /
    method: POST
- name: earthquake_data
  endpoint:
    path: /query
    method: GET
    data_selector: features
    params:
      format: geojson
      minmagnitude: 1.0
- name: comments
  endpoint:
    path: /api/v1/search
    method: GET
    data_selector: hits
    params:
      tags: comment
- name: checkout
  endpoint:
    path: /checkout/:key
    method: POST
- name: payment_webhook
  endpoint:
    path: /payment_webhook/:key/:status
    method: POST
- name: crash_application
  endpoint:
    path: /crash_application
    method: POST
- name: search_stories
  endpoint:
    path: /search
    method: GET
    data_selector: hits
    params:
      query: query
      hitsPerPage: maxResults
      tags: story
- name: get_comments
  endpoint:
    path: /search
    method: GET
    data_selector: hits
    params:
      tags: comment,story_{storyId}
      hitsPerPage: maxComments
- name: employee
  endpoint:
    path: /employee
    method: GET
    data_selector: employees
- name: alert_employee
  endpoint:
    path: /alert_employee
    method: GET
    data_selector: alerts
- name: example_queue
  endpoint:
    path: /example_queue
    method: POST
    data_selector: tasks
    params: {}
- name: pipeline_queue
  endpoint:
    path: /pipeline_queue
    method: POST
    data_selector: task
    params: {}
- name: workflows
  endpoint:
    path: /workflows
    method: GET
    data_selector: records
- name: workflows
  endpoint:
    path: /workflows
    method: GET
    data_selector: workflows
- name: workflow_steps
  endpoint:
    path: /workflows/steps
    method: GET
    data_selector: steps
- name: enqueued_workflows
  endpoint:
    path: /workflows/queued
    method: GET
    data_selector: queued_workflows
- name: cancel_workflow
  endpoint:
    path: /workflows/cancel
    method: POST
    data_selector: cancel
- name: resume_workflow
  endpoint:
    path: /workflows/resume
    method: POST
    data_selector: resume
- name: fork_workflow
  endpoint:
    path: /workflows/fork
    method: POST
    data_selector: fork
- name: kafka_messages
  endpoint:
    path: /kafka/messages
    method: POST
    data_selector: messages
- name: earthquake
  endpoint:
    path: /earthquake
    method: GET
    data_selector: earthquakes
- name: system_database
  endpoint:
    path: /config/system_database_url
    method: GET
- name: application_database
  endpoint:
    path: /config/application_database_url
    method: GET
- name: WorkflowHandle
  endpoint:
    path: /api/workflow/handle
    method: GET
    data_selector: records
    params: {}
- name: WorkflowHandleAsync
  endpoint:
    path: /api/workflow/handle/async
    method: GET
    data_selector: records
    params: {}
- name: resume_workflow
  endpoint:
    path: /resume_workflow
    method: POST
- name: resume_workflow_async
  endpoint:
    path: /resume_workflow_async
    method: POST
- name: fork_workflow
  endpoint:
    path: /fork_workflow
    method: POST
- name: fork_workflow_async
  endpoint:
    path: /fork_workflow_async
    method: POST
- name: debounce
  endpoint:
    path: /debounce
    method: POST
- name: db
  endpoint:
    path: /var/lib/postgresql/data
    method: POST
    data_selector: environment
    params:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: dbos
- name: app
  endpoint:
    path: /
    method: GET
    data_selector: application
    params: {}
- name: dbos-app
  endpoint:
    path: /
    method: GET
    data_selector: services
    params: {}
- name: postgres
  endpoint:
    path: /
    method: GET
    data_selector: services
    params: {}
- name: database_instance
  endpoint:
    path: /dbos-cloud/db
    method: POST
    data_selector: instances
    params: {}
- name: database_status
  endpoint:
    path: /dbos-cloud/db/status
    method: GET
    data_selector: status
    params: {}
- name: logs
  endpoint:
    path: /v1/logs
    method: POST
- name: traces
  endpoint:
    path: /v1/traces
    method: POST
- name: workflow_list
  endpoint:
    path: /workflow/list
    method: GET
- name: workflow_queue_list
  endpoint:
    path: /workflow/queue/list
    method: GET
- name: workflow_cancel
  endpoint:
    path: /workflow/cancel
    method: POST
- name: workflow_resume
  endpoint:
    path: /workflow/resume
    method: POST
- name: workflow_restart
  endpoint:
    path: /workflow/restart
    method: POST
- name: workflow
  endpoint:
    method: POST
    data_selector: workflow
- name: step
  endpoint:
    method: POST
    data_selector: step
- name: transaction
  endpoint:
    method: POST
    data_selector: transaction
- name: scheduled
  endpoint:
    method: POST
    data_selector: scheduled
- name: required_roles
  endpoint:
    method: POST
    data_selector: required_roles
- name: kafka_consumer
  endpoint:
    method: POST
    data_selector: kafka_consumer
- name: start_workflow
  endpoint:
    path: /start_workflow
    method: POST
    data_selector: workflow_handle
    params: {}
- name: start_workflow_async
  endpoint:
    path: /start_workflow_async
    method: POST
    data_selector: workflow_handle_async
    params: {}
- name: set_event
  endpoint:
    path: /set_event
    method: POST
    data_selector: event
    params: {}
- name: set_event_async
  endpoint:
    path: /set_event_async
    method: POST
    data_selector: event
    params: {}
- name: get_event
  endpoint:
    path: /get_event
    method: GET
    data_selector: event_value
    params: {}
- name: get_event_async
  endpoint:
    path: /get_event_async
    method: GET
    data_selector: event_value
    params: {}
- name: get_all_events
  endpoint:
    path: /get_all_events
    method: GET
    data_selector: events
    params: {}
- name: get_all_events_async
  endpoint:
    path: /get_all_events_async
    method: GET
    data_selector: events
    params: {}
- name: sleep
  endpoint:
    path: /sleep
    method: POST
    data_selector: sleep_duration
    params: {}
- name: sleep_async
  endpoint:
    path: /sleep_async
    method: POST
    data_selector: sleep_duration
    params: {}
- name: retrieve_workflow
  endpoint:
    path: /retrieve_workflow
    method: GET
    data_selector: workflow_handle
    params: {}
- name: retrieve_workflow_async
  endpoint:
    path: /retrieve_workflow_async
    method: GET
    data_selector: workflow_handle
    params: {}
- name: write_stream
  endpoint:
    path: /write_stream
    method: POST
    data_selector: stream_write
    params: {}
- name: write_stream_async
  endpoint:
    path: /write_stream_async
    method: POST
    data_selector: stream_write
    params: {}
- name: close_stream
  endpoint:
    path: /close_stream
    method: POST
    data_selector: stream_close
    params: {}
- name: close_stream_async
  endpoint:
    path: /close_stream_async
    method: POST
    data_selector: stream_close
    params: {}
- name: read_stream
  endpoint:
    path: /read_stream
    method: GET
    data_selector: stream_values
    params: {}
- name: read_stream_async
  endpoint:
    path: /read_stream_async
    method: GET
    data_selector: stream_values
    params: {}
- name: list_workflows
  endpoint:
    path: /list_workflows
    method: GET
    data_selector: workflow_status_list
    params: {}
- name: list_queued_workflows
  endpoint:
    path: /list_queued_workflows
    method: GET
    data_selector: queued_workflow_status_list
    params: {}
- name: list_workflow_steps
  endpoint:
    path: /list_workflow_steps
    method: GET
    data_selector: workflow_steps
    params: {}
- name: workflow_id
  endpoint:
    path: /workflow_id
    method: GET
- name: step_id
  endpoint:
    path: /step_id
    method: GET
- name: step_status
  endpoint:
    path: /step_status
    method: GET
- name: span
  endpoint:
    path: /span
    method: GET
- name: application_version
  endpoint:
    path: /application_version
    method: GET
- name: authenticated_user
  endpoint:
    path: /authenticated_user
    method: GET
- name: authenticated_roles
  endpoint:
    path: /authenticated_roles
    method: GET
- name: assumed_role
  endpoint:
    path: /assumed_role
    method: GET
- name: dbos-starter
  endpoint:
    path: /dbos/starter
    method: GET
    data_selector: records
- name: example_queue
  endpoint:
    path: /queues/example_queue
    method: POST
    data_selector: workflow_handle
- name: workflow_list
  endpoint:
    path: /workflow/list
    method: GET
    data_selector: workflow statuses
- name: workflow_get
  endpoint:
    path: /workflow/get
    method: GET
    data_selector: workflow status
- name: workflow_steps
  endpoint:
    path: /workflow/steps
    method: GET
    data_selector: workflow steps
- name: workflow_cancel
  endpoint:
    path: /workflow/cancel
    method: POST
    data_selector: workflow cancel status
- name: workflow_resume
  endpoint:
    path: /workflow/resume
    method: POST
    data_selector: workflow status
- name: workflow_fork
  endpoint:
    path: /workflow/fork
    method: POST
    data_selector: workflow status
- name: workflow_queue_list
  endpoint:
    path: /workflow/queue/list
    method: GET
    data_selector: workflow statuses
- name: migrate
  endpoint:
    path: /migrate
    method: POST
    data_selector: migrate status
- name: start
  endpoint:
    path: /start
    method: POST
    data_selector: start status
- name: init
  endpoint:
    path: /init
    method: POST
    data_selector: init status
- name: reset
  endpoint:
    path: /reset
    method: POST
    data_selector: reset status
- name: workflow_status
  endpoint:
    path: /dbos.workflow_status
    method: GET
    data_selector: records
- name: operation_outputs
  endpoint:
    path: /dbos.operation_outputs
    method: GET
    data_selector: records
- name: notifications
  endpoint:
    path: /dbos.notifications
    method: GET
    data_selector: records
- name: workflow_events
  endpoint:
    path: /dbos.workflow_events
    method: GET
    data_selector: records
- name: streams
  endpoint:
    path: /dbos.streams
    method: GET
    data_selector: records
- name: workflow
  endpoint:
    path: /workflows
    method: GET
- name: step
  endpoint:
    path: /steps
    method: GET
- name: WorkflowQueue
  endpoint:
    path: /typescript/reference/queues
    method: GET
    data_selector: queues
    params: {}
- name: KnexDataSource
  endpoint:
    path: /@dbos-inc/knex-datasource
    method: GET
    data_selector: records
- name: DrizzleDataSource
  endpoint:
    path: /@dbos-inc/drizzle-datasource
    method: GET
    data_selector: records
- name: KyselyDataSource
  endpoint:
    path: /@dbos-inc/kysely-datasource
    method: GET
    data_selector: records
- name: NodePostgresDataSource
  endpoint:
    path: /@dbos-inc/nodepg-datasource
    method: GET
    data_selector: records
- name: PostgresDataSource
  endpoint:
    path: /@dbos-inc/postgres-datasource
    method: GET
    data_selector: records
- name: PrismaDataSource
  endpoint:
    path: /@dbos-inc/prisma-datasource
    method: GET
    data_selector: records
- name: TypeORMDataSource
  endpoint:
    path: /@dbos-inc/typeorm-datasource
    method: GET
    data_selector: records
- name: workflow
  endpoint:
    path: /workflow
    method: GET
    data_selector: workflows
    params:
      limit: '10'
- name: GetEvent
  endpoint:
    path: /GetEvent
    method: GET
    data_selector: event
    params: {}
- name: SetEvent
  endpoint:
    path: /SetEvent
    method: POST
    data_selector: event
    params: {}
- name: Send
  endpoint:
    path: /Send
    method: POST
    data_selector: message
    params: {}
- name: Recv
  endpoint:
    path: /Recv
    method: GET
    data_selector: message
    params: {}
- name: Sleep
  endpoint:
    path: /Sleep
    method: POST
    data_selector: duration
    params: {}
- name: RetrieveWorkflow
  endpoint:
    path: /RetrieveWorkflow
    method: GET
    data_selector: workflowHandle
    params: {}
- name: ListWorkflows
  endpoint:
    path: /ListWorkflows
    method: GET
    data_selector: workflows
    params: {}
- name: GetWorkflowSteps
  endpoint:
    path: /GetWorkflowSteps
    method: GET
    data_selector: steps
    params: {}
- name: CancelWorkflow
  endpoint:
    path: /CancelWorkflow
    method: POST
    data_selector: status
    params: {}
- name: ResumeWorkflow
  endpoint:
    path: /ResumeWorkflow
    method: POST
    data_selector: workflowHandle
    params: {}
- name: ForkWorkflow
  endpoint:
    path: /ForkWorkflow
    method: POST
    data_selector: workflowHandle
    params: {}
- name: NewWorkflowQueue
  endpoint:
    path: /newworkflowqueue
    method: POST
    data_selector: workflow_queue
    params: {}
- name: workflow_status
  endpoint:
    path: /workflows/status
    method: GET
- name: workflow_queue
  endpoint:
    path: /golang/reference/queues
    method: GET
    data_selector: queues
notes:
- DBOS requires Python 3.9 or later.
- DBOS requires Node v20 or later.
- DBOS requires Go 1.23.0 or higher.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- DBOS provides lightweight durable workflows built on top of Postgres.
- DBOS makes your application resilient to any failure.
- DBOS provides a lightweight library for durable workflows built on top of Postgres.
- DBOS applications can be deployed to various environments including Kubernetes,
  EC2, or serverless platforms.
- DBOS uses a database to durably store workflow and step state.
- By default, it uses SQLite, which requires no configuration.
- For production use, we recommend connecting your DBOS application to a Postgres
  database.
- DBOS does NOT stand for anything.
- DBOS must always be configured with the constructor at the top and call DBOS.launch()
  in its main function.
- DBOS workflows and steps should NOT have side effects in memory outside of their
  own scope.
- All messages are persisted to the database, so if `send` completes successfully,
  the destination workflow is guaranteed to be able to `recv` it.
- If you're sending a message from a workflow, DBOS guarantees exactly-once delivery.
- If you're sending a message from normal Python code, you can use `SetWorkflowID`
  with an idempotency key to guarantee exactly-once delivery.
- NEVER async def a transaction.
- Idempotently start the checkout workflow in the background.
- All events are persisted to the database, so the latest version of an event is always
  retrievable.
- Workflows can stream data in real time to clients.
- You can optionally configure a step to automatically retry any exception a set number
  of times with exponential backoff.
- DBOS workflows provide durable execution.
- Workflows can be interrupted and automatically resumed.
- DBOS streams are immutable and append-only.
- DBOS must always be configured with the system_database_url environment variable.
- DBOS requires a Postgres database.
- DBOS does not support 'serverless' frameworks due to its long-running background
  jobs.
- DBOS programs MUST have a starting file (typically 'main.ts' or 'server.ts') that
  creates all objects and workflow functions during startup.
- You can set a deduplication ID for an enqueued workflow as an argument to DBOS.startWorkflow.
- Workflows with the same priority are dequeued in FIFO (first in, first out) order.
- DBOS requires a Postgres database
- Set the DBOS_SYSTEM_DATABASE_URL environment variable to your connection string
- When self-hosting, be careful when upgrading your application's code to prevent
  code compatibility issues.
- Each time you deploy an application, it creates a new version with a unique ID.
- Applications should serve requests from port 8000 (Python—the default port for FastAPI
  and Gunicorn) or 3000 (TypeScript—the default port for Express and Koa).
- Multiple applications can connect to the same Postgres database—they are deployed
  to isolated databases on that server.
- Uses API key for authorization.
- Handles failures gracefully by releasing reserved inventory.
- Agent autonomously searches for information on Hacker News
- Requires setup of research topic and max iterations
- Uses OpenAI API key for authentication.
- This app uses DBOS to fetch stock prices from Yahoo Finance, store them into Postgres,
  and display them using Streamlit.
- 'Asynchronous Processing: The approval process may take days, so the workflow must
  be invoked asynchronously in the background.'
- 'Workflow Reliability: The workflow must be durable and fault tolerant.'
- Asynchronous processing requires durable execution for reliable workflows.
- Earthquake data is sometimes updated later.
- Requires a Slack bot token supplied through an environment variable.
- API calls are limited by a 30-second timeout.
- DBOS needs a dbos.transaction_completion table for its own recordkeeping.
- Uses DBOS workflows for handling Kafka messages.
- Some objects like Contact may return nulls in deeply nested fields
- You cannot call, start, or enqueue workflows from within steps.
- All messages are persisted to the database, so if send completes successfully, the
  destination workflow is guaranteed to be able to recv it.
- If you're sending a message from normal Python code, you can use SetWorkflowID with
  an idempotency key to guarantee exactly-once delivery.
- 'Scheduled workflows must take in exactly two arguments: the time that the run was
  scheduled (as a datetime) and the time the run was actually started (as a datetime).'
- DBOS uses croniter to parse cron schedules, which is able to do second repetition.
- DBOS provides a pre-configured logger for you to use available at DBOS.logger.
- To use OpenTelemetry features such as tracing and export, you must install the DBOS
  OpenTelemetry dependencies.
- DBOS classes must be instantiated before DBOS.launch() is called.
- If config_name is not supplied, DBOS may not find the class instance it needs to
  recover a workflow.
- To minimize dependencies during testing, you may want to use a SQLite system database
  instead of Postgres.
- DBOSClient provides a programmatic way to interact with your DBOS application from
  external code.
- Container automatically restarts if the application crashes
- Check the services are running with `kubectl` or your favorite k8s admin tool.
- Retention policies require DBOS Python >= 1.5.0 or DBOS TypeScript >= 2.10.24
- By default, the rows threshold is set to 1M rows.
- By default, the global timeout is disabled.
- DBOS Cloud will serve HTTP traffic on port 8080.
- Applications are expected to listen for HTTP requests on port 3000 (TypeScript)
  or port 8000 (Python).
- Refresh tokens automatically expire after a year or after a month of inactivity.
- Treat refresh tokens as secrets and keep them safe.
- Both the database instance name and username must be 3 to 16 characters long and
  contain only lowercase letters, numbers and underscores.
- The username must start with a letter.
- The usernames dbosadmin, dbos, postgres and admin are reserved and cannot be used.
- The database password must contain between 8 and 128 characters, and cannot contain
  the characters /, ", @, ', or whitespaces.
- While it is occasionally necessary, be careful when manually changing the schema
  on a production database.
- Be careful making breaking schema changes such as deleting or renaming a column—they
  may break active workflows running on a previous application version.
- DBOS Cloud is currently hosted in AWS us-east-1.
- Logs are paginated and ordered chronologically.
- Traces are sorted chronologically and displayed hierarchically.
- Counts of warnings, errors and fatal errors are color coded as yellow, red and purple
  respectively.
- This tutorial shows how to configure your DBOS Cloud application to export OpenTelemetry
  logs and traces to a third party observability service.
- These steps require a DBOS Pro or Enterprise subscription.
- To make it easy to test changes to your application without affecting your production
  users, we recommend using separate staging and production environments.
- If you manually specify the application database name by setting app_db_name in
  dbos_config.yaml, you must ensure each environment uses a different value of app_db_name.
- Time-based retention is disabled by default.
- Secrets are key-value pairs that are securely stored in DBOS Cloud and made available
  to your application as environment variables.
- Redeploy your application for newly created or updated secrets to take effect.
- Should be called after all decorators run.
- You should not call a DBOS function until after DBOS is launched.
- DBOS must serialize data such as workflow inputs and outputs and step outputs to
  store it in the system database.
- By default, data is serialized with `pickle` then Base64-encoded, but you can optionally
  supply a custom serializer through DBOS configuration.
- 'DBOS automatically wraps your transaction functions in a SQLAlchemy ''begin once''
  block. Transaction functions automatically commit when they successfully complete
  and roll back if they throw an exception. Therefore, do not use DBOS.sql_session.commit()
  or DBOS.sql_session.rollback() in your transaction functions. Otherwise, you might
  see a sqlalchemy.exc.InvalidRequestError: Can''t operate on closed transaction inside
  context manager.'
- Debouncing delays workflow execution until some time has passed since the workflow
  has last been called.
- You can debounce workflows to prevent wasted work when triggered multiple times
  in quick succession.
- DBOS automatically wraps your transaction functions in a SQLAlchemy 'begin once'
  block.
- Transaction functions automatically commit when they successfully complete and roll
  back if they throw an exception.
- 'Timeouts are start-to-completion: if a workflow is enqueued, the timeout does not
  begin until the workflow is dequeued and starts execution.'
- 'Timeouts are durable: they are stored in the database and persist across restarts,
  so workflows can have very long timeouts.'
- Timeout deadlines are propagated to child workflows by default.
- May only be accessed from within a transaction.
- DBOS automatically wraps your transaction functions in a SQLAlchemy begin once block.
  Transaction functions automatically commit when they successfully complete and roll
  back if they throw an exception.
- Do not use DBOS.sql_session.commit() or DBOS.sql_session.rollback() in your transaction
  functions.
- By default, data is serialized with `pickle` then Base64-encoded
- Queues allow you to ensure that functions will be run, without starting them immediately.
- Queues are useful for controlling the number of functions run in parallel, or the
  rate at which functions are started.
- DBOS requires configuration with DBOS.setConfig before launch.
- DBOS class is a singleton and must be configured and launched exactly once.
- All queues should be created before DBOS.launch() is called.
- Use only in a development environment.
- Uses OpenTelemetry for tracing
- Logging facility is optional
- DBOS applications automatically save their state to Postgres every time a workflow
  step is executed.
- Uses durable sleep feature for workflows.
- Workflows can be scheduled using cron syntax.
- Workflows are versioned for recovery purposes.
- You can set a deduplication ID for an enqueued workflow using WithDeduplicationID
  when calling RunWorkflow.
- If a workflow with a deduplication ID is currently enqueued or actively executing
  (status ENQUEUED or PENDING), subsequent workflow enqueue attempts with the same
  deduplication ID in the same queue will return an error.
- Workflows with the same priority are dequeued in FIFO (first in, first out) order.
  Priority values can range from 1 to 2,147,483,647, where a low number indicates
  a higher priority.
- If using priority, you must set WithPriorityEnabled on your queue.
- Workflows without assigned priorities have the highest priority and are dequeued
  before workflows with assigned priorities.
- All workflows must be registered before the context is launched.
- Workflows may execute immediately or be enqueued for later execution based on options.
- Workflows provide durable execution so you can write programs that are resilient
  to any failure.
- Queues provide flow control, letting you manage how many workflows run at once or
  how often workflows are started.
- Worker concurrency limits are recommended for most use cases.
errors:
- '404 Not Found: Checkout failed to start'
- '500 Internal Server Error: Check server logs for more details.'
- '404 Not Found: Ensure the resource exists and the URL is correct.'
- 'Error: SENDGRID_API_KEY is not set'
- 'Error: SENDGRID_FROM_EMAIL is not set'
- 'Error: ADMIN_EMAIL is not set'
- 'Error fetching data from USGS: {status_code} {response.text}'
- 'HTTP error! status: {response.status}'
- 'Employee not found: Check if the employee exists before processing.'
- 'Alert timeout: Ensure timely response to alerts.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'sqlalchemy.exc.InvalidRequestError: Can''t operate on closed transaction inside
  context manager'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
auth_info:
  mentioned_objects:
  - refresh tokens
client:
  base_url: https://console.dbos.dev
  auth:
    type: oauth2
source_metadata: null
