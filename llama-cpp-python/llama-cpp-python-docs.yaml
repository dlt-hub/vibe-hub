resources:
- name: llama_vocab_p
  endpoint:
    path: /llama_vocab_p
    method: GET
- name: llama_model_p
  endpoint:
    path: /llama_model_p
    method: GET
- name: llama_context_p
  endpoint:
    path: /llama_context_p
    method: GET
- name: llama_memory_t
  endpoint:
    path: /llama_memory_t
    method: GET
- name: llama_kv_cache_p
  endpoint:
    path: /llama_kv_cache_p
    method: GET
- name: llama_pos
  endpoint:
    path: /llama_pos
    method: GET
- name: llama_token
  endpoint:
    path: /llama_token
    method: GET
- name: llama_seq_id
  endpoint:
    path: /llama_seq_id
    method: GET
- name: llama_token_data
  endpoint:
    path: /llama_token_data
    method: GET
- name: llama_chat_message
  endpoint:
    path: /llama_chat_message
    method: GET
- name: llama_model_default_params
  endpoint:
    path: /llama_model_default_params
    method: GET
- name: llama_backend_init
  endpoint:
    path: /llama_backend_init
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: model
  endpoint:
    path: /model
    method: GET
    data_selector: choices
    params:
      model_path: path/to/model
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: tokenize
  endpoint:
    path: /tokenize
    method: POST
    data_selector: tokens
    params: {}
- name: detokenize
  endpoint:
    path: /detokenize
    method: POST
    data_selector: detokenized_string
    params: {}
- name: reset
  endpoint:
    path: /reset
    method: POST
    data_selector: reset_status
    params: {}
- name: eval
  endpoint:
    path: /eval
    method: POST
    data_selector: evaluation_result
    params: {}
- name: sample
  endpoint:
    path: /sample
    method: POST
    data_selector: sampled_token
    params: {}
- name: generate
  endpoint:
    path: /generate
    method: POST
    data_selector: generated_tokens
    params: {}
- name: create_embedding
  endpoint:
    path: /create_embedding
    method: POST
    data_selector: embedding_object
    params: {}
- name: embed
  endpoint:
    path: /embed
    method: POST
    data_selector: embeddings
    params: {}
- name: llama_token_data
  endpoint:
    path: /llama/token_data
    method: GET
    data_selector: records
- name: llama_token_data_array
  endpoint:
    path: /llama/token_data_array
    method: GET
    data_selector: records
- name: llama_batch
  endpoint:
    path: /llama/batch
    method: GET
    data_selector: records
- name: llama_model_params
  endpoint:
    path: /llama/model_params
    method: GET
    data_selector: records
- name: llama_context_params
  endpoint:
    path: /llama/context_params
    method: GET
    data_selector: records
- name: llama_vocab_fim_rep
  endpoint:
    path: /llama_vocab_fim_rep
    method: GET
- name: llama_vocab_fim_sep
  endpoint:
    path: /llama_vocab_fim_sep
    method: GET
- name: llama_token_get_text
  endpoint:
    path: /llama_token_get_text
    method: GET
- name: llama_token_get_score
  endpoint:
    path: /llama_token_get_score
    method: GET
- name: llama_token_get_attr
  endpoint:
    path: /llama_token_get_attr
    method: GET
- name: llama_token_is_eog
  endpoint:
    path: /llama_token_is_eog
    method: GET
- name: llama_token_is_control
  endpoint:
    path: /llama_token_is_control
    method: GET
- name: llama_token_bos
  endpoint:
    path: /llama_token_bos
    method: GET
- name: llama_token_eos
  endpoint:
    path: /llama_token_eos
    method: GET
- name: llama_token_eot
  endpoint:
    path: /llama_token_eot
    method: GET
- name: llama_token_cls
  endpoint:
    path: /llama_token_cls
    method: GET
- name: llama_token_sep
  endpoint:
    path: /llama_token_sep
    method: GET
- name: llama_token_nl
  endpoint:
    path: /llama_token_nl
    method: GET
- name: llama_token_pad
  endpoint:
    path: /llama_token_pad
    method: GET
- name: llama_add_bos_token
  endpoint:
    path: /llama_add_bos_token
    method: GET
- name: llama_add_eos_token
  endpoint:
    path: /llama_add_eos_token
    method: GET
- name: llama_token_fim_pre
  endpoint:
    path: /llama_token_fim_pre
    method: GET
- name: llama_token_fim_suf
  endpoint:
    path: /llama_token_fim_suf
    method: GET
- name: llama_token_fim_mid
  endpoint:
    path: /llama_token_fim_mid
    method: GET
- name: llama_token_fim_pad
  endpoint:
    path: /llama_token_fim_pad
    method: GET
- name: llama_token_fim_rep
  endpoint:
    path: /llama_token_fim_rep
    method: GET
- name: llama_token_fim_sep
  endpoint:
    path: /llama_token_fim_sep
    method: GET
- name: llama_vocab_cls
  endpoint:
    path: /llama_vocab_cls
    method: GET
- name: llama_tokenize
  endpoint:
    path: /llama_tokenize
    method: GET
- name: llama_token_to_piece
  endpoint:
    path: /llama_token_to_piece
    method: GET
- name: llama_detokenize
  endpoint:
    path: /llama_detokenize
    method: GET
- name: llama_chat_apply_template
  endpoint:
    path: /llama_chat_apply_template
    method: GET
- name: llama_sampler_init_dry
  endpoint:
    path: /llama_sampler_init_dry
    method: POST
- name: llama_sampler_init_logit_bias
  endpoint:
    path: /llama_sampler_init_logit_bias
    method: POST
- name: llama_sampler_init_infill
  endpoint:
    path: /llama_sampler_init_infill
    method: POST
- name: llama_sampler_get_seed
  endpoint:
    path: /llama_sampler_get_seed
    method: GET
- name: llama_sampler_sample
  endpoint:
    path: /llama_sampler_sample
    method: POST
- name: llama_split_path
  endpoint:
    path: /llama_split_path
    method: POST
- name: llama_split_prefix
  endpoint:
    path: /llama_split_prefix
    method: POST
- name: llama_print_system_info
  endpoint:
    path: /llama_print_system_info
    method: GET
- name: llama_log_set
  endpoint:
    path: /llama_log_set
    method: POST
- name: llama_perf_context
  endpoint:
    path: /llama_perf_context
    method: POST
- name: llama_perf_context_print
  endpoint:
    path: /llama_perf_context_print
    method: POST
- name: llama_perf_context_reset
  endpoint:
    path: /llama_perf_context_reset
    method: POST
- name: llama_perf_sampler
  endpoint:
    path: /llama_perf_sampler
    method: POST
- name: llama_perf_sampler_print
  endpoint:
    path: /llama_perf_sampler_print
    method: POST
- name: llama_perf_sampler_reset
  endpoint:
    path: /llama_perf_sampler_reset
    method: POST
- name: llama_opt_init
  endpoint:
    path: /llama_opt_init
    method: POST
- name: llama_opt_epoch
  endpoint:
    path: /llama_opt_epoch
    method: POST
- name: EmbeddingUsage
  endpoint:
    path: /llama_cpp/embedding_usage
    method: GET
    data_selector: prompt_tokens, total_tokens
    params: {}
- name: Embedding
  endpoint:
    path: /llama_cpp/embedding
    method: GET
    data_selector: index, object, embedding
    params: {}
- name: CreateEmbeddingResponse
  endpoint:
    path: /llama_cpp/create_embedding_response
    method: GET
    data_selector: object, model, data, usage
    params: {}
- name: CompletionLogprobs
  endpoint:
    path: /llama_cpp/completion_logprobs
    method: GET
    data_selector: text_offset, token_logprobs, tokens, top_logprobs
    params: {}
- name: CompletionChoice
  endpoint:
    path: /llama_cpp/completion_choice
    method: GET
    data_selector: text, index, logprobs, finish_reason
    params: {}
- name: CompletionUsage
  endpoint:
    path: /llama_cpp/completion_usage
    method: GET
    data_selector: prompt_tokens, completion_tokens, total_tokens
    params: {}
- name: CreateCompletionResponse
  endpoint:
    path: /llama_cpp/create_completion_response
    method: GET
    data_selector: id, object, created, model, choices, usage
    params: {}
- name: ChatCompletionResponseFunctionCall
  endpoint:
    path: /llama_cpp/chat_completion_response_function_call
    method: GET
    data_selector: name, arguments
    params: {}
- name: ChatCompletionResponseMessage
  endpoint:
    path: /llama_cpp/chat_completion_response_message
    method: GET
    data_selector: content, tool_calls, role, function_call
    params: {}
- name: ChatCompletionFunction
  endpoint:
    path: /llama_cpp/chat_completion_function
    method: GET
    data_selector: name, description, parameters
    params: {}
- name: ChatCompletionTopLogprobToken
  endpoint:
    path: /llama_cpp/chat_completion_top_logprob_token
    method: GET
    data_selector: token, logprob, bytes
    params: {}
- name: ChatCompletionLogprobToken
  endpoint:
    path: /llama_cpp/chat_completion_logprob_token
    method: GET
    data_selector: token, logprob, bytes, top_logprobs
    params: {}
- name: ChatCompletionLogprobs
  endpoint:
    path: /llama_cpp/chat_completion_logprobs
    method: GET
    data_selector: content, refusal
    params: {}
- name: ChatCompletionResponseChoice
  endpoint:
    path: /llama_cpp/chat_completion_response_choice
    method: GET
    data_selector: index, message, logprobs, finish_reason
    params: {}
- name: CreateChatCompletionResponse
  endpoint:
    path: /llama_cpp/create_chat_completion_response
    method: GET
    data_selector: id, object, created, model, choices, usage
    params: {}
- name: ChatCompletionMessageToolCallChunkFunction
  endpoint:
    path: /llama_cpp/chat_completion_message_tool_call_chunk_function
    method: GET
    data_selector: name, arguments
    params: {}
- name: ChatCompletionMessageToolCall
  endpoint:
    path: /llama_cpp/chat_completion_message_tool_call
    method: GET
    data_selector: id, type, function
    params: {}
- name: ChatCompletionRequestSystemMessage
  endpoint:
    path: /llama_cpp/chat_completion_request_system_message
    method: GET
    data_selector: role, content
    params: {}
- name: ChatCompletionRequestUserMessage
  endpoint:
    path: /llama_cpp/chat_completion_request_user_message
    method: GET
    data_selector: role, content
    params: {}
- name: ChatCompletionRequestAssistantMessage
  endpoint:
    path: /llama_cpp/chat_completion_request_assistant_message
    method: GET
    data_selector: role, content, tool_calls, function_call
    params: {}
- name: ChatCompletionRequestToolMessage
  endpoint:
    path: /llama_cpp/chat_completion_request_tool_message
    method: GET
    data_selector: role, content, tool_call_id
    params: {}
- name: ChatCompletionRequestFunctionMessage
  endpoint:
    path: /llama_cpp/chat_completion_request_function_message
    method: GET
    data_selector: role, content, name
    params: {}
- name: UserDetail
  endpoint:
    path: /function/UserDetail
    method: POST
    data_selector: parameters
- name: functionary-small
  endpoint:
    path: /functionary-small-v2.2-GGUF
    method: GET
    data_selector: models
- name: multi-modal
  endpoint:
    path: /multi-modal-models
    method: GET
    data_selector: models
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
    params: {}
- name: functionary
  endpoint:
    path: /meetkai/functionary-small-v2.2-GGUF
    method: GET
    data_selector: data
    params: {}
- name: UserDetail
  endpoint:
    path: /function
    method: POST
    data_selector: choices
    params: {}
- name: llava-v1.5-7b
  endpoint:
    path: /mys/ggml_llava-v1.5-7b
    method: GET
    data_selector: models
    params: {}
- name: llava-v1.5-13b
  endpoint:
    path: /mys/ggml_llava-v1.5-13b
    method: GET
    data_selector: models
    params: {}
- name: llava-v1.6-34b
  endpoint:
    path: /cjpais/llava-v1.6-34B-gguf
    method: GET
    data_selector: models
    params: {}
- name: moondream2
  endpoint:
    path: /vikhyatk/moondream2
    method: GET
    data_selector: models
    params: {}
- name: nanollava
  endpoint:
    path: /abetlen/nanollava-gguf
    method: GET
    data_selector: models
    params: {}
- name: llama-3-vision-alpha
  endpoint:
    path: /abetlen/llama-3-vision-alpha-gguf
    method: GET
    data_selector: models
    params: {}
- name: minicpm-v-2.6
  endpoint:
    path: /openbmb/MiniCPM-V-2_6-gguf
    method: GET
    data_selector: models
    params: {}
- name: qwen2.5-vl
  endpoint:
    path: /unsloth/Qwen2.5-VL-3B-Instruct-GGUF
    method: GET
    data_selector: models
    params: {}
- name: multi-modal_models
  endpoint:
    path: /models/multi-modal
    method: GET
    data_selector: models
    params: {}
notes:
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- 'M1 Mac Performance Issue: Ensure Python supports arm64 architecture for optimal
  performance.'
- Detailed MacOS Metal GPU install documentation is available.
- Uses function calling with compatible formats for OpenAI
- Supports parallel function calling
- Image can be passed as base64 encoded data URIs
- Docker on termux (requires root) is currently the only known way to run this on
  phones.
- You needed xcode installed in order pip to build/compile the C++ code
- If you omit the `--n_gpu_layers 1` then CPU will be used
- Without GPU acceleration this is unlikely to be fast enough to be usable.
- Server options are available as environment variables.
- Add support for Llava1.5 multimodal models
- Add seed parameter to completion and chat_completion functions of Llama class
- Add JSON mode support to constrain chat completion to JSON objects
- Add support for Huggingface Autotokenizer Chat Formats
- Fix llama-2 chat format
- Add support for functionary chat format
- Update docs for gguf and add hw acceleration docs for server
- Add NUMA support
- 'M1 Mac Performance Issue: Make sure you have installed a version of Python that
  supports arm64 architecture.'
- 'M1 Mac Performance Issue: If you are using Apple Silicon (M1) Mac, make sure you
  have installed a version of Python that supports arm64 architecture.'
- If this fails, add --verbose to the pip install to see the full cmake build log.
- There is no need to provide the default system messages used in Functionary as they
  are added automatically in the Functionary chat handler.
- 'M1 Mac Performance Issue: Ensure Python supports arm64 architecture.'
- Some models may return different formats.
- n_ctx should be increased to accommodate the image embedding
- A Docker image is available on GHCR
- Docker on termux (requires root) is currently the only known way to run this on
  phones
- 'M1 Mac Performance Issue: Ensure you have installed a version of Python that supports
  arm64 architecture.'
- To upgrade and rebuild llama-cpp-python add --upgrade --force-reinstall --no-cache-dir
  flags to the pip install command.
- If installation fails, add --verbose to see the full cmake build log.
- Multi-modal models support tool calling and JSON mode.
- A Docker image is available on GHCR.
- Currently, the only known way to run this on phones is through Docker on termux
  (requires root).
- To upgrade and rebuild `llama-cpp-python`, use `--upgrade --force-reinstall --no-cache-dir`
  flags.
- Uses Docker to run the server
- Requires setup of models directory
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  base_url: https://huggingface.co/meetkai
  headers:
    Accept: application/json
source_metadata: null
