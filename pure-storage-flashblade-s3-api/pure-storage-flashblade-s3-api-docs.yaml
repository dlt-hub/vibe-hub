resources:
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: vSphere Plugin
  endpoint:
    path: /csh?topicname=c_release_notes_pure_storage_plugin_for_the_vsphere_client.html
    method: GET
    data_selector: release_notes
    params: {}
- name: VMware Appliance
  endpoint:
    path: /csh?topicname=c_release_notes_pure_storage_vmware_appliance.html
    method: GET
    data_selector: release_notes
    params: {}
- name: Offline Deployment Procedure
  endpoint:
    path: /vm-analytics-collector/purestorage-ova-latest.iso
    method: GET
- name: vSphere Plugin
  endpoint:
    path: /services/data/vXX.X/sobjects/vSpherePlugin
    method: POST
- name: vSphere Plugin
  endpoint:
    path: /services/data/vXX.X/sobjects/vSpherePlugin
    method: GET
    data_selector: records
    params: {}
- name: vCenter Server
  endpoint:
    path: /PureStorage/VMwareAppliance/vCenter
    method: GET
- name: Pure1
  endpoint:
    path: /PureStorage/VMwareAppliance/Pure1
    method: GET
- name: FlashArray
  endpoint:
    path: /PureStorage/VMwareAppliance/FlashArray
    method: GET
- name: FlashArray
  endpoint:
    path: /services/data/vSphere/FlashArray
    method: GET
    data_selector: records
- name: Pure1
  endpoint:
    path: /services/data/vSphere/Pure1
    method: GET
    data_selector: records
- name: FlashArray Connection
  endpoint:
    path: /services/data/FlashArrayConnection
    method: POST
    data_selector: connection
    params: {}
- name: FlashArray VASA Providers
  endpoint:
    path: /services/vsphere/flasharray/vasa/providers
    method: POST
    data_selector: providers
    params: {}
- name: Replication Manager Array View
  endpoint:
    path: /replication-manager/array-view
    method: GET
    data_selector: protection_groups
    params: {}
- name: Replication Group Management View
  endpoint:
    path: /replication-manager/group-management
    method: GET
    data_selector: active_operations
    params: {}
- name: pod_creation
  endpoint:
    path: /services/data/vSpherePlugin/CreatePod
    method: POST
    data_selector: pod
    params: {}
- name: volume_group
  endpoint:
    path: /services/data/vXX.X/volume_groups
    method: POST
    data_selector: volumeGroup
    params: {}
- name: Virtual Machine Management
  endpoint:
    path: /services/virtualmachine/management
    method: GET
- name: Pure Storage Plugin
  endpoint:
    path: /vcenter/plugin
    method: GET
    data_selector: folders
    params: {}
- name: FlashArray
  endpoint:
    path: /api/v1/flasharrays
    method: POST
    data_selector: results
    params: {}
- name: host_configuration
  endpoint:
    path: /api/host_configuration
    method: GET
    data_selector: hosts
    params: {}
- name: host_groups
  endpoint:
    path: /api/vsphere/host_groups
    method: POST
    data_selector: data
    params: {}
- name: host_group_creation
  endpoint:
    path: /vSphere/Plugin/HostGroupCreation
    method: POST
    data_selector: response
    params: {}
- name: Add/Update Host Group
  endpoint:
    path: /Add/UpdateHostGroup
    method: POST
    data_selector: Will be created
    params: {}
- name: iSCSI Configuration Workflow
  endpoint:
    path: /configure/iscsi
    method: POST
    data_selector: actions
    params: {}
- name: FlashArray VASA Providers
  endpoint:
    path: /vasa/providers
    method: POST
    data_selector: providers
    params: {}
- name: vVol Disk Snapshot
  endpoint:
    path: /create_snapshot/vVol_disk
    method: POST
    data_selector: snapshot
    params: {}
- name: vVol Copy
  endpoint:
    path: /api/vvol_copy
    method: POST
    data_selector: result
    params: {}
- name: VM Undelete
  endpoint:
    path: /vvols/datastore/undelete
    method: POST
    data_selector: VMs
    params: {}
- name: VMFS Datastore
  endpoint:
    path: /create/vmfs/datastore
    method: POST
    data_selector: datastore
    params: {}
- name: Restore VMFS Datastore
  endpoint:
    path: /api/vsphere/restore_vmfs_datastore
    method: POST
    data_selector: result
    params: {}
- name: VMFS Datastore Details
  endpoint:
    path: /vcenter/vmfs/details
    method: GET
    data_selector: datastore_details
    params: {}
- name: Protection Groups
  endpoint:
    path: /services/data/vXX.X/ProtectionGroups
    method: GET
    data_selector: records
    params: {}
- name: VMFS Datastores
  endpoint:
    path: /services/data/vXX.X/VMFSDatastores
    method: GET
    data_selector: records
    params: {}
- name: UNMAP
  endpoint:
    path: /running_and_scheduling_unmap_on_vmfs
    method: GET
    data_selector: data
    params: {}
- name: NFSv3 Datastore
  endpoint:
    path: /services/data/vXX.X/nfsdatastore
    method: POST
    data_selector: datastore
    params: {}
- name: NFSv4.1
  endpoint:
    path: /services/data/vXX.X/sobjects/NFSv4.1
    method: POST
    data_selector: records
    params: {}
- name: NFSv3 Datastore Creation
  endpoint:
    path: /NFSv3/Datastore/Creation
    method: GET
    data_selector: records
- name: NFSv4.1 Datastore Creation
  endpoint:
    path: /NFSv4.1/Datastore/Creation
    method: GET
    data_selector: records
- name: NFS Datastore Details
  endpoint:
    path: /path/to/nfs/datastore/details
    method: GET
    data_selector: details
- name: Edit NFS Datastore
  endpoint:
    path: /path/to/edit/nfs/datastore
    method: POST
    data_selector: edit
- name: Destroy NFS Datastore
  endpoint:
    path: /path/to/destroy/nfs/datastore
    method: DELETE
    data_selector: destroy
- name: Mount NFS Datastore
  endpoint:
    path: /path/to/mount/nfs/datastore
    method: POST
    data_selector: mount
- name: Edit NFS Datastore Policy (Custom Policy Outcome)
  endpoint:
    path: /edit/nfs/datastore/custom/policy
    method: POST
    data_selector: details
    params: {}
- name: Edit NFS Datastore Policy (Unrestricted Policy Outcome)
  endpoint:
    path: /edit/nfs/datastore/unrestricted/policy
    method: POST
    data_selector: details
    params: {}
- name: capacity_metrics
  endpoint:
    path: /path/to/capacity_metrics
    method: GET
    data_selector: metrics
    params: {}
- name: performance_metrics
  endpoint:
    path: /performance/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: Virtual Machine Recovery
  endpoint:
    path: /recover/vm
    method: POST
    data_selector: result
    params: {}
- name: replication_group
  endpoint:
    path: /services/data/vX.X/sobjects/ReplicationGroup
    method: GET
    data_selector: records
    params: {}
- name: Replication Manager Array View
  endpoint:
    path: /replication/manager/array/view
    method: GET
    data_selector: replicationGroups
    params: {}
- name: Replication Group Management View
  endpoint:
    path: /replication/manager/group/manage
    method: GET
    data_selector: replicationGroupDetails
    params: {}
- name: vSphere
  endpoint:
    path: /api/vsphere
    method: GET
    data_selector: plugin
    params: {}
- name: Replication Provider
  endpoint:
    path: /vasa/version3/replication
    method: GET
    data_selector: features
    params: {}
- name: Storage Capabilities
  endpoint:
    path: /vasa/version3/capabilities
    method: GET
    data_selector: capabilities
    params: {}
- name: Storage Policy
  endpoint:
    path: /vasa/version3/policies
    method: GET
    data_selector: policies
    params: {}
- name: Fault Domain
  endpoint:
    path: /vasa/version3/faultdomains
    method: GET
    data_selector: faultDomains
    params: {}
- name: Source Replication Group
  endpoint:
    path: /vasa/version3/sourcegroups
    method: GET
    data_selector: sourceGroups
    params: {}
- name: Target Replication Group
  endpoint:
    path: /vasa/version3/targetgroups
    method: GET
    data_selector: targetGroups
    params: {}
- name: vasa_versions
  endpoint:
    path: /vasa/versions
    method: GET
    data_selector: versions
- name: FlashArray Registration
  endpoint:
    path: /register/flasharray
    method: POST
    data_selector: registrationStatus
    params: {}
- name: Storage Provider Registration
  endpoint:
    path: /register/storageprovider
    method: POST
    data_selector: providerStatus
    params: {}
- name: CT0 Storage Provider
  endpoint:
    path: https://<controllerIP>:8084
    method: POST
    data_selector: provider
    params: {}
- name: CT1 Storage Provider
  endpoint:
    path: https://<controllerIP>:8084
    method: POST
    data_selector: provider
    params: {}
- name: VASA Provider Registration
  endpoint:
    path: /New-VasaProvider
    method: POST
    data_selector: results
    params: {}
- name: PFA VASA Provider Registration
  endpoint:
    path: /New-PfaVasaProvider
    method: POST
    data_selector: results
    params: {}
- name: storage_provider
  endpoint:
    path: /vcenter/storage-providers
    method: GET
    data_selector: providers
    params: {}
- name: VasaProvider
  endpoint:
    path: /Get-VasaProvider
    method: GET
    data_selector: records
    params: {}
- name: RemoveVasaProvider
  endpoint:
    path: /Remove-VasaProvider
    method: POST
    data_selector: records
    params: {}
- name: requirements
  endpoint:
    path: /requirements
    method: GET
    data_selector: records
    params: {}
- name: recommendations
  endpoint:
    path: /recommendations
    method: GET
    data_selector: records
    params: {}
- name: VASA Providers
  endpoint:
    path: /vasa/providers
    method: GET
    data_selector: providers
    params: {}
- name: requirements
  endpoint:
    path: /requirements
    method: GET
    data_selector: records
    params: {}
- name: object_limits
  endpoint:
    path: /object_limits
    method: GET
    data_selector: records
    params: {}
- name: unsupported_configurations
  endpoint:
    path: /unsupported_configurations
    method: GET
    data_selector: records
    params: {}
- name: pod_creation
  endpoint:
    path: /purepod/create
    method: POST
    data_selector: Name
    params: {}
- name: protocol_endpoint_creation
  endpoint:
    path: /purevol/create
    method: POST
    data_selector: Name
    params:
      container_version: 4
- name: protocol_endpoint_connection
  endpoint:
    path: /purevol/connect
    method: POST
    data_selector: Name
    params: {}
- name: acl_setting
  endpoint:
    path: /purevchost/connection/create
    method: POST
    data_selector: VC Host
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: create_nvme_vvols_storage_container
  endpoint:
    path: /creating_nvme-vvols-storage-containers
    method: POST
    data_selector: result
    params: {}
- name: host_object
  endpoint:
    path: /csh?topicname=c_managing_hosts_and_host_groups.html
    method: GET
    data_selector: instructions
- name: host_configuration
  endpoint:
    path: /api/vsphere/host/configuration
    method: GET
    data_selector: host
    params: {}
- name: Host Group Configuration
  endpoint:
    path: /r_viewing_host_group_configuration__r_viewing_host_group_configuration-tbl1
    method: GET
    data_selector: records
    params: {}
- name: Renaming a Host
  endpoint:
    path: /library/common_content/r_renaming_a_host.html#r_renaming_a_host__r_renaming_a_host-tbl1
    method: GET
    data_selector: records
    params: {}
- name: Creating a Host and Adding it to a Host Group
  endpoint:
    path: /library/common_content/t_creating_a_host_and_adding_it_to_a_host_group.html#t_creating_a_host_and_adding_it_to_a_host_group__t_creating_a_host_and_adding_it_to_a_host_group-tbl1
    method: GET
    data_selector: records
    params: {}
- name: Host
  endpoint:
    path: /api/vsphere/hosts
    method: POST
    data_selector: host
    params: {}
- name: volume_groups
  endpoint:
    path: /api/v1/volume_groups
    method: GET
    data_selector: volume_groups
    params: {}
- name: destroyed_volume_groups
  endpoint:
    path: /api/v1/destroyed_volume_groups
    method: GET
    data_selector: destroyed_volume_groups
    params: {}
- name: recover_deleted_vvols
  endpoint:
    path: /recovering/deleted/vvols
    method: GET
    data_selector: records
- name: restore_destroyed_vVol
  endpoint:
    path: /restore/destroyed_vVol
    method: POST
    data_selector: workflow
    params: {}
- name: overwrite_vVol
  endpoint:
    path: /overwrite/vVol
    method: POST
    data_selector: workflow
    params: {}
- name: vVol Restoration
  endpoint:
    path: /restoring/vvol
    method: POST
    data_selector: workflow
    params: {}
- name: protocol_endpoints
  endpoint:
    path: /protocol_endpoints
    method: GET
    data_selector: protocol_endpoints
    params: {}
- name: protocol_endpoint
  endpoint:
    path: /purevol
    method: GET
    data_selector: protocol_endpoints
    params: {}
- name: vVols
  endpoint:
    path: /vvols
    method: GET
    data_selector: records
    params: {}
- name: vVol Datastore
  endpoint:
    path: /vvols/datastores
    method: GET
    data_selector: datastores
    params: {}
- name: vVol Datastore Mounting
  endpoint:
    path: /mount/vVolDatastore
    method: POST
    data_selector: mountDetails
    params: {}
- name: vVol Snapshots
  endpoint:
    path: /api/vvols/snapshots
    method: GET
    data_selector: snapshots
    params: {}
- name: managed_snapshots
  endpoint:
    path: /api/vvols/snapshots
    method: POST
    data_selector: snapshots
- name: Virtual Machine vVol Details
  endpoint:
    path: /path/to/virtual/machine/vvol/details
    method: GET
    data_selector: vVolDetails
    params: {}
- name: restore_destroyed_vVol
  endpoint:
    path: /restore/destroyed/vVol
    method: POST
    data_selector: restoreStatus
    params: {}
- name: overwrite_vVol
  endpoint:
    path: /overwrite/vVol
    method: POST
    data_selector: overwriteStatus
    params: {}
- name: vVol Copy
  endpoint:
    path: /vvol/copy
    method: POST
    data_selector: import
    params: {}
- name: Replication Provider
  endpoint:
    path: /vvols/replication/provider
    method: GET
    data_selector: replication_features
    params: {}
- name: Storage Capabilities
  endpoint:
    path: /vvols/storage/capabilities
    method: GET
    data_selector: capabilities
    params: {}
- name: Storage Policy
  endpoint:
    path: /vvols/storage/policy
    method: GET
    data_selector: storage_policies
    params: {}
- name: Fault Domain
  endpoint:
    path: /vvols/fault/domain
    method: GET
    data_selector: fault_domains
    params: {}
- name: Source Replication Group
  endpoint:
    path: /vvols/source/replication/group
    method: GET
    data_selector: source_groups
    params: {}
- name: Target Replication Group
  endpoint:
    path: /vvols/target/replication/group
    method: GET
    data_selector: target_groups
    params: {}
- name: replication_policies
  endpoint:
    path: /replication/policies
    method: GET
    data_selector: replication_policies
    params: {}
- name: replication_groups
  endpoint:
    path: /replication/groups
    method: GET
    data_selector: replication_groups
    params: {}
- name: failure_domains
  endpoint:
    path: /failure/domains
    method: GET
    data_selector: failure_domains
    params: {}
- name: SyncReplicationGroup_Task
  endpoint:
    path: /api/vvols/sync_replication_group
    method: POST
    data_selector: task
    params: {}
- name: TestFailoverReplicationGroupStart_Task
  endpoint:
    path: /api/vvols/test_failover_replication_group/start
    method: POST
    data_selector: task
    params: {}
- name: TestFailoverReplicationGroupStop_Task
  endpoint:
    path: /api/vvols/test_failover_replication_group/stop
    method: POST
    data_selector: task
    params: {}
- name: PromoteReplicationGroup_Task
  endpoint:
    path: /api/vvols/promote_replication_group
    method: POST
    data_selector: task
    params: {}
- name: PrepareFailoverReplicationGroup_Task
  endpoint:
    path: /api/vvols/prepare_failover_replication_group
    method: POST
    data_selector: task
    params: {}
- name: FailoverReplicationGroup_Task
  endpoint:
    path: /api/vvols/failover_replication_group
    method: POST
    data_selector: task
    params: {}
- name: ReverseReplicateGroup_Task
  endpoint:
    path: /api/vvols/reverse_replicate_group
    method: POST
    data_selector: task
    params: {}
- name: QueryReplicationGroup
  endpoint:
    path: /api/vvols/query_replication_group
    method: GET
    data_selector: replication_group
    params: {}
- name: QueryPointInTimeReplica
  endpoint:
    path: /api/vvols/query_point_in_time_replica
    method: GET
    data_selector: replica
    params: {}
- name: QueryReplicationPeer
  endpoint:
    path: /api/vvols/query_replication_peer
    method: GET
    data_selector: replication_peer
    params: {}
- name: SyncReplicationGroup_Task
  endpoint:
    path: /vvols/SyncReplicationGroup_Task
    method: POST
    data_selector: task
    params: {}
- name: TestFailoverReplicationGroupStart_Task
  endpoint:
    path: /vvols/TestFailoverReplicationGroupStart_Task
    method: POST
    data_selector: task
    params: {}
- name: TestFailoverReplicationGroupStop_Task
  endpoint:
    path: /vvols/TestFailoverReplicationGroupStop_Task
    method: POST
    data_selector: task
    params: {}
- name: PromoteReplicationGroup_Task
  endpoint:
    path: /vvols/PromoteReplicationGroup_Task
    method: POST
    data_selector: task
    params: {}
- name: PrepareFailoverReplicationGroup_Task
  endpoint:
    path: /vvols/PrepareFailoverReplicationGroup_Task
    method: POST
    data_selector: task
    params: {}
- name: FailoverReplicationGroup_Task
  endpoint:
    path: /vvols/FailoverReplicationGroup_Task
    method: POST
    data_selector: task
    params: {}
- name: ReverseReplicateGroup_Task
  endpoint:
    path: /vvols/ReverseReplicateGroup_Task
    method: POST
    data_selector: task
    params: {}
- name: QueryReplicationGroup
  endpoint:
    path: /vvols/QueryReplicationGroup
    method: GET
    data_selector: replication_group
    params: {}
- name: QueryPointInTimeReplica
  endpoint:
    path: /vvols/QueryPointInTimeReplica
    method: GET
    data_selector: point_in_time_replica
    params: {}
- name: QueryReplicationPeer
  endpoint:
    path: /vvols/QueryReplicationPeer
    method: GET
    data_selector: replication_peer
    params: {}
- name: SpbmFaultDomain
  endpoint:
    path: /Get-SpbmFaultDomain
    method: GET
    data_selector: ''
    params: {}
- name: SpbmReplicationGroup
  endpoint:
    path: /Get-SpbmReplicationGroup
    method: GET
    data_selector: ''
    params: {}
- name: SpbmReplicationPair
  endpoint:
    path: /Get-SpbmReplicationPair
    method: GET
    data_selector: ''
    params: {}
- name: SpbmPointInTimeReplica
  endpoint:
    path: /Get-SpbmPointInTimeReplica
    method: GET
    data_selector: ''
    params: {}
- name: SpbmStoragePolicy
  endpoint:
    path: /Get-SpbmStoragePolicy
    method: GET
    data_selector: ''
    params: {}
- name: SyncSpbmReplicationGroup
  endpoint:
    path: /Sync-SpbmReplicationGroup
    method: POST
    data_selector: ''
    params: {}
- name: StartSpbmReplicationTestFailover
  endpoint:
    path: /Start-SpbmReplicationTestFailover
    method: POST
    data_selector: ''
    params: {}
- name: StartSpbmReplicationPromote
  endpoint:
    path: /Start-SpbmReplicationPromote
    method: POST
    data_selector: ''
    params: {}
- name: StopSpbmReplicationTestFailover
  endpoint:
    path: /Stop-SpbmReplicationTestFailover
    method: POST
    data_selector: ''
    params: {}
- name: StartSpbmReplicationPrepareFailover
  endpoint:
    path: /Start-SpbmReplicationPrepareFailover
    method: POST
    data_selector: ''
    params: {}
- name: StartSpbmReplicationFailover
  endpoint:
    path: /Start-SpbmReplicationFailover
    method: POST
    data_selector: ''
    params: {}
- name: StartSpbmReplicationReverse
  endpoint:
    path: /Start-SpbmReplicationReverse
    method: POST
    data_selector: ''
    params: {}
- name: Get-SpbmFaultDomain
  endpoint:
    path: /Get-SpbmFaultDomain
    method: GET
    data_selector: FaultDomain
    params: {}
- name: Get-SpbmReplicationGroup
  endpoint:
    path: /Get-SpbmReplicationGroup
    method: GET
    data_selector: ReplicationGroup
    params: {}
- name: Get-SpbmReplicationPair
  endpoint:
    path: /Get-SpbmReplicationPair
    method: GET
    data_selector: ReplicationPair
    params: {}
- name: Get-SpbmPointInTimeReplica
  endpoint:
    path: /Get-SpbmPointInTimeReplica
    method: GET
    data_selector: PointInTimeReplica
    params: {}
- name: Get-SpbmStoragePolicy
  endpoint:
    path: /Get-SpbmStoragePolicy
    method: GET
    data_selector: StoragePolicy
    params: {}
- name: Sync-SpbmReplicationGroup
  endpoint:
    path: /Sync-SpbmReplicationGroup
    method: POST
    data_selector: ReplicationGroup
    params: {}
- name: Start-SpbmReplicationTestFailover
  endpoint:
    path: /Start-SpbmReplicationTestFailover
    method: POST
    data_selector: ReplicationTestFailover
    params: {}
- name: Start-SpbmReplicationPrepareFailover
  endpoint:
    path: /Start-SpbmReplicationPrepareFailover
    method: POST
    data_selector: ReplicationPrepareFailover
    params: {}
- name: Start-SpbmReplicationFailover
  endpoint:
    path: /Start-SpbmReplicationFailover
    method: POST
    data_selector: ReplicationFailover
    params: {}
- name: Start-SpbmReplicationReverse
  endpoint:
    path: /Start-SpbmReplicationReverse
    method: POST
    data_selector: ReplicationReverse
    params: {}
- name: storage_policy
  endpoint:
    path: /services/data/v1.0/storage_policy
    method: POST
    data_selector: policies
    params: {}
- name: VMs and Virtual Disks Compliance
  endpoint:
    path: /services/data/vXX.X/vm_storage_policy_compliance
    method: GET
    data_selector: compliance_status
    params: {}
- name: Assigning a Storage Policy to a VM
  endpoint:
    path: /assigning/storage/policy/to/vm
    method: GET
    data_selector: records
- name: Changing a VM's Storage Policy
  endpoint:
    path: /changing/vm/storage/policy
    method: GET
    data_selector: records
- name: Assigning a Policy during Storage Migration
  endpoint:
    path: /assigning/policy/during/storage/migration
    method: GET
    data_selector: records
- name: performance_history
  endpoint:
    path: /performance/history
    method: GET
    data_selector: performanceData
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: FlashArray Protection Groups
  endpoint:
    path: /path/to/flasharray/protection/groups
    method: GET
    data_selector: data
    params: {}
- name: VM vVol details
  endpoint:
    path: /path/to/vm/vvol/details
    method: GET
    data_selector: details
    params: {}
- name: flasharray
  endpoint:
    path: /services/data/vXX.X/sobjects/FlashArray
    method: GET
    data_selector: records
- name: flashblade
  endpoint:
    path: /services/data/vXX.X/sobjects/FlashBlade
    method: GET
    data_selector: records
- name: requirements
  endpoint:
    path: /requirements
    method: GET
    data_selector: records
    params: {}
- name: recommendations
  endpoint:
    path: /recommendations
    method: GET
    data_selector: records
    params: {}
- name: pure-protocol-endpoint
  endpoint:
    path: /pure-protocol-endpoint
    method: GET
    data_selector: records
- name: VASA Providers
  endpoint:
    path: /vasa/providers
    method: GET
    data_selector: providers
    params: {}
- name: CT0_storage_provider
  endpoint:
    path: /services/data/vXX.X/sobjects/CT0_storage_provider
    method: POST
    data_selector: records
    params: {}
- name: CT1_storage_provider
  endpoint:
    path: /services/data/vXX.X/sobjects/CT1_storage_provider
    method: POST
    data_selector: records
    params: {}
- name: New-VasaProvider
  endpoint:
    path: /New-VasaProvider
    method: POST
    data_selector: ProviderDetails
    params:
      Name: MyProvider
      Username: UserName
      Password: Password
      Url: MyUrl
- name: New-PfaVasaProvider
  endpoint:
    path: /New-PfaVasaProvider
    method: POST
    data_selector: ProviderDetails
    params:
      Flasharray: $Global:DefaultFlashArray
      Credentials: (Get-Credential)
- name: VASA Provider
  endpoint:
    path: /Get-VasaProvider
    method: GET
    data_selector: records
    params: {}
- name: Remove VASA Provider
  endpoint:
    path: /Remove-VasaProvider
    method: POST
    data_selector: response
    params:
      confirm: 'false'
- name: stretched_storage_container
  endpoint:
    path: /activecluster/vvols
    method: POST
    data_selector: container
    params: {}
- name: stretchable_container
  endpoint:
    path: /activecluster/vvols/stretchable-container
    method: POST
    data_selector: result
    params: {}
- name: sync_status
  endpoint:
    path: /activecluster/vvols/sync-status
    method: GET
    data_selector: status
    params: {}
- name: connect_pe
  endpoint:
    path: /activecluster/vvols/connect-pe
    method: POST
    data_selector: result
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: create_flasharray_host_object
  endpoint:
    path: /api/v1/flasharray/host
    method: POST
    data_selector: host
    params: {}
- name: create_flasharray_host_group_object
  endpoint:
    path: /api/v1/flasharray/hostgroup
    method: POST
    data_selector: hostgroup
    params: {}
- name: create_pod
  endpoint:
    path: /api/pod/create
    method: POST
    data_selector: pod
    params:
      quota_limit: 500 TB
- name: create_protocol_endpoint
  endpoint:
    path: /api/protocol_endpoint/create
    method: POST
    data_selector: protocol_endpoint
    params: {}
- name: connect_protocol_endpoint
  endpoint:
    path: /api/protocol_endpoint/connect
    method: POST
    data_selector: connection
    params:
      host_group: NVMe-vVols-Host-Group-FC
- name: Add/Update Host Group
  endpoint:
    path: /Add/Update/HostGroup
    method: POST
- name: rename_host
  endpoint:
    path: /rename/host
    method: POST
    data_selector: host_renamed
- name: iSCSI Configuration Workflow
  endpoint:
    path: /api/iscsi/configuration
    method: POST
    data_selector: workflow
    params: {}
- name: protocol_endpoint
  endpoint:
    path: /api/protocol_endpoints
    method: GET
    data_selector: endpoints
- name: Protocol Endpoints
  endpoint:
    path: /path/to/protocol/endpoints
    method: GET
    data_selector: records
    params: {}
- name: protocol_endpoint
  endpoint:
    path: /protocol-endpoints
    method: GET
    data_selector: protocol_endpoints
    params: {}
- name: vVol Datastore
  endpoint:
    path: /vmfs/volumes/
    method: GET
    data_selector: contents
- name: volume_group
  endpoint:
    path: /api/vvols
    method: GET
    data_selector: volumes
    params: {}
- name: vVol-based VM Datastores
  endpoint:
    path: /vVols/vm_datastore_structures
    method: GET
    data_selector: vVols
    params: {}
- name: Conventional VM Datastores
  endpoint:
    path: /conventional/vm_datastore_structures
    method: GET
    data_selector: conventional
    params: {}
- name: configuration_vVol
  endpoint:
    path: /services/data/vXX.X/sobjects/ConfigurationVVol
    method: GET
    data_selector: records
    params: {}
- name: data_vvol
  endpoint:
    path: /data/vVols
    method: GET
    data_selector: vVols
    params: {}
- name: VM suspension
  endpoint:
    path: /services/data/vXX.X/vm/suspend
    method: POST
    data_selector: memory_vvol
    params: {}
- name: VM snapshots
  endpoint:
    path: /services/data/vXX.X/vm/snapshot
    method: POST
    data_selector: memory_vvol
    params: {}
- name: restore_destroyed_vVol
  endpoint:
    path: /restore_destroyed_vVol
    method: POST
    data_selector: result
    params: {}
- name: overwrite_vVol
  endpoint:
    path: /overwrite_vVol
    method: POST
    data_selector: result
    params: {}
- name: vVol Binding Scenarios
  endpoint:
    path: /vvol/binding/scenarios
    method: GET
    data_selector: scenarios
- name: snapshots
  endpoint:
    path: /snapshots/vVols
    method: GET
    data_selector: snapshots
    params: {}
- name: unmanaged_snapshots
  endpoint:
    path: /unmanaged_snapshots
    method: GET
    data_selector: snapshots
    params: {}
- name: Virtual Machine vVol Details
  endpoint:
    path: /vSphere/Plugin/VirtualMachineVVolDetails
    method: GET
    data_selector: vVolDetails
- name: vVol Copy
  endpoint:
    path: /api/vvol/copy
    method: POST
    data_selector: result
    params: {}
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- The VMware appliance can only support a single integration per deployment; when
  multiple integrations are needed, more than one VMware appliance will need to be
  deployed.
- A remote plugin OVA instance may only be registered against a single vCenter instance
  or a set of vCenters that are in enhanced linked-mode.
- Note that the remote-based plugin (the Remote vSphere Plugin, versions 5.x and higher)
  is the best choice because VMware does not support local plugins (4.x plugin versions)
  on vSphere 8.0 and later.
- When performing online deployment, upgrades, queries and tasks with `puresw`, access
  to `deb.cloud-support.purestorage.com` via port 443 is required.
- The appliance will need access to each vCenter it will be registered with on ports
  8443 and 443.
- The FlashArray's that will be used with the plugin will need to be accessible via
  port 443 to the appliance.
- If your environment does not have access to Pure1 during the VMware appliance deployment
  please see the **Offline Deployment Procedure** for this step.
- In order to change the Docker networking configuration after the VMware appliance
  has been successfully deployed, a new appliance will need to be deployed.
- Please ensure that the subnet provided is not in use in your environment. This should
  be a unique and private network that only the appliance will use for communicating
  between the plugin container and the appliance networking.
- 'Reminder: The Pure Storage VMware Appliance was released to support deployments
  of Pure Storage integrations such as the Remote Plugin for the vSphere Client and
  the VM Analytics Collector.'
- Please ensure that the subnet provided is not in use in your environment.
- This should be a unique and private network that only the appliance will use for
  communicating between the plugin container and the appliance networking.
- This includes all of the in-depth User Guides for best practices and usage of the
  various integrations.
- Follow steps 1 through 9 in the Online Deployment Procedure section above.
- On step 10, select None (Offline Installation).
- If the vCenter's fully qualified domain name (FQDN) contains .local, run the following
  command from the command line of the VMware appliance to ensure you can add arrays
  properly from the plugin in vCenter.
- Requires SSH connection to the appliance using the OVA VM's DNS name or IP address
  displayed in vCenter.
- The Pure VMware appliance OVA must remain powered on after the plugin registration.
- There is a plugin upgrade ongoing. After the upgrade is complete, please run `pureplugin
  update-registration` to update the registration on vCenter(s).
- The vSphere Remote Plugin will require that the extensions are re-registered in
  order to ensure the updated extensions are pushed to the vCenter. You will need
  to run pureplugin update-registration which will require a username and password
  in order to re-register the extensions. This is required as the credentials to register
  the extensions is not stored on the appliance.
- Login with the pureuser account.
- The strong mode does not support SHA-1 as a cryptographic hash function.
- The strong mode also supports full SSL certificates as trust exchange mechanism.
- Requires setup of connected app in Pure Storage
- Pure Storage recommends using a service account to simplify auditing.
- No provisioning workflows are blocked when Pure1 is not authenticated, though intelligent
  provisioning and other insights are disabled.
- Access to Pure1 is not required from ESXi hosts or the vCenter Server
- No network access is required from ESXi or the vCenter Server to the target FlashArray
- Users can choose to create local FlashArray users or use LDAP-connected users.
- It is recommended to provision a service account for plugin access to the FlashArray.
- Using plugin managed keys is preferred if user managed keys are not required.
- It is recommended to use the plugin managed keys instead of the user managed keys.
- There is no method to remove a specific Pure1 authentication in the vSphere Plugin
  today.
- The public key that pairs with the private key used to generate it can be removed
  from Pure1.
- FQDN is always preferred for Array URL
- Virtual address can be verified from the array on Settings > Network > Subnets &
  Interfaces
- No existing storage will be affected, but the FlashArray represented by that connection
  can no longer be managed within the plugin unless it is re-authenticated.
- 'Minimum Plugin Version: Remote Plugin 5.0.0'
- 'Minimum Purity Version: 4.10'
- 'Minimum vCenter Version: 6.7U3'
- All versions of the Pure Storage plugin are compatible with vCenter RBAC.
- If further information is required outside of what is provided above, please refer
  to the references listed below for an in-depth review of RBAC.
- Certain features of the plugin do not map directly to native vSphere objects (VMs,
  datastores, folders, etc) and are therefore placed in the home screen of the plugin.
- Recommended practice is to have a local FlashArray Array Admin user to register
  the storage providers.
- To utilize replication workflows between two vCenters, they must be in Enhanced
  Linked Mode
- The replication workflows will not failover virtual machines between two unlinked
  vCenters
- When using the replication workflows with two (or more) linked vCenters, replication
  groups must not be shared between vCenters.
- A single plugin instance must be able to communicate with both vCenters in order
  to be able to get a list of the virtual machines in the replication groups, perform
  virtual machine specific tasks and track tasks/requests; a single plugin instance
  is not able to communicate with multiple unlinked vCenters.
- When using the replication workflows with one vCenter, that vCenter must be connected
  to both arrays that are replicating and access to vVol Datastores on both arrays.
- This page is designed to give a high level overview of all the array's protection
  groups.
- The format for a replication group is the name of the array followed by the name
  of the array's protection group.
- The workflows function with a single vCenter Server as well.
- Single vCenter Server can have multiple arrays connected.
- In the Name field, enter a unique name for the pod.
- In the Quota field, enter a storage limit for the pod.
- Editing the name is currently the only option.
- If a volume in the group has its own limit set, a limit cannot be set on the group
  until the volume limit is cleared.
- Datastores will appear as standalone volumes.
- If an array has volumes that aren't connected to this vCenter or the ESXi hosts
  it manages, the unrelated volume information will be obscured with ... [X] unrelated
  volumes.
- The load metric is not a live value, so it can be delayed by up to an hour, though
  usually less.
- The timestamp is set in UTC time (the time zone advertised in the Pure1 REST API).
- Tags can currently not be created, edited, or deleted with the vSphere Plugin.
- 'If tags are not appearing, it is for one of the following reasons: Pure1 is not
  authenticated with the plugin or the selected FlashArray is not in the authenticated
  Pure1 organization.'
- A key feature of the remote plugin is Role-Based Access Control (RBAC).
- There is a workaround to a vSphere limitation that enables FlashArray objects to
  have permissions assigned and used against them within vCenter.
- It is critical that the Pure Storage Plugin folder as well as the alpha-numeric
  folder names are not changed as this will break the permissions mapping between
  the plugin and the FlashArray instance.
- It is important to remove the renamed top-level plugin folder along with the duplicate
  sub-folder entries.
- The prerequisite for this article is that the vSphere Plugin has been installed
  via the Pure Storage VMware Appliance.
- The Pure Storage Plugin for the vSphere Client provides the ability to VMware users
  to have insight into and control of their Pure Storage FlashArray environment while
  directly logged into the vSphere Client.
- To view the host configuration on a specific FlashArray, select the FlashArray in
  the drop-down.
- For clustered hosts it is recommended to always put a host in a host group.
- All hosts must be in a VMware cluster - the plugin does not support creating host
  groups for ESXi hosts that are not in a cluster.
- The new host will be shown as Will be created, indicating that it was not found
  on the FlashArray and the plugin will proceed with configuring it and adding it
  to the existing host group.
- Any new name must be unique and follow FlashArray object naming conventions.
- An invalid name will be rejected.
- The plugin requires the host to be in maintenance mode.
- Deconfiguring a host does not remove the iSCSI targets from the ESXi host itself
  for the specified FlashArray.
- These actions are completely non-disruptive for existing iSCSI connections to other
  Pure Storage FlashArrays and 3rd party storage vendors.
- When creating the new host / host group objects on the FlashArray with the option,
  it is not necessary to execute the workflow separately.
- Datastore name will always be customizable as long as permissions in vCenter are
  correct, but if the FlashArray is on a Purity version less than 6.4.4, container
  name and container size will not be customizable because the default storage container
  for vVols can only be modified with a support case.
- Purity 6.4.4 introduced multiple storage containers; additional storage containers
  are customizable.
- The Pure Storage vSphere plugin has the ability to recover a destroyed vVol within
  24 hours of when the vVol was destroyed.
- There is also an integration to overwrite an existing vVol with a previous FlashArray
  snapshot of the vVol.
- The source can be either a FlashArray Snapshot or a Managed Snapshot.
- In order to be able to execute VM Undelete within the eradication timer (default
  24 hours), at minimum an array snapshot of the VM's config vVol is required.
- Pure Storage does recommend leveraging SPBM and local snapshot protection placement
  rules for vVols based VMs.
- When a vVols based VM does not have a snapshot of the Config vVol, the VM will report
  that it does not have Undelete Protection
- For these workflows, a Storage Policy is created that has Local Snapshot Protection
  placement rule sets and the FlashArray Group specified
- The VM that showed that Undelete Protection was not current had the vVol No Requirements
  Policy applied to it
- The VM storage policy is changed to the one previously covered and applied to the
  whole VM
- Once the protection group snapshot schedule triggers, we now see that the VM has
  Undelete Protection
- Looking at the VM Compliance for the Policy we see the 4 VMs that have it applied
  and that they are all in a Compliant state
- Storage policies are not applied or reviewed when registering a VM in vSphere. Please
  apply the appropriate storage policy for the VM to ensure that the VM is still protected.
- As with the standard Undelete process, the policy association is lost in vSphere
  when the VM is deleted. Ensure that the storage policy is reapplied to the VM to
  ensure that protection is still applied.
- VMFS 6 is recommended for its automatic space reclamation support.
- Protection Groups can be configured with the VMFS datastore.
- Uses Pure1 authentication for recommended FlashArray selection.
- Only VMFS volume expansions are supported within the plugin.
- Direct mounts found. This means that one or more hosts in the cluster have the datastore
  directly connected to one or more hosts instead of the host group.
- Mounted Directly to the host. This means that the datastore is connected directly
  to the host and not to the host group the host is in. While this is not specifically
  a dangerous situation, it is recommended to connect storage via the host group for
  a cluster to ensure uniform access.
- Multiple Host Groups found. This means that more than one host group has been found
  for the cluster. This can cause provisioning to be non-uniform/irregular and should
  be investigated to ensure the configuration is intended.
- Pure Storage snapshots are extremely space efficient, immutable and enable multiple
  recovery and copy options both on and off of the FlashArray.
- By default, the snapshot will be retained for 24 hours on the FlashArray after deletion
  but there is a selectable option to Eradicate the snapshot which will cause it to
  be destroyed immediately, freeing up capacity for use.
- The Eradicate operation is at the expense of that snapshot not being recoverable
  for the normal 24 hour period after deletion from the FlashArray.
- VMs must be powered off and unregistered before restoring the VMFS datastore.
- The restore operation overwrites the existing volume and resignatures it for immediate
  use.
- The plugin 'Restore from Snapshot' VMFS feature does not support encrypted VMs.
- There is a known issue in the vSphere plugin where right-clicking on a VMFS-backed
  VM will give a recovery wizard. This option should be grayed out and is not expected
  to function. This will be fixed in a future version of the vSphere plugin.
- By default, the volume and all snapshots are fully recoverable from the FlashArray
  for 24 hours after deletion.
- Prior to destroying a VMFS datastore, it is required to make sure that all VMs that
  reside on that volume have either been deleted, storage vMotioned to a different
  datastore or powered off and removed from vSphere inventory.
- Some details do not appear for all datastores (lag for instance is only for ActiveDR
  datastores).
- Protection Groups must be created and setup at the FlashArray level prior to being
  available for use in the plugin.
- If SafeMode is enabled on the array or volume, reducing protection may fail.
- UNMAP can either be run on demand or set to run on a schedule.
- It is advisable to set a schedule to run UNMAP during lower utilization times.
- This KB article assumes that the remote vSphere Plugin has been installed to vCenter,
  that one or more FlashArray(s) have been added as a connection to it, that one or
  more ESXi host(s) or clusters have been configured for use with the plugin and that
  the FlashArray(s) are on Purity version 6.4.0 or later.
- NFS traffic should only go down interfaces that are designed for this traffic.
- There can be complexity when managing a file system and all of its policies on the
  FlashArray.
- The vSphere Plugin helps simplify this process for vSphere Users.
- 'Currently there are three metrics visible: Latency, IOPS, Bandwidth.'
- To utilize replication workflows between two vCenters, they must be in Enhanced
  Linked Mode.
- The replication workflows will not failover virtual machines between two unlinked
  vCenters.
- It is not supported to use linked vCenters that are registered with the same array's
  VASA providers.
- A single plugin instance must be able to communicate with both vCenters in order
  to be able to get a list of the virtual machines in the replication groups, perform
  virtual machine specific tasks and track tasks/requests.
- A single plugin instance is not able to communicate with multiple unlinked vCenters.
- Users can failover VMs between different vCenter Servers.
- Workflows can be executed against target replication groups.
- An array needs to be selected before accessing the Replication Manager.
- Replication group format is {array-name}:{pgroup-name}.
- The information contained in this documentation is subject to change without notice.
- Management requests are issued via HTTPS and TLS 1.2 over port 8084 for the FlashArray
  VASA Provider.
- The Data Path is established over the storage fabric, meaning iSCSI or Fibre Channel.
- The VMware environment must be running vSphere Version 6.5 or a newer version in
  both ESXi hosts and vCenter.
- Pure Storage recommends running vSphere 6.7 U3 p03 or higher for various fixes and
  improvements.
- The management of VASA Provider certificates is supported by the FlashArray with
  the release of Purity//FA 5.3
- Best practice is to never use the default array admin, pureuser, with any integration
  with the FlashArray.
- The recommended practice is to have a local FlashArray Array Admin user to register
  the storage providers with.
- In the event that the vCenter is in Enhanced Linked Mode, the option to choose which
  vCenter to register the storage providers with will be given.
- Best practice is to use names that make operational sense for VASA provider.
- HTTPS is required, the controller’s IP address must be specified, and port 8084
  is required.
- Requires setup of Pure Storage PowerShell SDK
- Requires connection to vCenter Server and FlashArray
- vCenter can only have a single Active storage provider for a given storage array.
- There are a couple ways to remove a storage provider in the event that the end user
  needs to remove and re-register a Storage Provider or simply wants to remove the
  storage providers. This can be done either from the vCenter Server UI or with PowerShell
  via PowerCLI.
- The workflow is using the VASA provider ID due to inconsistent behavior when using
  the VASA provider name.
- Ensure that all hosts, vCenter servers and arrays are all synced
- The 'pure-protocol-endpoint' must not be destroyed
- Configure a syslog server for the vSphere environment
- Not meant to be Best Practices deep dives nor a comprehensive outline of all best
  practices when using vVols with Pure Storage.
- A Best Practices deep dive will be given in the future.
- Pure Storage recommends that customers running vVols upgrade to Purity//FA 6.2.10
  or higher.
- NTP must be configured the same across all ESXi hosts and vCenter Servers in the
  environment. The time and data must be configured to the current date/time.
- Configure Syslog forwarding for vSphere environment.
- Network port 8084 must be open and accessible from vCenter Servers and ESXi hosts
  to the FlashArray that will be used for vVols.
- Use Virtual Machine Hardware version 14 or higher.
- Do not run vCenter servers on vVols.
- Either configure a SPBM policy to snapshot all of the vVol VM's Config vVols or
  manually put Config vVols in a FlashArray protection group with snapshot scheduled
  enabled.
- The FlashArray Protocol Endpoint object 'pure-protocol-endpoint' must exist. The
  FlashArray admin must not rename, delete or otherwise edit the default FlashArray
  Protocol Endpoint.
- Pure Storage is working to correct this and improve this implementation in a future
  Purity release.
- 'Requirement: Register both VASA Providers, CT0 and CT1, respectively.'
- 'Recommendation: Do not use a Active Directory user to register the storage providers.'
- 'Recommendation: User a local array admin created to register the storage providers.'
- 'Recommendation: Should the FlashArray be running Purity 5.3.6 or higher, Import
  CA signed certificates to VASA-CT0 and VASA-CT1.'
- Massive improvements to vVols performance at scale and load has been released with
  the FlashArray VASA Provider 2.0.0 with Purity//FA 6.2 and 6.3
- Pure Storage's recommendation when using vVols with the FlashArray is to upgrade
  to a Purity//FA 6.2.10 or higher.
- AWS offload is not supported with storage policies.
- Do not change the default storage policy with the vVols Datastore. This could cause
  issues in the vSphere UI when provisioning to the vVols Datastore.
- Any FlashArray should be running Purity 6.1.8 or higher when using vVols before
  enabling SafeMode.
- vSphere Environment running 7.0 U1 or higher is ideal to leverage the allocated
  bitmap hint as part of VASA 3.5.
- Object count, object count, object count. Seriously, the biggest impact that enabling
  SafeMode will have is on object count.
- The use of Auto-RG for SPBM when assigning replication groups to a VM should not
  be used.
- Once a VM has a storage policy replication group assigned, VASA will be unable to
  assign a different replication group.
- Failover replication group workflows will not be able to disable replication group
  schedules.
- Environments that are frequently powering on/off VMs or vMotioning between hosts
  will have higher amounts of swap vVols pending eradication.
- When changed block tracking (CBT) is enabled the first time, this will increase
  the amount of volume snapshots pending eradication.
- vCenter Server 8.0 U3 (Build 24022515) or later required
- ESXi 8.0 U3 (Build 24022510) or later required
- Purity//FA 6.6.11 or later required for ActiveCluster + vVols to be enabled by default
- Feature support for stretched vVols is now enabled by default in Purity//FA 6.6.11
  and higher.
- In order to stretch vVols and leverage ActiveCluster vVols, upgrade to Purity//FA
  6.6.11 or later.
- Creating a stretchable container is only possible via the CLI at this time.
- The workflow for stretching a stretchable container is straightforward.
- Depending on the size of the container, replication link, and array models, the
  time it takes for the container to be in sync can vary.
- NVMe-vVols on the Pure Storage FlashArray is certified with VMware for vSphere 8.0
  U1 and later with Purity//FA 6.6.2 and later.
- The process will require ESXi hosts and vCenter Server running 8.0 U1 or later.
- FlashArray must be running Purity//FA 6.6.2 or later.
- If using both NVMe-oF and SCSI for vVols on the same ESXi host, it is strongly recommended
  to create two separate host objects for each host on the FlashArray.
- If using both VMFS and vVols with NVMe-oF on the same ESXi host, it is strongly
  recommended to create two separate host objects for each host on the FlashArray.
- When creating the new pod in the FA GUI you do not have the ability to set the quota
  on creation.
- Overall vSphere 8.0 U2 has a lot more quality of life updates in the GUI and CLI
  for NVMe vVols.
- For FlashArrays running 5.3.6 or earlier, DO NOT make this change online.
- If the ESXi host personality has not been set for a host object, this screen will
  show that and offer a button to FIX this issue.
- In order to remove a host, the plugin requires the host to be in maintenance mode.
  This requirement is a precaution to prevent accidental removal of storage for running
  virtual machines. This requirement may be adjusted or lifted in the future as more
  specific workflows are developed.
- Deconfiguring a host does not remove the iSCSI targets from the ESXi host itself
  for the specified FlashArray. Only the array-side configuration is removed. The
  removal of iSCSI target information will be included in a future release.
- These actions are completely non-disruptive for existing iSCSI connections to other
  Pure Storage FlashArrays and 3rd party storage vendors. This is due to the configuration
  changes only being applied at the individual iSCSI sessions level rather than being
  set at a global level.
- The benefits of vVols are rooted in the increased storage granularity achieved by
  implementing each vVol-based virtual disk as a separate volume on the array.
- FlashArray syntax limits volume group names to letters, numbers and dashes; arrays
  remove other characters that are valid in virtual machine names during volume group
  creation.
- The length of the volume group name is limited between 1 and 63 characters.
- In the event that the VM name is longer than 46 characters the VM name will be truncated
  as part of the volume group name.
- vVols do not change the fundamental VM architecture
- With vVol-based VMs, there is no file system, but VMware makes the structure appear
  to be the same as that of a conventional VM.
- A FlashArray creates a config vVol for each vVol-based VM.
- Arrays name config vVols by concatenating the volume group name with config-<UUID>.
- VMware enforces the 62 terabyte maximum to enable vVols to be moved to VMFS or NFS,
  both of whose maximum virtual disk size is 62 terabytes.
- At this time VMware does not support expanding a Volume that is configured with
  a SCSI controller that is enabled with sharing.
- VMware creates a vswp file for the VM’s memory image when it is swapped out and
  another for ESXi administrative purposes.
- Like all FlashArray volumes, swap vVols are thin-provisioned—they occupy no space
  until data is written to them.
- VMware destroys and immediately eradicates swap vVols from the array. (They do not
  remain in the Destroyed Volumes folder for 24 hours.)
- Deleted data and config vVols are both recoverable within 24 hours of deletion.
- Recovering a config vVol requires at least one pre-existing array-based snapshot.
- The plugin can recover a destroyed vVol within 24 hours of destruction.
- PEs greatly extend the number of vVols that can be connected to an ESXi cluster;
  each PE can have up to 16,383 vVols per host bound to it simultaneously.
- A FlashArray automatically creates a default pure-protocol-endpoint PE when its
  first VASA provider is registered.
- 'BEST PRACTICE: Use one PE per vVol container. All hosts should share the same PE
  and vVol to host bindings are host-specific, so multi-tenancy is inherently supported.'
- 'BEST PRACTICE: Configure the round robin path selection policy for PEs.'
- 'BEST PRACTICE: Leave PE queue depth limits at the default values unless performance
  problems occur.'
- Binding and unbinding is automatic. There is never a need for a VMware or FlashArray
  administrator to manually bind a vVol to an ESXi host.
- FlashArrays only bind vVols to ESXi hosts that make requests; they do not bind them
  to host groups.
- A vVol datastore is sometimes referred to as a storage container.
- With the release of Purity//FA version 6.4.1, the VASA provider now supports multiple
  storage containers.
- FlashArrays require two items to create a volume—a size and a name.
- Creating multiple containers through Pure's vSphere plugin is not currently supported
  but will be in an upcoming release of the plugin.
- Best Practice is to create and mount the vVol Datastore against the ESXi Cluster
  which would be mapped to a FlashArray Host Group.
- vVols replace LUN-based datastores formatted with VMFS.
- There is no file system on a vVol datastore, nor are vVol-based virtual disks encapsulated
  in files.
- Array-based snapshots are generally preferable, particularly for their lower performance
  impact.
- FlashArray snapshots have negligible performance impact and can be retained for
  longer periods.
- FlashArray volume names are auto-generated, but VMware tools list the snapshot name
  supplied by the VMware administrator.
- All FlashArray snapshots are crash consistent, so snapshots of vVol based-VMs that
  they host are likewise at least crash consistent.
- All FlashArray snapshots are crash consistent.
- Snapshots created with VMware tools are called managed snapshots. Snapshots created
  by external means, such the FlashArray GUI, CLI, and REST interfaces and protection
  group policies, are referred to as unmanaged.
- A source snapshot or volume must be of the same size as the target data vVol.
- Overwriting a data vVol while it is in use typically causes the application to fail
  or produce incorrect results.
- Config vVols should only be overwritten with their own snapshots.
- Memory vVols should never be overwritten.
- Undelete Protection indicates that there is a FlashArray Snapshot of the VM's Config
  vVol.
- The page allows workflows to Import a virtual disk (vVol), restore a destroyed vVol,
  or Overwrite an existing vVol.
- The Pure Storage plugin has the ability to create a new snapshot of only a vVol
  virtual disk.
- The Pure Storage vSphere plugin can recover a destroyed vVol within 24 hours.
- Integration to overwrite an existing vVol with a previous FlashArray snapshot.
- VMware requires that a VM’s config vVol and data vVols be replicated within a single
  failure domain.
- Replication policies can only be assigned to config vVols and data vVols.
- API calls for FailoverReplicationGroup and TestFailoverReplicationGroup do not register
  VMs, power them on or change networks.
- The vVols replication management APIs just make sure the VM storage is ready on
  the target site.
- Full support with SRM and vVols FA replication is GA with the release of Pure Storage's
  VASA 1.1.0 (available with Purity 5.3.6+) and VMware's Site Recovery Manager 8.3.
- This integration is certified with VMware and Pure Storage.
- This type of error is an expected outcome for a syncReplicationGroup Task. The key
  here is that the 'error' is saying that there is an 'ongoing task'. This means that
  a replication job was started and is now in progress.
- 'BEST PRACTICE: Only virtual hardware versions 11 and later are supported. If a
  VM has VMware-managed VMFS-based memory snapshots and is at virtual hardware level
  10 or earlier, delete the memory snapshots prior to migration. Upgrading the virtual
  hardware does not resolve this issue.'
- Storage vMotion can migrate VMs from VMFS, NFS, or Raw Device Mappings (RDMs) to
  vVols.
- Only VMs and virtual disks configured with vVols can be compliant. VMFS-based VMs
  are never compliant, even if their volume is on a compliant FlashArray.
- A policy can be created even if no registered vVol datastores are compatible with
  it, but it cannot be assigned to any VMs or vVols.
- Each policy’s status is either Compliant, Noncompliant, or Out-of-date.
- 'BEST PRACTICES: Pure Storage recommends assigning local snapshot policies to all
  config vVols to simplify VM restoration.'
- All FlashArray volumes are thin provisioned, so the Thin Provision virtual disk
  format should always be selected. With FlashArray volumes, there is no performance
  impact for thin provisioning.
- Best practices recommend assigning local snapshot policies to all config vVols to
  simplify VM restoration.
- All FlashArray volumes are thin provisioned, so the Thin Provision virtual disk
  format should always be selected.
- When a VM is renamed in vCenter the volume group is not automatically renamed on
  the FlashArray.
- In the event that a VM's name is changed in vCenter then the volume group name would
  need to either be updated manually or could be done via a PowerCLI or python workflow
  as well.
- Beginning in Purity 5.1.3 Managed Snapshots behavior was changed to copy the Data
  Volumes to new volumes in the Array Volume Group vs taking array based snapshots
  of the data volumes.
- As part of this update, data reduction numbers will now differ.
- Since VMware is essentially asking the array to create several identical volumes
  through VASA and the Array will oblige and dedup them appropriately.
- Which means that the more Managed Snapshots that are taken, the higher the data
  reduction on number on the Volume Group will become.
- Overall increasing the Array data reduction numbers.
- The graphs show the selected object’s performance history over time intervals from
  24 hours to 1 year.
- Some operations may fail if vVols are not properly configured
- Some objects may return nulls in deeply nested fields
- Both management interfaces must be configured on both controllers and both arrays
  with enabled and active links.
- The 'pure-protocol-endpoint' must not be destroyed. This namespace must exist for
  vVols management path to operate correctly.
- Do not run vCenter Servers on vVols
- The minimum required vSphere version is 6.5 GA release.
- vSphere 6.5 and 6.7 are both be end of life since October of 2022 and will be at
  end of technical guidance in October of 2023.
- vSphere 7.0 Update 3 released several improvements for vVols and running vVols at
  scale.
- vSphere 7.0 Update 3f should be the minimum vSphere version to target for running
  vVols with Pure Storage in your vSphere environment.
- NTP must be configured the same across all ESXi hosts and vCenter Servers in the
  environment.
- The FlashArray Protocol Endpoint object 'pure-protocol-endpoint' must exist.
- The FlashArray admin must not rename, delete or otherwise edit the default FlashArray
  Protocol Endpoint.
- Register both VASA Providers, CT0 and CT1, respectively.
- Do not use an Active Directory user to register the storage providers.
- User a local array admin created to register the storage providers.
- Should the FlashArray be running Purity 5.3.6 or higher, Import CA signed certificates
  to VASA-CT0 and VASA-CT1.
- The process of VMware taking a managed snapshot is fairly serialized.
- The Management Path is done via HTTPS and TLS 1.2 over port 8084 for the FlashArray
  VASA Provider.
- The Data Path is established over the storage fabric. Today this means iSCSI or
  Fibre Channel.
- The Control/Management Path is separate from the Data Path.
- Because the FlashArray vVol implementation uses VASA Version 3, the VMware environment
  must be running vSphere Version 6.5 or a newer version in both ESXi hosts and vCenter.
- Pure Storage does recommend running vSphere 6.7 U3 p03 or higher for the various
  fixes and improvements found in this release.
- The management of VASA Provider certificates is supported by the FlashArray with
  the release of Purity//FA 5.3 and VASA 1.1.0.
- Once the Storage Provider is successfully registered, navigate to the vCenter Server
  page, then Config and the Storage Providers tab. Confirm that the storage providers
  are online and healthy.
- A friendly name for the VASA provider should make operational sense.
- Use HTTPS for the controller’s VASA provider URL.
- Using PowerCLI script is preferable for registering multiple VASA providers.
- Requires a connection to a vCenter Server and FlashArray.
- There are a couple ways to remove a storage provider in the event that the end user
  needs to remove and re-register a Storage Provider or simply wants to remove the
  storage providers.
- Inconsistent behavior when using VASA provider name; use provider ID instead
- ActiveCluster + vVols is enabled by default in Purity//FA 6.6.11 and later.
- Support for AC+vVols shipped in Purity//FA 6.6.8, but required a support case to
  enable.
- New concepts were created In order to support both the VASA 6 specification and
  Metro Cluster Storage for vVols.
- There will be no ability or workflow to convert an existing storage container or
  the default storage container into a stretched container.
- Only vVol containers created with version 4 may be stretched
- Stretched and stretchable storage containers should not be shared between vCenter
  Servers; they must be private containers for a single vCenter Server.
- Creating a stretchable container is only possible via the CLI.
- Stretched and stretchable storage containers should not be shared between vCenter
  Servers.
- When VMware releases vSphere 8.0 GA they introduced the support for NVMe-oF with
  vVols for Fibre Channel.
- In vSphere 8.0 U1 support for NVMe-oF with vVols for TCP was released.
- Pure Storage supports both NVMe-oF with TCP and FC for vVols on the FlashArray beginning
  with Purity//FA 6.6.2.
- FlashArray running Purity//FA 6.6.2 or later.
- ESXi hosts configured to use NVMe-oF with TCP or FC.
- FlashArray configured to use NVMe-oF with TCP or FC.
- When creating the new pod in the CLI, you can set the quota for the pod.
- In the FA GUI, you do not have the ability to set the quota on creation.
- Differences in vSphere 8.0 U2 and vSphere 8.0 U1 GUI Views
- A PE is a volume of zero capacity with a special setting in its Vital Product Data
  (VPD) page that ESXi detects during a SCSI inquiry.
- Do not rename, destroy or eradicate the pure-protocol-endpoint PE on the FlashArray.
- Purity//FA Version 5.0.0 and newer versions have the VASA service as a core part
  of the Purity OS.
- Delete files from datastore is not a default—if it is not selected, the vVol is
  detached from the VM, but remains on the array.
- VMware creates memory vVols when suspending VMs and creating snapshots with memory
  state.
- In order to be able to execute PiT VM Undelete of a vVols based VM that has been
  eradicated, a FA protection group snapshot of all the VMs Data vVols, managed snapshots
  and Config vVols are required.
- In order to be able to execute a PiT VM Revert of a vVols based VM, a FA protection
  group snapshot of all vVol disks of the currently configured VM are required.
- One thing to keep in mind is that storage policies are not applied or reviewed when
  registering an VM in vSphere. As such, please apply the appropriate storage policy
  for the VM to ensure that the VM is still protected.
- The VM PiT Revert can only be executed against a VM that is powered off.
- Undelete workflow is specific to arrays running Purity//FA 6.1 and lower.
- Undelete Protection means that there is currently a FlashArray Snapshot of this
  VMs Config vVol. This is required for the Undelete workflow.
- By default, a volume on the FlashArray that is destroyed has 24 hours until it is
  eradicated.
- FlashArray snapshots are created instantaneously, have negligible performance impact,
  and initially occupy no space.
- With FlashArray based snapshots of vVols, there is no reconsolidation.
- Snapshots created with VMware tools are called managed snapshots.
- Snapshots created by external means must be managed with external tools.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Unauthorized: Ensure correct username and password are used for vSphere account.'
- '401 Unauthorized: Recheck user permissions or authentication'
- Array address is not reachable from vCenter
- 'FAILEDOVER: The replication group is in a FAILEDOVER state.'
- Pure1 is not authenticated with the plugin
- The selected FlashArray is not in the authenticated Pure1 organization.
- Pure1 is not authenticated with the plugin.
- 'Not Configured: If there is no host for the ESXi host.'
- 'Connected: If there is a correctly configured host.'
- 'Not Connected: If a host is created but is not seen as online by the FlashArray.'
- Restore button grayed out if VMs are powered on.
- 'Not Configured: This means that the ESXi host does not have a corresponding host
  on the FlashArray at all.'
- 'Not Connected: This means the ESXi host has a corresponding host object for that
  protocol on the array but the FlashArray DOES NOT see it as online.'
- Delete files from datastore is not a default—if it is not selected, the vVol is
  detached from the VM, but remains on the array.
- DO NOT DESTROY THE DEFAULT PURE-PROTOCOL-ENDPOINT
- 'Request limit exceeded: Throttle API calls or reduce frequency.'
- 'Query timeout: Break down filters or add selectivity.'
- 'Error doing ''Sync'' on replication group ''30488813-7524-3538-868d-66c8037a6d39/395a60c2-5803-40be-95b7-029b1b3ffc3e:62''.
  Reason: Error 1: Sync of the replication group is ongoing.'
- 'Unauthorized: Check your authentication settings'
- 'Not Found: Verify the endpoint path'
- 'Internal Server Error: Contact support'
- VMware ESXi 6.5 Update 3 - Build 13932383
- Recovering a VM’s config vVol results in an empty folder.
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - pureuser
  - vSphere Account
  - FlashArray
  - vVols
  - vVol
client:
  base_url: https://pure1.purestorage.com
  auth:
    type: plugin managed keys
source_metadata: null
