resources:
- name: chainlet
  endpoint:
    path: /chains
    method: GET
    data_selector: chains
- name: models
  endpoint:
    path: /api/v1/models
    method: GET
    data_selector: data
    params: {}
- name: chains
  endpoint:
    path: /api/v1/chains
    method: GET
    data_selector: data
    params: {}
- name: inference
  endpoint:
    path: /api/v1/inference
    method: POST
    data_selector: data
    params: {}
- name: management
  endpoint:
    path: /api/v1/management
    method: GET
    data_selector: data
    params: {}
- name: RemoteErrorDetail
  endpoint:
    path: /truss_chains/RemoteErrorDetail
    method: GET
    data_selector: parameters
    params: {}
- name: GenericRemoteException
  endpoint:
    path: /truss_chains/GenericRemoteException
    method: GET
    data_selector: parameters
    params: {}
- name: model_api
  endpoint:
    path: /api/v1/models
    method: GET
    data_selector: models
    params: {}
- name: training_jobs
  endpoint:
    path: /api/v1/training_jobs
    method: GET
    data_selector: jobs
    params: {}
- name: model_api
  endpoint:
    path: /api/v1/models
    method: GET
    data_selector: data
    params: {}
- name: environments
  endpoint:
    path: /deployment/environments
    method: GET
    data_selector: environments
    params: {}
- name: instance_type
  endpoint:
    path: /deployment/resources
    method: GET
    data_selector: instances
- name: autoscaling
  endpoint:
    path: /reference/management-api/deployments/autoscaling
    method: GET
    data_selector: autoscaling_config
- name: environments
  endpoint:
    path: /reference/management-api/environments
    method: GET
    data_selector: environments
- name: instance_types
  endpoint:
    path: /deployment/resources
    method: GET
    data_selector: resources
    params: {}
- name: predict
  endpoint:
    path: /predict
    method: POST
    data_selector: model input
- name: async_predict
  endpoint:
    path: /async_predict
    method: POST
    data_selector: model input
- name: sync
  endpoint:
    path: /sync/{route}
    method: GET
    data_selector: various routes
- name: autoscaling
  endpoint:
    path: /reference/management-api/deployments/autoscaling
    method: GET
    data_selector: settings
    params: {}
- name: model_prediction
  endpoint:
    path: /models/MODEL_ID/predict
    method: POST
    data_selector: content
    params: {}
- name: predict
  endpoint:
    path: /predict
    method: POST
    data_selector: model input
- name: async_predict
  endpoint:
    path: /async_predict
    method: POST
    data_selector: model input
- name: sync
  endpoint:
    path: /sync/{route}
    method: GET
    data_selector: custom server routes
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: data
    params: {}
- name: check_async_predict
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: data
    params: {}
- name: predict
  endpoint:
    path: /models/MODEL_ID/predict
    method: POST
    data_selector: content
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: request_id
- name: check_async_request
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: data
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: data
    params: {}
- name: check_async_predict_results
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: data
    params: {}
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: request_id
    params: {}
- name: check_async_result
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: request_id
    params: {}
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: ''
    params: {}
- name: async_request
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: ''
    params: {}
- name: TrainingJob
  endpoint:
    path: /training/job
    method: POST
    data_selector: job
    params: {}
- name: async_predict
  endpoint:
    path: /production/async_predict
    method: POST
    data_selector: request_id
- name: async_request_status
  endpoint:
    path: /async_request/{request_id}
    method: GET
    data_selector: model_id
- name: TrainingJob
  endpoint:
    path: /training/jobs
    method: POST
    data_selector: job
    params: {}
- name: TrainingJob
  endpoint:
    path: /api/training/jobs
    method: POST
    data_selector: job
- name: async_queue_metrics
  endpoint:
    path: /observability/async-queue-metrics
    method: GET
    data_selector: metrics
    params: {}
- name: inference_volume
  endpoint:
    path: /observability/inference-volume
    method: GET
    data_selector: metrics
    params: {}
- name: response_time
  endpoint:
    path: /observability/response-time
    method: GET
    data_selector: metrics
    params: {}
- name: request_response_size
  endpoint:
    path: /observability/request-response-size
    method: GET
    data_selector: metrics
    params: {}
- name: replicas
  endpoint:
    path: /observability/replicas
    method: GET
    data_selector: metrics
    params: {}
- name: cpu_usage_memory
  endpoint:
    path: /observability/cpu-usage-memory
    method: GET
    data_selector: metrics
    params: {}
- name: gpu_usage_memory
  endpoint:
    path: /observability/gpu-usage-memory
    method: GET
    data_selector: metrics
    params: {}
- name: model_statuses
  endpoint:
    path: /observability/health
    method: GET
    data_selector: statuses
    params: {}
- name: model_statuses
  endpoint:
    path: /status/status
    method: GET
    data_selector: statuses
- name: environment_variables
  endpoint:
    path: environment_variables
    method: SET
    data_selector: variables
    params: {}
- name: requirements
  endpoint:
    path: requirements
    method: SET
    data_selector: packages
    params: {}
- name: system_packages
  endpoint:
    path: system_packages
    method: SET
    data_selector: packages
    params: {}
- name: resources
  endpoint:
    path: resources
    method: SET
    data_selector: resource_config
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: model
  endpoint:
    path: /reference/model
    method: GET
    data_selector: resources
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params:
      model: deepseek-ai/DeepSeek-V3-0324
- name: predict_environment
  endpoint:
    path: /environments/{env_name}/predict
    method: POST
- name: predict_development
  endpoint:
    path: /development/predict
    method: POST
- name: predict_deployment
  endpoint:
    path: /deployment/{deployment_id}/predict
    method: POST
- name: async_predict_environment
  endpoint:
    path: /environments/{env_name}/async_predict
    method: POST
- name: async_predict_development
  endpoint:
    path: /development/async_predict
    method: POST
- name: async_predict_deployment
  endpoint:
    path: /deployment/{deployment_id}/async_predict
    method: POST
- name: websocket_environment
  endpoint:
    path: /environments/{env_name}/websocket
    method: WEBSOCKET
- name: websocket_development
  endpoint:
    path: /development/websocket
    method: WEBSOCKET
- name: websocket_deployment
  endpoint:
    path: /deployment/{deployment_id}/websocket
    method: WEBSOCKET
- name: async_request_status
  endpoint:
    path: /async_request/{request_id}
    method: GET
- name: cancel_async_request
  endpoint:
    path: /async_request/{request_id}
    method: DEL
- name: async_queue_status_environment
  endpoint:
    path: /environments/{env_name}/async_queue_status
    method: GET
- name: async_queue_status_development
  endpoint:
    path: /development/async_queue_status
    method: GET
- name: async_queue_status_deployment
  endpoint:
    path: /deployment/{deployment_id}/async_queue_status
    method: GET
- name: wake_production
  endpoint:
    path: /production/wake
    method: POST
- name: wake_development
  endpoint:
    path: /development/wake
    method: POST
- name: wake_deployment
  endpoint:
    path: /deployment/{deployment_id}/wake
    method: POST
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chains
  endpoint:
    path: /v1/chains
    method: GET
- name: predict_environment
  endpoint:
    path: /environments/{env_name}/predict
    method: POST
- name: predict_development
  endpoint:
    path: /development/predict
    method: POST
- name: predict_deployment
  endpoint:
    path: /deployment/{deployment_id}/predict
    method: POST
- name: async_predict_environment
  endpoint:
    path: /environments/{env_name}/async_predict
    method: POST
- name: async_predict_development
  endpoint:
    path: /development/async_predict
    method: POST
- name: async_predict_deployment
  endpoint:
    path: /deployment/{deployment_id}/async_predict
    method: POST
- name: websocket_environment
  endpoint:
    path: /environments/{env_name}/websocket
    method: WEBSOCKET
- name: websocket_development
  endpoint:
    path: /development/websocket
    method: WEBSOCKET
- name: websocket_deployment
  endpoint:
    path: /deployment/{deployment_id}/websocket
    method: WEBSOCKET
- name: async_request_status
  endpoint:
    path: /async_request/{request_id}
    method: GET
- name: cancel_async_request
  endpoint:
    path: /async_request/{request_id}
    method: DEL
- name: async_queue_status_environment
  endpoint:
    path: /environments/{env_name}/async_queue_status
    method: GET
- name: async_queue_status_development
  endpoint:
    path: /development/async_queue_status
    method: GET
- name: async_queue_status_deployment
  endpoint:
    path: /deployment/{deployment_id}/async_queue_status
    method: GET
- name: wake_production
  endpoint:
    path: /production/wake
    method: POST
- name: wake_development
  endpoint:
    path: /development/wake
    method: POST
- name: wake_deployment
  endpoint:
    path: /deployment/{deployment_id}/wake
    method: POST
- name: TrainingJob
  endpoint:
    path: /reference/training
    method: GET
    data_selector: training_jobs
    params: {}
- name: TrainingProject
  endpoint:
    path: /reference/training/project
    method: GET
    data_selector: training_projects
    params: {}
- name: base_image
  endpoint:
    path: /development/model/base-images
    method: GET
    data_selector: base_image
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: chains
  endpoint:
    path: /v1/chains
    method: GET
- name: activate_model_deployment
  endpoint:
    path: /v1/models/{model_id}/environments/{env_name}/activate
    method: POST
- name: deactivate_model_deployment
  endpoint:
    path: /v1/models/{model_id}/environments/{env_name}/deactivate
    method: POST
- name: promote_model_deployment
  endpoint:
    path: /v1/models/{model_id}/environments/{env_name}/promote
    method: POST
- name: HelloWorld
  endpoint:
    path: /development/predict
    method: POST
    data_selector: response
    params: {}
- name: build_commands
  endpoint:
    path: /build_commands
    method: POST
    data_selector: commands
    params: {}
- name: TrainingJob
  endpoint:
    path: /reference/trainingjob
    method: GET
    data_selector: records
- name: TrainingProject
  endpoint:
    path: /reference/trainingproject
    method: GET
    data_selector: records
- name: TrainingJob
  endpoint:
    path: /training/jobs
    method: POST
    data_selector: job
    params: {}
- name: TrainingProject
  endpoint:
    path: /training/projects
    method: GET
    data_selector: projects
    params: {}
- name: base_image
  endpoint:
    path: /development/model/base-images
    method: GET
    data_selector: records
- name: RunningTotalCalculator
  endpoint:
    path: /predict
    method: POST
    data_selector: result
    params: {}
- name: model
  endpoint:
    path: /development/predict
    method: POST
    data_selector: response
    params: {}
- name: hello_world_chain
  endpoint:
    path: /run_remote
    method: POST
    data_selector: output
    params: {}
- name: poem_generator_chain
  endpoint:
    path: /run_remote
    method: POST
    data_selector: output
    params: {}
- name: hello_chain
  endpoint:
    path: /hello_chain
    method: GET
    data_selector: records
    params: {}
- name: poems
  endpoint:
    path: /poems
    method: GET
    data_selector: records
    params: {}
- name: RunningTotalCalculator
  endpoint:
    path: /predict
    method: POST
    data_selector: result
- name: chain_inference
  endpoint:
    path: /development/run_remote
    method: POST
    data_selector: json
- name: Chainlet
  endpoint:
    path: /chains
    method: GET
    data_selector: chains
    params: {}
- name: MyBaseChainlet
  endpoint:
    path: /development/chain/mybasechainlet
    method: GET
    data_selector: records
    params: {}
- name: Chainlet16Core
  endpoint:
    path: /development/chain/chainlet16core
    method: GET
    data_selector: records
    params: {}
notes:
- Uses API key for authentication.
- Requires setup of connected app in api
- Requires API key for authentication
- Uses OAuth2 with refresh token — requires setup of connected app in Baseten
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Requires API key for access.
- Baseten provides scalable infrastructure for running containerized training jobs.
- Model APIs offer a fast, reliable path to production for LLM-powered features.
- Develop locally in a containerized environment that mirrors production, test confidently,
  and ship your model without surprises.
- Deploy dedicated models with full control
- Run high-performance inference
- Baseten supports multiple entry points depending on your workflow.
- Deployments can be deactivated to suspend inference execution while preserving configuration.
- Deleted deployments are purged from the dashboard but retained in usage logs.
- Truss is the open-source package you use to turn any ML model into a production-ready
  API on Baseten - without needing to learn Docker or build custom infrastructure.
- Environments provide structured management for deployments, ensuring controlled
  rollouts, stable endpoints, and autoscaling.
- Deleted environments are removed from the overview but remain in billing history.
- Deployments are automatically wrapped in a REST API.
- Scale to zero is enabled by default in the standard autoscaling config.
- Models that have not received any traffic for more than two weeks will be automatically
  deactivated.
- Production environment cannot be deleted unless the entire model is removed.
- Autoscaling settings are per deployment and inherited when promoting a model to
  production unless overridden.
- When using a streaming endpoint with cURL, use the --no-buffer flag to stream output
  as it is received.
- Async requests can remain queued for up to 72 hours and run for up to 1 hour.
- Async requests are not compatible with streaming model output.
- Baseten does not store async predict results.
- Use the --no-buffer flag to stream output as it is received.
- Baseten does not store model outputs.
- Webhook endpoints must use HTTPS.
- Secrets from the baseten workspace like API keys are referenced using SecretReference.
- Async requests can be queued for up to 72 hours and run for up to 1 hour.
- Ensure any secrets referenced via SecretReference are defined in your Baseten workspace
  settings.
- Uses OAuth2 for authentication.
- The presigned URLs expire after 7 days from generation
- These URLs are primarily intended for evaluation and testing purposes, not for long-term
  inference deployments
- This feature works with HuggingFace compatible LLMs, allowing you to easily deploy
  fine-tuned language models directly from your training checkpoints with a single
  command.
- The presigned URLs expire after 7 days from generation.
- These URLs are primarily intended for evaluation and testing purposes, not for long-term
  inference deployments.
- High CPU/memory usage may degrade performance—consider upgrading to a larger instance
  type.
- Low CPU/memory usage may indicate overprovisioning—switch to a smaller instance
  to reduce costs.
- High GPU load can slow inference—check response time metrics.
- High memory usage may cause out-of-memory failures.
- Low utilization may indicate overprovisioning—consider a smaller GPU.
- Large queue size indicates requests are queued faster than they are processed.
- To improve async throughput, increase the max replicas or adjust autoscaling concurrency.
- Every model deployment in your Baseten workspace has a status to represent its activity
  and health.
- Tracing helps diagnose performance bottlenecks but introduces minor overhead, so
  it is disabled by default.
- Recommended 1-minute interval (metrics update every 30 seconds).
- 6 requests per minute per organization
- Ensure that any SecretReference used has been set in your Baseten Workspace.
- Requires setup of a Baseten account and API key.
- Reference a secret named 'WANDB_API_KEY'
- Timeout of 30 minutes for load method after deployment.
- Development models have slightly worse performance, and have more limited scaling
  properties, so it’s highly recommended to not use these for any production use-case.
- This feature is still in beta.
- RemoteConfig does not support all the options exposed by the traditional config.yaml.
- This new framework does not support preprocess or postprocess hooks.
- Requires setup of Baseten account and API key
- To deploy Chains remotely, you also need a Baseten account
- It is handy to export your API key to the current shell session or permanently in
  your .bashrc
- Timeout of 30 minutes for load method
- Chains are designed for production in replicated remote deployments.
- Local debugging enables seamless local testing and cloud deployments.
- Large weights increase deployment size, making it slower. Consider cloud storage
  instead.
- Chains use the same inference API as models.
- Uses Python driven configuration for models
- This feature is still in beta
- Requires a Baseten account.
- Export your API key to the shell session or .bashrc
- To deploy Chains remotely, you need a Baseten account.
- Export your API key to the current shell session or permanently in your .bashrc
- Streaming outputs is useful for returning partial results to the client, before
  all data has been processed.
- Numeric data or audio/video are most efficiently transmitted as bytes.
- Other representations such as JSON or base64 encoding lose precision, add significant
  parsing overhead and increase message sizes.
- Some version combinations of msgpack and msgpack_numpy give errors, we know that
  msgpack = '>=1.0.2' and msgpack-numpy = '>=0.4.8' work.
- Chains exists to help you build multi-step, multi-model pipelines.
- Complementary to the purely local development Chains also has a 'watch' mode.
- Error handling in Chains follows the principle that the root cause bubbles up until
  the entrypoint - which returns an error response.
- It is possible to add error handling around each remote Chainlet invocation.
- Chains tries to raise the same exception class on the caller Chainlet as was raised
  in the dependency Chainlet.
- Development deployments are intended for testing and can’t scale past one replica.
- Each time you deploy to an environment, a new deployment is created.
- Stubs can be integrated into Chainlets by passing in a URL of the deployed model.
- Chains support async invocation.
- 'No run_remote implementation: Unlike regular Chainlets, EngineBuilderLLMChainlet
  does not require users to implement run_remote(). Instead, it automatically wires
  into the deployed engine’s API.'
- No run_local and watch Standard Chains support a local debugging mode and watch.
- Live-patch deployed code
- The watch command combines local development and full deployment
- Some version combinations of `msgpack` and `msgpack_numpy` give errors, we know
  that `msgpack = '>=1.0.2'` and `msgpack-numpy = '>=0.4.8'` work.
- Engine Builder models are pre-trained models optimized for specific inference tasks.
errors:
- '401 Unauthorized: Recheck API key or token expiration'
- '401 Unauthorized: Check your API key'
- '404 Not Found: The requested resource does not exist'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Check API key.'
- Exception raised when calling a remote chainlet results in an error
- API requests return a 404 error while deactivated.
- API requests return a 404 error post-deletion.
- '404 Not Found: Returned when API requests are made to deactivated or deleted deployments.'
- 'COLD_START: Time required to initialize a new replica when scaling up.'
- If traffic spikes again before the scale-down delay ends, replicas remain active.
- Models that have not received any traffic for more than two weeks will be automatically
  deactivated.
- '401 Unauthorized: Ensure correct API key is used.'
- 'RequestTimeout: The request took too long to complete.'
- '401 Unauthorized: Recheck API key'
- Errors in executing the async prediction will be included in the errors field of
  the async predict result.
- 'SERVICE_PROVIDER_NOT_FOUND: Service provider example-service-provider not found
  in ~/.trussrc'
- 'UNAUTHENTICATED_ACCESS: You have to log in to perform the request'
- 'please check the api-key you provided: Invalid API key'
- 'Service provider not found: Service provider example-service-provider not found
  in ~/.trussrc'
- 'You have to log in to perform the request: API key in ~/.trussrc is missing or
  incorrect'
- 'please check the api-key you provided: invalid API key in request header'
- 'HTTP 429 (Too Many Requests): Exceeding the limit results in this response.'
- '400: Bad request (malformed input)'
- '401: Unauthorized (invalid or missing API key)'
- '402: Payment required'
- '404: Model not found'
- '429: Rate limit exceeded'
- '500: Internal server error'
- 'HTTP 429 (Too Many Requests): Exceeding the rate limit of 6 requests per minute
  per organization.'
- 'Unauthorized: Check your API key or permissions.'
- 'ValueError: This input is too long: 100.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - SecretReference
  - hf_access_token
client:
  base_url: https://app.baseten.co
  auth:
    type: apikey
    location: header
    header_name: Authorization
source_metadata: null
