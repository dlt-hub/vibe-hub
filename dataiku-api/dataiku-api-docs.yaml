resources:
- name: DSSClient
  endpoint:
    path: /api/client
    method: GET
- name: DSSProject
  endpoint:
    path: /api/projects
    method: GET
- name: DSSDataset
  endpoint:
    path: /api/datasets
    method: GET
- name: DSSStatisticsWorksheet
  endpoint:
    path: /api/statistics
    method: GET
- name: DSSManagedFolder
  endpoint:
    path: /api/managed_folders
    method: GET
- name: DSSStreamingEndpoint
  endpoint:
    path: /api/streaming_endpoints
    method: GET
- name: DSSRecipe
  endpoint:
    path: /api/recipes
    method: GET
- name: DSSMLTask
  endpoint:
    path: /api/ml_tasks
    method: GET
- name: DSSMLflowExtension
  endpoint:
    path: /api/mlflow
    method: GET
- name: DSSCodeStudioObject
  endpoint:
    path: /api/code_studios
    method: GET
- name: DSSJob
  endpoint:
    path: /api/jobs
    method: GET
- name: DSSScenario
  endpoint:
    path: /api/scenarios
    method: GET
- name: DSSAPIService
  endpoint:
    path: /api/api_services
    method: GET
- name: DSSMeaning
  endpoint:
    path: /api/meanings
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: time_series
  endpoint:
    path: /time_series
    method: GET
    data_selector: data_points
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Google Analytics datasets
  endpoint:
    method: GET
- name: application_tiles
  endpoint:
    path: /tiles
    method: GET
- name: application_as_recipe
  endpoint:
    path: /application-as-recipe
    method: GET
- name: application_instance_home
  endpoint:
    path: /instance-home
    method: GET
- name: files_based_partitioning
  endpoint:
    path: /fs_datasets.html
    method: GET
- name: column_based_partitioning
  endpoint:
    path: /column-based-partitioning
    method: GET
- name: SQL datasets
  endpoint:
    path: /datasets
    method: GET
    data_selector: datasets
    params: {}
- name: SQL write and execution
  endpoint:
    path: /write_and_execute
    method: POST
    data_selector: execution_results
    params: {}
- name: Partitioning
  endpoint:
    path: /partition
    method: GET
    data_selector: partitions
    params: {}
- name: SQL pipelines in DSS
  endpoint:
    path: /pipelines
    method: GET
    data_selector: pipelines
    params: {}
- name: metrics
  endpoint:
    path: /metrics
    method: GET
- name: checks
  endpoint:
    path: /checks
    method: GET
- name: data_quality_rules
  endpoint:
    path: /data-quality-rules
    method: GET
- name: custom_probes_and_checks
  endpoint:
    path: /custom-probes-and-checks
    method: GET
- name: data_quality_templates
  endpoint:
    path: /data-quality-templates
    method: GET
- name: sampling_methods
  endpoint:
    path: /sampling_methods
    method: GET
- name: visualization_themes
  endpoint:
    path: /visualization/themes
    method: GET
    data_selector: themes
- name: SQL tables
  endpoint:
    path: /sql/tables
    method: GET
    data_selector: datasets
    params: {}
- name: SQL queries
  endpoint:
    path: /sql/queries
    method: GET
    data_selector: datasets
    params: {}
- name: SharePoint Document
  endpoint:
    path: /sites/{site}/drives/{drive}/items/{item}
    method: GET
- name: SharePoint List
  endpoint:
    path: /sites/{site}/lists/{list}
    method: GET
- name: managed_folder_creation
  endpoint:
    path: /managed_folders/create
    method: POST
    data_selector: folder_id
- name: managed_folder_usage
  endpoint:
    path: /managed_folders/{folder_id}/usage
    method: GET
    data_selector: usage_data
- name: files_in_folder
  endpoint:
    path: /internal/files_from_folder
    method: GET
- name: metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: records
- name: cluster_tasks
  endpoint:
    path: /internal/stats/cluster_tasks
    method: GET
    data_selector: tasks
    params: {}
- name: commits
  endpoint:
    path: /internal/stats/commits
    method: GET
    data_selector: commits
    params: {}
- name: jobs
  endpoint:
    path: /internal/stats/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: scenario_runs
  endpoint:
    path: /internal/stats/scenario_runs
    method: GET
    data_selector: runs
    params: {}
- name: object_states
  endpoint:
    path: /internal/stats/object_states
    method: GET
    data_selector: states
    params: {}
- name: ftp_datasets
  endpoint:
    path: /path/to/ftp/datasets
    method: GET
    data_selector: datasets
    params:
      writable: 'true'
- name: remote_file
  endpoint:
    path: http://HOST/stats/20140102.log
    method: GET
    data_selector: file
    params: {}
- name: managed_dataset_sql
  endpoint:
    path: /managed/dataset/sql
    method: POST
    data_selector: settings
    params:
      schema: ${projectKey}
      table_name_prefix: ${myvar1}_
      table_name_suffix: _dss
- name: managed_dataset_hdfs
  endpoint:
    path: /managed/dataset/hdfs
    method: POST
    data_selector: settings
    params:
      path_prefix: ${projectKey}/
      path_suffix: test
      table_name_prefix: ${myvar1}_
      table_name_suffix: _dss
      hive_database: ${projectKey}_dss
- name: non_managed_datasets
  endpoint:
    path: /clearing/non-managed-datasets
    method: POST
    data_selector: result
    params:
      dku.connection.externalDatasets.allowClear: 'true'
- name: SQL table dataset
  endpoint:
    path: /sql/table/dataset
    method: GET
- name: File-based dataset
  endpoint:
    path: /file-based/dataset
    method: GET
- name: cassandra_connection
  endpoint:
    path: /cassandra/connection
    method: POST
    data_selector: response
    params: {}
- name: cassandra_dataset
  endpoint:
    path: /cassandra/dataset
    method: GET
    data_selector: data
    params: {}
- name: charts_interface
  endpoint:
    path: /dss/latest/charts
    method: GET
- name: basic_charts
  endpoint:
    path: /dss/latest/charts/basic-charts
    method: GET
- name: scatter_chart
  endpoint:
    path: /scatter/charts
    method: GET
    data_selector: charts
    params: {}
- name: Boxplot
  endpoint:
    path: /other_charts/boxplot
    method: GET
- name: 2D Distribution
  endpoint:
    path: /other_charts/2d_distribution
    method: GET
- name: Lift Chart
  endpoint:
    path: /other_charts/lift_chart
    method: GET
- name: Treemap
  endpoint:
    path: /other_charts/treemap
    method: GET
- name: KPI
  endpoint:
    path: /other_charts/kpi
    method: GET
- name: Radar
  endpoint:
    path: /other_charts/radar
    method: GET
- name: Sankey
  endpoint:
    path: /other_charts/sankey
    method: GET
- name: Gauge
  endpoint:
    path: /other_charts/gauge
    method: GET
- name: common_chart_elements
  endpoint:
    path: /common/chart/elements
    method: GET
    data_selector: records
- name: reusable_dimension
  endpoint:
    path: /reusable/dimensions
    method: GET
    data_selector: dimensions
    params: {}
- name: reference_lines
  endpoint:
    path: /reference/lines
    method: GET
- name: Tableau
  endpoint:
    path: /product/plugins/tableau-hyper-export/
    method: GET
- name: Microsoft PowerBI
  endpoint:
    path: /product/plugins/microsoft-power-bi-v2
    method: GET
- name: Qlik
  endpoint:
    path: /product/plugins/qlik-qvx
    method: GET
- name: Microstrategy
  endpoint:
    path: /product/plugins/microstrategy
    method: GET
- name: inserting_recipe
  endpoint:
    path: /inserting_recipe
    method: POST
    data_selector: recipe_details
    params: {}
- name: removing_recipe
  endpoint:
    path: /removing_recipe
    method: DELETE
    data_selector: removal_details
    params: {}
- name: date_parser
  endpoint:
    path: /dss/latest/date-parser
    method: GET
    data_selector: records
- name: date_formatter
  endpoint:
    path: /dss/latest/date-formatter
    method: GET
    data_selector: records
- name: date_components_extractor
  endpoint:
    path: /dss/latest/date-components-extractor
    method: GET
    data_selector: records
- name: Split and Fold
  endpoint:
    path: /processors/split-fold.html
    method: GET
- name: Fold multiple columns
  endpoint:
    path: /processors/fold-columns-by-name.html
    method: GET
- name: Fold multiple columns by pattern
  endpoint:
    path: /processors/fold-columns-by-pattern.html
    method: GET
- name: Unfold
  endpoint:
    path: /processors/unfold.html
    method: GET
- name: Unfold an array
  endpoint:
    path: /processors/unfold-array.html
    method: GET
- name: Split and Unfold
  endpoint:
    path: /processors/split-unfold.html
    method: GET
- name: Triggered Unfold
  endpoint:
    path: /processors/triggered-unfold.html
    method: GET
- name: Geopoint converters
  endpoint:
    path: /processors/geopoint-create
    method: GET
    data_selector: records
- name: Resolve GeoIP
  endpoint:
    path: /processors/geoip
    method: GET
    data_selector: records
- name: Reverse geocoding
  endpoint:
    path: /geographic/geocoding
    method: GET
    data_selector: records
- name: Zipcode geocoding
  endpoint:
    path: /geographic/geocoding
    method: GET
    data_selector: records
- name: Change coordinates system
  endpoint:
    path: /processors/change-crs
    method: GET
    data_selector: records
- name: Compute distances between geospatial objects
  endpoint:
    path: /processors/geo-distance
    method: GET
    data_selector: records
- name: Create area around a geopoint
  endpoint:
    path: /processors/geopoint-buffer
    method: GET
    data_selector: records
- name: Extract from geo column
  endpoint:
    path: /processors/geo-info-extractor
    method: GET
    data_selector: records
- name: grouping
  endpoint:
    path: /grouping
    method: GET
    data_selector: aggregations
- name: join
  endpoint:
    path: /join
    method: POST
    data_selector: datasets
    params: {}
- name: fuzzy_join
  endpoint:
    path: /fuzzy_join
    method: POST
    data_selector: results
    params: {}
- name: Geo join recipe
  endpoint:
    path: /geo-join
    method: POST
    data_selector: results
- name: sample_filter
  endpoint:
    path: /sample/filter
    method: GET
    data_selector: records
- name: filtering
  endpoint:
    path: /filtering
    method: GET
    data_selector: records
- name: pivot_recipe
  endpoint:
    path: /pivot
    method: GET
    data_selector: records
    params: {}
- name: generate_features
  endpoint:
    path: /generate/features
    method: POST
    data_selector: records
    params: {}
- name: download_recipe
  endpoint:
    path: /download/recipe
    method: POST
    data_selector: output_folder
    params: {}
- name: List Folder Contents
  endpoint:
    path: /list/folder/contents
    method: GET
    data_selector: files
    params: {}
- name: export_recipe
  endpoint:
    path: /dynamic_recipe_repeat/export
    method: POST
    data_selector: results
- name: download_recipe
  endpoint:
    path: /dynamic_recipe_repeat/download
    method: POST
    data_selector: results
- name: Generate Recipe
  endpoint:
    path: /generate-recipe
    method: GET
    data_selector: supported_recipes
- name: extract_failed_rows
  endpoint:
    path: /extract/failed_rows
    method: POST
    data_selector: records
- name: Sharepoint folder
  endpoint:
    path: /path/to/sharepoint/folder
    method: GET
    data_selector: files
    params: {}
- name: Manual mapping dataset
  endpoint:
    path: /path/to/manual/mapping/dataset
    method: GET
    data_selector: mappings
    params: {}
- name: output_dataset_name
  endpoint:
    method: POST
    data_selector: output
    params: {}
- name: hive_recipe
  endpoint:
    path: /hive/recipes
    method: POST
    data_selector: results
    params: {}
- name: SparkSQL recipe
  endpoint:
    path: /dss/latest/sparksql
    method: POST
    data_selector: results
    params: {}
- name: dynamic_recipe_repeat
  endpoint:
    path: /dss/latest/dynamic-recipe-repeat
    method: GET
    data_selector: recipe
    params: {}
- name: storage_types
  endpoint:
    path: /storage/types
    method: GET
    data_selector: types
- name: meanings
  endpoint:
    path: /meanings
    method: GET
    data_selector: meanings
- name: data_preparation
  endpoint:
    path: /data/preparation/schema
    method: GET
    data_selector: schema
    params: {}
- name: external_dataset
  endpoint:
    path: /dss/external/dataset
    method: GET
    data_selector: schema
    params: {}
- name: managed_dataset
  endpoint:
    path: /dss/managed/dataset
    method: GET
    data_selector: schema
    params: {}
- name: Text
  endpoint: {}
  data_selector: {}
- name: Decimal
  endpoint: {}
  data_selector: {}
- name: Integer
  endpoint: {}
  data_selector: {}
- name: Boolean
  endpoint: {}
  data_selector: {}
- name: Datetime with zone
  endpoint: {}
  data_selector: {}
- name: Datetime no zone
  endpoint: {}
  data_selector: {}
- name: Date only
  endpoint: {}
  data_selector: {}
- name: Object / Array
  endpoint: {}
  data_selector: {}
- name: Natural language
  endpoint: {}
  data_selector: {}
- name: Latitude / Longitude
  endpoint: {}
  data_selector: {}
- name: Geopoint
  endpoint: {}
  data_selector: {}
- name: Geometry
  endpoint: {}
  data_selector: {}
- name: Country
  endpoint: {}
  data_selector: {}
- name: US State
  endpoint: {}
  data_selector: {}
- name: IP Address
  endpoint: {}
  data_selector: {}
- name: URL
  endpoint: {}
  data_selector: {}
- name: HTTP Query String
  endpoint: {}
  data_selector: {}
- name: User Agent
  endpoint: {}
  data_selector: {}
- name: E-Mail address
  endpoint: {}
  data_selector: {}
- name: Anthropic
  endpoint:
    path: /api/anthropic
    method: GET
    data_selector: models
    params: {}
- name: AWS Bedrock
  endpoint:
    path: /api/aws/bedrock
    method: GET
    data_selector: models
    params: {}
- name: AWS SageMaker LLM
  endpoint:
    path: /api/aws/sagemaker
    method: GET
    data_selector: models
    params: {}
- name: Azure OpenAI
  endpoint:
    path: /api/azure/openai
    method: GET
    data_selector: models
    params: {}
- name: Azure LLM
  endpoint:
    path: /api/azure/llm
    method: GET
    data_selector: models
    params: {}
- name: Cohere
  endpoint:
    path: /api/cohere
    method: GET
    data_selector: models
    params: {}
- name: Databricks Mosaic AI
  endpoint:
    path: /api/databricks/mosaic
    method: GET
    data_selector: models
    params: {}
- name: Google Vertex Generative AI
  endpoint:
    path: /api/google/vertex
    method: GET
    data_selector: models
    params: {}
- name: Mistral AI
  endpoint:
    path: /api/mistral
    method: GET
    data_selector: models
    params: {}
- name: OpenAI
  endpoint:
    path: /api/openai
    method: GET
    data_selector: models
    params: {}
- name: Snowflake Cortex
  endpoint:
    path: /api/snowflake/cortex
    method: GET
    data_selector: models
    params: {}
- name: Stability AI
  endpoint:
    path: /api/stability
    method: GET
    data_selector: models
    params: {}
- name: model_metadata
  endpoint:
    path: /model_metadata.json
    method: GET
    data_selector: model_metadata
- name: image_input
  endpoint:
    path: /llm-mesh/image-input
    method: GET
- name: image_output
  endpoint:
    path: /llm-mesh/image-output
    method: GET
- name: Trace Explorer
  endpoint:
    path: /trace-explorer
    method: GET
- name: Generalized Linear Models
  endpoint:
    path: /generalized-linear-models
    method: GET
    data_selector: models
    params: {}
- name: model_comparisons
  endpoint:
    path: /model_comparisons
    method: GET
    data_selector: model_comparisons
    params: {}
- name: worksheet
  endpoint:
    path: /worksheets
    method: GET
    data_selector: worksheets
- name: cards
  endpoint:
    path: /cards
    method: GET
    data_selector: cards
- name: Bivariate Analysis
  endpoint:
    path: /bivariate-analysis
    method: GET
    data_selector: analysis_options
- name: Histogram
  endpoint:
    path: /histogram
    method: GET
    data_selector: histogram_options
- name: Box Plot
  endpoint:
    path: /box-plot
    method: GET
    data_selector: box_plot_options
- name: Mosaic Plot
  endpoint:
    path: /mosaic-plot
    method: GET
    data_selector: mosaic_plot_options
- name: Scatter Plot
  endpoint:
    path: /scatter-plot
    method: GET
    data_selector: scatter_plot_options
- name: Summary Stats
  endpoint:
    path: /summary-stats
    method: GET
    data_selector: summary_stats_options
- name: Frequency Table
  endpoint:
    path: /frequency-table
    method: GET
    data_selector: frequency_table_options
- name: one_sample_tests
  endpoint:
    path: /one-sample-tests
    method: GET
    data_selector: tests
    params: {}
- name: two_sample_tests
  endpoint:
    path: /two-sample-tests
    method: GET
    data_selector: tests
    params: {}
- name: n_sample_tests
  endpoint:
    path: /n-sample-tests
    method: GET
    data_selector: tests
    params: {}
- name: categorical_tests
  endpoint:
    path: /categorical-tests
    method: GET
    data_selector: tests
    params: {}
- name: Stationarity and unit root tests
  endpoint:
    path: /time-series/unit-root-tests
    method: GET
    data_selector: tests
- name: Trend
  endpoint:
    path: /time-series/trend
    method: GET
    data_selector: trend_analysis
- name: Auto-correlation
  endpoint:
    path: /time-series/auto-correlation
    method: GET
    data_selector: auto_correlation_analysis
- name: prebuilt_notebooks
  endpoint:
    path: /predefined/notebooks
    method: GET
    data_selector: notebooks
- name: list_available_extensions
  endpoint:
    path: /bin/dssadmin/jupyter-nbextensions/available
    method: GET
- name: list_enabled_extensions
  endpoint:
    path: /bin/dssadmin/jupyter-nbextensions/list
    method: GET
- name: enable_extension
  endpoint:
    path: /bin/dssadmin/jupyter-nbextensions/enable/EXTENSION_NAME
    method: POST
- name: disable_extension
  endpoint:
    path: /bin/dssadmin/jupyter-nbextensions/disable/EXTENSION_NAME
    method: POST
- name: customize_extension
  endpoint:
    path: /DSS_HOME/jupyter-run/jupyter/config/nbconfig/notebook.json
    method: PATCH
- name: project_libraries
  endpoint:
    path: /home/dataiku/workspace/project-lib-versioned
    method: GET
    data_selector: files
- name: project_resources
  endpoint:
    path: /home/dataiku/workspace/project-lib-resources
    method: GET
    data_selector: files
- name: code_recipes
  endpoint:
    path: /home/dataiku/workspace/recipes
    method: GET
    data_selector: files
- name: code_notebooks
  endpoint:
    path: /home/dataiku/workspace/notebooks
    method: GET
    data_selector: files
- name: code_studio_versioned_files
  endpoint:
    path: /home/dataiku/workspace/code_studio-versioned
    method: GET
    data_selector: files
- name: code_studio_resource_files
  endpoint:
    path: /home/dataiku/workspace/code_studio-resources
    method: GET
    data_selector: files
- name: user_config_files
  endpoint:
    path: /home/dataiku/workspace/user-versioned
    method: GET
    data_selector: files
- name: user_resource_files
  endpoint:
    path: /home/dataiku/workspace/user-resources
    method: GET
    data_selector: files
- name: Code Studios
  endpoint:
    path: /project_exports_and_bundles
    method: GET
    data_selector: resources
    params: {}
- name: standard_webapps
  endpoint:
    path: /webapps/standard
    method: GET
- name: shiny_webapps
  endpoint:
    path: /webapps/shiny
    method: GET
- name: bokeh_webapps
  endpoint:
    path: /webapps/bokeh
    method: GET
- name: dash_webapps
  endpoint:
    path: /webapps/dash
    method: GET
- name: wiki_articles
  endpoint:
    path: /wikis/articles
    method: GET
- name: contacts
  endpoint:
    path: /contacts
    method: GET
    data_selector: records
    params: {}
- name: project_tags
  endpoint:
    path: /tags/project
    method: GET
    data_selector: tags
    params: {}
- name: global_tag_categories
  endpoint:
    path: /tags/global
    method: GET
    data_selector: categories
    params: {}
- name: notebook_import
  endpoint:
    path: /import/notebooks
    method: POST
    data_selector: notebooks
    params: {}
- name: notebook_export
  endpoint:
    path: /export/notebooks
    method: POST
    data_selector: notebooks
    params: {}
- name: project_access_request
  endpoint:
    path: /requests/project_access
    method: POST
    data_selector: request
    params: {}
- name: application_execution_request
  endpoint:
    path: /requests/application_execution
    method: POST
    data_selector: request
    params: {}
- name: object_sharing_request
  endpoint:
    path: /requests/object_sharing
    method: POST
    data_selector: request
    params: {}
- name: plugin_install_update_request
  endpoint:
    path: /requests/plugin_install_update
    method: POST
    data_selector: request
    params: {}
- name: code_environment_request
  endpoint:
    path: /requests/code_environment
    method: POST
    data_selector: request
    params: {}
- name: profile_upgrade_request
  endpoint:
    path: /requests/profile_upgrade
    method: POST
    data_selector: request
    params: {}
- name: dashboard_export
  endpoint:
    path: /export/dashboard
    method: GET
    data_selector: exports
    params: {}
- name: filters
  endpoint:
    path: /filters
    method: GET
    data_selector: filters
    params: {}
- name: chart
  endpoint:
    path: /insights/chart
    method: GET
    data_selector: insights
- name: dataset_table
  endpoint:
    path: /insights/dataset-table
    method: GET
    data_selector: insights
- name: model_report
  endpoint:
    path: /insights/model-report
    method: GET
    data_selector: insights
- name: managed_folder
  endpoint:
    path: /insights/managed-folder
    method: GET
    data_selector: insights
- name: jupyter_notebook
  endpoint:
    path: /insights/jupyter-notebook
    method: GET
    data_selector: insights
- name: webapp
  endpoint:
    path: /insights/webapp
    method: GET
    data_selector: insights
- name: metric
  endpoint:
    path: /insights/metric
    method: GET
    data_selector: insights
- name: data_quality
  endpoint:
    path: /insights/data-quality
    method: GET
    data_selector: insights
- name: scenarios
  endpoint:
    path: /insights/scenarios
    method: GET
    data_selector: insights
- name: wiki_article
  endpoint:
    path: /insights/wiki-article
    method: GET
    data_selector: insights
- name: workspace_settings
  endpoint:
    path: /workspaces/settings
    method: GET
- name: manage_workspace_users
  endpoint:
    path: /workspaces/users
    method: GET
- name: workspace_roles
  endpoint:
    path: /workspaces/roles
    method: GET
- name: Data Collections
  endpoint:
    path: /data-collections
    method: GET
    data_selector: datasets
    params: {}
- name: popular_datasets
  endpoint:
    path: /most-used-datasets
    method: GET
    data_selector: datasets
    params: {}
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- A plugin can only be installed by a user with administrative privileges.
- For an introduction to the exploration component of Data Science Studio, we recommend
  that you first consult the Knowledge Base.
- Users without permissions can initiate application instantiation requests.
- Application instances receive a project variable named projectRandomKey.
- Graph data structure natively represents relationships between data.
- Some of the key benefits are enhanced data relationships & insights, identifying
  unusual connections, and faster query processing.
- The Deployer is usually deployed as a dedicated node in your DSS cluster, but may
  also be run locally on a Design or Automation node.
- The DSS Automation Node provides a way to separate your DSS development and production
  environments.
- The Deployer is either available as a dedicated node or attached to a Design node.
- The Deployer location is configured by administrator for the whole instance on each
  Design node in Administration > Settings > Deployer.
- Dataiku Cloud manages the Elastic AI compute for its customers. Hence the setup
  / configuration described in the following chapters are not applicable for Dataiku
  Cloud.
- This feature is not available on Dataiku Cloud.
- Dataiku Cloud is configured to use DSS itself as a virtual metastore. This property
  cannot be customized.
- User Isolation Framework was previously called Multi-User-Security.
- If using Dataiku Cloud Stacks installation, User Isolation is automatically managed
  for you, and you do not need to follow these instructions.
- If using Dataiku Cloud, User Isolation is automatically managed for you, and you
  do not need to follow these instructions.
- Streaming features are not enabled by default in DSS. You need to manually activate
  them by going to Administration > Settings > Misc. > Other, check the “Enable streaming”
  box and restart DSS.
- Many parts of DSS support sampling data to extract subsets and/or reduce the size
  of data to process.
- If you migrated existing projects to DSS 14.0.0, please note that the charts and
  dashboards from these projects will not automatically have a theme applied; they
  will retain their previous appearance prior to the migration.
- 'Azure Blob as a filesystem-like storage comes with a few limitations: keys must
  not start with a `/`, ''files'' with names containing `/` are not supported, ''folders''
  (prefixes) `.` and `..` are not supported, a file and a folder with the same name
  are not supported, multiple successive / are not supported.'
- 'DSS uses the same filesystem-like mechanism when accessing GCS: when you specify
  a container, you can browse it to quickly find your data.'
- Use a service account if your DSS users don’t have direct access to the resources
  in GCP.
- Use OAuth2 if your DSS users have access to your GCP project and particularly to
  GCS.
- Even when the uploaded files are put on HDFS, the dataset can’t be used as Hive
  table. Use a normal HDFS dataset pointing at the same location as the dataset instead.
- SharePoint list views are not supported by the native connector.
- Default site should contain the name for an existing SharePoint site.
- Default drive should contain the name of an existing drive.
- Support for Elasticsearch 1.x and 2.x is now deprecated and will be removed in a
  future release.
- When setting up your connection, set the port to 443 and check Enable HTTPS.
- DSS offers unstructured storage handles in the form of 'Managed Folders'.
- Not all storage systems have a native concept of 'directory'.
- Dataset cannot be used as Hive table when on HDFS.
- Editable Datasets are limited to 100 000 rows and can only be modified from the
  UI.
- They can’t be edited from the dataiku Python API.
- Connection to kdb+ is Experimental and not fully supported
- You must install the 'kdb+' plugin to enable support for kdb+
- Creating FTP connections can only be done by DSS administrators (except if you use
  'personal connections')
- Using FTP passive mode is often mandatory when there is a firewall between the Data
  Science Studio server and the FTP server.
- You can use the DSS Download recipe to cache the contents from a SCP/SFTP server.
- Data will be fetched from the HTTP source each time you access this dataset in Explore
  or Charts and the sample needs to be refreshed.
- Download recipe will check the HTTP server for updates when its output folder is
  rebuilt.
- DSS can directly read data stored on HTTP or HTTPS servers using the HTTP dataset.
- This 'remote' dataset can only be used as input in DSS.
- Creating a cached version of a HTTP source with the download recipe implies creating
  a download recipe, and its associated output managed folder.
- Filesystem connections can only be created and managed by DSS administrators.
- Enabling the following option can result in unrecoverable data loss. The option
  should only be enabled if all input data for the specified connection can be fully
  cleared and deleted by DSS, without any way to recover the underlying data from
  DSS.
- Non-managed datasets are datasets that point to an existing SQL table or external
  filesystem store. It’s generally expected that non-managed (external) datasets are
  a source of truth that exist outside of DSS and should not be modified by DSS.
- Any external SQL table or non-partitioned file-based dataset can be configured to
  be repeating.
- By default a variable is created for each column of the parameters dataset.
- This capability is provided by the 'pi-system' plugin, which you need to install.
- The PI System servers you want to access must have PIWebAPI enabled.
- This plugin is Not supported
- This capability is provided by the 'google-drive' plugin, which you need to install.
- Not supported
- This capability is provided by the 'googlesheets' plugin, which you need to install.
- This capability is provided by the “google-analytics-4” plugin, which you need to
  install.
- At this point, although the preset is operational, you can’t test it yet as your
  user hasn’t authorized Dataiku to access Google Analytics on their behalf.
- Data transfer out of Dataiku Cloud is limited to 1,000 GB per month on Dataiku Cloud
  (all Dataiku Cloud editions).
- Note that this 1,000 GB limit can be lifted upon motivated customer request.
- Data Science Studio can connect to clusters running Cassandra version 2.0.1 and
  later.
- It is mandatory to configure at least one contact host.
- By default, the first 10,000 records of your dataset are selected for the sample.
- For best performance, it is recommended that you do not use samples above 200,000
  records.
- The DSS Charts Engine does not require data to fit in memory, however it stores
  its optimized format on the disk on which DSS resides.
- For large samples, you need to make sure that you have enough space on this disk
  to store your data extracts.
- 'It generally does not make sense to use the ''AVERAGE'' aggregation when creating
  subgroups of bars. Only aggregations that ''naturally stack'' should be used: SUM
  and COUNT.'
- Hexagonal binning is incompatible with live in-database processing.
- Map charts cannot be downloaded as images. To share map charts, publish them to
  a dashboard or take a screenshot of the map to generate an image of the map chart.
- Administrative maps are considered a BETA feature. You might encounter issues while
  using it and we do not guarantee that this feature will keep working the same way
  in future versions.
- Axes and measures/dimensions are independent, meaning that if you want consistency
  between the axes tick labels and the measure/dimension values you have to apply
  the same options to the axes and the measures/dimensions from their dedicated configuration
  menus.
- Editing a reusable dimension that is used in multiple charts will automatically
  update it everywhere it is used.
- Similarly, deleting a reusable dimension will remove it from all charts where it
  was used.
- Microsoft will retire the capability that DSS is using to publish datasets on PowerBI
  on October 31st, 2027.
- Flow zones are a completely optional feature.
- Zones do not define new security boundaries, they are only used for laying out the
  flow.
- Some options allow for automated schema propagation.
- Inserting a recipe can be done by right-clicking the dataset or using the More Actions
  menu.
- Removing a recipe requires that it not be at the end of the Flow and must have only
  one input dataset.
- By default, only 5 activities can be executed simultaneously on a DSS instance.
- Administrators can change this value in the Administration > Settings > Resource
  control > Concurrent jobs and activities screen.
- The graphics export feature must be setup before the Flow graph can be exported.
- Files generated are fully customizable.
- To use this feature, the graphical export library must be activated on your DSS
  instance. Please check with your administrator.
- All data goes through the DSS server but does not need to be in memory (as it is
  streamed).
- When Spark is installed, preparation recipe jobs can run on Spark.
- A subset of the preparation processors can be translated to SQL queries.
- The boundaries of the numerical range are inclusive.
- Both lower and upper boundaries are optional.
- If the column does not contain a valid numerical value for a row, this row is considered
  as being out of the range (and thus non-matching).
- The provided time zone will be used to filter dates.
- 'Date periods are calendar periods : ‘last 3 months’ will be a range that only includes
  the last 3 complete calendar months.'
- Dates are parsed into UTC timezone for server's locale.
- Ensure date columns are in valid format before operations.
- Unfolding a column with a large number of values will create a large number of columns.
- It is highly recommended not to unfold columns with more than 100 values.
- The sync recipe allows you to synchronize one dataset to another.
- DSS creates the output dataset with the same partitioning and puts a 'equals' dependency.
- DSS automatically chooses the engine to execute the synchronization.
- DSS handles inner, left outer, right outer, full outer, cross and advanced joins.
- Optionally add additional output datasets to capture unmatched rows resulting from
  your join.
- The fuzzy join recipe is dedicated to joins between two datasets when join keys
  don’t match exactly.
- If all of the join conditions are set to strict equality then a fuzzy join recipe
  will be equivalent to a regular join recipe.
- Join types (INNER, LEFT, RIGHT, FULL or CROSS) are defined for each individual join
  clause.
- All geometries are expressed in the SRID 4326.
- If no grouping key is provided, the only considered subset will be the whole input
  dataset.
- At least one order column is required.
- When two rows have the same values for both the key and order columns, the order
  between those two rows is not deterministic and can change over the different builds
  (different rows may be retrieved for the same input values and recipe settings).
- The remaining rows output is used to retrieve the rows from the original dataset
  that does not match the Top N recipe definitions.
- However, when two rows have the same value according to the key and order columns,
  some engines may retrieve both rows in the two outputs (because the computations
  for the outputs are run separately). If one needs to strictly split the input dataset
  in two, one could use the DSS engine, or a Top N recipe plus a join recipe on the
  output and the original dataset to retrieve the non-matching rows.
- Since DSS v4.1, null values are ordered in a specific way, take a look at Null values
  handling
- '4 types of filtering are available: Rules based, Formula based, SQL expression
  based, Elasticsearch query string.'
- The 'sort' recipe allows you to order a dataset.
- The output dataset must preserve the writing order.
- The recipe can have pre-filters.
- The generate features recipe only supports datasets that are compatible with Spark
  or with SQL views.
- The recipe does not support partitioned datasets.
- The recipe does not support SQL pipelines.
- Editable Datasets are limited to 100K rows.
- The first time you run a Push to editable recipe, it will copy the whole content
  of the regular dataset to the editable dataset.
- If you make changes to the content in the editable dataset, and then rerun the push
  to editable recipe, it will copy over all data that was new or changed in the original
  dataset but will preserve every modification you did in the editable dataset.
- 'The download recipe only deals with files: it does not interpret the files, and
  does not create a directly usable dataset.'
- Name collisions between downloaded files can be avoided by applying renaming rules.
- To enable a repeating recipe, make sure Enable is checked and a dataset is selected
  in the Parameters dataset dropdown.
- The Generate Recipe feature is not enabled by default.
- To generate visual recipes for multiple datasets, select all the datasets from the
  flow.
- The extract failed rows recipe allows users to create a new dataset containing the
  records that failed the Data Quality rules defined on the input dataset.
- at least one upsert key is required to identify rows
- DSS does not enforce or control the unicity of rows in the input or output dataset.
  If several rows have the same combination of values for the upsert keys, the behavior
  is undefined
- If Azure AD groups readable by is set to Everybody, all users can retrieve the DSS
  groups and their respective Azure AD mappings.
- The manual mapping dataset will override existing mappings if the DSS group is already
  mapped to Azure AD groups.
- The dataset is "streamed" and it is not required to fit in RAM.
- R recipes can read and write datasets, whatever their storage backend is.
- You can now write your R code.
- DSS allows you to build datasets by executing SQL statements.
- The SQL query recipe is the simplest recipe. Its usage should generally be preferred.
- DSS cannot detect the output schema of the output datasets.
- Automatic filling of the schema from the table can be disabled in the Advanced settings
  of the SQL script recipe.
- If you modify the code of a SQL script recipe to generate a differently-shaped table,
  you need to run the SQL script recipe.
- By default, in a SQL script recipe, all the inputs and outputs must be in the same
  database connection.
- When using the Allow SQL across connections option, you are responsible for making
  sure that all the inputs and outputs can be accessed from the selected connection.
- SQL script recipes on PostgreSQL are run using the psql client tool.
- Each Hive recipe runs in a separate Hive environment (called a metastore).
- If your input datasets are partitioned, only the partitions that are needed by the
  dependencies system are available.
- Spark is not installed. See [Setting up Spark integration](../spark/installation.html)
  for more information
- You don’t have [write access](../security/permissions.html) on the project
- You don’t have the proper [user profile](../security/user-profiles.html). Your administrator
  needs to grant you an appropriate user profile
- If the Pyspark icon is not enabled (greyed out), it can be because Spark is not
  installed.
- You don’t have write access on the project.
- You don’t have the proper user profile. Your administrator needs to grant you an
  appropriate user profile.
- 'Tier 2 support: Support for SparkR and sparklyr is covered by Tier 2 support'
- 'Limited compatibility: SparkR and sparklyr cannot be used on Cloudera, nor on Elastic
  AI / Spark-on-Kubernetes setups'
- 'Features and security: sparklyr has concurrency and security limitations'
- If the SparkSQL icon is not enabled (greyed out), it can be because Spark is not
  installed or the user doesn’t have write access on the project.
- The first time you validate a SparkSQL recipe after DSS startup, validation can
  take up to one minute. Subsequent validations are faster.
- 'In global metastore mode, validation is disabled: it is not possible to validate
  your code anymore.'
- DSS allows for one of the input datasets to be piped in the script, via the standard
  input.
- The data is expected as tab-separated CSV.
- Any SQL query recipe can be configured to be repeating.
- A variable is created for each column of the parameters dataset.
- The explanations are AI-generated and as such are subject to errors.
- DSS automatically detects the column names and types from the data.
- If the schema of the underlying table changes, DSS will automatically update the
  schema.
- An administrator needs to define connections to LLM models.
- Some models/regions require an inference profile to work on AWS’ side.
- Running local large scale Hugging Face models is a complex and very costly setup,
  and both quality and performance tend to be below proprietary LLM APIs.
- We recommend that you make your first experiments in the domain of LLMs using hosted
  LLM APIs.
- Fine-tuning can be resource-intensive, so evaluate your needs carefully.
- The LLM fine-tuning recipe is available to customers with the Advanced LLM Mesh
  add-on.
- It is strongly advised to use a GPU to fine-tune HuggingFace models.
- You need a licence with Advanced LLM Mesh to customize quotas.
- A query can match multiple quotas, in that case all matching quotas see their current
  spend incremented.
- Rate limits are enforced per LLM model and per provider.
- 'Rate Limiting is not supported for the following LLM connections: Amazon SageMaker
  LLM, Databricks Mosaic AI, Snowflake Cortex, Local Hugging Face.'
- At the heart of a Visual Agent, there is a LLM that acts as the central coordinator.
- Agent Hub requires three LLM connections to function correctly.
- It is recommended to create the Agent Hub in a separate project.
- This capability is provided by the Trace Explorer plugin, which you need to install.
- Ensure your chosen column has valid JSON. Rows with invalid JSON are skipped.
- Unlike supervised machine learning, you don’t need a target to conduct unsupervised
  machine learning
- During this optimization of hyperparameters, DSS never uses the test set, which
  must remain 'pristine' for the final evaluation of the model quality.
- Hyperparameter points that yield undefined metric values (NaN or infinity) are ignored
  during the search.
- This only applies to the 'Python in-memory' training engine
- Classes cannot be declared directly in the Models > Design tab. They should be packaged
  in a library and imported.
- Only models trained with the same preprocessing settings can be ensembled.
- It is not possible to ensemble models with different time ordering or weight parameters.
- It is not possible to ensemble models trained using K-Fold cross-test.
- It is not possible to ensemble models trained with vector or image features.
- It is not possible to ensemble partitioned models.
- It is not possible to ensemble computer vision models.
- After deployment as a saved model, it is not possible to retrain an ensemble model
  on the automation node.
- After deployment as a saved model, it is not possible to retrain an ensemble model
  if the original models have been removed from the analysis screen.
- Any placeholders starting with the keyword design will be linked to the current
  status of your visual analysis. If you change parameters in your model, DSS will
  consider that placeholder values are outdated and will replace these placeholders
  with blanks in the generated documentation.
- If you delete your visual analysis and then generate a document from a saved model,
  any placeholders starting with the keyword design will not provide any result.
- You need to upgrade the table of contents manually after a document generation
- The Model Document Generator is currently not compatible with ensemble models
- The Model Document Generator is currently not compatible with Keras nor computer
  vision models
- The Model Document Generator is currently not compatible with models imported from
  MLflow
- The Model Document Generator is not compatible with plugin algorithms
- When generating a document from a visual analysis, the model document generator
  will not display outdated design information. To fix this issue, you can either
  run a new analysis or revert the design to match the analysis
- When generating a document from a saved model version, some information may be extracted
  from the original Visual Analysis Design settings. As a result, any placeholders
  starting with design will be empty when the Visual Analysis was modified or erased
- Each part of a conditional placeholder must necessarily be in a new line
- Table and conditional placeholders are not supported on headers and footers
- Scoring engines are only used to actually predict rows.
- Some algorithms may not generate probabilities with SQL scoring.
- This scoring uses the dataiku-scoring open source Python package.
- Note that this feature is not available in all Dataiku licenses.
- The Python export feature does not support preparation scripts.
- Export to a Databricks Registry is covered by Tier 2 support.
- MLflow export is covered by Tier 2 support.
- MLflow export is covered by Tier 2 support
- The MLflow export feature does not support preparation scripts.
- This feature is not available in all Dataiku licenses. You may need to reach out
  to your Dataiku Account Manager or Customer Success Manager.
- Only Visual Machine Learning with Python backend is supported. Deep Learning and
  computer vision models are not supported.
- Only prediction models are supported. Clustering models are not supported.
- All partitions must have enough data samples to properly train a model. For example,
  in the case of a multiclass classification problem, all target classes must be represented
  in each partition.
- A saved model cannot have a mix of partitioned and unpartitioned versions.
- The SQL engine does not support the Partition dispatch mode of the scoring recipe.
- Only top-level (overall model) metrics and checks are available.
- Test set might be too small for reliable performance estimation
- Target variable distribution in test data does not match the training data distribution,
  metrics could be misleading
- Training set might be too small for robust training
- The dataset is imbalanced, metrics can be misleading
- Only a subset of all time series charts are displayed
- Too many zero target values for the MAPE metric
- The treatment variable is imbalanced
- The combination of treatment and outcome variables is imbalanced
- 'Outlier detection: The mini-cluster size threshold may be too high with respect
  to the training dataset size.'
- Feature handling configuration on columns X, Y and Z is causing N% of the test/train/input
  dataset rows to be dropped during preprocessing
- Calculation of X failed
- N remote workers failed
- N remote workers taking a long time to start
- N remote workers took more than T to start
- Requested GPU but no device is available, using CPU
- The algorithm seems to have overfit the train set, all the leaves in the model are
  pure
- Number of tree leaves is too large with respect to dataset size
- Too good to be true?
- Feature has suspiciously high importance which could be indicative of data leakage
  or overfitting
- When running a scoring recipe for a statistical time series forecasting model, time
  series that were not seen during training are ignored.
- Time series forecasting models require input time series to have a minimum length.
- Time series that are too short are ignored.
- Time series forecasting models trained with external features require one horizon
  of future values of these external features to make forecasts.
- Time series that don’t have enough future values of external features are ignored.
- Only time series with at least 2 valid time steps (i.e. with valid target value)
  can be resampled.
- Time series with less than 2 time steps are ignored in the scoring and evaluation
  recipes.
- The annotated dataset is typically used to train a supervised machine learning model.
- Labeling can also be used for human reviews in other situations, e.g. of a model’s
  predictions.
- Defining Feature Groups requires the “Manage Feature Store” permission.
- You may experience a latency of a few seconds before a Feature Group appears in
  the Feature Store and is usable.
- All prediction models trained in Dataiku may be used in a Model Comparison.
- Only evaluations from the same prediction type - binary classification, multi-class
  classification or regression - can be added to the same model comparison.
- MLflow model import feature is supported, and Dataiku tests it with a variety of
  different MLflow models.
- Different MLflow models are allowed to behave in arbitrary non-standard ways and
  to return completely different kind of data.
- Most cloud vendors impose few constraints on models, and endpoints are allowed to
  behave in arbitrary ways and most noticeably to return any kind of data.
- External Models can not be included in an API package. External Models can not be
  exported.
- Project Standards is only available with a Dataiku for Enterprise AI license.
- The Worksheet is a visual summary of exploratory data analysis (EDA) tasks.
- You do not need an API key to create a public API client.
- Instance- and project-level code libraries are available in containerized notebooks,
  without needing to rebuild the container base image.
- Code that saves files to the current working directory will not be effective in
  containerized notebooks because these files will only live in the container.
- You do not need to stop DSS to perform these actions.
- Some extensions need a specific package installed in the code env of the notebook
  to be able to run.
- Only connections compatible with ES dialect v7 and higher are supported.
- The Search notebook does not allow explicit selection of partitions.
- In Dataiku Cloud, a space-admin needs to activate the feature « Code-Studio » in
  the Launchpad (extension tab > add an extension). The feature will be ready to use
  without needing any additional requirements.
- Templates are built from blocks, each adding some configuration to the Code Studios.
  The blocks are applied sequentially, so blocks can rely on the modifications defined
  in previous blocks.
- Webapps can have more than 1 pod running a given Code Studio.
- Webapps cannot write back to the server’s filesystem.
- Only Code Studios that are published as webapps are included in exports or bundles.
- Project resources are not exported by default, and need to be manually checked.
- Contrary to Bokeh, Dash is not installed by default in DSS.
- DSS supports Dash versions 1.x (earlier 0.x versions may work but have not been
  qualified).
- URL usually requires authentication and will redirect users to DSS login.
- Webapps are normally only available for logged-in DSS users who have at least the
  “Read dashboards” permission on the DSS project if the webapp is accessible to dashboard
  users.
- It is also possible to make webapps public. When a webapp is public, being an authenticated
  user is not necessary in order to access the webapp, and no authorization control
  is performed.
- Webapps require users to be authenticated.
- This capability is provided by the 'Send emails' plugin, which you need to install.
- Interaction with SSH-based remotes requires that the UNIX account running DSS (the
  dssuser) can connect to the repository without any prompt.
- Never use `.*` as a whitelisted URL, because that allows the user to clone local
  repositories as the `dssuser`.
- DSS comes builtin with Git-based version control.
- By default, a Dataiku project uses the 'Auto' commit mode.
- It is strongly recommended to have a good understanding of the Git model and wording
  before using this feature.
- You can import multiple external repositories
- Changes to a library will be taken into account in all projects
- DSS saves the reference of the remote git repository during a Notebook import.
- DSS checks for potential conflicts when pushing or pulling Notebooks.
- Requests can be initiated by any user to gain access to projects, datasets or other
  objects.
- Code environment requests are available only on Design nodes.
- The Generate Metadata feature is not enabled by default.
- Administrators can enable it under Administration > Settings > AI Services > Enable
  Generate Steps, Recipe, and Metadata.
- In order to start using the AI Code Assistant, the feature needs to be enabled by
  an admin.
- Using a self-signed certificate may prevent the feature from being fully functional
  on all browsers.
- SQL Assistant is only supported on 'regular' SQL recipes, not on Impala, Hive or
  Spark SparkSQL
- SQL Assistant is only supported on 'regular' SQL notebooks, not on Impala, Hive
  or Spark SparkSQL
- AI Explain can generate explanations of project Flows and Code Recipes.
- Explanations are AI-generated and as such are subject to errors.
- The dashboards export feature must be setup prior to being usable.
- Filters can be applied to a dashboard page to refine its insights.
- Chart and dataset insights can be filtered themselves.
- If filters are present in the dashboard page’s filters panel but absent from your
  URL, their default state will be applied.
- Opening a generated URL with a filters query parameter ensures that the insights
  in the dashboard are filtered as they were at generation time.
- Dataiku Stories is not available in all Dataiku licenses. You may need to reach
  out to your Dataiku Account Manager or Customer Success Manager.
- Popular datasets are not detected across multiple DSS instances.
errors:
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- 'ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_ON_RECIPE_TYPE: Cannot check schema
  consistency on this kind of recipe'
- 'ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_WITH_RECIPE_CONFIG: Cannot check schema
  consistency because of recipe configuration'
- 'ERR_RECIPE_CANNOT_CHANGE_ENGINE: Not compatible with Spark'
- 'ERR_RECIPE_CANNOT_USE_ENGINE: Cannot use the selected engine for this recipe'
- 'ERR_RECIPE_ENGINE_NOT_DWH: Error in recipe engine: SQLServer is not Data Warehouse
  edition'
- 'ERR_RECIPE_INCONSISTENT_I_O: Inconsistent recipe input or output'
- 'ERR_RECIPE_SYNC_AWS_DIFFERENT_REGIONS: Error in recipe engine: Redshift and S3
  are in different AWS regions'
- 'ERR_RECIPE_PDEP_UPDATE_REQUIRED: Partition dependency update required'
- 'ERR_RECIPE_SPLIT_INVALID_COMPUTED_COLUMNS: Invalid computed column'
- 'ERR_SCENARIO_INVALID_STEP_CONFIG: Invalid scenario step configuration'
- 'ERR_SECURITY_CRUD_INVALID_SETTINGS: The user attributes submitted for a change
  are invalid'
- 'ERR_SECURITY_GROUP_EXISTS: The new requested group already exists'
- 'ERR_SECURITY_INVALID_NEW_PASSWORD: The new password is invalid'
- 'ERR_SECURITY_INVALID_PASSWORD: The password hash from the database is invalid'
- 'ERR_SECURITY_DECRYPTION_FAILED: Decryption failed due to invalid HMAC'
- 'ERR_SECURITY_MUS_USER_UNMATCHED: The DSS user is not configured to be matched onto
  a system user'
- 'ERR_SECURITY_PATH_ESCAPE: The requested file is not within any allowed directory'
- 'ERR_SECURITY_UPLOAD_WITHOUT_CONNECTION: Your site’s policy does not allow you to
  upload datasets without a valid target connection'
- 'ERR_SECURITY_USER_EXISTS: The requested user for creation already exists'
- 'ERR_SECURITY_WRONG_PASSWORD: The old password provided for password change is invalid'
- 'ERR_SPARK_FAILED_DRIVER_OOM: Spark failure: out of memory in driver'
- 'ERR_SPARK_FAILED_TASK_OOM: Spark failure: out of memory in task'
- 'ERR_SPARK_FAILED_YARN_KILLED_MEMORY: Spark failure: killed by YARN (excessive memory
  usage)'
- 'ERR_SPARK_PYSPARK_CODE_FAILED_UNSPECIFIED: Pyspark code failed'
- 'ERR_SPARK_SQL_LEGACY_UNION_SUPPORT: Your current Spark version doesn’t support
  UNION clause but only supports UNION ALL, which does not remove duplicates'
- 'ERR_SQL_CANNOT_LOAD_DRIVER: Failed to load database driver'
- 'ERR_SQL_DB_UNREACHABLE: Failed to reach database'
- 'ERR_SQL_IMPALA_MEMORYLIMIT: Impala memory limit exceeded'
- 'ERR_SQL_POSTGRESQL_TOOMANYSESSIONS: too many sessions open concurrently'
- 'ERR_SQL_TABLE_NOT_FOUND: SQL Table not found'
- 'ERR_SQL_VERTICA_TOOMANYROS: Error in Vertica: too many ROS'
- 'ERR_SQL_VERTICA_TOOMANYSESSIONS: Error in Vertica: too many sessions open concurrently'
- 'ERR_SYNAPSE_CSV_DELIMITER: Bad delimiter setup'
- 'ERR_TRANSACTION_FAILED_ENOSPC: Out of disk space'
- 'ERR_TRANSACTION_GIT_COMMMIT_FAILED: Failed committing changes'
- 'ERR_USER_ACTION_FORBIDDEN_BY_PROFILE: Your user profile does not allow you to perform
  this action'
- 'INFO_RECIPE_POTENTIAL_FAST_PATH: Potential fast path configuration'
- 'INFO_RECIPE_IMPALA_POTENTIAL_FAST_PATH: Potential Impala fast path configuration'
- 'WARN_ACTIVITY_WAITING_K8S_CONTAINERSTARTING_CLOUD: Execution container is initializing'
- 'WARN_ACTIVITY_WAITING_K8S_POD_PENDING_CLOUD: Container will start soon'
- 'WARN_ACTIVITY_WAITING_K8S_QUOTA_EXCEEDED_CLOUD: You have exceeded your RAM and
  CPU quotas'
- 'WARN_ACTIVITY_WAITING_QUEUED_CLOUD: Your activity is queued'
- 'WARN_APP_AS_RECIPE_HAS_ORPHAN_INSTANCES: Too many Application-As-Recipe instances'
- 'WARN_APP_AS_RECIPE_TOO_MANY_INSTANCES: Application-As-Recipe instance has orphan
  instances'
- 'WARN_CLUSTERS_NONE_SELECTED_GLOBAL: No default cluster selected'
- 'WARN_CLUSTERS_NONE_SELECTED_PROJECT: No cluster selected in project'
- 'WARN_CONNECTION_HDFS_ACL_SUBDIRECTORY: subdirectory ACL synchronization mode'
- 'WARN_CONNECTION_NO_HADOOP_INTERFACE: no Hadoop interface set'
- 'WARN_CONNECTION_DATABRICKS_NO_AUTOFASTWRITE: automatic fast-write disabled'
- 'WARN_CONNECTION_SNOWFLAKE_NO_AUTOFASTWRITE: automatic fast-write disabled'
- 'WARN_CONNECTION_SPARK_NO_GROUP_WITH_DETAILS_READ_ACCESS: no groups allowed to read
  connection details'
- 'WARN_FOLDER_CONNECTION_TYPE_ERROR: Invalid connection linked to a managed folder'
- 'WARN_JOBS_MAX_OVER_MAX_ACTIVITIES: Jobs - Max jobs is over max activities'
- 'WARN_JOBS_MAX_TOO_HIGH: Jobs - Max value too high'
- 'WARN_JOBS_NO_LIMIT: Jobs - No limits set'
- 'WARN_JVM_CONFIG_XMX_IN_RED_ZONE: Sub optimal Xmx value'
- 'WARN_JVM_CONFIG_KERNEL_XMX_OVER_THRESHOLD: Xmx value for kernel over threshold'
- 'WARN_MISC_AUDIT_NO_LOG4J_LOCAL_TARGET: No Log4j local target'
- 'WARN_MISC_BASE_IMAGE_FROM_OLDER_DSS_IMAGE: Custom Base Image was built with an
  older DSS Version.'
- 'WARN_MISC_BASE_IMAGE_NOT_FOUND: Custom Base Image not found'
- 'WARN_MISC_CODE_ENV_BUILTIN_MODIFIED: Built-in code env modified'
- 'WARN_MISC_CODE_ENV_DEPRECATED_INTERPRETER: Deprecated Python interpreter'
- 'WARN_MISC_CODE_ENV_IMAGE_OUT_OF_DATE: Code Environment images out of date'
- 'WARN_MISC_CODE_ENV_USES_PYSPARK: pyspark installed in a code environment'
- 'WARN_MISC_DISK_MOUNT_TYPE: non recommended filesystem type'
- 'WARN_MISC_DISK_NOEXEC_FLAG: noexec flag'
- 'WARN_MISC_DISK_ROTATIONAL: Rotational hard drives'
- 'WARN_MISC_ENVVAR_SPECIAL_CHAR: Environment variables with special characters'
- 'WARN_MISC_EVENT_SERVER_NO_TARGET: No target'
- 'WARN_MISC_JDBC_JARS_CONFLICT: JDBC drivers - some JARs are prone to version conflicts'
- 'WARN_MISC_LARGE_INTERNAL_DB: internal runtime database is too large'
- 'WARN_PROJECT_LARGE_JOB_HISTORY: Projects - Too old or too many job logs'
- 'WARN_PROJECT_LARGE_SCENARIO_HISTORY: Projects - Too old or too many scenario run
  logs'
- 'WARN_PROJECT_LARGE_STREAMING_HISTORY: Projects - Too old or too many continuous
  activities logs'
- 'WARN_RECIPE_SPARK_INDIRECT_HDFS: No direct access to read/write HDFS dataset'
- 'WARN_RECIPE_SPARK_INDIRECT_S3: No direct access to read/write S3 dataset'
- 'WARN_SPARK_NON_DISTRIBUTED_READ: Input dataset is read in a non-distributed way'
- 'WARN_SPARK_NON_DISTRIBUTED_WRITE: Output dataset is written in a non-distributed
  way'
- 'WARN_SPARK_UDFS_MAY_BE_BROKEN: Python UDFs may fail'
- 'WARN_SPARK_TASK_OOM: Some Spark tasks encountered out of memory'
- 'WARN_SPARK_TASK_DISKFULL: Some Spark tasks encountered disk space issues'
- 'WARN_SPARK_K8S_KILLED_EXECUTORS: Some Kubernetes executors were killed'
- 'WARN_SPARK_MISSING_DRIVER_TO_EXECUTOR_CONNECTIVITY: The Spark driver cannot call
  into the executors'
- 'WARN_SPARK_WITH_DATABRICKS_DATASET: Not leveraging Databricks compute'
- 'WARN_SECURITY_NO_CGROUPS: cgroups for resource control are not enabled'
- 'WARN_SECURITY_UIF_NOT_ENABLED: User Isolation Framework is not enabled'
- Undocumented error
- 'AADSTS700016: Application with identifier ‘[app id]’ was not found in the directory
  ‘[other id]’. This can happen if the application has not been installed by the administrator
  of the tenant or consented to by any user in the tenant.'
- 'AADSTS7000218: The request body must contain the following parameter: ‘client_assertion’
  or ‘client_secret’.'
- '413 error: Reverse proxies or corporate firewalls running between DSS and your
  browser may impose size limitations.'
- '403 Forbidden: Check permissions for the API or resource'
- '404 Not Found: Verify the site or document path'
- Default memory usage of a chart is limited to 150MB.
- Default number of bins on a chart is limited to 50000.
- 'Cost tracking is not supported for the following LLM providers: Amazon SageMaker
  LLM, Databricks Mosaic AI, Snowflake Cortex.'
- The SQL engine does not support the Partition dispatch mode of the scoring recipe.
auth_info:
  mentioned_objects:
  - DSSClient
  - DSSProject
  - DSSDataset
  - DSSStatisticsWorksheet
  - DSSManagedFolder
  - DSSStreamingEndpoint
  - DSSRecipe
  - DSSMLTask
  - DSSMLflowExtension
  - DSSCodeStudioObject
  - DSSJob
  - DSSScenario
  - DSSAPIService
  - DSSMeaning
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  base_url: https://doc.dataiku.com/
  headers:
    Accept: application/json
source_metadata: null
