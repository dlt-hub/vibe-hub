resources:
- name: example_bash_operator
  endpoint:
    path: /api/v1/dags/example_bash_operator
    method: GET
    data_selector: records
- name: example_bash_operator
  endpoint:
    path: /api/v1/dags/example_bash_operator
    method: GET
    data_selector: records
    params: {}
- name: example_bash_operator
  endpoint:
    path: /api/v1/dags/example_bash_operator
    method: GET
    data_selector: records
    params: {}
- name: example_bash_operator
  endpoint:
    path: /dags/example_bash_operator
    method: POST
    data_selector: tasks
    params:
      start-date: '2015-01-01'
      end-date: '2015-01-02'
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: DAG Runs
  endpoint:
    path: /dags/{dag_id}/dagRuns
    method: POST
- name: dagRuns
  endpoint:
    path: /dags/{dag_id}/dagRuns
    method: POST
    data_selector: null
    params:
      logical_date: null
- name: task_instance
  endpoint:
    path: /api/v1/task_instances
    method: GET
    data_selector: records
    params: {}
- name: dag_run
  endpoint:
    path: /api/v1/dag_runs
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: api-server
  endpoint:
    path: /api-server
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
- name: backfill
  endpoint:
    path: /backfill
    method: GET
- name: cheat-sheet
  endpoint:
    path: /cheat-sheet
    method: GET
- name: config
  endpoint:
    path: /config
    method: GET
- name: connections
  endpoint:
    path: /connections
    method: GET
- name: dag-processor
  endpoint:
    path: /dag-processor
    method: GET
- name: dags
  endpoint:
    path: /dags
    method: GET
- name: db
  endpoint:
    path: /db
    method: GET
- name: db-manager
  endpoint:
    path: /db-manager
    method: GET
- name: info
  endpoint:
    path: /info
    method: GET
- name: jobs
  endpoint:
    path: /jobs
    method: GET
- name: kerberos
  endpoint:
    path: /kerberos
    method: GET
- name: plugins
  endpoint:
    path: /plugins
    method: GET
- name: pools
  endpoint:
    path: /pools
    method: GET
- name: providers
  endpoint:
    path: /providers
    method: GET
- name: rotate-fernet-key
  endpoint:
    path: /rotate-fernet-key
    method: GET
- name: scheduler
  endpoint:
    path: /scheduler
    method: GET
- name: standalone
  endpoint:
    path: /standalone
    method: GET
- name: tasks
  endpoint:
    path: /tasks
    method: GET
- name: triggerer
  endpoint:
    path: /triggerer
    method: GET
- name: variables
  endpoint:
    path: /variables
    method: GET
- name: version
  endpoint:
    path: /version
    method: GET
- name: dags
  endpoint:
    path: /dags
    method: GET
- name: jobs
  endpoint:
    path: /jobs
    method: GET
- name: dag_runs
  endpoint:
    path: /dags/{dag_id}/dag_runs
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
    data_selector: assets
    params:
      limit: 50
      offset: 0
- name: backfills
  endpoint:
    path: /backfills
    method: GET
    data_selector: backfills
    params:
      limit: 50
      offset: 0
- name: Get Dag Run
  endpoint:
    path: /get/api/v2/auth/refresh
    method: GET
- name: Delete Dag Run
  endpoint:
    path: /delete/api/v2/auth/refresh
    method: DELETE
- name: Patch Dag Run
  endpoint:
    path: /patch/api/v2/auth/refresh
    method: PATCH
- name: Get Upstream Asset Events
  endpoint:
    path: /get/api/v2/auth/refresh
    method: GET
- name: Clear Dag Run
  endpoint:
    path: /clear/api/v2/auth/refresh
    method: POST
- name: Get Dag Runs
  endpoint:
    path: /get/api/v2/auth/refresh
    method: GET
- name: Trigger Dag Run
  endpoint:
    path: /trigger/api/v2/auth/refresh
    method: POST
- name: Get Event Log
  endpoint:
    path: /api/v2/event_log
    method: GET
    data_selector: null
    params:
      event_log_id: required
- name: Get Dag Source
  endpoint:
    path: /api/v2/dag_source
    method: GET
    data_selector: null
    params:
      dag_id: required
- name: Get Dag Stats
  endpoint:
    path: /api/v2/dag_stats
    method: GET
    data_selector: null
    params:
      dag_ids: null
- name: Get Config
  endpoint:
    path: /api/v2/config
    method: GET
    data_selector: null
    params:
      section: null
- name: Get Dag
  endpoint:
    path: /api/v2/dag
    method: GET
    data_selector: null
    params:
      dag_id: required
- name: event_logs
  endpoint:
    path: /get/event_logs
    method: GET
    data_selector: records
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: extra_links
  endpoint:
    path: /get/extra_links
    method: GET
    data_selector: records
    params:
      map_index: -1
- name: task_instance
  endpoint:
    path: /get/task_instance
    method: GET
    data_selector: records
    params: {}
- name: mapped_task_instances
  endpoint:
    path: /get/mapped_task_instances
    method: GET
    data_selector: records
    params:
      limit: 50
      offset: 0
      order_by:
      - map_index
- name: api-server
  endpoint:
    path: /api-server
    method: GET
- name: assets
  endpoint:
    path: /assets
    method: GET
- name: backfill
  endpoint:
    path: /backfill
    method: GET
- name: cheat-sheet
  endpoint:
    path: /cheat-sheet
    method: GET
- name: config
  endpoint:
    path: /config
    method: GET
- name: connections
  endpoint:
    path: /connections
    method: GET
- name: dag-processor
  endpoint:
    path: /dag-processor
    method: GET
- name: dags
  endpoint:
    path: /dags
    method: GET
- name: db
  endpoint:
    path: /db
    method: GET
- name: db-manager
  endpoint:
    path: /db-manager
    method: GET
- name: info
  endpoint:
    path: /info
    method: GET
- name: jobs
  endpoint:
    path: /jobs
    method: GET
- name: kerberos
  endpoint:
    path: /kerberos
    method: GET
- name: plugins
  endpoint:
    path: /plugins
    method: GET
- name: pools
  endpoint:
    path: /pools
    method: GET
- name: providers
  endpoint:
    path: /providers
    method: GET
- name: rotate-fernet-key
  endpoint:
    path: /rotate-fernet-key
    method: GET
- name: scheduler
  endpoint:
    path: /scheduler
    method: GET
- name: standalone
  endpoint:
    path: /standalone
    method: GET
- name: tasks
  endpoint:
    path: /tasks
    method: GET
- name: triggerer
  endpoint:
    path: /triggerer
    method: GET
- name: variables
  endpoint:
    path: /variables
    method: GET
- name: version
  endpoint:
    path: /version
    method: GET
- name: task_instance
  endpoint:
    path: /api/v2/task_instances
    method: GET
    data_selector: records
    params:
      dag_id: required
      dag_run_id: required
- name: variables
  endpoint:
    path: /airflow/variables
    method: GET
    data_selector: records
- name: import_errors
  endpoint:
    path: /import_errors
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: jobs
  endpoint:
    path: /jobs
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: providers
  endpoint:
    path: /providers
    method: GET
    params:
      limit: 50
      offset: 0
- name: pools
  endpoint:
    path: /pools
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: assets
  endpoint:
    path: /assets
    method: GET
    data_selector: assets
- name: backfills
  endpoint:
    path: /backfills
    method: GET
    data_selector: backfills
- name: get_tasks
  endpoint:
    path: /get/tasks
    method: GET
    data_selector: null
    params:
      order_by: task_id
- name: get_variable
  endpoint:
    path: /get/variable
    method: GET
    data_selector: null
    params: {}
- name: post_variable
  endpoint:
    path: /post/variable
    method: POST
    data_selector: null
    params: {}
- name: get_health
  endpoint:
    path: /get/health
    method: GET
    data_selector: null
    params: {}
- name: get_version
  endpoint:
    path: /get/version
    method: GET
    data_selector: null
    params: {}
- name: get_dag_run
  endpoint:
    path: /dag_runs/{dag_id}/{dag_run_id}
    method: GET
- name: delete_dag_run
  endpoint:
    path: /dag_runs/{dag_id}/{dag_run_id}
    method: DELETE
- name: patch_dag_run
  endpoint:
    path: /dag_runs/{dag_id}/{dag_run_id}
    method: PATCH
- name: get_upstream_asset_events
  endpoint:
    path: /dag_runs/{dag_id}/{dag_run_id}/upstream_asset_events
    method: GET
- name: clear_dag_run
  endpoint:
    path: /dag_runs/{dag_id}/{dag_run_id}/clear
    method: POST
- name: get_dag_runs
  endpoint:
    path: /dag_runs/{dag_id}
    method: GET
- name: trigger_dag_run
  endpoint:
    path: /dag_runs/{dag_id}/trigger
    method: POST
- name: event_log
  endpoint:
    path: /get/event_log
    method: GET
    data_selector: 'null'
    params:
      event_log_id: integer
- name: dag_bundle_config_list
  endpoint:
    path: /dags-folder
    method: GET
    data_selector: records
- name: dag_bundle_storage_path
  endpoint:
    path: /dag_bundle_storage_path
    method: GET
    data_selector: records
- name: event_logs
  endpoint:
    path: /event_logs
    method: GET
    data_selector: records
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: extra_links
  endpoint:
    path: /extra_links
    method: GET
    data_selector: records
    params:
      map_index: -1
- name: task_instance
  endpoint:
    path: /task_instance
    method: GET
    data_selector: records
    params:
      map_index: -1
- name: mapped_task_instances
  endpoint:
    path: /mapped_task_instances
    method: GET
    data_selector: records
    params:
      limit: 50
      offset: 0
      order_by:
      - map_index
- name: task_instance_dependencies
  endpoint:
    path: /task_instance_dependencies
    method: GET
    data_selector: records
    params:
      map_index: -1
- name: task_instance_tries
  endpoint:
    path: /task_instance_tries
    method: GET
    data_selector: records
    params:
      map_index: -1
- name: task_instances
  endpoint:
    path: /task_instances
    method: GET
    data_selector: records
    params: {}
- name: bulk_task_instances
  endpoint:
    path: /task_instances/bulk
    method: POST
    data_selector: records
    params: {}
- name: task_instance_try_details
  endpoint:
    path: /task_instances/{dag_id}/{dag_run_id}/{task_id}/{task_try_number}
    method: GET
    data_selector: records
    params: {}
- name: mapped_task_instance_try_details
  endpoint:
    path: /task_instances/{dag_id}/{dag_run_id}/{task_id}/{task_try_number}/{map_index}
    method: GET
    data_selector: records
    params: {}
- name: clear_task_instances
  endpoint:
    path: /task_instances/clear/{dag_id}
    method: POST
    data_selector: records
    params: {}
- name: log
  endpoint:
    path: /task_instances/{dag_id}/{dag_run_id}/{task_id}/logs/{try_number}
    method: GET
    data_selector: records
    params: {}
- name: external_log_url
  endpoint:
    path: /task_instances/{dag_id}/{dag_run_id}/{task_id}/external_log_url/{try_number}
    method: GET
    data_selector: records
    params: {}
- name: update_hitl_detail
  endpoint:
    path: /hitl/{dag_id}/{dag_run_id}/{task_id}/{map_index}
    method: PUT
    data_selector: records
    params: {}
- name: get_hitl_detail
  endpoint:
    path: /hitl/{dag_id}/{dag_run_id}/{task_id}/{map_index}
    method: GET
    data_selector: records
    params: {}
- name: get_hitl_details
  endpoint:
    path: /getHitlDetails
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - ti_id
- name: get_import_errors
  endpoint:
    path: /getImportErrors
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: get_jobs
  endpoint:
    path: /getJobs
    method: GET
    params:
      limit: 50
      offset: 0
      order_by:
      - id
- name: get_plugins
  endpoint:
    path: /getPlugins
    method: GET
    params:
      limit: 50
      offset: 0
- name: get_providers
  endpoint:
    path: /getProviders
    method: GET
    params:
      limit: 50
      offset: 0
- name: get_xcom_entries
  endpoint:
    path: /getXcomEntries
    method: GET
    params:
      limit: 50
      offset: 0
- name: apache-airflow-core
  endpoint:
    path: /apache-airflow-core
    method: GET
- name: apache-airflow-task-sdk
  endpoint:
    path: /apache-airflow-task-sdk
    method: GET
- name: apache-airflow-providers-*
  endpoint:
    path: /apache-airflow-providers-*
    method: GET
- name: apache-airflow
  endpoint:
    path: /apache-airflow
    method: GET
- name: arangodb
  endpoint:
    path: /services/data/vXX.X/sobjects/ArangoDB
    method: GET
    data_selector: records
- name: celery
  endpoint:
    path: /services/data/vXX.X/sobjects/Celery
    method: GET
    data_selector: records
- name: docker
  endpoint:
    path: /services/data/vXX.X/sobjects/Docker
    method: GET
    data_selector: records
- name: elasticsearch
  endpoint:
    path: /services/data/vXX.X/sobjects/Elasticsearch
    method: GET
    data_selector: records
- name: mysql
  endpoint:
    path: /services/data/vXX.X/sobjects/MySQL
    method: GET
    data_selector: records
- name: get_tasks
  endpoint:
    path: /get/tasks
    method: GET
    data_selector: null
    params:
      order_by: task_id
- name: get_variable
  endpoint:
    path: /get/variable
    method: GET
    data_selector: null
    params:
      variable_key: string
- name: delete_variable
  endpoint:
    path: /delete/variable
    method: DELETE
    data_selector: null
    params:
      variable_key: string
- name: get_variables
  endpoint:
    path: /get/variables
    method: GET
    data_selector: null
    params:
      limit: 50
      offset: 0
- name: post_variable
  endpoint:
    path: /post/variable
    method: POST
    data_selector: null
- name: refresh_auth
  endpoint:
    path: /refresh
    method: GET
    data_selector: null
- name: database_migrations
  endpoint:
    path: /migrations
    method: GET
    data_selector: migrations
    params: {}
- name: datasource
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: results
    params: {}
- name: dag
  endpoint:
    path: /dags
    method: GET
- name: datasource_list
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: datasources
    params: {}
- name: wikipedia_schema
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: columns
    params: {}
- name: wikipedia_count
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: count
    params: {}
- name: email
  endpoint:
    path: /email
    method: GET
    data_selector: records
    params: {}
- name: datasource
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: records
    params: {}
- name: dag_processor_child_process_log_directory
  endpoint:
    path: /services/data/vXX.X/sobjects/DagProcessorChildProcessLogDirectory
    method: GET
    data_selector: records
- name: dag_processor_log_format
  endpoint:
    path: /services/data/vXX.X/sobjects/DagProcessorLogFormat
    method: GET
    data_selector: records
- name: log_filename_template
  endpoint:
    path: /services/data/vXX.X/sobjects/LogFilenameTemplate
    method: GET
    data_selector: records
- name: remote_logging
  endpoint:
    path: /services/data/vXX.X/sobjects/RemoteLogging
    method: GET
    data_selector: records
- name: DruidOperator
  endpoint:
    path: /api/airflow/providers/apache/druid/operators/druid/index.html
    method: GET
    data_selector: json_index_file
    params: {}
- name: smtp_host
  endpoint:
    path: AIRFLOW__SMTP__SMTP_HOST
    method: GET
    data_selector: string
- name: smtp_mail_from
  endpoint:
    path: AIRFLOW__SMTP__SMTP_MAIL_FROM
    method: GET
    data_selector: string
- name: smtp_port
  endpoint:
    path: AIRFLOW__SMTP__SMTP_PORT
    method: GET
    data_selector: integer
- name: smtp_retry_limit
  endpoint:
    path: AIRFLOW__SMTP__SMTP_RETRY_LIMIT
    method: GET
    data_selector: integer
- name: smtp_ssl
  endpoint:
    path: AIRFLOW__SMTP__SMTP_SSL
    method: GET
    data_selector: boolean
- name: smtp_starttls
  endpoint:
    path: AIRFLOW__SMTP__SMTP_STARTTLS
    method: GET
    data_selector: boolean
- name: smtp_timeout
  endpoint:
    path: AIRFLOW__SMTP__SMTP_TIMEOUT
    method: GET
    data_selector: integer
- name: DruidOperator
  endpoint:
    path: /api/airflow/providers/apache/druid/operators/druid
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: Batch Ingestion
- name: DruidOperator
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: ''
      ingestion_type: ''
- name: DruidOperator
  endpoint:
    path: /druid/operator
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: ingestion_type
- name: DruidOperator
  endpoint:
    path: /_api/airflow/providers/apache/druid/operators/druid
    method: POST
- name: airflow-core
  endpoint:
    path: /apache-airflow-core
    method: GET
- name: airflow-task-sdk
  endpoint:
    path: /apache-airflow-task-sdk
    method: GET
- name: apache-airflow-providers
  endpoint:
    path: /apache-airflow-providers-*
    method: GET
- name: Druid Ingestion
  endpoint:
    path: /druid/ingestion
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: Batch Ingestion
- name: DruidOperator
  endpoint:
    path: json_index.json
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: ''
      ingestion_type: ''
- name: DruidOperator
  endpoint:
    params:
      json_index_file: json_index.json
- name: common-compat
  endpoint:
    path: apache-airflow[common-compat]
    method: INSTALL
    data_selector: compatibility code for old Airflow
- name: common-io
  endpoint:
    path: apache-airflow[common-io]
    method: INSTALL
    data_selector: Core IO Operators
- name: common-messaging
  endpoint:
    path: apache-airflow[common-messaging]
    method: INSTALL
    data_selector: Core Messaging Operators
- name: common-sql
  endpoint:
    path: apache-airflow[common-sql]
    method: INSTALL
    data_selector: Core SQL Operators
- name: ftp
  endpoint:
    path: apache-airflow[ftp]
    method: INSTALL
    data_selector: FTP hooks and operators
- name: http
  endpoint:
    path: apache-airflow[http]
    method: INSTALL
    data_selector: HTTP hooks, operators and sensors
- name: sftp
  endpoint:
    path: apache-airflow[sftp]
    method: INSTALL
    data_selector: SFTP hooks, operators and sensors
- name: database_migrations
  endpoint:
    path: /migrate
    method: GET
    data_selector: migrations
    params: {}
- name: druid_ingest
  endpoint:
    path: /druid/ingest
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_conn_id
- name: datasource
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: datasources
    params: {}
- name: DruidOperator
  endpoint:
    path: druidOperator
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
- name: druid_ingest
  endpoint:
    path: druid_ingest
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
- name: list_datasources
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: ''
    params:
      sql: SELECT DISTINCT datasource FROM sys.segments WHERE is_published = 1
- name: describe_wikipedia
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: ''
    params:
      sql: SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME
        = 'wikipedia'
- name: select_count_from_datasource
  endpoint:
    path: /druid/v2/sql/
    method: POST
    data_selector: ''
    params:
      sql: SELECT COUNT(*) FROM sys.segments WHERE datasource = 'wikipedia'
- name: DruidOperator
  endpoint:
    path: /druid/operator
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: Druid Connection ID
- name: DruidOperator
  endpoint:
    path: /druid/operator
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_conn_id
- name: json_index_file
  endpoint:
    path: apache/druid/tests/system/apache/druid/example_druid_dag.py
    method: POST
    data_selector: JSON_INDEX_STR
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: Batch Ingestion
- name: DruidOperator
  endpoint:
    path: /_api/airflow/providers/apache/druid/operators/druid
    method: GET
    data_selector: example_content
    params: {}
- name: DruidOperator
  endpoint:
    path: /_api/airflow/providers/apache/druid/operators/druid/index.html
    method: GET
    data_selector: spec
    params: {}
- name: DruidOperator
  endpoint:
    path: /druid/ingest
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: '{{ conn.druid_ingest_conn_id }}'
      ingestion_type: '{{ ingestion_type }}'
- name: list_datasources
  endpoint:
    sql: SELECT DISTINCT datasource FROM sys.segments WHERE is_published = 1
- name: describe_wikipedia
  endpoint:
    sql: SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME
      = 'wikipedia'
- name: select_count_from_datasource
  endpoint:
    sql: SELECT COUNT(*) FROM sys.segments WHERE datasource = 'wikipedia'
- name: DruidOperator
  endpoint:
    path: /druid/operator
    method: POST
    data_selector: datasource_prd
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: Batch Ingestion
- name: DruidOperator
  endpoint:
    path: /_api/airflow/providers/apache/druid/operators/druid/index.html
    method: GET
    data_selector: json_index_file
- name: DruidOperator
  endpoint:
    path: /druid
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: ingestion_type
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
- name: DruidOperator
  endpoint:
    path: /druid_operator
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_conn_id
      ingestion_type: Batch Ingestion
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params: {}
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params:
      filepath: filepath
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: file_or_folder
    params: {}
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1
    method: GET
    data_selector: file
    params:
      filepath: path/to/file
- name: json_index_file
  endpoint:
    path: /json_index.json
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1/
    method: GET
    data_selector: fileStatus
    params:
      filepath: ''
- name: druid_index
  endpoint:
    path: /druid/ingestion
    method: POST
    data_selector: spec
    params:
      json_index_file: json_index.json
      druid_ingest_conn_id: druid_ingest_connection
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params: {}
- name: DruidOperator
  endpoint:
    path: /druid/ingest
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
- name: json_index
  endpoint:
    path: /example_druid_dag.py
    method: POST
    data_selector: json_index_file
    params: {}
- name: DruidOperator
  endpoint:
    path: /druid/operator
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
- name: DruidOperator
  endpoint:
    path: /submit_job
    method: POST
    data_selector: json_index_file
    params:
      druid_ingest_conn_id: druid_ingest_conn_id
- name: list_datasources
  endpoint:
    path: /sys/segments
    method: GET
    data_selector: datasource
    params:
      is_published: 1
- name: describe_wikipedia
  endpoint:
    path: /INFORMATION_SCHEMA/COLUMNS
    method: GET
    data_selector: COLUMN_NAME, DATA_TYPE
    params:
      TABLE_NAME: wikipedia
- name: select_count_from_datasource
  endpoint:
    path: /sys/segments
    method: GET
    data_selector: COUNT
    params:
      datasource: wikipedia
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1/
    method: GET
    data_selector: file
    params:
      filepath: path/to/file/or/folder
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params:
      filepath: parameter
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1/
    method: GET
    data_selector: fileStatus
    params:
      filepath: path/to/file
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1
    method: GET
    data_selector: file
    params:
      filepath: path/to/file
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params:
      filepath: required
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
- name: WebHdfsSensor
  endpoint:
    path: /webhdfs/v1/
    method: GET
    data_selector: fileStatus
    params:
      filepath: ''
- name: WebHdfsSensor
  endpoint:
    path: /_api/airflow/providers/apache/hdfs/sensors/web_hdfs
    method: GET
    data_selector: records
    params: {}
- name: WebHdfsSensor
  endpoint:
    path: /api/v1/webhdfs
    method: GET
    data_selector: file
    params:
      filepath: path/to/file
- name: campaign
  endpoint:
    path: /v10/customers/{customerId}/campaigns
    method: GET
    data_selector: results
    params: {}
- name: ad_group
  endpoint:
    path: /v10/customers/{customerId}/adGroups
    method: GET
    data_selector: results
    params: {}
- name: DataflowJob
  endpoint:
    path: /dataflow/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: BigQuery
  endpoint:
    path: /bigquery/v2/projects/{projectId}/datasets/{datasetId}/tables
    method: GET
    data_selector: tables
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID
  endpoint:
    path: /tests/system/google/DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID
    method: GET
    data_selector: example-project
- name: campaigns
  endpoint:
    path: /v10/customers/{customerId}/campaigns
    method: GET
    data_selector: campaigns
    params:
      incremental: last_modified_time
- name: ad_groups
  endpoint:
    path: /v10/customers/{customerId}/adGroups
    method: GET
    data_selector: adGroups
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: DEFAULT_GCP_SYSTEM_TEST_PROJECT_ID
  endpoint:
    path: /example-project
    method: GET
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: DbtCloudRunJobOperator
  endpoint:
    path: /_api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudRunJobOperator
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /_api/airflow/providers/dbt/cloud/sensors/dbt/index.html#airflow.providers.dbt.cloud.sensors.dbt.DbtCloudJobRunSensor
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /_api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudGetJobRunArtifactOperator
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    path: /_api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudListJobsOperator
    method: GET
- name: DbtCloudRunJobOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudRunJobOperator
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt/index.html#airflow.providers.dbt.cloud.sensors.dbt.DbtCloudJobRunSensor
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudGetJobRunArtifactOperator
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html#airflow.providers.dbt.cloud.operators.dbt.DbtCloudListJobsOperator
    method: GET
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs
    method: POST
    data_selector: run_results
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs
    method: POST
    data_selector: run_results
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: trigger_job_run3
  endpoint:
    path: /api/v2/jobs
    method: POST
    data_selector: run_results
    params:
      project_name: my_dbt_project
      environment_name: prod
      job_name: my_dbt_job
      check_interval: 10
      timeout: 300
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: job_status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: job_run_sensor_deferred
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: job_status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/jobs/artifacts
    method: GET
    data_selector: artifacts
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: run_id
    params:
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: run_id
    params:
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: trigger_job_run3
  endpoint:
    path: /api/v2/jobs/run
    method: POST
    data_selector: run_id
    params:
      project_name: my_dbt_project
      environment_name: prod
      job_name: my_dbt_job
      check_interval: 10
      timeout: 300
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: status
    params:
      timeout: 20
- name: job_run_sensor_deferred
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: status
    params:
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/jobs/artifacts
    method: GET
    data_selector: artifact
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: output
    params:
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: output
    params:
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/v2/job_runs/{run_id}
    method: GET
    data_selector: status
    params:
      timeout: 20
- name: job_run_sensor_deferred
  endpoint:
    path: /api/v2/job_runs/{run_id}
    method: GET
    data_selector: status
    params:
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/job_runs/{run_id}/artifacts
    method: GET
    data_selector: artifacts
    params:
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/v2/accounts/{account_id}/jobs
    method: GET
    data_selector: jobs
    params:
      project_id: 160645
- name: job_run
  endpoint:
    path: /api/v2/jobs
    method: POST
    data_selector: job_run
    params: {}
- name: job_run_status
  endpoint:
    path: /api/v2/jobs/{job_id}/run
    method: GET
    data_selector: run_status
    params: {}
- name: job_run_artifact
  endpoint:
    path: /api/v2/jobs/{job_id}/artifacts
    method: GET
    data_selector: artifacts
    params: {}
- name: list_jobs
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: DbtCloudRunJobOperator
  endpoint:
    path: /api/v2/jobs/run
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/v2/jobs/run/status
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /api/v2/jobs/run/artifacts
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    path: /api/v2/jobs
    method: GET
- name: trigger_job_run1
  endpoint:
    path: /dbt/cloud/jobs/run
    method: POST
    data_selector: job_run
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run3
  endpoint:
    path: /dbt/cloud/jobs/run
    method: POST
    data_selector: job_run
    params:
      project_name: my_dbt_project
      environment_name: prod
      job_name: my_dbt_job
      check_interval: 10
      timeout: 300
- name: list_dbt_jobs
  endpoint:
    path: /dbt/cloud/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: DbtCloudRunJobOperator
  endpoint:
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    method: GET
- name: DbtCloudRunJobOperator
  endpoint:
    path: /jobs/run
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /jobs/run/status
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /jobs/run/artifact
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    path: /jobs
    method: GET
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs/run
    method: POST
    data_selector: output
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs/run
    method: POST
    data_selector: output
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: job_run_sensor_defered
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/jobs/artifact
    method: GET
    data_selector: artifact
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: job_run
  endpoint:
    path: /api/v2/jobs/{job_id}/run/
    method: POST
    data_selector: data
    params: {}
- name: job_run_status
  endpoint:
    path: /api/v2/jobs/{job_id}/run/{run_id}/
    method: GET
    data_selector: data
    params: {}
- name: job_run_artifact
  endpoint:
    path: /api/v2/jobs/{job_id}/run/{run_id}/artifacts/
    method: GET
    data_selector: data
    params: {}
- name: list_jobs
  endpoint:
    path: /api/v2/accounts/{account_id}/jobs/
    method: GET
    data_selector: data
    params: {}
- name: trigger_job_run1
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/DbtCloudRunJobOperator
    method: POST
    data_selector: output
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/DbtCloudRunJobOperator
    method: POST
    data_selector: output
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt/DbtCloudJobRunSensor
    method: GET
    data_selector: status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: job_run_sensor_defered
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt/DbtCloudJobRunSensor
    method: GET
    data_selector: status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/DbtCloudGetJobRunArtifactOperator
    method: GET
    data_selector: artifact
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/DbtCloudListJobsOperator
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html
    method: POST
    data_selector: output
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html
    method: POST
    data_selector: output
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt/index.html
    method: GET
    data_selector: output
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: job_run_sensor_deferred
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt/index.html
    method: GET
    data_selector: output
    params:
      run_id: trigger_job_run2.output
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html
    method: GET
    data_selector: output
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt/index.html
    method: GET
    data_selector: output
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: job_run_sensor_defered
  endpoint:
    params:
      run_id: trigger_job_run2.output
      timeout: 20
      deferrable: true
- name: get_run_results_artifact
  endpoint:
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    params:
      account_id: 106277
      project_id: 160645
- name: job_run
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: jobs
- name: list_jobs
  endpoint:
    path: /api/v2/accounts/{account_id}/jobs
    method: GET
    data_selector: jobs
- name: DbtCloudRunJobOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: POST
    data_selector: output
    params:
      account_id: account_id
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt
    method: GET
    data_selector: job_status
    params:
      account_id: account_id
      run_id: trigger_job_run2.output
      timeout: 20
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: GET
    data_selector: artifacts
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: DbtCloudListJobsOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: jobs
  endpoint:
    path: /accounts/{account_id}/jobs
    method: GET
    data_selector: jobs
- name: job_run_artifact
  endpoint:
    path: /jobs/{job_id}/run_results
    method: GET
    data_selector: run_results
- name: job_run
  endpoint:
    path: /api/v2/jobs/run
    method: POST
    data_selector: data
    params: {}
- name: job_status
  endpoint:
    path: /api/v2/jobs/status
    method: GET
    data_selector: data
    params: {}
- name: job_artifact
  endpoint:
    path: /api/v2/jobs/artifacts
    method: GET
    data_selector: data
    params: {}
- name: list_jobs
  endpoint:
    path: /api/v2/jobs
    method: GET
    data_selector: data
    params: {}
- name: DbtCloudRunJobOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: POST
    data_selector: job_run
    params: {}
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/airflow/providers/dbt/cloud/sensors/dbt
    method: GET
    data_selector: job_run_status
    params: {}
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: GET
    data_selector: artifacts
    params: {}
- name: DbtCloudListJobsOperator
  endpoint:
    path: /api/airflow/providers/dbt/cloud/operators/dbt
    method: GET
    data_selector: jobs
    params: {}
- name: DbtCloudRunJobOperator
  endpoint:
    path: /api/v2/jobs/run
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/v2/jobs/run/status
    method: GET
- name: DbtCloudGetJobRunArtifactOperator
  endpoint:
    path: /api/v2/jobs/run/artifacts
    method: GET
- name: DbtCloudListJobsOperator
  endpoint:
    path: /api/v2/jobs
    method: GET
- name: job_run
  endpoint:
    path: /api/v2/jobs/{job_id}/run
    method: POST
    data_selector: run_id
- name: job_run_artifact
  endpoint:
    path: /api/v2/run_artifacts/{run_id}/{path}
    method: GET
    data_selector: artifact
- name: job_run_status
  endpoint:
    path: /api/v2/jobs/{job_id}/runs/{run_id}
    method: GET
    data_selector: status
- name: job_run
  endpoint:
    path: /api/v2/jobs/{job_id}/run
    method: POST
    data_selector: run
    params: {}
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/{job_id}/run/status
    method: GET
    data_selector: status
    params: {}
- name: download_artifact
  endpoint:
    path: /api/v2/jobs/{job_id}/artifacts/{path}
    method: GET
    data_selector: artifact
    params: {}
- name: list_jobs
  endpoint:
    path: /api/v2/accounts/{account_id}/jobs
    method: GET
    data_selector: jobs
    params: {}
- name: DbtCloudRunJob
  endpoint:
    path: /api/v2/runs
    method: POST
- name: DbtCloudJobRunSensor
  endpoint:
    path: /api/v2/runs/{run_id}
    method: GET
- name: DbtCloudGetJobRunArtifact
  endpoint:
    path: /api/v2/runs/{run_id}/artifacts
    method: GET
- name: DbtCloudListJobs
  endpoint:
    path: /api/v2/accounts/{account_id}/jobs
    method: GET
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: job_run
    params:
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: job_run
    params:
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/run/status
    method: GET
    data_selector: job_run_status
    params:
      timeout: 20
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/jobs/runs/artifact
    method: GET
    data_selector: artifact
    params:
      path: run_results.json
- name: trigger_job_run1
  endpoint:
    path: /api/dbt_cloud/job_runs
    method: POST
    data_selector: job_run
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/dbt_cloud/job_runs
    method: POST
    data_selector: job_run
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/dbt_cloud/job_runs/status
    method: GET
    data_selector: status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: get_run_results_artifact
  endpoint:
    path: /api/dbt_cloud/job_runs/artifacts
    method: GET
    data_selector: artifact
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/dbt_cloud/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: output
    params:
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/v2/jobs/48617/run
    method: POST
    data_selector: output
    params:
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/v2/jobs/run
    method: GET
    data_selector: status
    params:
      timeout: 20
- name: get_run_results_artifact
  endpoint:
    path: /api/v2/jobs/run/artifacts
    method: GET
    data_selector: artifacts
    params:
      path: run_results.json
- name: trigger_job_run1
  endpoint:
    path: /api/dbt_cloud/jobs
    method: POST
    data_selector: job_run
    params:
      job_id: 48617
      check_interval: 10
      timeout: 300
- name: trigger_job_run2
  endpoint:
    path: /api/dbt_cloud/jobs
    method: POST
    data_selector: job_run
    params:
      job_id: 48617
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: job_run_sensor
  endpoint:
    path: /api/dbt_cloud/job_run
    method: GET
    data_selector: job_run_status
    params:
      run_id: trigger_job_run2.output
      timeout: 20
- name: get_run_results_artifact
  endpoint:
    path: /api/dbt_cloud/artifacts
    method: GET
    data_selector: artifact
    params:
      run_id: trigger_job_run1.output
      path: run_results.json
- name: list_dbt_jobs
  endpoint:
    path: /api/v1/jobs
    method: GET
    data_selector: jobs
    params:
      account_id: 106277
      project_id: 160645
- name: trigger_job_run1
  endpoint:
    path: /api/v1/jobs/48617/run
    method: POST
    data_selector: run
    params:
      check_interval: 10
      timeout: 300
- name: get_run_results_artifact
  endpoint:
    path: /api/v1/runs/{run_id}/artifacts
    method: GET
    data_selector: artifacts
    params:
      path: run_results.json
- name: trigger_job_run2
  endpoint:
    path: /api/v1/jobs/48617/run
    method: POST
    data_selector: run
    params:
      wait_for_termination: false
      additional_run_config:
        threads_override: 8
- name: trigger_job_run3
  endpoint:
    path: /api/v1/projects/my_dbt_project/environments/prod/jobs/my_dbt_job/run
    method: POST
    data_selector: run
    params:
      check_interval: 10
      timeout: 300
- name: job_run_sensor
  endpoint:
    path: /api/v1/runs/{run_id}/status
    method: GET
    data_selector: status
    params:
      timeout: 20
- name: job_run_sensor_deferred
  endpoint:
    path: /api/v1/runs/{run_id}/status
    method: GET
    data_selector: status
    params:
      timeout: 20
      deferrable: true
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: records
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /export
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: body
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /exportDatabase
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: jobs
  endpoint:
    path: /jobs
    method: GET
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: body
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: records
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: body
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: records
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: export_database
  endpoint:
    path: /v1/projects/{project_id}/databases/{database_id}/documents:export
    method: POST
    data_selector: documents
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: records
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: records
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: records
- name: export_database
  endpoint:
    path: /google/cloud/firestore/export
    method: POST
    data_selector: outputUriPrefix
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: records
- name: export_database_to_gcs
  endpoint:
    path: /export_database_to_gcs
    method: POST
    data_selector: results
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /export_database_to_gcs
    method: POST
    data_selector: body
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: outputUriPrefix
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: body
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /v1/projects/{project_id}/databases/{database_id}:exportDocuments
    method: POST
    data_selector: operation
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: outputUriPrefix
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: firestore_data
  endpoint:
    path: /v1/projects/{PROJECT_ID}/databases/(default)/documents/{EXPORT_COLLECTION_ID}
    method: GET
    data_selector: documents
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: outputUriPrefix
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database_to_gcs
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: records
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore
    method: POST
    data_selector: outputUriPrefix
    params: {}
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: CloudFirestoreExportDatabaseOperator
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/CloudFirestoreExportDatabaseOperator
    method: POST
    data_selector: records
    params: {}
- name: create_workflow
  endpoint:
    path: /workflows/create
    method: POST
    data_selector: workflow
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow: WORKFLOW
      workflow_id: WORKFLOW_ID
- name: update_workflow
  endpoint:
    path: /workflows/update
    method: PATCH
    data_selector: workflow
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
      update_mask: FieldMask(paths=["name", "description"])
- name: get_workflow
  endpoint:
    path: /workflows/get
    method: GET
    data_selector: workflow
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
- name: list_workflows
  endpoint:
    path: /workflows/list
    method: GET
    data_selector: workflows
    params:
      project_id: PROJECT_ID
      location: LOCATION
- name: delete_workflow
  endpoint:
    path: /workflows/delete
    method: DELETE
    data_selector: workflow
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
- name: create_execution
  endpoint:
    path: /executions/create
    method: POST
    data_selector: execution
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
      execution: EXECUTION
- name: get_execution
  endpoint:
    path: /executions/get
    method: GET
    data_selector: execution
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
      execution_id: create_execution_id
- name: list_executions
  endpoint:
    path: /executions/list
    method: GET
    data_selector: executions
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: WORKFLOW_ID
- name: cancel_execution
  endpoint:
    path: /executions/cancel
    method: POST
    data_selector: execution
    params:
      project_id: PROJECT_ID
      location: LOCATION
      workflow_id: SLEEP_WORKFLOW_ID
      execution_id: cancel_execution_id
- name: export_database
  endpoint:
    path: /_api/airflow/providers/google/firebase/operators/firestore/index.html
    method: POST
    data_selector: body
    params: {}
- name: export_database
  endpoint:
    path: /export_database
    method: POST
    data_selector: body
    params:
      project_id: PROJECT_ID
      body:
        outputUriPrefix: EXPORT_DESTINATION_URL
        collectionIds:
        - EXPORT_COLLECTION_ID
- name: ADLSCreateObjectOperator
  endpoint:
    path: /api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: upload_data
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: remove_file
    method: DELETE
    data_selector: path
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: adls_files
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: POST
    data_selector: records
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: DELETE
    data_selector: records
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: GET
    data_selector: records
- name: cloud_firestore_export
  endpoint:
    path: /v1/projects/{PROJECT_ID}/databases/(default):exportDocuments
    method: POST
    data_selector: response
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: records
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: records
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: records
- name: upload_data
  endpoint:
    path: upload_data
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: delete_task
  endpoint:
    path: delete_task
    method: DELETE
    data_selector: path
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: adls_files
  endpoint:
    path: adls_files
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: create_workflow
  endpoint:
    path: /workflows
    method: POST
    data_selector: workflow
    params: {}
- name: update_workflow
  endpoint:
    path: /workflows/{workflow_id}
    method: PATCH
    data_selector: workflow
    params: {}
- name: get_workflow
  endpoint:
    path: /workflows/{workflow_id}
    method: GET
    data_selector: workflow
    params: {}
- name: list_workflows
  endpoint:
    path: /workflows
    method: GET
    data_selector: workflows
    params: {}
- name: delete_workflow
  endpoint:
    path: /workflows/{workflow_id}
    method: DELETE
    data_selector: workflow
    params: {}
- name: create_execution
  endpoint:
    path: /workflows/{workflow_id}/executions
    method: POST
    data_selector: execution
    params: {}
- name: get_execution
  endpoint:
    path: /workflows/{workflow_id}/executions/{execution_id}
    method: GET
    data_selector: execution
    params: {}
- name: list_executions
  endpoint:
    path: /workflows/{workflow_id}/executions
    method: GET
    data_selector: executions
    params: {}
- name: cancel_execution
  endpoint:
    path: /workflows/{workflow_id}/executions/{execution_id}:cancel
    method: POST
    data_selector: execution
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
    params: {}
- name: ADLSCreateObject
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
- name: ADLSDelete
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
- name: ADLSList
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
- name: ADLSCreateObjectOperator
  endpoint:
    path: /api/adls/create
    method: POST
    data_selector: data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /api/adls/delete
    method: DELETE
    data_selector: data
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /api/adls/list
    method: GET
    data_selector: data
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: POST
    data_selector: upload_data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: DELETE
    data_selector: remove_file
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: GET
    data_selector: adls_files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObject
  endpoint:
    path: /path/to/adls/create
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDelete
  endpoint:
    path: /path/to/adls/delete
    method: DELETE
    data_selector: data
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSList
  endpoint:
    path: /path/to/adls/list
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: records
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: records
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: records
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: data
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: data
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /upload_data
    method: POST
    data_selector: records
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /delete_task
    method: DELETE
    data_selector: records
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /adls_files
    method: GET
    data_selector: records
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: records
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: records
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: records
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /adls/create
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /adls/delete
    method: DELETE
    data_selector: data
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /adls/list
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObject
  endpoint:
    path: /path/to/adls/create
    method: POST
    data_selector: data
    params: {}
- name: ADLSDelete
  endpoint:
    path: /path/to/adls/delete
    method: DELETE
    data_selector: data
    params: {}
- name: ADLSList
  endpoint:
    path: /path/to/adls/list
    method: GET
    data_selector: data
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /ADLSCreateObjectOperator
    method: POST
    data_selector: records
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /ADLSDeleteOperator
    method: DELETE
    data_selector: records
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /ADLSListOperator
    method: GET
    data_selector: records
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: create_object
  endpoint:
    path: /adls/create
    method: POST
    data_selector: records
    params: {}
- name: delete_object
  endpoint:
    path: /adls/delete
    method: DELETE
    data_selector: records
    params: {}
- name: list_objects
  endpoint:
    path: /adls/list
    method: GET
    data_selector: records
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: POST
    data_selector: remove_file
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: POST
    data_selector: adls_files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObject
  endpoint:
    path: /createObject
    method: POST
    data_selector: data
    params: {}
- name: ADLSDelete
  endpoint:
    path: /deleteObject
    method: DELETE
    data_selector: data
    params: {}
- name: ADLSList
  endpoint:
    path: /listObjects
    method: GET
    data_selector: data
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html#airflow.providers.microsoft.azure.operators.adls.ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html#airflow.providers.microsoft.azure.operators.adls.ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html#airflow.providers.microsoft.azure.operators.adls.ADLSListOperator
    method: GET
    data_selector: adls_files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSDeleteOperator
  endpoint:
    path: /delete_file
    method: POST
    data_selector: results
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSDeleteOperator
  endpoint:
    path: /adls/delete
    method: POST
    data_selector: result
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSCreateObjectOperator
  endpoint:
    path: /upload
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /delete
    method: DELETE
    data_selector: data
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /list
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSDeleteOperator
  endpoint:
    path: /adls/delete
    method: POST
    data_selector: response
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: POST
    data_selector: upload_data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: DELETE
    data_selector: remove_file
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/index.html
    method: GET
    data_selector: adls_files
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: records
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: records
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: records
- name: remote_logs
  endpoint:
    path: /path/to/logs
    method: GET
    data_selector: logs
    params: {}
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: records
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: records
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: records
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /upload
    method: POST
    data_selector: response
    params:
      file_system_name: Fabric
      file_name: REMOTE_FILE_PATH
      data: Hello world
      replace: true
- name: ADLSDeleteOperator
  endpoint:
    path: /delete
    method: DELETE
    data_selector: response
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: ADLSListOperator
  endpoint:
    path: /list
    method: GET
    data_selector: files
    params:
      path: folder/output/*.parquet
      azure_data_lake_conn_id: azure_data_lake_default
- name: ADLSCreateObjectOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSCreateObjectOperator
    method: POST
    data_selector: upload_data
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSDeleteOperator
    method: DELETE
    data_selector: remove_file
    params: {}
- name: ADLSListOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/adls/ADLSListOperator
    method: GET
    data_selector: adls_files
    params: {}
- name: ADLSDeleteOperator
  endpoint:
    path: /adlsdelete
    method: DELETE
    data_selector: files
    params:
      path: REMOTE_FILE_PATH
      recursive: true
- name: secrets
  endpoint:
    path: /secrets
    method: GET
    data_selector: secrets
- name: remote_logs
  endpoint:
    path: /logs
    method: GET
    data_selector: logs
    params: {}
- name: upload_data
  endpoint:
    path: /upload
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: remote.txt
      data: Hello world
      replace: true
- name: delete_file
  endpoint:
    path: /delete
    method: DELETE
    data_selector: status
    params:
      path: remote.txt
      recursive: true
- name: upload_task
  endpoint:
    path: /upload
    method: POST
    data_selector: result
    params:
      local_path: localfile.txt
      remote_path: remote.txt
- name: delete_task
  endpoint:
    path: /delete
    method: DELETE
    data_selector: result
    params:
      path: remote.txt
      recursive: true
- name: adls_files
  endpoint:
    path: /folder/output/*.parquet
    method: GET
    data_selector: files
    params: {}
- name: AzureDataFactoryRunPipelineOperator
  endpoint:
    path: /operators/adf_run_pipeline.html
    method: POST
    data_selector: pipeline_run
- name: AzureDataFactoryPipelineRunStatusSensor
  endpoint:
    path: /sensors/data_factory/index.html
    method: GET
    data_selector: pipeline_run_status
- name: Create Queue
  endpoint:
    path: /createQueue
    method: POST
    data_selector: records
    params: {}
- name: Send Message
  endpoint:
    path: /sendMessage
    method: POST
    data_selector: records
    params: {}
- name: Receive Message
  endpoint:
    path: /receiveMessage
    method: GET
    data_selector: records
    params: {}
- name: Delete Queue
  endpoint:
    path: /deleteQueue
    method: DELETE
    data_selector: records
    params: {}
- name: Create Topic
  endpoint:
    path: /createTopic
    method: POST
    data_selector: records
    params: {}
- name: Delete Topic
  endpoint:
    path: /deleteTopic
    method: DELETE
    data_selector: records
    params: {}
- name: Create Subscription
  endpoint:
    path: /createSubscription
    method: POST
    data_selector: records
    params: {}
- name: Update Subscription
  endpoint:
    path: /updateSubscription
    method: PUT
    data_selector: records
    params: {}
- name: Receive Subscription Message
  endpoint:
    path: /receiveSubscriptionMessage
    method: GET
    data_selector: records
    params: {}
- name: Delete Subscription
  endpoint:
    path: /deleteSubscription
    method: DELETE
    data_selector: records
    params: {}
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: instance
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: instance
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: instance
    params: {}
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: instance
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: instances
    params:
      page_size: 100
- name: upload_data
  endpoint:
    path: /upload
    method: POST
    data_selector: data
    params:
      file_system_name: Fabric
      file_name: remote.txt
      data: Hello world
      replace: true
- name: delete_file
  endpoint:
    path: /delete
    method: DELETE
    data_selector: data
    params:
      path: remote.txt
      recursive: true
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: upload_file
  endpoint:
    path: /upload
    method: POST
    data_selector: task_id
    params:
      local_path: localfile.txt
      remote_path: remote.txt
- name: remove_file
  endpoint:
    path: /delete
    method: POST
    data_selector: task_id
    params:
      path: remote.txt
      recursive: true
- name: adls_files
  endpoint:
    path: folder/output/*.parquet
    method: GET
- name: AzureDataFactoryRunPipelineOperator
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/data_factory/index.html
    method: POST
    data_selector: pipeline_run
    params: {}
- name: AzureDataFactoryPipelineRunStatusSensor
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/sensors/data_factory/index.html
    method: GET
    data_selector: pipeline_status
    params: {}
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: instances
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get_instance
    method: GET
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover_instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_response
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask: '{"paths": ["memory_size_gb"]}'
      instance: '{"memory_size_gb": 2}'
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_response
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_service_bus_queue
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusCreateQueueOperator
    method: POST
    data_selector: ''
    params: {}
- name: send_message_to_service_bus_queue
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusSendMessageOperator
    method: POST
    data_selector: ''
    params: {}
- name: receive_message_service_bus_queue
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusReceiveMessageOperator
    method: POST
    data_selector: ''
    params: {}
- name: delete_service_bus_queue
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusDeleteQueueOperator
    method: POST
    data_selector: ''
    params: {}
- name: create_service_bus_topic
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusTopicCreateOperator
    method: POST
    data_selector: ''
    params: {}
- name: delete_asb_topic
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusTopicDeleteOperator
    method: POST
    data_selector: ''
    params: {}
- name: create_service_bus_subscription
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusSubscriptionCreateOperator
    method: POST
    data_selector: ''
    params: {}
- name: update_service_bus_subscription
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusUpdateSubscriptionOperator
    method: POST
    data_selector: ''
    params: {}
- name: receive_message_service_bus_subscription
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.ASBReceiveSubscriptionMessageOperator
    method: POST
    data_selector: ''
    params: {}
- name: delete_service_bus_subscription
  endpoint:
    path: /_api/airflow/providers/microsoft/azure/operators/asb/index.html#airflow.providers.microsoft.azure.operators.asb.AzureServiceBusSubscriptionDeleteOperator
    method: POST
    data_selector: ''
    params: {}
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: instance
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: instance
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: instance
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: instance
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: instances
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: instance
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: instance
    params: {}
- name: update_instance
  endpoint:
    path: /cloud_memorystore_update_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore_scale_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: output
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: DELETE
    data_selector: output
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: GET
    data_selector: output
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: GET
    data_selector: output
    params:
      page_size: 100
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: output
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: output
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: output
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_mask
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: memory_size_gb
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode:
        mode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: create_instance
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: delete_instance
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: export_instance
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: import_instance
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: get_instance
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: list_instances
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: failover_instance
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /v1/{location}/instances
    method: POST
    data_selector: instance
    params:
      location: '{{ location }}'
      instance_id: '{{ instance_id }}'
      project_id: '{{ project_id }}'
- name: delete_instance
  endpoint:
    path: /v1/{location}/instances/{instance}
    method: DELETE
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: export_instance
  endpoint:
    path: /v1/{location}/instances/{instance}:export
    method: POST
    data_selector: output_config
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: import_instance
  endpoint:
    path: /v1/{location}/instances/{instance}:import
    method: POST
    data_selector: input_config
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: get_instance
  endpoint:
    path: /v1/{location}/instances/{instance}
    method: GET
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: list_instances
  endpoint:
    path: /v1/{location}/instances
    method: GET
    data_selector: instances
    params:
      location: '{{ location }}'
      page_size: '{{ page_size }}'
      project_id: '{{ project_id }}'
- name: failover_instance
  endpoint:
    path: /v1/{location}/instances/{instance}:failover
    method: POST
    data_selector: data_protection_mode
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: create_instance
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: delete_instance
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: export_instance
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreImportOperator
    method: POST
    data_selector: import_instance
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: get_instance
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: list_instances
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: failover_instance
    params: {}
- name: update_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreUpdateInstanceOperator
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreScaleInstanceOperator
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /cloud_memorystore_update_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore_scale_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /operators/cloud/cloud_memorystore/create_instance
    method: POST
    data_selector: instance
    params:
      location: '{{ location }}'
      instance_id: '{{ instance_id }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: delete_instance
  endpoint:
    path: /operators/cloud/cloud_memorystore/delete_instance
    method: DELETE
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: export_instance
  endpoint:
    path: /operators/cloud/cloud_memorystore/export_instance
    method: POST
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      output_config: '{{ output_config }}'
      project_id: '{{ project_id }}'
- name: import_instance
  endpoint:
    path: /operators/cloud/cloud_memorystore/import_instance
    method: POST
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      input_config: '{{ input_config }}'
      project_id: '{{ project_id }}'
- name: get_instance
  endpoint:
    path: /operators/cloud/cloud_memorystore/get_instance
    method: GET
    data_selector: instance
    params:
      location: '{{ location }}'
      instance: '{{ instance }}'
      project_id: '{{ project_id }}'
- name: list_instances
  endpoint:
    path: /operators/cloud/cloud_memorystore/list_instances
    method: GET
    data_selector: instances
    params:
      location: '{{ location }}'
      page_size: '{{ page_size }}'
      project_id: '{{ project_id }}'
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: update_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreUpdateInstanceOperator
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreScaleInstanceOperator
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: instance
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: instance
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: instance
    params: {}
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover_instance
    method: POST
    data_selector: instance
    params: {}
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get_instance
    method: GET
    data_selector: instance
    params: {}
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: instance
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: instances
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: update_instance
  endpoint:
    path: /cloud_memorystore/update_instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore/scale_instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: create_instance
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: delete_instance
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: export_instance
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreImportOperator
    method: POST
    data_selector: import_instance
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: get_instance
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: list_instances
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: failover_instance
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: instance
  endpoint:
    path: /cloud/memorystore/instance
    method: GET
    data_selector: instances
    params: {}
- name: scale_instance
  endpoint:
    path: /cloud/memorystore/scale_instance
    method: POST
    params:
      memory_size_gb: '3'
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
- name: create_instance_and_import
  endpoint:
    path: /cloud/memorystore/create_instance_and_import
    method: POST
- name: export_and_delete_instance
  endpoint:
    path: /cloud/memorystore/export_and_delete_instance
    method: POST
- name: instance
  endpoint:
    path: /v1/instances
    method: GET
    data_selector: instances
    params: {}
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: delete_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params: {}
- name: export_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: get_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params: {}
- name: import_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params: {}
- name: list_instances
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params: {}
- name: update_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreUpdateInstanceOperator
    method: PATCH
    data_selector: result
    params: {}
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: output_config
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: output_config
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: output_config
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreImportOperator
    method: POST
    data_selector: output_config
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: output_config
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: output_config
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/index.html#airflow.providers.google.cloud.operators.cloud_memorystore.CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: output_config
    params: {}
- name: scale_instance
  endpoint:
    path: /cloud/memorystore/scale_instance
    method: POST
    data_selector: response
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance_and_import
  endpoint:
    path: /cloud/memorystore/create_instance_and_import
    method: POST
    data_selector: response
    params: {}
- name: export_and_delete_instance
  endpoint:
    path: /cloud/memorystore/export_and_delete_instance
    method: POST
    data_selector: response
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_mask
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: memory_size_gb
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: instances
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: scale_instance
  endpoint:
    path: /cloudmemorystore/scale_instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: output_config
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: Instance.Tier.BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: output_config
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: output_config
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: output_config
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: output_config
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreUpdateInstanceOperator
    method: PATCH
    data_selector: output_config
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /api/cloud/memorystore/scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance_and_import
  endpoint:
    path: /api/cloud/memorystore/create-instance-and-import
    method: POST
    data_selector: instance
    params: {}
- name: export_and_delete_instance
  endpoint:
    path: /api/cloud/memorystore/export-and-delete-instance
    method: POST
    data_selector: instance
    params: {}
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create-instance
    method: POST
    data_selector: instance
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete-instance
    method: DELETE
    data_selector: instance
    params: {}
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get-instance
    method: GET
    data_selector: instance
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list-instances
    method: GET
    data_selector: instances
    params:
      page_size: 100
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import-instance
    method: POST
    data_selector: instance
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export-instance
    method: POST
    data_selector: instance
    params: {}
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover-instance
    method: POST
    data_selector: instance
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: POST
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: records
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: records
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: create_instance
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: DELETE
    data_selector: delete_instance
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: export_instance
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: failover_instance
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: GET
    data_selector: get_instance
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: POST
    data_selector: import_instance
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore
    method: GET
    data_selector: list_instances
    params:
      page_size: '100'
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    method: POST
- name: delete_instance
  endpoint:
    method: DELETE
- name: export_instance
  endpoint:
    method: POST
- name: failover_instance
  endpoint:
    method: POST
- name: get_instance
  endpoint:
    method: GET
- name: import_instance
  endpoint:
    method: POST
- name: list_instances
  endpoint:
    method: GET
- name: update_instance
  endpoint:
    method: PATCH
- name: create_instance
  endpoint:
    path: /cloud/memorystore/instances
    method: POST
    data_selector: instance
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/instances/{instance}
    method: DELETE
    data_selector: instance
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/instances/{instance}/export
    method: POST
    data_selector: export
    params: {}
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/instances/{instance}/failover
    method: POST
    data_selector: failover
    params: {}
- name: get_instance
  endpoint:
    path: /cloud/memorystore/instances/{instance}
    method: GET
    data_selector: instance
    params: {}
- name: import_instance
  endpoint:
    path: /cloud/memorystore/instances/{instance}/import
    method: POST
    data_selector: import
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/instances
    method: GET
    data_selector: instances
    params:
      page_size: '100'
- name: scale_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/scale_instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance_and_import
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/create_instance_and_import
    method: POST
    data_selector: create_instance_and_import
    params: {}
- name: export_and_delete_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/export_and_delete_instance
    method: POST
    data_selector: export_and_delete_instance
    params: {}
- name: update_instance
  endpoint:
    path: /cloud_memorystore/update_instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore/scale_instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params:
      page_size: '100'
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: result
    params: {}
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete
    method: DELETE
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get
    method: GET
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list
    method: GET
    data_selector: instances
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /cloud/memorystore/update
    method: PATCH
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore/scale_instance
    method: POST
    data_selector: instance
    params:
      memory_size_gb: 3
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
- name: create_instance_and_import
  endpoint:
    path: /cloud_memorystore/create_instance_and_import
    method: POST
    data_selector: instance
    params: {}
- name: export_and_delete_instance
  endpoint:
    path: /cloud_memorystore/export_and_delete_instance
    method: POST
    data_selector: instance
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: response
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: response
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: response
    params: {}
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover_instance
    method: POST
    data_selector: response
    params: {}
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get_instance
    method: GET
    data_selector: response
    params: {}
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: response
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: response
    params:
      page_size: 100
- name: instance
  endpoint:
    path: /cloud/memorystore/instances
    method: GET
    data_selector: instances
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_mask
    params:
      memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: memory_size_gb
    params:
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /cloud_memorystore/create_instance
    method: POST
- name: delete_instance
  endpoint:
    path: /cloud_memorystore/delete_instance
    method: DELETE
- name: export_instance
  endpoint:
    path: /cloud_memorystore/export_instance
    method: POST
- name: failover_instance
  endpoint:
    path: /cloud_memorystore/failover_instance
    method: POST
- name: get_instance
  endpoint:
    path: /cloud_memorystore/get_instance
    method: GET
- name: import_instance
  endpoint:
    path: /cloud_memorystore/import_instance
    method: POST
- name: list_instances
  endpoint:
    path: /cloud_memorystore/list_instances
    method: GET
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: Instance.Tier.BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create
    method: POST
    data_selector: result
    params: {}
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete
    method: DELETE
    data_selector: result
    params: {}
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export
    method: POST
    data_selector: result
    params: {}
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover
    method: POST
    data_selector: result
    params: {}
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get
    method: GET
    data_selector: result
    params: {}
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import
    method: POST
    data_selector: result
    params: {}
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list
    method: GET
    data_selector: result
    params:
      page_size: '100'
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      update_mask: '{"paths": ["memory_size_gb"]}'
      instance: '{"memory_size_gb": 2}'
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /api/cloud/memorystore/create-instance
    method: POST
    data_selector: create_instance
    params: {}
- name: delete_instance
  endpoint:
    path: /api/cloud/memorystore/delete-instance
    method: DELETE
    data_selector: delete_instance
    params: {}
- name: export_instance
  endpoint:
    path: /api/cloud/memorystore/export-instance
    method: POST
    data_selector: export_instance
    params: {}
- name: failover_instance
  endpoint:
    path: /api/cloud/memorystore/failover-instance
    method: POST
    data_selector: failover_instance
    params: {}
- name: get_instance
  endpoint:
    path: /api/cloud/memorystore/get-instance
    method: GET
    data_selector: get_instance
    params: {}
- name: import_instance
  endpoint:
    path: /api/cloud/memorystore/import-instance
    method: POST
    data_selector: import_instance
    params: {}
- name: list_instances
  endpoint:
    path: /api/cloud/memorystore/list-instances
    method: GET
    data_selector: list_instances
    params:
      page_size: 100
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get_instance
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask: '{"paths": ["memory_size_gb"]}'
      instance: '{"memory_size_gb": 2}'
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: instance
  endpoint:
    path: /cloud/memorystore/instances
    method: GET
    data_selector: instances
    params: {}
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: Instance.Tier.BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode:
        data_protection_mode: FailoverInstanceRequest.DataProtectionMode.LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: scale_instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance: FIRST_INSTANCE
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode:
        DataProtectionMode:
          LIMITED_DATA_LOSS: true
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_instance
  endpoint:
    path: /services/data/vXX.X/sobjects/CreateInstance
    method: POST
    data_selector: result
- name: get_instance
  endpoint:
    path: /services/data/vXX.X/sobjects/GetInstance
    method: GET
    data_selector: result
- name: list_instances
  endpoint:
    path: /services/data/vXX.X/sobjects/ListInstances
    method: GET
    data_selector: result
- name: delete_instance
  endpoint:
    path: /services/data/vXX.X/sobjects/DeleteInstance
    method: DELETE
    data_selector: result
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: Instance.Tier.BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode:
        DataProtectionMode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: create_sink
  endpoint:
    path: /cloud_logging_sink/create
    method: POST
    data_selector: sink_config
    params:
      project_id: PROJECT_ID
- name: update_sink
  endpoint:
    path: /cloud_logging_sink/update
    method: POST
    data_selector: sink_config
    params:
      project_id: PROJECT_ID
- name: list_sinks
  endpoint:
    path: /cloud_logging_sink/list
    method: GET
    data_selector: sinks
    params:
      project_id: PROJECT_ID
- name: delete_sink
  endpoint:
    path: /cloud_logging_sink/delete
    method: DELETE
    data_selector: sink_name
    params:
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /cloud_memorystore/update_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /cloud_memorystore/scale_instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: create_memcached_instance
  endpoint:
    path: /create-instance
    method: POST
    data_selector: instance
    params: {}
- name: delete_memcached_instance
  endpoint:
    path: /delete-instance
    method: DELETE
    data_selector: instance
    params: {}
- name: get_memcached_instance
  endpoint:
    path: /get-instance
    method: GET
    data_selector: instance
    params: {}
- name: list_memcached_instances
  endpoint:
    path: /list-instances
    method: GET
    data_selector: instances
    params: {}
- name: update_memcached_instance
  endpoint:
    path: /update-instance
    method: PATCH
    data_selector: instance
    params: {}
- name: update_memcached_parameters
  endpoint:
    path: /update-parameters
    method: PATCH
    data_selector: parameters
    params: {}
- name: apply_memcached_parameters
  endpoint:
    path: /apply-parameters
    method: POST
    data_selector: parameters
    params: {}
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: create_instance
  endpoint:
    path: /cloud/memorystore/create_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance: FIRST_INSTANCE
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /cloud/memorystore/delete_instance
    method: DELETE
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /cloud/memorystore/export_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /cloud/memorystore/import_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /cloud/memorystore/get_instance
    method: GET
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /cloud/memorystore/list_instances
    method: GET
    data_selector: output
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /cloud/memorystore/failover_instance
    method: POST
    data_selector: output
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode: LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
    params: {}
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entities
    params: {}
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
    params: {}
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
    params: {}
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      instance:
        tier: Instance.Tier.BASIC
        memory_size_gb: 1
      project_id: PROJECT_ID
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      output_config:
        gcs_destination:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      input_config:
        gcs_source:
          uri: EXPORT_GCS_URL
      project_id: PROJECT_ID
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      do_xcom_push: true
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: result
    params:
      location: '-'
      page_size: 100
      project_id: PROJECT_ID
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: result
    params:
      location: LOCATION
      instance: MEMORYSTORE_REDIS_INSTANCE_NAME_2
      data_protection_mode: FailoverInstanceRequest.DataProtectionMode.LIMITED_DATA_LOSS
      project_id: PROJECT_ID
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: update_mask
    params:
      memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: memory_size_gb
    params:
      memory_size_gb: 3
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: create_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreCreateInstanceOperator
    method: POST
    data_selector: create_instance
    params: {}
- name: delete_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreDeleteInstanceOperator
    method: DELETE
    data_selector: delete_instance
    params: {}
- name: export_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreExportInstanceOperator
    method: POST
    data_selector: export_instance
    params: {}
- name: failover_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreFailoverInstanceOperator
    method: POST
    data_selector: failover_instance
    params: {}
- name: get_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreGetInstanceOperator
    method: GET
    data_selector: get_instance
    params: {}
- name: import_instance
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreImportOperator
    method: POST
    data_selector: import_instance
    params: {}
- name: list_instances
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreListInstancesOperator
    method: GET
    data_selector: list_instances
    params:
      page_size: 100
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: sentences
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: update_instance
  endpoint:
    path: /update-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME
      project_id: PROJECT_ID
      update_mask:
        paths:
        - memory_size_gb
      instance:
        memory_size_gb: 2
- name: scale_instance
  endpoint:
    path: /scale-instance
    method: POST
    data_selector: instance
    params:
      location: LOCATION
      instance_id: MEMORYSTORE_REDIS_INSTANCE_NAME_3
      project_id: PROJECT_ID
      memory_size_gb: 3
- name: instance
  endpoint:
    path: /v1/{location}/instances
    method: GET
- name: analyze_entities
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/index.html#airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeEntitiesOperator
    method: GET
- name: analyze_entity_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/index.html#airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeEntitySentimentOperator
    method: GET
- name: analyze_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/index.html#airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeSentimentOperator
    method: GET
- name: analyze_classify_text
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/index.html#airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageClassifyTextOperator
    method: GET
- name: create_sink
  endpoint:
    path: /logging/sink
    method: POST
    data_selector: sink_config
    params:
      project_id: PROJECT_ID
- name: update_sink
  endpoint:
    path: /logging/sink/{sink_name}
    method: PUT
    data_selector: sink_config
    params:
      project_id: PROJECT_ID
- name: list_sinks
  endpoint:
    path: /logging/sinks
    method: GET
    data_selector: sinks
    params:
      project_id: PROJECT_ID
- name: delete_sink
  endpoint:
    path: /logging/sink/{sink_name}
    method: DELETE
    data_selector: response
    params:
      project_id: PROJECT_ID
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
    params: {}
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
    params: {}
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
    params: {}
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
    params: {}
- name: create_memcached_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedCreateInstanceOperator
    method: POST
    data_selector: instance
- name: delete_memcached_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedDeleteInstanceOperator
    method: DELETE
    data_selector: instance
- name: get_memcached_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedGetInstanceOperator
    method: GET
    data_selector: instance
- name: list_memcached_instances
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedListInstancesOperator
    method: GET
    data_selector: instances
- name: update_memcached_instance
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedUpdateInstanceOperator
    method: PATCH
    data_selector: instance
- name: update_memcached_parameters
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedUpdateParametersOperator
    method: PATCH
    data_selector: parameters
- name: apply_memcached_parameters
  endpoint:
    path: /airflow/providers/google/cloud/operators/cloud_memorystore/CloudMemorystoreMemcachedApplyParametersOperator
    method: POST
    data_selector: apply_parameters
- name: analyze_entities
  endpoint:
    path: /analyze/entities
    method: POST
    data_selector: entities
    params: {}
- name: analyze_entity_sentiment
  endpoint:
    path: /analyze/entitySentiment
    method: POST
    data_selector: entitySentiment
    params: {}
- name: analyze_sentiment
  endpoint:
    path: /analyze/sentiment
    method: POST
    data_selector: sentiment
    params: {}
- name: classify_text
  endpoint:
    path: /classify/text
    method: POST
    data_selector: categories
    params: {}
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: sentiment
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
- name: analyze_entities
  endpoint:
    path: /v1/documents:analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /v1/documents:analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /v1/documents:analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /v1/documents:classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/CloudNaturalLanguageAnalyzeEntitiesOperator
    method: POST
    data_selector: output
- name: analyze_entity_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/CloudNaturalLanguageAnalyzeEntitySentimentOperator
    method: POST
    data_selector: output
- name: analyze_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/CloudNaturalLanguageAnalyzeSentimentOperator
    method: POST
    data_selector: output
- name: classify_text
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language/CloudNaturalLanguageClassifyTextOperator
    method: POST
    data_selector: output
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: sentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: sentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entitySentiment
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: sentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entities
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: documentSentiment
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
- name: analyze_entities
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language.CloudNaturalLanguageAnalyzeEntitiesOperator
    method: POST
    data_selector: output
- name: analyze_entity_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language.CloudNaturalLanguageAnalyzeEntitySentimentOperator
    method: POST
    data_selector: output
- name: analyze_sentiment
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language.CloudNaturalLanguageAnalyzeSentimentOperator
    method: POST
    data_selector: output
- name: analyze_classify_text
  endpoint:
    path: /_api/airflow/providers/google/cloud/operators/natural_language.CloudNaturalLanguageClassifyTextOperator
    method: POST
    data_selector: output
- name: analyze_entities
  endpoint:
    path: /analyzeEntities
    method: POST
    data_selector: entities
    params: {}
- name: analyze_entity_sentiment
  endpoint:
    path: /analyzeEntitySentiment
    method: POST
    data_selector: entities
    params: {}
- name: analyze_sentiment
  endpoint:
    path: /analyzeSentiment
    method: POST
    data_selector: documentSentiment
    params: {}
- name: classify_text
  endpoint:
    path: /classifyText
    method: POST
    data_selector: categories
    params: {}
notes:
- Airflow is dedicated to providing a harassment-free experience for everyone.
- Successful installation requires a Python 3 environment.
- Starting with Airflow 3.1.0, Airflow supports Python 3.10, 3.11, 3.12, 3.13.
- While there have been successes with using other tools like poetry or pip-tools,
  they do not share the same workflow as pip or uv.
- Officially supported installation methods include `pip` and `uv`.
- Only pip installation is currently officially supported.
- Out of the box, Airflow uses a SQLite database.
- Only `pip` installation is currently officially supported.
- There are known issues with `bazel` that might lead to circular dependencies when
  using it to install Airflow.
- Airflow uses a SQLite database out of the box.
- Successful installation requires a Python 3 environment. Starting with Airflow 2.7.0,
  Airflow supports Python 3.8, 3.9, 3.10, 3.11 and 3.12.
- There are known issues with bazel that might lead to circular dependencies when
  using it to install Airflow. Please switch to pip if you encounter such problems.
- There are known issues with bazel that might lead to circular dependencies when
  using it to install Airflow.
- Successful installation requires a Python 3 environment. Starting with Airflow 2.7.0,
  Airflow supports Python 3.8, 3.9, 3.10 and 3.11.
- While there have been successes with using other tools like poetry or pip-tools,
  they do not share the same workflow as pip - especially when it comes to constraint
  vs. requirements management.
- Out of the box, Airflow uses a SQLite database, which you should outgrow fairly
  quickly since no parallelization is possible using this database backend.
- Airflow uses a SQLite database by default.
- Python 3.11 is not yet supported.
- Airflow uses a SQLite database, which you should outgrow fairly quickly since no
  parallelization is possible using this database backend.
- On November 2020, new version of PIP (20.3) has been released with a new, 2020 resolver.
  This resolver does not yet work with Apache Airflow and might leads to errors in
  installation - depends on your choice of extras. In order to install Airflow you
  need to either downgrade pip to version 20.2.4 or, in case you use Pip 20.3, you
  need to add option --use-deprecated legacy-resolver to your pip install command.
- On November 2020, new version of PIP (20.3) has been released with a new, 2020 resolver.
  This resolver does not yet work with Apache Airflow and might leads to errors in
  installation - depends on your choice of extras. In order to install Airflow you
  need to either downgrade pip to version 20.2.4 `pip upgrade --pip==20.2.4` or, in
  case you use Pip 20.3, you need to add option `--use-deprecated legacy-resolver`
  to your pip install command.
- Requires setup of connected app in api
- Users are responsible for setting up database.
- Managed Services usually provide everything you need to run Airflow.
- Setting up the sandbox in the Quick Start section was easy; building a production-grade
  environment requires a bit more work!
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Avoid using Airflow Variables/Connections or accessing Airflow database at the top
  level of your timetable code.
- 'Creating a new Dag is a three-step process: writing Python code to create a Dag
  object, testing if the code meets your expectations, configuring environment dependencies
  to run your Dag.'
- You should avoid writing the top level code which is not necessary to create Operators
  and build Dag relations between them.
- You should wait for your Dag to appear in the UI to be able to trigger it.
- Keep a staging environment to test the complete Dag run before deploying in the
  production.
- Do not hard code values inside the Dag and then change them manually according to
  the environment.
- The watcher pattern is how we call a Dag with a task that is 'watching' the states
  of the other tasks.
- Change the file_parsing_sort_mode to modified_time, raise the min_file_process_interval
  to 600 (10 minutes), 6000 (100 minutes) or a higher value.
- The Dag parser will skip the min_file_process_interval check if a file is recently
  modified.
- This might not work for case where the Dag is imported/created from a separate file.
- Ensure sorting method matches your sync strategy.
- Common causes for disappearing Dags include total parsing of all Dags is too long,
  inconsistent dynamic Dag generation, file processing configuration issues, file
  synchronization problems, and time synchronization issues.
- Dags should have various associated tests to ensure that they produce expected results.
- Total parsing of all Dags is too long - If parsing takes longer than dagbag_import_timeout,
  files may not be processed completely.
- Inconsistent dynamic Dag generation - Dags created through dynamic generation must
  produce stable Dag IDs across parses.
- File processing configuration issues - A certain combination of parameters may lead
  to scenarios which certain Dags are less likely to be processed at each loop.
- File synchronization problems - Common with git-sync setups.
- Time synchronization issues - Ensure all nodes (database, schedulers, workers) use
  NTP with <1s clock drift.
- Its always a wise idea to backup the metadata database before undertaking any operation
  modifying the database.
- You might consider disabling the Airflow cluster while you perform such maintenance.
- It is recommended against using dynamic values as start_date.
- When using depends_on_past=True, its important to pay special attention to start_date.
- When scheduling Dag, the `next_ds` `next_ds_nodash` `prev_ds` `prev_ds_nodash` are
  calculated using `logical_date` and the Dags schedule (if applicable). If you set
  `schedule` as `None` or `@once`, the `next_ds`, `next_ds_nodash`, `prev_ds`, `prev_ds_nodash`
  values will be set to `None.
- When manually triggering Dag, the schedule will be ignored, and `prev_ds == next_ds
  == ds`.
- Ensure sorting method matches your sync strategy
- 'Check these parameters: file_parsing_sort_mode, parsing_processes, parsing_cleanup_interval.'
- 'File synchronization problems - Common with git-sync setups: Symbolic link swapping
  delays, Permission changes during sync, mtime preservation issues.'
- If a tasks Dag failed to parse on the worker, the scheduler may mark the task as
  failed. If confirmed, consider increasing core.dagbag_import_timeout and dag_processor.dag_file_processor_timeout.
- The scheduler will mark a task as failed if the task has been queued for longer
  than scheduler.task_queued_timeout.
- If a task instances heartbeat times out, it will be marked failed by the scheduler.
- A user marked the task as successful or failed in the Airflow UI.
- An external script or process used the Airflow REST API to change the state of a
  task.
- A dag run timeout can be specified by dagrun_timeout in the dags definition.
- When a task process consumes too much memory for a worker, the best case scenario
  is it is killed with SIGKILL (exit code -9). Depending on configuration and infrastructure,
  it is also possible that the whole worker will be killed due to OOM and then the
  tasks would be marked as failed after failing to heartbeat.
- The workers.socket_cleanup_timeout option controls how long the supervisor waits
  after the task finishes before force-closing any remaining sockets. If you observe
  leftover supervisor processes, consider increasing this delay.
- It is recommended against using dynamic values as start_date, especially datetime.now()
  as it can be quite confusing.
- Airflow will now auto align the start_date and the schedule, by using the start_date
  as the moment to start looking.
- The smtp provider is now pre-installed when you install Airflow.
- Deadline Alerts are experimental in 3.1 and may change in future versions based
  on user feedback.
- When a task process consumes too much memory for a worker, the best case scenario
  is it is killed with SIGKILL (exit code -9).
- Under very high concurrency the socket handlers inside the task supervisor may miss
  the final EOF events from the task process.
- Some objects like Contact may return nulls in deeply nested fields
- The behavior of the `POST /dags/{dag_id}/dagRuns` endpoint has changed. If a `logical_date`
  is not explicitly provided when triggering a DAG via the REST API, it now defaults
  to `None`.
- 'Service-Oriented Architecture: A new Task Execution API and airflow api-server
  enable task execution in remote environments with improved isolation and flexibility
  (AIP-72).'
- 'Edge Executor: A new executor that supports distributed, event-driven, and edge-compute
  workflows (AIP-69), now generally available.'
- 'Stable Authoring Interface: DAG authors should now use the new airflow.sdk namespace
  to import core DAG constructs like @dag, @task, and DAG.'
- 'Scheduler-Managed Backfills: Backfills are now scheduled and tracked like regular
  DAG runs, with native UI and API support (AIP-78).'
- 'DAG Versioning: Airflow now tracks structural changes to DAGs over time, enabling
  inspection of historical DAG definitions via the UI and API (AIP-66).'
- 'Asset-Based Scheduling: The dataset model has been renamed and redesigned as assets,
  with a new @asset decorator and cleaner event-driven DAG definition (AIP-74, AIP-75).'
- 'Support for ML and AI Workflows: DAGs can now run with logical_date=None, enabling
  use cases such as model inference, hyperparameter tuning, and non-interval workflows
  (AIP-83).'
- 'Removal of Legacy Features: SLAs, SubDAGs, DAG and Xcom pickling, and several internal
  context variables have been removed. Use the upgrade tools to detect deprecated
  usage.'
- 'Split CLI and API Changes: The CLI has been split into airflow and airflowctl (AIP-81),
  and REST API now defaults to logical_date=None when triggering a new DAG run.'
- 'Modern React UI: A complete UI overhaul built on React and FastAPI includes version-aware
  views, backfill management, and improved DAG and task introspection (AIP-38, AIP-84).'
- 'Migration Tooling: Use ruff and airflow config update to validate DAGs and configurations.
  Upgrade requires Airflow 2.7 or later and Python 3.93.12.'
- The v2 API is now the stable, fully supported interface for programmatic access
  to Airflow.
- DAG bundles are not initialized in the triggerer.
- Triggers cannot come from a DAG bundle.
- DAGs must now use the unified `schedule` field for all time- and event-based scheduling
  logic.
- Airflow no longer supports triggering DAG runs with a logical date in the future.
- Some objects may return nulls in deeply nested fields
- Fix log retrieval failures for in-progress tasks by properly configuring JWT authentication
- Triggers can come from anywhere else on `sys.path`.
- HITL operators require apache-airflow-providers-standard package and Airflow 3.1+.
- Deadline Alerts currently support only asynchronous callbacks (AsyncCallback).
- Airflow 3.0 introduces several configuration and interface updates that improve
  consistency.
- The default value of `catchup_by_default` is now `False`.
- The default value of `create_cron_data_intervals` is now `False`.
- Ease migration to Airflow 3
- Deployments can opt-out of data collection by setting the '[usage_data_collection]enabled'
  option to 'False', or the 'SCARF_ANALYTICS=false' environment variable.
- Support for Microsoft SQL-Server for Airflow Meta Database has been removed.
- Datasets must use a URI that conform to rules laid down in AIP-60.
- Airflow no longer silently updates configuration options that retain deprecated
  default values.
- Several configuration defaults have changed in Airflow 3.0 to better reflect modern
  usage patterns.
- It is advised to upgrade user code to use pendulum 3 as soon as possible.
- Graphviz will remain to be installed for most users.
- To ensure Airflow is secure by default, the raw HTML support in trigger UI has been
  super-seeded by markdown support via the description_md attribute.
- Add stronger warning that MSSQL is not supported and no longer functional
- Remove Python 3.7 support
- Old Graph View is removed
- The trigger UI form is skipped in web UI if no parameters are defined in a DAG
- The db init, db upgrade commands and [database] load_default_connections configuration
  options are deprecated
- In case of SMTP SSL connection, the context now uses the default context
- Disable default allowing the testing of connections in UI, API and CLI
- The xcomEntries API disables support for the deserialize flag by default
- Change of the default Celery application name
- The default value for scheduler.max_tis_per_query is changed from 512 to 16
- Some executors have been moved to corresponding providers
- Pendulum 3 introduced some subtle incompatibilities that you might rely on in your
  code - for example default rendering of dates is missing 'T' in the rendered date
  representation, which is not ISO8601 compliant.
- Airflow extras now get extras normalized to '-' (following PEP-685) instead of '_'
  and '.' when you install airflow with such extras (for example 'dbt.core' or 'all_dbs').
- Raw HTML code in DAG docs and DAG params descriptions is disabled by default.
- Change default MySQL client to MariaDB
- Mark daskexecutor provider as removed
- Clarify UI user types in security model
- Add links to DAGRun / DAG / Task in templates-ref.rst
- Add docs of how to test for DAG Import Errors
- Clean-up of our new security page
- Cleans up Extras reference page
- Update Dag trigger API and command docs
- Add deprecation info to the Airflow modules and classes docstring
- Formatting installation doc to improve readability
- Fix triggerer HA doc
- Add type annotation to code examples
- Document cron and delta timetables
- Update index.rst doc to correct grammar
- Fixing small typo in python.py
- Separate out and clarify policies for providers
- 'Fix docs: add an apache prefix to pip install'
- Default allowed pattern of a run_id has been changed to `^[A-Za-z0-9_.~:+-]+$`.
- Default permissions of file task handler log directories and files has been changed
  to 'owner + group' writeable.
- SLA callbacks no longer add files to the dag processor managers queue.
- Default permissions of file task handler log directories and files has been changed
  to 'owner + group' writable.
- Some commands may require additional parameters or settings.
- Use treat-dag-id-as-regex to target multiple DAGs by treating the dag-id as a
  regex pattern.
- Default output format is 'table'.
- Default verbose logging is False.
- To authenticate Airflow API requests, clients must include a JWT in the Authorization
  header of each request.
- This is an experimental endpoint and may change or be removed without notice.
- Use caution when running commands that modify the database.
- Default output format for commands is table.
- 'Output format. Allowed values: json, yaml, plain, table (default: table)'
- Make logging output more verbose
- Default limit for paginated responses is 50.
- Clients must include a JWT in the Authorization header of each request.
- The token generated using the secret key has a short expiry time though - make sure
  that time on ALL the machines that you run Airflow components on is synchronized
  (for example using ntpd) otherwise you might get 'forbidden' errors when the logs
  are accessed.
- Added in version 2.5.0.
- The ability to allow testing connections across Airflow UI, API and CLI.
- Added in version 3.0.0.
- Airflow is now split into several independent and isolated distribution packages.
- The `apache-airflow` package is a meta-package that installs all the other packages.
- Uses OAuth2 with password bearer token.
- The ER diagram shows the snapshot of the database structure valid for Airflow version
  3.1.1 and it should be treated as an internal detail.
- It might be changed at any time and you should not directly access the database
  to retrieve information from it or modify the data.
- The token generated using the secret key has a short expiry time
- Make sure that time on ALL the machines that you run Airflow components on is synchronized
- Airflow comes with an SQLite backend by default.
- Running the default setup in production can lead to data loss in multiple scenarios.
- If you want to run production-grade Airflow, make sure to configure the backend
  to be an external database such as PostgreSQL or MySQL.
- Airflow uses LocalExecutor by default.
- For a multi-node setup, you should use the Kubernetes executor or the Celery executor.
- Best practice to ensure worker workloads have no access to the Keytab but only access
  to the periodically refreshed, temporary authentication tokens.
- Added in version 2.8.2.
- Added in version 2.7.0.
- Added in version 2.3.0.
- Added in version 2.3.4.
- Added in version 1.10.3.
- Added in version 1.10.6.
- Added in version 2.4.0.
- Added in version 2.2.0.
- Added in version 2.0.0.
- Added in version 2.1.0.
- Added in version 2.9.0.
- Added in version 1.10.10.
- Added in version 2.6.0.
- Make sure you have installed the apache-airflow-providers-apache-druid package to
  enable Druid support.
- Make sure you have installed the 'apache-airflow-providers-apache-druid' package
  to enable Druid support.
- If you choose to override the base_log_folder you may need to update the logging
  settings.
- Uses SMTP for sending email notifications
- The maximum number of retry attempts to the execution API server is 5.
- The maximum amount of time to wait before retrying a failed API request is 90.0
  seconds.
- The minimum amount of time to wait before retrying a failed API request is 1.0 seconds.
- The timeout for HTTP requests from workers to the Execution API server is 5.0 seconds.
- The maximum number of consecutive failed heartbeats before terminating the task
  instance process is 3.
- The minimum interval at which the worker checks the task instances heartbeat status
  with the API server is 5 seconds.
- The full class name of secrets backend to enable for workers is '' (empty).
- The seconds to wait after a task process exits before forcibly closing any remaining
  communication sockets is 60.0 seconds.
- You must configure a Druid Connection first.
- Airflow is now split into several independent and isolated distribution packages
- The apache-airflow package is now a meta-package that installs all the other distribution
  packages
- To use DruidOperator, you must configure a Druid Connection first.
- Airflow comes with an SQLite backend by default. This allows the user to run Airflow
  without any external database.
- If you want to run production-grade Airflow, make sure you configure the backend
  to be an external database such as PostgreSQL or MySQL.
- Prior to Airflow version 2.7.0, airflow db upgrade was used to apply migrations,
  however, it has been deprecated in favor of airflow db migrate.
- Best practice to implement proper security mechanism is to ensure worker workloads
  have no access to the Keytab but only have access to periodically refreshed, temporary
  authentication tokens.
- In a Kubernetes environment, the concept of sidecar is used where both Kerberos
  token refresher and worker are part of the same Pod.
- Make sure you have installed the `apache-airflow-providers-apache-druid` package
  to enable Druid support.
- The minimum Apache Airflow version supported by this provider distribution is 2.10.0.
- WebHDFS provides web services access to data stored in HDFS.
- Submit a task directly to Druid, you need to provide the filepath to the Druid index
  specification json_index_file, and the connection id of the Druid overlord druid_ingest_conn_id
  which accepts index jobs in Airflow Connections.
- The host to connect to can be 'local', 'yarn' or an URL.
- For Web HDFS Hook it is possible to specify multiple hosts as a comma-separated
  list.
- Minimum Apache Airflow version supported is 2.10.0.
- Minimum Apache Airflow version supported is 2.10.0
- The minimum Apache Airflow version supported by this provider distribution is 2.9.0.
- The minimum Apache Airflow version supported by this provider package is 2.9.0.
- The minimum Apache Airflow version supported by this provider package is 2.8.0.
- The minimum Apache Airflow version supported by this provider package is 2.7.0.
- This release of provider is only available for Airflow 2.10+ as explained in the
  Apache Airflow providers support policy.
- This release of provider is only available for Airflow 2.8+.
- This release of provider is only available for Airflow 2.6+ as explained in the
  Apache Airflow providers support policy
- ONLY v12 version of google ads is supported. You should set v12 when your create
  an operator or client.
- The host to connect to can be local, yarn or an URL.
- Requires setup of OAuth2 credentials in Google Cloud Console
- Updated GoogleAdsHook to support newer API versions after google deprecated v5.
  Google Ads v8 is the new default API.
- This release of provider is only available for Airflow 2.8+ as explained in the
  Apache Airflow providers support policy.
- The GCSToGCSOperator now retains the nested folder structure when moving or copying
  a single object, aligning its behavior with the behavior for multiple objects.
- The change above has been reverted in the 10.21.0 release.
- This release of provider is only available for Airflow 2.5+ as explained in the
  Apache Airflow providers support policy.
- Remove 'location' - replaced with 'region' (#23250)
- Removing `xcom_push` (#23252)
- '''bigquery_conn_id'' and ''google_cloud_storage_conn_id'' is removed. Please use
  ''gcp_conn_id'' (#23326)'
- Removal of deprecated method call (blob.download_as_string) (#20091)
- Minimum Apache Airflow version supported by this provider distribution is 2.9.0
- The GCSToGCSOperator now retains the nested folder structure when moving or copying
  a single object.
- Several AutoML operators have stopped being supported following the shutdown of
  a legacy version of AutoML Natural Language, Tables, Vision, and Video Intelligence
  services.
- This release dropped support for Python 3.7
- Minimum Apache Airflow version supported is 2.9.0
- Minimum Apache Airflow version supported by this provider package is 2.8.0.
- Minimum Apache Airflow version supported is 2.8.0.
- Remove 'location' - replaced with 'region'
- Use 'project_id' instead of 'project' in various operators
- Uses Google Cloud Secret Manager to manage secrets
- This authentication mechanism only works for Airflow 2.x
- It is also highly recommended to configure an OAuth2 audience so that the generated
  tokens are restricted to use by Airflow only.
- Uses OAuth2 with refresh token  requires setup of Google Cloud project
- Some fields may not be returned based on user permissions
- Due to apply_default decorator removal, this version of the provider requires Airflow
  2.1.0+.
- This release of provider is only available for Airflow 2.9+ as explained in the
  Apache Airflow providers support policy.
- Google provider delete deprecated reaching removal date (December 2024)
- The previous Search Ads 360 Reporting API was decommissioned on June 30, 2024.
- This release of provider is only available for Airflow 2.3+ as explained in the
  Apache Airflow providers support policy.
- 'Affected functions are: cancel_job, create_workflow_template, get_batch_client,
  get_cluster_client, get_job, get_job_client, get_template_client, instantiate_inline_workflow_template,
  instantiate_workflow_template, submit_job, update_cluster, wait_for_job'
- This version has no code changes. Its released due to yank of previous version
  due to packaging issues.
- The previous Search Ads 360 Reporting API has been decommissioned.
- The GCSToGCSOperator now retains the nested folder structure when moving or copying
  a single object, aligning its behavior with the behavior for multiple objects. If
  this change impacts your workflows, you may need to adjust your source_object parameter
  to include the full path up to the folder containing your single file and specify
  destination_object explicitly to ignore nested folders.
- The change above has been reverted in the 10.21.0 release. The behavior of the GCSToGCSOperator
  has been restored to the previous behavior.
- Prepare release for Sep 2025 2nd wave of providers
- Remove deprecated from google provider scheduled for September 2025 except 30 September
  2025
- The `account_id` must be supplied either through the connection or supplied as a
  parameter to the task.
- Dataflow operators don't not always create a virtualenv
- The account_id must be supplied either through the connection or as a parameter.
- Each operator can be tied to a specific dbt Cloud Account by providing the Account
  ID.
- The account_id for the operators is referenced within the default_args of the example
  DAG.
- Operators may require the account_id parameter.
- Deferrable mode can be used with job_run_sensor.
- Requires a valid account ID for API calls.
- Rate limits may apply depending on account type.
- Operators can execute dbt Cloud jobs, poll for status, and download run artifacts.
- Each operator can be tied to a specific dbt Cloud Account by providing the Account
  ID or through the Airflow Connection.
- Operators can execute dbt Cloud jobs and poll for their status.
- If an Account ID is not provided in an Airflow connection, account_id must be explicitly
  passed to an operator or hook method.
- If a Tenant domain is not provided, cloud.getdbt.com will be used as the default
  value assuming a multi-tenant instance in the North America region.
- All hooks and operators related to dbt Cloud use `dbt_cloud_default` by default.
- If an Account ID is not provided in an Airflow connection, `account_id` must be
  explicitly passed to an operator or hook method.
- Exports a copy of all or a subset of documents from Google Cloud Firestore to Google
  Cloud Storage.
- Export a copy of all or a subset of documents from Google Cloud Firestore to Google
  Cloud Storage.
- Select or create a Cloud Platform project using the Cloud Console.
- Enable billing for your project, as described in the Google Cloud documentation.
- Enable the API, as described in the Cloud Console documentation.
- Install API libraries via pip.
- Setup a Google Cloud Connection is required.
- Enable billing for your project.
- This API allows you to interact with dbt Cloud jobs.
- Enable the API.
- You can use Jinja templating with body, gcp_conn_id, api_version, impersonation_chain
  parameters which allows you to dynamically determine values.
- Exports a copy of all or a subset of documents from Google Cloud Firestore to Google
  Cloud Storage
- Select or create a Cloud Platform project using the Cloud Console
- Enable billing for your project
- Enable the API
- Requires setup of Google Cloud Connection
- Export database to Google Cloud Storage is performed with CloudFirestoreExportDatabaseOperator.
- You can use Jinja templating with body, gcp_conn_id, api_version, impersonation_chain
  parameters.
- Airflow is a complex system that requires continuous monitoring and adjustment of
  resources.
- Requires setup of Google Cloud project and enabling billing
- Must enable the API for the project
- Connection which uses key from the Secret Manager requires that Application Default
  Credentials (ADC) have permission to access payloads of secrets.
- Alternative way of storing connections besides storing only key in Secret Manager.
- You must select or create a Cloud Platform project using the Cloud Console.
- You must enable billing for your project.
- You must enable the API.
- Enable billing for your project as described in the Google Cloud documentation.
- Enable the API as described in the Cloud Console documentation.
- Example DAG for exporting Firestore data to Google Cloud Storage and BigQuery.
- The Helm Chart is managed by the same people who build Airflow, and they are committed
  to keep it updated whenever new features and capabilities of Airflow are released.
- This example creates collections in Firestore and exports the database to Cloud
  Storage.
- Create necessary resources using AZURE PORTAL or AZURE CLI.
- remote_base_log_folder must be prefixed with wasb:// for Airflow to use the correct
  log handler
- If you want to look up some and not others in Azure Key Vault you may do so by setting
  the relevant *_prefix parameter of the ones to be excluded as null.
- The remote_base_log_folder must be prefixed with wasb:// for Airflow to use the
  correct log handler.
- An incorrect format can cause a misleading ResourceNotFoundError, even if the container
  exists.
- Only one authorization method can be used at a time.
- If you need to manage multiple credentials or keys then you should configure multiple
  connections.
- This test needs watcher in order to properly mark success/failure when 'tearDown'
  task with trigger rule is part of the DAG
- Performs polling on the Airflow Triggerer thus freeing up resources on Airflow Worker
- Only one authorization method can be used at a time. If you need to manage multiple
  credentials or keys then you should configure multiple connections.
- Ensure billing is enabled for your project before using the operators.
- Permissions for the bucket must be configured for import/export.
- Operators uses Instance for representing instance.
- Jinja templating is supported for various parameters.
- Configure permissions for the bucket to import and export data.
- Use Jinja templating to dynamically determine values.
- Requires setup of Google Cloud project and enabling APIs.
- Too find the service account for your instance, run the CloudMemorystoreCreateInstanceOperator
  or CloudMemorystoreGetInstanceOperator and make a use of the service account listed
  under persistenceIamIdentity.
- Make sure to enable billing for your project.
- You must enable the API for your project.
- Operators uses an Instance for representing instance.
- You can use Jinja templating with certain parameters which allows you to dynamically
  determine values.
- You can use Jinja templating with parameters.
- Requires setup of Google Cloud project and enabling the API
- Use Jinja templating for dynamic values in parameters
- Jinja templating can be used to dynamically determine values.
- It is necessary to configure permissions for the bucket to import and export data.
- Use Jinja templating with parameters to dynamically determine values.
- You must create a Cloud Platform project and enable billing.
- Enable the API before using these operators.
- Jinja templating supported for dynamic values
- Operators uses a Instance for representing instance.
- You can use Jinja templating with location, instance_id, instance, project_id, retry,
  timeout, metadata, gcp_conn_id, impersonation_chain parameters.
- Example Airflow DAG for Google Cloud Memorystore service.
- Enable the API for your project in the Google Cloud Console.
- To use these operators, you must select or create a Cloud Platform project using
  the Cloud Console.
- You must enable the API before using.
- Requires setup of a Google Cloud project and API libraries.
- Requires setup of a Google Cloud project and enabling billing.
- API must be enabled in the Google Cloud Console.
- Setup a Google Cloud Connection required.
- Create a sink using a Python dictionary.
- Update a sink using a Protobuf object.
- Setup a Google Cloud Connection
- You must select or create a Cloud Platform project and enable billing.
- You must install API libraries via pip.
- You must have a Google Cloud project and enable the Natural Language API.
- Billing must be enabled for your project.
errors:
- Circular dependencies encountered when using bazel for installation.
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- If a tasks Dag failed to parse on the worker, the scheduler may mark the task as
  failed.
- The scheduler will mark a task as failed if the task has been queued for longer
  than scheduler.task_queued_timeout.
- If a task instances heartbeat times out, it will be marked failed by the scheduler.
- A dag run timeout can be specified by dagrun_timeout in the dags definition.
- When a task process consumes too much memory for a worker, the best case scenario
  is it is killed with SIGKILL (exit code -9).
- '422: Validation error'
- '400: Bad request'
- 422 errors instead of 400
- DAG runs can no longer be triggered with a future logical date.
- '422: Validation Error'
- 200 Successful Response
- 307 Temporary Redirect
- 422 Validation Error
- '200: Successful Response'
- '307: Temporary Redirect'
- 'Invalid command: Check the command syntax'
- '422 Validation Error: Check request parameters and payload.'
- Connection testing can be done maliciously leading to undesired and insecure outcomes.
- 'forbidden: errors when the logs are accessed'
- Increase the timeout value if you experience timeout errors under high load.
- 'INVALID_ARGUMENT: Check the request parameters'
- 'UNAUTHENTICATED: Check OAuth token or permissions'
- 'NOT_FOUND: Resource does not exist'
- '403 Forbidden: Check user permissions for the requested resource'
- '400 Bad Request: Validate input parameters and data format'
- 'RESOURCE_NOT_FOUND: The specified resource was not found.'
- 'INVALID_ARGUMENT: The provided argument is invalid.'
- Catch Permission Denied exception when getting secret from GCP Secret Manager
- '400 Bad Request: Check the request parameters or body.'
- '404 Not Found: Verify that the specified job or account exists.'
- '404 Not Found: Check the job or account ID.'
- '401 Unauthorized: Ensure proper authentication credentials.'
- '400 Bad Request: Check the request parameters.'
- '404 Not Found: Ensure the job or account ID is correct.'
- '404 Not Found: Check if the account_id or job_id is correct'
- '401 Unauthorized: Ensure proper authentication credentials are provided'
- 'NOT_FOUND: The specified document does not exist.'
- 'PERMISSION_DENIED: Access denied for the operation.'
- 'PERMISSION_DENIED: Ensure the user has the correct permissions'
- 'ResourceNotFoundError: An incorrect format can cause this error even if the container
  exists'
- 'Unauthorized: Check client credentials or permissions.'
- 'ResourceNotFoundError: Check the format of remote_base_log_folder.'
- 'RESOURCE_NOT_FOUND: Check if the resource exists.'
- '400 Bad Request: Invalid request parameters.'
- '404 Not Found: The specified instance does not exist.'
- '403 Forbidden: Insufficient permissions to perform the operation.'
auth_info:
  mentioned_objects:
  - Connection
  - Variable
  - XCom
  - OauthToken
  - AuthProvider
  - NamedCredential
  - OAuth2PasswordBearer
  - HTTPBearer
  - KDC
  - Keytab
  - Workload Identity
  - ComputeEngineHook
  - SSHHook
  - Principal
  - Web Identity Federation
  - OAuth2Client
  - GoogleAdsClient
  - OAuth2Token
  - DefaultAzureCredential
  - ClientSecretCredential
  - Instance
  - CloudMemorystoreCreateInstanceOperator
  - CloudMemorystoreGetInstanceOperator
  - GCSBucketCreateAclEntryOperator
  - CloudMemorystoreDeleteInstanceOperator
  - CloudMemorystoreExportInstanceOperator
  - CloudMemorystoreFailoverInstanceOperator
  - CloudMemorystoreImportOperator
  - CloudMemorystoreListInstancesOperator
  - CloudMemorystoreUpdateInstanceOperator
  - Document
  - CloudNaturalLanguageAnalyzeEntitiesOperator
  - CloudNaturalLanguageAnalyzeEntitySentimentOperator
  - CloudNaturalLanguageAnalyzeSentimentOperator
  - CloudNaturalLanguageClassifyTextOperator
client:
  base_url: https://pypi.org/project/apache-airflow-providers-google/
source_metadata: null
