resources:
- name: generate_completion
  endpoint:
    path: /api/generate
    method: POST
    data_selector: embedding
    params: {}
- name: generate_chat_completion
  endpoint:
    path: /api/chat
    method: POST
    data_selector: embedding
    params: {}
- name: create_model
  endpoint:
    path: /api/create
    method: POST
    data_selector: embedding
    params: {}
- name: check_blob_exists
  endpoint:
    path: /api/blobs/:digest
    method: HEAD
    data_selector: ''
    params: {}
- name: create_blob
  endpoint:
    path: /api/blobs/:digest
    method: POST
    data_selector: ''
    params: {}
- name: list_local_models
  endpoint:
    path: /api/tags
    method: GET
    data_selector: ''
    params: {}
- name: show_model_information
  endpoint:
    path: /api/show
    method: POST
    data_selector: ''
    params: {}
- name: generate_embeddings
  endpoint:
    path: /api/embed
    method: POST
    data_selector: embedding
    params: {}
- name: list_running_models
  endpoint:
    path: /api/ps
    method: GET
    data_selector: models
    params: {}
- name: generate_response
  endpoint:
    path: /api/generate
    method: POST
    data_selector: null
    params: {}
- name: chat
  endpoint:
    path: /api/chat
    method: POST
    data_selector: null
    params: {}
- name: generateCompletion
  endpoint:
    path: /api/generate
    method: POST
    data_selector: embedding
    params: {}
- name: generateChatCompletion
  endpoint:
    path: /api/chat
    method: POST
    data_selector: embedding
    params: {}
- name: createModel
  endpoint:
    path: /api/create
    method: POST
    data_selector: embedding
    params: {}
- name: listLocalModels
  endpoint:
    path: /api/tags
    method: GET
    data_selector: embedding
    params: {}
- name: showModelInformation
  endpoint:
    path: /api/show
    method: POST
    data_selector: embedding
    params: {}
- name: generate_embedding
  endpoint:
    path: /api/embed
    method: POST
    data_selector: embedding
- name: list_running_models
  endpoint:
    path: /api/ps
    method: GET
    data_selector: models
- name: import_model
  endpoint:
    path: /import/model
    method: POST
    data_selector: model_import_response
    params: {}
- name: quantization
  endpoint:
    path: /quantization/model
    method: POST
    data_selector: quantization_response
    params: {}
- name: share_model
  endpoint:
    path: /share/model
    method: POST
    data_selector: share_response
    params: {}
- name: generate
  endpoint:
    path: /api/generate
    method: POST
    data_selector: Content
    params: {}
- name: ollama_cpu
  endpoint:
    path: /docker/cpu
    method: POST
    data_selector: response
    params: {}
- name: ollama_nvidia_gpu
  endpoint:
    path: /docker/nvidia_gpu
    method: POST
    data_selector: response
    params: {}
- name: ollama_amd_gpu
  endpoint:
    path: /docker/amd_gpu
    method: POST
    data_selector: response
    params: {}
- name: model_file
  endpoint:
    path: /modelfile
    method: GET
    data_selector: records
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: GET
- name: completions
  endpoint:
    path: /v1/completions
    method: GET
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: model_details
  endpoint:
    path: /v1/models/{model}
    method: GET
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: GET
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
    params: {}
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: data
    params: {}
- name: chat_completion
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: completion
  endpoint:
    path: /completions
    method: POST
    data_selector: choices
    params: {}
- name: list_models
  endpoint:
    path: /models
    method: GET
    data_selector: data
    params: {}
- name: retrieve_model
  endpoint:
    path: /models/{model_id}
    method: GET
    data_selector: data
    params: {}
- name: embeddings
  endpoint:
    path: /embeddings
    method: POST
    data_selector: data
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: ''
    params: {}
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: ''
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: ''
    params: {}
- name: model_details
  endpoint:
    path: /v1/models/{model}
    method: GET
    data_selector: ''
    params: {}
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: ''
    params: {}
- name: generate_response
  endpoint:
    path: /generate
    method: POST
    data_selector: response
- name: chat_with_model
  endpoint:
    path: /chat
    method: POST
    data_selector: response
- name: model_import
  endpoint:
    path: /import/model
    method: POST
    data_selector: model
    params: {}
- name: model_run
  endpoint:
    path: /run/model
    method: POST
    data_selector: model
    params: {}
- name: ollama_container
  endpoint:
    path: /ollama/container
    method: POST
    data_selector: container
    params: {}
- name: model
  endpoint:
    path: /api/models
    method: GET
    data_selector: models
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
- name: models
  endpoint:
    path: /v1/models
    method: GET
- name: model_details
  endpoint:
    path: /v1/models/{model}
    method: GET
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: choices
    params:
      model: llava
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: choices
    params:
      model: llama3.2
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
    params: {}
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: data
    params:
      model: all-minilm
- name: chat_completion
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: messages
- name: completion
  endpoint:
    path: /completions
    method: POST
    data_selector: prompt
- name: model_list
  endpoint:
    path: /models
    method: GET
    data_selector: models
- name: model_retrieve
  endpoint:
    path: /models/{model_id}
    method: GET
    data_selector: model
- name: embedding
  endpoint:
    path: /embeddings
    method: POST
    data_selector: input
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: messages
- name: completions
  endpoint:
    path: /v1/completions
    method: POST
    data_selector: prompt
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: ''
- name: model_details
  endpoint:
    path: /v1/models/{model}
    method: GET
    data_selector: ''
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: input
- name: generate
  endpoint:
    path: /api/generate
    method: POST
    data_selector: response
    params: {}
- name: chat
  endpoint:
    path: /api/chat
    method: POST
    data_selector: response
    params: {}
notes:
- The `/api/embeddings` endpoint has been superseded by `/api/embed`.
- 要启用额外的调试日志以帮助解决问题，首先从托盘菜单中退出正在运行的应用程序，然后在 PowerShell 终端中 $env:OLLAMA_DEBUG="1" &
  "ollama app.exe"
- Ollama supports multiple model architectures including Llama, Mistral, and Gemma.
- 虽然 AMD 已将 `amdgpu` 驱动程序贡献给了官方的 Linux 内核源代码，但该版本较旧，可能不支持所有 ROCm 功能。我们建议你从 https://www.amd.com/en/support/linux-drivers
  安装最新驱动程序，以获得对你 Radeon GPU 的最佳支持。
- If you are running on an NVIDIA JetPack system, Ollama cannot automatically discover
  the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6
  to the container to select version 5 or 6.
- '`Modelfile` syntax is under development'
- '`Modelfile` is case insensitive'
- OpenAI compatibility is experimental and may undergo significant changes.
- prompt 目前仅接受字符串
- Ollama 在 macOS 和 Windows 上会自动下载更新
- Ollama 默认绑定 127.0.0.1 端口 11434
- Ollama 应在 Windows 上使用 clang 构建
- ROCm 需要提升的权限才能在运行时访问 GPU
- If you run into problems on Linux and want to install an older version, or you'd
  like to try out a pre-release before it's officially released, you can tell the
  install script which version to install.
- If your system is configured with the 'noexec' flag where Ollama stores its temporary
  executable files, you can specify an alternate location by setting OLLAMA_TMPDIR
  to a location writable by the user ollama runs as.
- While AMD has contributed the `amdgpu` driver upstream to the official linux kernel
  source, the version is older and may not support all ROCm features. We recommend
  you install the latest driver from https://www.amd.com/en/support/linux-drivers
  for best support of your Radeon GPU.
- Ollama uses unicode characters for progress indication, which may render as unknown
  squares in some older terminal fonts in Windows 10.
- If you're running on an NVIDIA JetPack system, Ollama can't automatically discover
  the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6
  to the container to select version 5 or 6.
- Modelfile syntax is in development
- OpenAI compatibility is experimental and is subject to major adjustments including
  breaking changes.
- Ollama runs locally, and conversation data does not leave your machine.
- By default, Ollama uses a context window size of 2048 tokens.
- Due to bugs in the GCC C++ library for unicode support, Ollama should be built with
  clang on windows.
errors:
- '3: not initialized'
- '46: device unavailable'
- '100: no device'
- '999: unknown'
auth_info:
  mentioned_objects: []
client:
  base_url: http://localhost:11434
  headers:
    Accept: application/json
source_metadata: null
