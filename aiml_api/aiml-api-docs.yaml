client:
  auth: To access the AI/ML API, an API Key is required, which can be obtained from
    the [API Key](https://aimlapi.com/app/keys) page, and you should follow the instructions
    in the [Quickstart guide](https://aimlapi.com/) to set it up.
  most_recurring_base_url: https://api.aimlapi.com
  paginator: Pagination is required and can be applied using the "before" and "after"
    cursors, which are object IDs that define your place in the list.
endpoints:
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-kontext-max-text-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/quickstart/setting-up':
  - /`
  - /v1`
  - /v1
- 'endpoints source: https://docs.aimlapi.com/quickstart/supported-sdks':
  - /chat/completions
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/model-database':
  - /models
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm':
  - /v1/chat/completions`
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models':
  - /v2/generate/video/minimax/generation
- 'endpoints source: https://docs.aimlapi.com/api-references/music-models':
  - /v2/generate/audio
- 'endpoints source: https://docs.aimlapi.com/api-references/moderation-safety-models':
  - /chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/3d-generating-models':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models':
  - /v1/embeddings
  - /v1
- 'endpoints source: https://docs.aimlapi.com/use-cases/create-images-illustrate-an-article':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/use-cases/animate-images-a-childrens-encyclopedia':
  - /v2
- 'endpoints source: https://docs.aimlapi.com/use-cases/create-an-assistant-to-discuss-a-specific-document': []
- 'endpoints source: https://docs.aimlapi.com/use-cases/create-a-3d-model-from-an-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/use-cases/create-a-looped-gif-for-a-web-banner':
  - /v1/images/generations
- ? 'endpoints source: https://docs.aimlapi.com/use-cases/read-text-aloud-and-describe-images-ai-tool-to-support-people-with-visual-impairments'
  : - /v1/tts
- 'endpoints source: https://docs.aimlapi.com/use-cases/find-relevant-answers-semantic-search-with-text-embeddings':
  - /v2
- 'endpoints source: https://docs.aimlapi.com/use-cases/summarize-websites-with-ai-powered-chrome-extension':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/capabilities/function-calling':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/capabilities/image-to-text-vision':
  - /chat/completions
- 'endpoints source: https://docs.aimlapi.com/capabilities/batch-processing':
  - /v1
  - /batches`
  - /batches?batch_id={batch_id
  - /v1/batches
  - /v1/batches/cancel/{batch\_id
  - /batches/cancel/{batch_id
- 'endpoints source: https://docs.aimlapi.com/capabilities/anthropic':
  - /messages
- 'endpoints source: https://docs.aimlapi.com/capabilities/models-comparsion':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/faq/call-api-in-the-asynchronous-mode': []
- 'endpoints source: https://docs.aimlapi.com/faq/openai-sdk-doesnt-work':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/glossary/concepts':
  - /`
  - /v1`
- 'endpoints source: https://docs.aimlapi.com/integrations/aider':
  - /v1`
  - /v1
- 'endpoints source: https://docs.aimlapi.com/integrations/autogpt':
  - http://localhost:3000`.
  - http://localhost:3000/build
- 'endpoints source: https://docs.aimlapi.com/integrations/continue.dev':
  - /v1`
  - /v1
- 'endpoints source: https://docs.aimlapi.com/integrations/cursor':
  - /`
  - /openai/deployments/google/gemini-2.5-pro/chat/completions?api-version=2024-12-01-preview
- 'endpoints source: https://docs.aimlapi.com/integrations/elizaos':
  - http://localhost:3000
- 'endpoints source: https://docs.aimlapi.com/integrations/gpt-researcher-gptr':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/integrations/litellm':
  - /v2
  - /v1
- 'endpoints source: https://docs.aimlapi.com/integrations/sillytavern':
  - http://localhost:8000`
- 'endpoints source: https://docs.aimlapi.com/integrations/toolhouse':
  - /v1`
  - /v1
- 'endpoints source: https://docs.aimlapi.com/quickstart':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-3.5-turbo':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4-preview':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4-turbo':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o-mini':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o-audio-preview':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o-mini-audio-preview':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o-search-preview':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4o-mini-search-preview':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o1':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o1-mini':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o3':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o3-mini':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o3-pro':
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4.1':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4.1-mini':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-4.1-nano':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/o4-mini':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-oss-20b':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-oss-120b':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5-mini':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5-nano':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5-chat':
  - /v1/chat/completions
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5-pro':
  - /v1/responses
- 'endpoints source: https://docs.aimlapi.com/api-references':
  - /models
- 'endpoints source: https://docs.aimlapi.com/api-references/text-models-llm/nvidia/llama-3.1-nemotron-70b':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-pro':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-pro-v1.1':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-pro-v1.1-ultra':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-realism':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-dev':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-dev-image-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-schnell':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-kontext-max-image-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-kontext-pro-text-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-kontext-pro-image-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-srpo-text-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/flux/flux-srpo-image-to-image':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/image-models/bytedance/uso':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.1-plus-text-to-video':
  - /v2/generate/video/alibaba/generation
  - /v2/video/generations`**.**
  - /v2
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.1-turbo-text-to-video':
  - /v2/generate/video/alibaba/generation
  - /v2/video/generations`**.**
  - /v2
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.2-plus-text-to-video':
  - /v2/generate/video/alibaba/generation
  - /v2/video/generations`**.**
  - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.2-14b-animate-replace-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.2-14b-animate-move-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-reframe-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-outpainting-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-inpainting-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-pose-image-to-video'
  : - /v2/video/generations
    - /v2
- ? 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-depth-image-to-video'
  : - /v2/video/generations
    - /v2
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.5-preview-text-to-video':
  - /v2/generate/video/alibaba/generation
  - /v2/video/generations`**.**
  - /v2
- 'endpoints source: https://docs.aimlapi.com/api-references/video-models/alibaba-cloud/wan-2.5-preview-image-to-video':
  - /v2/generate/video/alibaba/generation
  - /v2/video/generations`**.**
  - /v2
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/speech-to-text':
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/text-to-speech/alibaba-cloud/qwen3-tts-flash':
  - /v1/tts
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-2':
  - /v1/embeddings
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-code-2':
  - /v1/embeddings
  - /v1
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-finance-2':
  - /v1/embeddings
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-large-2':
  - /v1/embeddings
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-large-2-instruct':
  - /v1/embeddings
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-law-2':
  - /v1/embeddings
- 'endpoints source: https://docs.aimlapi.com/api-references/embedding-models/anthropic/voyage-multilingual-2':
  - /v1/embeddings
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/speech-to-text/stt-legacy':
  - /v1/stt
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/speech-to-text/assembly-ai/slam-1':
  - /v1/stt/create
  - /v1
  - /v1/stt/{generation\_id
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/speech-to-text/assembly-ai/universal':
  - /v1
  - /v1/stt/create
  - /v1/stt/{generation\_id
- 'endpoints source: https://docs.aimlapi.com/api-references/speech-models/voice-chat/elevenlabs/v3_alpha':
  - /v1/tts
- 'endpoints source: https://docs.aimlapi.com/api-references/music-models/elevenlabs/eleven_music':
  - /v2/generate/audio
- ? 'endpoints source: https://docs.aimlapi.com/api-references/vision-models/ocr-optical-character-recognition/google/google-ocr'
  : - /v1/ocr
- 'endpoints source: https://docs.aimlapi.com/api-references/vision-models/ofr-optical-feature-recognition':
  - /vision
- 'endpoints source: https://docs.aimlapi.com/api-references/3d-generating-models/stability-ai/triposr':
  - /v1/images/generations
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine':
  - /v1/chat/completions
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/find-links':
  - /v1/bagoodex/links
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/find-images':
  - /v1/bagoodex/images
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/find-videos':
  - /v1/bagoodex/videos
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/find-the-weather':
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea2db13b12386fa4894bed175a78b1a73d4.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdc545ed9c61f19c827f0c61fdfb6829e6.png
  - /v1/bagoodex/weather
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea2672ffbd0b88fdead232eb139fe4be010.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea237f4cdb1738823c11db9661a9008b26b.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea2487fa0071c8c05c5d8cab80602121baf.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea24e9a609cde5258c4721caaca9f044f2b.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea29a6d5d11c931cb1c6b8961c5a701ac4a.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea231f7bed0f82344fc6f02aff2997c4fbf.png
  - https://serpapi.com/searches/67b753af5f068c54e9730a02/images/bfaadf278c5af1fdfcd2f19c8fed7ea2bf3a11e7710bbb1110889fa0b00f8ffd.png
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/find-a-local-map':
  - /v1/bagoodex/local-map
- 'endpoints source: https://docs.aimlapi.com/solutions/bagoodex/ai-search-engine/get-a-knowledge-structure':
  - /v1/bagoodex/knowledge
- 'endpoints source: https://docs.aimlapi.com/solutions/openai/assistants': []
- 'endpoints source: https://docs.aimlapi.com/solutions/openai/assistants/assistant-api':
  - /assistants
  - /assistants/{assistantId
  - /assistants`
- 'endpoints source: https://docs.aimlapi.com/solutions/openai/assistants/threads':
  - /threads/{threadId
  - /threads
  - /threads`
- 'endpoints source: https://docs.aimlapi.com/solutions/openai/assistants/messages':
  - /threads/{threadId
- 'endpoints source: https://docs.aimlapi.com/solutions/openai/assistants/runs':
  - /threads/{threadId
  - /threads/runs
  - /threads/runs`
