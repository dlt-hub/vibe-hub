resources:
- name: bucket
  endpoint:
    path: /api/v2/buckets
    method: GET
    data_selector: buckets
    params: {}
- name: organization
  endpoint:
    path: /api/v2/orgs
    method: GET
    data_selector: organizations
    params: {}
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: results
    params: {}
- name: query_data
  endpoint:
    path: /api/v2/query
    method: POST
    data_selector: results
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: data
    params: {}
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: data
    params: {}
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: data
  endpoint:
    path: /data
    method: GET
    data_selector: records
    params: {}
- name: home_sensor_data
  endpoint:
    path: /write
    method: POST
    data_selector: line protocol
- name: query_sql
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SELECT * FROM cpu LIMIT 5
- name: query_influxql
  endpoint:
    path: /api/v3/query_influxql
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SELECT * FROM home
- name: write_lp
  endpoint:
    path: /api/v3/write_lp
    method: POST
- name: v2_write
  endpoint:
    path: /api/v2/write
    method: POST
- name: v1_write
  endpoint:
    path: /write
    method: POST
- name: home
  endpoint:
    path: /write
    method: POST
    data_selector: points
    params: {}
- name: home
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: points
    params: {}
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
- name: write_lp
  endpoint:
    path: /api/v3/write_lp
    method: POST
- name: v1_write
  endpoint:
    path: /write
    method: POST
- name: v2_write
  endpoint:
    path: /api/v2/write
    method: POST
- name: write_lp
  endpoint:
    path: /api/v3/write_lp
    method: POST
    params:
      accept_partial: 'true'
      no_sync: 'false'
- name: v2_write
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: database_name
    params:
      bucket: DATABASE_NAME
      precision: s
- name: v1_write
  endpoint:
    path: /write
    method: POST
    data_selector: database_name
    params:
      db: DATABASE_NAME
      precision: s
- name: v2_write
  endpoint:
    path: /api/v2/write
    method: POST
    params:
      bucket: DATABASE_NAME
      precision: s
- name: v1_write
  endpoint:
    path: /write
    method: POST
    params:
      db: DATABASE_NAME
      precision: s
      p: DATABASE_TOKEN
- name: outputs.influxdb
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: records
    params:
      database: DATABASE_NAME
      password: DATABASE_TOKEN
- name: write_metrics
  endpoint:
    path: /api/v2/write
    method: POST
- name: influxdb_3_core
  endpoint:
    path: /outputs/influxdb_v2
    method: POST
    data_selector: urls
    params:
      bucket: DATABASE_NAME
      organization: ''
- name: influxdb_v2_oss
  endpoint:
    path: /outputs/influxdb_v2
    method: POST
    data_selector: urls
    params:
      bucket: BUCKET_NAME_OSS
      organization: ORG_NAME_OSS
- name: csv_file
  endpoint:
    path: /path/to/example.csv
    method: GET
    data_selector: records
    params: {}
- name: csv_file
  endpoint:
    path: /path/to/example.csv
    method: GET
    data_selector: records
    params: {}
- name: home
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: ''
    params:
      org: ignored
      bucket: DATABASE_NAME
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: ''
    params: {}
- name: home
  endpoint:
    path: /home.lp
    method: POST
    data_selector: records
    params: {}
- name: Processing Engine
  endpoint:
    path: /influxdb3/core/plugins/
    method: GET
- name: threshold_monitor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      threshold: '90'
      notify_email: admin@example.com
      database: my_database
- name: plugin_files
  endpoint:
    path: /_internal/system.plugin_files
    method: GET
    data_selector: records
- name: threshold_monitor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_spec
    params:
      threshold: '90'
      notify_email: admin@example.com
      database: my_database
- name: async_processor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_spec
    params:
      database: my_database
- name: plugin
  endpoint:
    path: /api/v3/engine/webhook
    method: GET
    data_selector: plugins
    params: {}
- name: threshold_monitor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      trigger-spec: every:1h
      plugin-filename: threshold_check.py
      trigger-arguments: threshold=90,notify_email=admin@example.com
      database: my_database
- name: async_processor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      trigger-spec: table:metrics
      plugin-filename: heavy_process.py
      run-asynchronous: ''
      database: my_database
- name: critical_processor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      trigger-spec: table:important_data
      plugin-filename: critical_process.py
      error-behavior: retry
      database: my_database
- name: auto_disable_processor
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      trigger-spec: request:webhook
      plugin-filename: webhook_handler.py
      error-behavior: disable
      database: my_database
- name: plugin
  endpoint:
    path: /api/v3/engine/webhook
    method: GET
    data_selector: plugins
    params: {}
- name: metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: records
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: write_reports
  endpoint:
    path: /write_reports
    method: GET
    data_selector: table_name
    params: {}
- name: write_reports
  endpoint:
    path: /write_reports
    method: GET
    data_selector: records
- name: write_monitoring
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: ''
    params:
      database: monitoring
      plugin_filename: examples/wal_plugin/wal_plugin.py
      trigger_spec: all_tables
      trigger_name: wal_monitoring
- name: write_monitoring_special
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: ''
    params:
      database: monitoring
      plugin_filename: examples/wal_plugin/wal_plugin.py
      trigger_spec: all_tables
      trigger_arguments: double_count_table=temperature
      trigger_name: wal_monitoring_special
- name: write_reports
  endpoint:
    path: /write_reports
    method: GET
    data_selector: records
    params: {}
- name: temperature
  endpoint:
    path: /services/data/vXX.X/sobjects/temperature
    method: GET
    data_selector: records
    params: {}
- name: sensor_data
  endpoint:
    path: /services/data/vXX.X/sobjects/sensor_data
    method: GET
    data_selector: records
    params: {}
- name: raw_temps
  endpoint:
    path: /services/data/vXX.X/sobjects/raw_temps
    method: GET
    data_selector: records
    params: {}
- name: temps_fahrenheit
  endpoint:
    path: /services/data/vXX.X/sobjects/temps_fahrenheit
    method: GET
    data_selector: records
    params: {}
- name: temperature
  endpoint:
    path: /services/data/vXX.X/sobjects/Temperature
    method: POST
    data_selector: records
    params:
      measurement: temperature
      target_measurement: temperature_normalized
      window: 24h
      names_transformations: temp:"snake"
      values_transformations: temp:"convert_degC_to_degF"
- name: sensor_data
  endpoint:
    path: /services/data/vXX.X/sobjects/SensorData
    method: POST
    data_selector: records
    params:
      measurement: sensor_data
      target_measurement: sensor_data_clean
      names_transformations: .*:"snake alnum_underscore_only"
- name: downsampling
  endpoint:
    path: /downsample
    method: POST
    data_selector: data
    params:
      source_measurement: cpu_metrics
      target_measurement: cpu_hourly
      interval: 1h
      window: 6h
      calculations: avg
      specific_fields: usage_user.usage_system
- name: downsampling
  endpoint:
    params:
      source_measurement: cpu_metrics
      target_measurement: cpu_hourly
      interval: 1h
      window: 6h
      calculations: avg
      specific_fields: usage_user.usage_system
- name: Downsampler Plugin
  endpoint:
    path: /downsampler
    method: POST
    data_selector: records
- name: scheduled_downsampling
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_spec
    params:
      source_measurement: cpu_metrics
      target_measurement: cpu_hourly
      interval: 1h
      window: 6h
      calculations: avg
      specific_fields: usage_user.usage_system
- name: on_demand_downsampling
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_spec
    params:
      source_measurement: environment
      target_measurement: environment_10min
      interval: 10min
      window: 30min
      calculations: temperature:avg.humidity:avg.pressure:max
- name: forecast_validation
  endpoint:
    path: /forecast_validation
    method: POST
    data_selector: results
    params:
      forecast_measurement: temperature_forecast
      actual_measurement: temperature_actual
      forecast_field: predicted_temp
      actual_field: temp
      error_metric: rmse
      error_thresholds: INFO-"0.5":WARN-"0.9":ERROR-"1.2":CRITICAL-"1.5"
      window: 1h
      senders: slack
- name: Forecast Error Evaluator Plugin
  endpoint:
    path: /influxdb3/core/plugins/forecast-error-evaluator
    method: GET
- name: forecast_validation
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: records
    params:
      forecast_measurement: temperature_forecast
      actual_measurement: temperature_actual
      forecast_field: predicted_temp
      actual_field: temp
      error_metric: rmse
      error_thresholds: INFO-"0.5":WARN-"0.9":ERROR-"1.2":CRITICAL-"1.5"
      window: 1h
      senders: slack
      slack_webhook_url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
- name: forecast_error_evaluator_plugin
  endpoint:
    path: /forecast/error/evaluator
    method: GET
    data_selector: records
- name: forecast_error_validation
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger_spec
    params:
      forecast_measurement: temperature_forecast
      actual_measurement: temperature_actual
      forecast_field: predicted_temp
      actual_field: temp
      error_metric: rmse
      error_thresholds: INFO-"0.5":WARN-"0.9":ERROR-"1.2":CRITICAL-"1.5"
      window: 1h
      senders: slack
- name: forecast_error_evaluator
  endpoint:
    path: /forecast/error/evaluator
    method: GET
    data_selector: records
- name: forecast_validation
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_arguments
    params:
      forecast_measurement: temperature_forecast
      actual_measurement: temperature_actual
      forecast_field: predicted_temp
      actual_field: temperature
      error_metric: rmse
      error_thresholds: INFO-"0.5":WARN-"1.0":ERROR-"2.0":CRITICAL-"3.0"
      window: 1h
      senders: slack
      slack_webhook_url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
      min_condition_duration: 10m
- name: critical_forecast_alert
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger_arguments
    params:
      forecast_measurement: demand_forecast
      actual_measurement: demand_actual
      forecast_field: predicted_demand
      actual_field: actual_demand
      error_metric: mse
      error_thresholds: CRITICAL-"100000"
      window: 15m
      senders: sms
      twilio_from_number: '+1234567890'
      twilio_to_number: +0987654321
      notification_text: 'CRITICAL: Production demand forecast error exceeded threshold.
        MSE: $$error'
      min_condition_duration: 2m
- name: system_processing_engine_logs
  endpoint:
    path: /_internal/system.processing_engine_logs
    method: GET
    data_selector: '*'
- name: forecast_validation
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params: {}
- name: scheduled_data_transfer
  endpoint:
    path: /api/v3/engine/replicate
    method: POST
    data_selector: trigger_arguments
    params:
      measurement: cpu
      window: 1h
      catalog_configs: eyJ1cmkiOiAiaHR0cDovL25lc3NpZTo5MDAwIn0=
      namespace: monitoring
      table_name: cpu_metrics
- name: http_api_endpoint
  endpoint:
    path: /api/v3/engine/replicate
    method: POST
    data_selector: trigger_arguments
    params: {}
- name: replicate
  endpoint:
    path: /api/v3/engine/replicate
    method: POST
    data_selector: data
    params:
      measurement: cpu
      window: 1h
      catalog_configs: eyJ1cmkiOiAiaHR0cDovL25lc3NpZTo5MDAwIn0=
      namespace: monitoring
      table_name: cpu_metrics
- name: scheduler_trigger_parameters
  endpoint:
    path: /scheduler/trigger/parameters
    method: GET
    data_selector: parameters
    params: {}
- name: replicate
  endpoint:
    path: /api/v3/engine/replicate
    method: POST
    data_selector: ''
    params: {}
- name: temperature_history
  endpoint:
    path: /weather/temperature_history
    method: GET
    data_selector: records
    params: {}
- name: sensor_data
  endpoint:
    path: /sensor_data
    method: GET
    data_selector: records
    params: {}
- name: data_transfer
  endpoint:
    path: /influxdb3/core/data_transfer
    method: POST
- name: measurement
  endpoint:
    path: /api/v3/engine/replicate
    method: POST
    data_selector: measurement
    params: {}
- name: anomaly_detection
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger_arguments
    params:
      measurement: cpu
      mad_thresholds: temp:2.5:20:5@load:3:10:2m
      senders: slack
      slack_webhook_url: https://hooks.slack.com/services/...
- name: example_usage
  endpoint:
    path: /write
    method: POST
    data_selector: test_data
    params:
      database: sensors
      measurement: environment
- name: real_time_anomaly_detection
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params:
      measurement: cpu
      mad_thresholds: temp:2.5:20:5@load:3:10:2m
      senders: slack
      slack_webhook_url: https://hooks.slack.com/services/...
- name: basic_count_based_anomaly_detection
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params:
      measurement: environment
      mad_thresholds: temperature:2.5:20:5
      senders: slack
      slack_webhook_url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
- name: anomaly_detection
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: result
    params:
      measurement: cpu
      mad_thresholds: temp:2.5:20:5@load:3:10:2m
      senders: slack
- name: MAD-based anomaly detection
  endpoint:
    path: /mad-anomaly-detection
    method: POST
    data_selector: anomalies
    params: {}
- name: measurement
  endpoint:
    params:
      measurement: cpu
      mad_thresholds: temp:2.5:20:5@load:3:10:2m
      senders: slack
- name: mad_anomaly_config
  endpoint:
    path: /plugins/mad_check_plugin
    method: POST
    data_selector: configuration
    params: {}
- name: notify
  endpoint:
    path: /api/v3/engine/notify
    method: POST
    data_selector: notification_text
    params: {}
- name: scheduled_trigger
  endpoint:
    path: /api/v3/engine/forecast
    method: POST
    data_selector: measurement
    params:
      measurement: temperature
      field: value
      window: 30d
      forecast_horizont: 2d
      tag_values: region:us-west.device:sensor1
      target_measurement: temperature_forecast
      model_mode: train
      unique_suffix: 20250619_v1
- name: http_trigger
  endpoint:
    path: /api/v3/engine/forecast
    method: POST
    data_selector: measurement
    params:
      measurement: temperature
      field: value
      forecast_horizont: 7d
      tag_values:
        region: us-west
        device: sensor1
      target_measurement: temperature_forecast
      unique_suffix: model_v1_20250722
      start_time: '2025-05-20T00:00:00Z'
      end_time: '2025-06-19T00:00:00Z'
- name: scheduled_trigger
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params:
      measurement: cpu
      field_change_count: temp:3.load:2
      window: 10m
      senders: slack
- name: data_write_trigger
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params:
      measurement: cpu
      field_thresholds: temp:30:10@status:ok:1h
      senders: slack
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: adtk_anomaly_detection
  endpoint:
    path: /create/trigger
    method: POST
    data_selector: trigger
    params:
      measurement: cpu
      field: usage
      detectors: QuantileAD.LevelShiftAD
      detector_params: eyJRdWFudGlsZUFKIjogeyJsb3ciOiAwLjA1LCAiaGlnaCI6IDAuOTV9LCAiTGV2ZWxTaGlmdEFKIjogeyJ3aW5kb3ciOiA1fX0=
      window: 10m
      senders: slack
- name: Stateless ADTK detector plugin
  endpoint:
    path: /services/data/vXX.X/sobjects/StatelessADTK
    method: POST
    data_selector: records
    params: {}
- name: anomaly_detection_trigger
  endpoint:
    path: /create_trigger
    method: POST
    data_selector: trigger
    params:
      measurement: cpu
      field: usage
      detectors: QuantileAD.LevelShiftAD
      detector_params: eyJRdWFudGlsZUFEIjogeyJsb3ciOiAwLjEsICJoaWdoIjogMC45fSwgIkxldmVsU2hpZnRBRCI6IHsid2luZG93IjogMTB9fQ==
      window: 10m
      senders: slack
- name: adtk_anomaly_detection
  endpoint:
    path: /adtk/anomaly_detection
    method: POST
    data_selector: results
    params: {}
- name: system_metrics
  endpoint:
    path: /system_metrics
    method: GET
    data_selector: metrics
    params:
      hostname: localhost
      include_cpu: true
      include_memory: true
      include_disk: true
      include_network: true
      max_retries: 3
- name: system_metrics
  endpoint:
    path: /system/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: system_cpu
  endpoint:
    path: /system_cpu
    method: GET
    data_selector: records
- name: system_disk_usage
  endpoint:
    path: /system_disk_usage
    method: GET
    data_selector: records
- name: system_cpu
  endpoint:
    path: /system_cpu
    method: GET
    data_selector: records
    params: {}
- name: system_memory
  endpoint:
    path: /system_memory
    method: GET
    data_selector: records
    params: {}
- name: system_disk_usage
  endpoint:
    path: /system_disk_usage
    method: GET
    data_selector: records
    params: {}
- name: system_network
  endpoint:
    path: /system_network
    method: GET
    data_selector: records
    params: {}
- name: scheduled_trigger_parameters
  endpoint:
    path: /scheduled_trigger_parameters
    method: GET
    data_selector: parameters
    params:
      measurement: required
      senders: required
      window: required
- name: data_write_trigger_parameters
  endpoint:
    path: /data_write_trigger_parameters
    method: GET
    data_selector: parameters
    params:
      measurement: required
      field_conditions: required
      senders: required
- name: threshold_check_parameters
  endpoint:
    path: /threshold_check_parameters
    method: GET
    data_selector: parameters
    params:
      field_aggregation_values: none
      deadman_check: 'false'
      interval: 5min
      trigger_count: 1
- name: notification_parameters
  endpoint:
    path: /notification_parameters
    method: GET
    data_selector: parameters
    params:
      influxdb3_auth_token: env var
      notification_deadman_text: template
      notification_threshold_text: template
      notification_text: template
      notification_path: notify
      port_override: 8181
      config_file_path: none
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: query
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SELECT * FROM home
- name: v1_query
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SELECT * FROM home
- name: query
  endpoint:
    path: /influxdb3/query
    method: POST
    data_selector: results
- name: query_sql
  endpoint:
    path: /api/v3/query_sql
    method: POST
    data_selector: results
    params:
      db: mydb
      q: SELECT * FROM system.queries LIMIT 2
      format: jsonl
- name: query_influxql
  endpoint:
    path: /api/v3/query_influxql
    method: POST
    data_selector: results
    params:
      db: mydb
      q: SELECT * FROM cpu LIMIT 5
      format: jsonl
- name: query
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SELECT * FROM home
- name: write_data
  endpoint:
    path: /v3/write
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /v3/query
    method: GET
    data_selector: records
- name: data
  endpoint:
    path: /home
    method: GET
    data_selector: records
- name: InfluxDB3
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: home
  endpoint:
    path: /home
    method: SELECT
    data_selector: time, temp, location
    params:
      time: now() - INTERVAL '1 hour'
- name: home
  endpoint:
    path: /home
    method: GET
    data_selector: records
    params: {}
- name: parameterized_queries
  endpoint:
    path: /parameterized-queries
    method: GET
    data_selector: parameterized_queries
    params: {}
- name: measurements
  endpoint:
    path: /show/measurements
    method: GET
    data_selector: name
- name: field_keys
  endpoint:
    path: /show/field_keys
    method: GET
    data_selector: fieldKey
- name: tag_keys
  endpoint:
    path: /show/tag_keys
    method: GET
    data_selector: tagKey
- name: tag_values
  endpoint:
    path: /show/tag_values
    method: GET
    data_selector: value
- name: home
  endpoint:
    path: /home
    method: SELECT
    data_selector: '*'
    params: {}
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
- name: admin_tokens
  endpoint:
    path: /influxdb3/core/admin/tokens/
    method: GET
    data_selector: tokens
- name: influxdb
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: write_data
  endpoint:
    path: /api/v3/write
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /api/v3/query
    method: POST
    data_selector: results
    params: {}
- name: database
  endpoint:
    path: /influxdb3/core/admin/databases/create/
    method: POST
    data_selector: database
    params: {}
- name: databases
  endpoint:
    path: /manage/databases
    method: GET
- name: create_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: ''
    params:
      retention_period: 30d
- name: list_databases
  endpoint:
    path: /api/v3/configure/database
    method: GET
    data_selector: ''
    params: {}
- name: delete_database
  endpoint:
    path: /api/v3/configure/database
    method: DELETE
    data_selector: ''
    params: {}
- name: databases
  endpoint:
    path: /admin/databases
    method: GET
    data_selector: databases
    params: {}
- name: tables
  endpoint:
    path: /admin/tables
    method: GET
    data_selector: tables
    params: {}
- name: databases
  endpoint:
    path: /admin/databases
    method: GET
    data_selector: databases
    params: {}
- name: tables
  endpoint:
    path: /admin/tables
    method: GET
    data_selector: tables
    params: {}
- name: database_create
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: db
    params:
      retention_period: 30d
- name: database_list
  endpoint:
    path: /api/v3/configure/database
    method: GET
    data_selector: databases
    params: {}
- name: database_delete
  endpoint:
    path: /api/v3/configure/database
    method: DELETE
    data_selector: db
    params: {}
- name: create_database
  endpoint:
    path: /configure/database
    method: POST
    data_selector: db
    params: {}
- name: create_database
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/create/database/
    method: GET
- name: http_api
  endpoint:
    path: /influxdb3/core/api/v3/
    method: GET
- name: influxdb3_explorer
  endpoint:
    path: /influxdb3/explorer/
    method: GET
- name: create_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: db
    params: {}
- name: create_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: db
    params: {}
- name: create_database
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/create/database/
    method: POST
- name: create_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: db
- name: databases
  endpoint:
    path: /api/v3/configure/database
    method: GET
    data_selector: '[]'
- name: delete_database
  endpoint:
    path: /api/v3/configure/database
    method: DELETE
    params:
      db: DATABASE_NAME
- name: tables
  endpoint:
    path: /influxdb3/core/admin/tables/
    method: GET
    data_selector: tables
    params: {}
- name: configure_table
  endpoint:
    path: /api/v3/configure/table
    method: POST
    data_selector: db
    params: {}
- name: list_tables
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: SHOW TABLES
    params: {}
- name: write_data
  endpoint:
    path: /api/v3/write
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /api/v3/query
    method: GET
    data_selector: results
    params: {}
- name: create_table
  endpoint:
    path: /api/v3/configure/table
    method: POST
    data_selector: ''
    params: {}
- name: tables
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: DATABASE_NAME
      q: SHOW TABLES
      format: json
- name: delete_table
  endpoint:
    path: /api/v3/configure/table
    method: DELETE
    params:
      db: DATABASE_NAME
      table: TABLE_NAME
      hard_delete_at: '2025-12-31T23:59:59Z'
- name: create_admin_token
  endpoint:
    path: /api/v3/configure/token/admin
    method: POST
- name: list_admin_tokens
  endpoint:
    path: /api/v3/query_sql
    method: GET
    params:
      db: _internal
      q: SELECT * FROM system.tokens WHERE permissions = '*:*:*'
- name: regenerate_admin_token
  endpoint:
    path: /api/v3/configure/token/admin/regenerate
    method: POST
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
- name: admin_token
  endpoint:
    path: /api/v3/configure/token/admin
    method: POST
    data_selector: token string
    params: {}
- name: tokens
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: _internal
      q: SELECT name, permissions FROM system.tokens WHERE permissions = '*:*:*'
      format: csv
- name: regenerate_operator_token
  endpoint:
    path: /api/v3/configure/token/admin/regenerate
    method: POST
- name: admin_token
  endpoint:
    path: /influxdb3/create/token
    method: POST
    data_selector: token
    params:
      name: _admin
      expiry: DURATION
      offline: true
      output-file: path/to/admin-token.json
- name: admin_token
  endpoint:
    path: /create/token
    method: POST
    data_selector: token
    params:
      name: _admin
      expiry: DURATION
      output-file: path/to/admin-token.json
- name: admin_token
  endpoint:
    path: /create/token
    method: POST
    data_selector: token
    params:
      name: _admin
      expiry: DURATION
      output-file: path/to/admin-token.json
- name: last_value_cache
  endpoint:
    path: /create_last_cache
    method: POST
    data_selector: result
    params:
      database: example-db
      token: 00xoXX0xXXx0000XxxxXx0Xx0xx0
      table: home
      key-columns: room,wall
      value-columns: temp,hum,co
      count: 5
      ttl: 30mins
- name: show_last_value_caches
  endpoint:
    path: /show_system_table
    method: GET
    data_selector: caches
    params:
      database: example-db
      token: 00xoXX0xXXx0000XxxxXx0Xx0xx0
- name: delete_last_value_cache
  endpoint:
    path: /delete_last_cache
    method: DELETE
    data_selector: result
    params:
      database: example-db
      token: 00xoXX0xXXx0000XxxxXx0Xx0xx0
      table: home
      cache-name: homeLastCache
- name: last_cache
  endpoint:
    path: /api/v3/configure/last_cache
    method: POST
    data_selector: ''
    params: {}
- name: last_cache
  endpoint:
    path: /last_cache
    method: GET
    data_selector: results
    params:
      table_name: table_name
      cache_name: cache_name
- name: last_caches
  endpoint:
    path: /api/v3/query_sql
    method: POST
    data_selector: result
    params: {}
- name: last_cache
  endpoint:
    path: /api/v3/configure/last_cache
    method: DELETE
    data_selector: ''
    params: {}
- name: distinct_value_cache
  endpoint:
    path: /influxdb3/core/admin/distinct-value-cache
    method: POST
    data_selector: cache
    params:
      database: example-db
      token: 00xoXX0xXXx0000XxxxXx0Xx0xx0
      table: wind_data
      columns: country,county,city
      max-cardinality: 10000
      max-age: 24h
      cache_name: windDistinctCache
- name: distinct_cache
  endpoint:
    path: /api/v3/configure/distinct_cache
    method: POST
    data_selector: ''
    params: {}
- name: query_sql
  endpoint:
    path: /api/v3/query_sql
    method: POST
    data_selector: ''
    params: {}
- name: distinct_caches
  endpoint:
    path: /api/v3/query_sql
    method: POST
    data_selector: results
    params: {}
- name: distinct_cache
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/distinct_cache/
    method: DELETE
- name: distinct_cache
  endpoint:
    path: /influxdb3/delete/distinct_cache
    method: DELETE
    params:
      database: '{{ dlt.secrets[''influxdb3_database_name''] }}'
      table: '{{ dlt.secrets[''table_name''] }}'
      cache_name: '{{ dlt.secrets[''dvc_name''] }}'
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
    params: {}
- name: influxdb3
  endpoint:
    path: /influxdb3
    method: GET
- name: queries
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: mydb
      q: SELECT * FROM system.queries LIMIT 2
      format: jsonl
- name: plugin_files
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: results
    params:
      db: _internal
      q: SELECT * FROM system.plugin_files
      format: jsonl
- name: install_script
  endpoint:
    path: /install_influxdb3.sh
    method: GET
- name: download_latest_binary
  endpoint:
    path: /influxdb/releases/influxdb3-core-3.6.0-windows_amd64.zip
    method: GET
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: admin-token-recovery-http-bind
  endpoint:
    path: /admin/token/recovery
    method: GET
    data_selector: recovery
    params: {}
- name: admin-token-file
  endpoint:
    path: /admin/token/file
    method: POST
    data_selector: tokenFile
    params: {}
- name: node
  endpoint:
    path: /serve/node
    method: GET
    data_selector: records
    params: {}
- name: datafusion
  endpoint:
    path: /datafusion
    method: GET
    data_selector: settings
    params: {}
- name: http
  endpoint:
    path: /http
    method: GET
    data_selector: settings
    params: {}
- name: memory
  endpoint:
    path: /memory
    method: GET
    data_selector: settings
    params: {}
- name: wal
  endpoint:
    path: /wal
    method: GET
    data_selector: settings
    params: {}
- name: compaction
  endpoint:
    path: /compaction
    method: GET
    data_selector: settings
    params: {}
- name: caching
  endpoint:
    path: /caching
    method: GET
    data_selector: settings
    params: {}
- name: processing_engine
  endpoint:
    path: /processing_engine
    method: GET
    data_selector: settings
    params: {}
- name: create
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/create/
    method: GET
- name: delete
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/
    method: GET
- name: disable
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/disable/
    method: GET
- name: enable
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/enable/
    method: GET
- name: query
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/query/
    method: GET
- name: serve
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/serve/
    method: GET
- name: show
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/show/
    method: GET
- name: test
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/test/
    method: GET
- name: update
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/update/
    method: GET
- name: write
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/write/
    method: GET
- name: database
  endpoint:
    path: /influxdb3/create/database
    method: POST
- name: last_cache
  endpoint:
    path: /influxdb3/create/last_cache
    method: POST
- name: distinct_cache
  endpoint:
    path: /influxdb3/create/distinct_cache
    method: POST
- name: table
  endpoint:
    path: /influxdb3/create/table
    method: POST
- name: token
  endpoint:
    path: /influxdb3/create/token
    method: POST
- name: trigger
  endpoint:
    path: /influxdb3/create/trigger
    method: POST
- name: distinct_cache
  endpoint:
    path: /create/distinct_cache
    method: POST
    data_selector: ''
    params:
      database: <DATABASE_NAME>
      token: <AUTH_TOKEN>
      table: <TABLE>
      columns: <COLUMNS>
      cache_name: <CACHE_NAME>
- name: last_cache
  endpoint:
    path: /create/last_cache
    method: POST
    data_selector: cache
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
- name: administer_influxdb
  endpoint:
    path: /influxdb3/core/admin/
    method: GET
    data_selector: records
- name: operator_token
  endpoint:
    path: /create/token
    method: POST
- name: named_admin_token
  endpoint:
    path: /create/token
    method: POST
- name: database
  endpoint:
    path: /create/database
    method: POST
- name: create_trigger
  endpoint:
    path: /influxdb3/create/trigger
    method: POST
    params: {}
- name: database
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/database/
    method: DELETE
- name: last_cache
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/last_cache/
    method: DELETE
- name: distinct_cache
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/distinct_cache/
    method: DELETE
- name: table
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/table/
    method: DELETE
- name: token
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/token/
    method: DELETE
- name: trigger
  endpoint:
    path: /influxdb3/core/reference/cli/influxdb3/delete/trigger/
    method: DELETE
- name: token
  endpoint:
    path: /api/v3/token
    method: DELETE
    params:
      token: '{{ INFLUXDB3_AUTH_TOKEN }}'
      token-name: TOKEN_TO_DELETE
- name: delete_database
  endpoint:
    path: /delete/database
    method: DELETE
    data_selector: DATABASE_NAME
    params:
      token: AUTH_TOKEN
- name: distinct_cache
  endpoint:
    path: /delete/distinct_cache
    method: DELETE
    params:
      database: <DATABASE_NAME>
      token: <AUTH_TOKEN>
      table: <TABLE>
- name: last_cache
  endpoint:
    path: /delete/last_cache
    method: DELETE
    data_selector: records
    params:
      database: DATABASE_NAME
      token: AUTH_TOKEN
      table: TABLE_NAME
      cache_name: CACHE_NAME
- name: delete_trigger
  endpoint:
    path: /delete/trigger
    method: DELETE
    data_selector: trigger
    params:
      database: <DATABASE_NAME>
      token: <AUTH_TOKEN>
      trigger_name: <TRIGGER_NAME>
- name: trigger
  endpoint:
    path: /disable/trigger
    method: POST
    data_selector: ''
    params:
      database: '{{ dlt.secrets[''INFLUXDB3_DATABASE_NAME''] }}'
      trigger_name: <TRIGGER_NAME>
- name: trigger
  endpoint:
    path: /enable/trigger
    method: POST
    data_selector: trigger_response
    params:
      database: <DATABASE_NAME>
      trigger_name: <TRIGGER_NAME>
- name: package
  endpoint:
    path: /influxdb3/install/package
    method: POST
    data_selector: result
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params: {}
- name: node
  endpoint:
    path: /api/v1/nodes
    method: GET
    data_selector: nodes
    params: {}
- name: object-store
  endpoint:
    path: /api/v1/object-store
    method: GET
    data_selector: objectStores
    params: {}
- name: databases
  endpoint:
    path: /influxdb3/show/databases
    method: GET
- name: plugins
  endpoint:
    path: /influxdb3/show/plugins
    method: GET
- name: system
  endpoint:
    path: /influxdb3/show/system
    method: GET
- name: tokens
  endpoint:
    path: /influxdb3/show/tokens
    method: GET
- name: plugins
  endpoint:
    path: /show/plugins
    method: GET
    data_selector: ''
    params: {}
- name: plugins
  endpoint:
    path: /show/plugins
    method: GET
- name: plugin_files
  endpoint:
    path: /_internal/system.plugin_files
    method: GET
    data_selector: ''
    params: {}
- name: write_data
  endpoint:
    path: /api/v3/write
    method: POST
    data_selector: data
    params: {}
- name: query_data
  endpoint:
    path: /api/v3/query
    method: POST
    data_selector: data
    params: {}
- name: databases
  endpoint:
    path: /show/databases
    method: GET
    data_selector: databases
    params: {}
- name: system
  endpoint:
    path: /show/system
    method: GET
- name: system_summary
  endpoint:
    path: /show/system
    method: GET
    params:
      limit: '10'
- name: system_table
  endpoint:
    path: /show/system/table
    method: GET
    data_selector: entries
    params:
      limit: '10'
- name: system_table
  endpoint:
    path: /influxdb3/show/system/table
    method: GET
- name: parquet_files
  endpoint:
    path: /show/system/table/parquet_files
    method: GET
    data_selector: json
- name: system_table
  endpoint:
    path: /show/system
    method: GET
    data_selector: table
- name: parquet_files
  endpoint:
    path: /show/system/table/parquet_files
    method: GET
    data_selector: records
    params: {}
- name: system_table_list
  endpoint:
    path: /system/table-list
    method: GET
    data_selector: summary
    params:
      database: DATABASE_NAME
- name: tokens
  endpoint:
    path: /show/tokens
    method: GET
    data_selector: tokens
- name: wal_plugin
  endpoint:
    path: /test/wal_plugin
    method: POST
    params:
      database: <DATABASE_NAME>
      token: <AUTH_TOKEN>
- name: schedule_plugin
  endpoint:
    path: /test/schedule_plugin
    method: POST
    params:
      database: DATABASE_NAME
      token: AUTH_TOKEN
- name: v3_write_lp
  endpoint:
    path: /influxdb3/core/write-data/http-api/v3-write-lp/
    method: POST
    data_selector: records
- name: v1_and_v2_compatibility
  endpoint:
    path: /influxdb3/core/write-data/http-api/compatibility-apis/
    method: POST
    data_selector: records
- name: database
  endpoint:
    path: /update/database
    method: POST
    data_selector: DATABASE_NAME
    params: {}
- name: update_trigger
  endpoint:
    path: /update/trigger
    method: POST
    data_selector: ''
    params:
      database: DATABASE_NAME
      trigger_name: TRIGGER_NAME
      token: AUTH_TOKEN
- name: update_trigger
  endpoint:
    path: /influxdb3/update/trigger
    method: POST
    data_selector: trigger
    params:
      database: <DATABASE_NAME>
      trigger-name: <TRIGGER_NAME>
- name: trigger
  endpoint:
    path: /update/trigger
    method: POST
    data_selector: ''
    params:
      database: DATABASE_NAME
      trigger-name: TRIGGER_NAME
      token: AUTH_TOKEN
- name: update_trigger
  endpoint:
    path: /influxdb3/update/trigger
    method: POST
- name: trigger
  endpoint:
    path: /update/trigger
    method: POST
    data_selector: response
    params:
      database: DATABASE_NAME
      trigger-name: TRIGGER_NAME
- name: update_trigger
  endpoint:
    path: /influxdb3/update/trigger
    method: POST
    data_selector: trigger
    params:
      database: <DATABASE_NAME>
      trigger-name: <TRIGGER_NAME>
- name: write
  endpoint:
    path: /write
    method: POST
    data_selector: line_protocol
    params:
      database: DATABASE_NAME
      token: AUTH_TOKEN
- name: line_protocol
  endpoint:
    path: /line/protocol
    method: POST
    data_selector: points
    params: {}
- name: line_protocol
  endpoint:
    path: /line/protocol
    method: POST
    data_selector: points
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: write_data
  endpoint:
    path: /write_data
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /query_data
    method: GET
    data_selector: records
    params: {}
- name: array_agg
  endpoint:
    path: /functions/array_agg
    method: GET
    data_selector: records
- name: avg
  endpoint:
    path: /functions/avg
    method: GET
    data_selector: records
- name: bit_and
  endpoint:
    path: /functions/bit_and
    method: GET
    data_selector: records
- name: bit_or
  endpoint:
    path: /functions/bit_or
    method: GET
    data_selector: records
- name: bit_xor
  endpoint:
    path: /functions/bit_xor
    method: GET
    data_selector: records
- name: bool_and
  endpoint:
    path: /functions/bool_and
    method: GET
    data_selector: records
- name: bool_or
  endpoint:
    path: /functions/bool_or
    method: GET
    data_selector: records
- name: count
  endpoint:
    path: /functions/count
    method: GET
    data_selector: records
- name: first_value
  endpoint:
    path: /functions/first_value
    method: GET
    data_selector: records
- name: grouping
  endpoint:
    path: /functions/grouping
    method: GET
    data_selector: records
- name: last_value
  endpoint:
    path: /functions/last_value
    method: GET
    data_selector: records
- name: max
  endpoint:
    path: /functions/max
    method: GET
    data_selector: records
- name: mean
  endpoint:
    path: /functions/mean
    method: GET
    data_selector: records
- name: median
  endpoint:
    path: /functions/median
    method: GET
    data_selector: records
- name: min
  endpoint:
    path: /functions/min
    method: GET
    data_selector: records
- name: nth_value
  endpoint:
    path: /functions/nth_value
    method: GET
    data_selector: records
- name: string_agg
  endpoint:
    path: /functions/string_agg
    method: GET
    data_selector: records
- name: sum
  endpoint:
    path: /functions/sum
    method: GET
    data_selector: records
- name: corr
  endpoint:
    path: /functions/corr
    method: GET
    data_selector: records
- name: covar
  endpoint:
    path: /covar
    method: GET
    data_selector: records
    params: {}
- name: covar_pop
  endpoint:
    path: /covar_pop
    method: GET
    data_selector: records
    params: {}
- name: covar_samp
  endpoint:
    path: /covar_samp
    method: GET
    data_selector: records
    params: {}
- name: regr_avgx
  endpoint:
    path: /regr_avgx
    method: GET
    data_selector: records
    params: {}
- name: regr_avgy
  endpoint:
    path: /regr_avgy
    method: GET
    data_selector: records
    params: {}
- name: regr_count
  endpoint:
    path: /regr_count
    method: GET
    data_selector: records
    params: {}
- name: regr_intercept
  endpoint:
    path: /regr_intercept
    method: GET
    data_selector: records
    params: {}
- name: regr_r2
  endpoint:
    path: /regr_r2
    method: GET
    data_selector: records
    params: {}
- name: regr_slope
  endpoint:
    path: /regr_slope
    method: GET
    data_selector: records
    params: {}
- name: regr_sxx
  endpoint:
    path: /regr_sxx
    method: GET
    data_selector: records
    params: {}
- name: regr_syy
  endpoint:
    path: /regr_syy
    method: GET
    data_selector: records
    params: {}
- name: regr_sxy
  endpoint:
    path: /regr_sxy
    method: GET
    data_selector: records
    params: {}
- name: stddev
  endpoint:
    path: /stddev
    method: GET
    data_selector: records
    params: {}
- name: stddev_pop
  endpoint:
    path: /stddev_pop
    method: GET
    data_selector: records
    params: {}
- name: stddev_samp
  endpoint:
    path: /stddev_samp
    method: GET
    data_selector: records
    params: {}
- name: var
  endpoint:
    path: /var
    method: GET
    data_selector: records
    params: {}
- name: var_pop
  endpoint:
    path: /var_pop
    method: GET
    data_selector: records
    params: {}
- name: var_samp
  endpoint:
    path: /var_samp
    method: GET
    data_selector: records
    params: {}
- name: current_date
  endpoint:
    path: /current_date
    method: GET
    data_selector: DATE32
    params: {}
- name: current_time
  endpoint:
    path: /current_time
    method: GET
    data_selector: TIME
    params: {}
- name: current_timestamp
  endpoint:
    path: /current_timestamp
    method: GET
    data_selector: TIMESTAMP
    params: {}
- name: date_bin
  endpoint:
    path: /date_bin
    method: GET
    data_selector: interval
    params: {}
- name: date_bin_gapfill
  endpoint:
    path: /date_bin_gapfill
    method: GET
    data_selector: interval
    params: {}
- name: date_bin_wallclock
  endpoint:
    path: /date_bin_wallclock
    method: GET
    data_selector: interval
    params: {}
- name: date_bin_wallclock_gapfill
  endpoint:
    path: /date_bin_wallclock_gapfill
    method: GET
    data_selector: interval
    params: {}
- name: date_bin_wallclock_gapfill
  endpoint:
    path: /date_bin_wallclock_gapfill
    method: SELECT
    data_selector: results
    params: {}
- name: date_trunc
  endpoint:
    path: /date_trunc
    method: SELECT
    data_selector: results
    params: {}
- name: date_part
  endpoint:
    path: /date_part
    method: SELECT
    data_selector: results
    params: {}
- name: extract
  endpoint:
    path: /extract
    method: SELECT
    data_selector: results
    params: {}
- name: from_unixtime
  endpoint:
    path: /from_unixtime
    method: SELECT
    data_selector: results
    params: {}
- name: make_date
  endpoint:
    path: /make_date
    method: SELECT
    data_selector: results
    params: {}
- name: now
  endpoint:
    path: /now
    method: SELECT
    data_selector: results
    params: {}
- name: to_char
  endpoint:
    path: /to_char
    method: SELECT
    data_selector: results
    params: {}
- name: to_date
  endpoint:
    path: /to_date
    method: SELECT
    data_selector: results
    params: {}
- name: to_local_time
  endpoint:
    path: /to_local_time
    method: SELECT
    data_selector: results
    params: {}
- name: to_local_time
  endpoint:
    path: /to_local_time
    method: GET
    data_selector: local time
- name: to_timestamp
  endpoint:
    path: /to_timestamp
    method: GET
    data_selector: to_timestamp
- name: to_timestamp_micros
  endpoint:
    path: /to_timestamp_micros
    method: GET
    data_selector: to_timestamp_micros
- name: to_timestamp_millis
  endpoint:
    path: /to_timestamp_millis
    method: GET
    data_selector: to_timestamp_millis
- name: to_timestamp_nanos
  endpoint:
    path: /to_timestamp_nanos
    method: GET
    data_selector: to_timestamp_nanos
- name: to_timestamp_seconds
  endpoint:
    path: /to_timestamp_seconds
    method: GET
    data_selector: to_timestamp_seconds
- name: to_unixtime
  endpoint:
    path: /to_unixtime
    method: GET
    data_selector: unixtime
- name: tz
  endpoint:
    path: /tz
    method: GET
    data_selector: time_tz
- name: write_data
  endpoint:
    path: /v3/write_lp
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /v3/query
    method: POST
    data_selector: results
- name: math_functions
  endpoint:
    path: /functions/math
    method: GET
    data_selector: functions
    params: {}
- name: string_functions
  endpoint:
    path: /functions/string
    method: GET
    data_selector: functions
    params: {}
- name: string_functions
  endpoint:
    path: /functions/string
    method: GET
    data_selector: functions
- name: decode
  endpoint:
    path: /decode
    method: POST
    data_selector: results
    params: {}
- name: encode
  endpoint:
    path: /encode
    method: POST
    data_selector: results
    params: {}
- name: array_any_value
  endpoint:
    path: /array_any_value
    method: GET
    data_selector: array_any_value
    params: {}
- name: array_append
  endpoint:
    path: /array_append
    method: GET
    data_selector: array_append
    params: {}
- name: array_cat
  endpoint:
    path: /array_cat
    method: GET
    data_selector: array_cat
    params: {}
- name: array_concat
  endpoint:
    path: /array_concat
    method: GET
    data_selector: array_concat
    params: {}
- name: array_contains
  endpoint:
    path: /array_contains
    method: GET
    data_selector: array_contains
    params: {}
- name: array_dims
  endpoint:
    path: /array_dims
    method: GET
    data_selector: array_dims
    params: {}
- name: array_distance
  endpoint:
    path: /array_distance
    method: GET
    data_selector: array_distance
    params: {}
- name: array_distinct
  endpoint:
    path: /array_distinct
    method: GET
    data_selector: array_distinct
    params: {}
- name: array_element
  endpoint:
    path: /array_element
    method: GET
    data_selector: array_element
    params: {}
- name: array_empty
  endpoint:
    path: /array_empty
    method: GET
    data_selector: array_empty
    params: {}
- name: array_except
  endpoint:
    path: /array_except
    method: GET
    data_selector: array_except
    params: {}
- name: array_has
  endpoint:
    path: /array_has
    method: GET
    data_selector: array_has
    params: {}
- name: array_has_all
  endpoint:
    path: /array_has_all
    method: GET
    data_selector: array_has_all
    params: {}
- name: array_has_any
  endpoint:
    path: /array_has_any
    method: GET
    data_selector: array_has_any
    params: {}
- name: array_indexof
  endpoint:
    path: /array_indexof
    method: GET
    data_selector: array_indexof
    params: {}
- name: array_intersect
  endpoint:
    path: /array_intersect
    method: GET
    data_selector: array_intersect
    params: {}
- name: array_join
  endpoint:
    path: /array_join
    method: GET
    data_selector: array_join
    params: {}
- name: array_length
  endpoint:
    path: /array_length
    method: GET
    data_selector: array_length
    params: {}
- name: array_max
  endpoint:
    path: /array_max
    method: GET
    data_selector: array_max
    params: {}
- name: array_min
  endpoint:
    path: /array_min
    method: GET
    data_selector: array_min
    params: {}
- name: array_ndims
  endpoint:
    path: /array_ndims
    method: GET
    data_selector: array_ndims
    params: {}
- name: array_pop_back
  endpoint:
    path: /array_pop_back
    method: GET
    data_selector: array_pop_back
    params: {}
- name: array_pop_front
  endpoint:
    path: /array_pop_front
    method: GET
    data_selector: array_pop_front
    params: {}
- name: array_position
  endpoint:
    path: /array_position
    method: GET
    data_selector: array_position
    params: {}
- name: array_positions
  endpoint:
    path: /array_positions
    method: GET
    data_selector: array_positions
    params: {}
- name: array_prepend
  endpoint:
    path: /array_prepend
    method: GET
    data_selector: array_prepend
    params: {}
- name: array_prepend
  endpoint:
    path: /array_prepend
    method: SELECT
    data_selector: array_prepend
    params: {}
- name: array_remove
  endpoint:
    path: /array_remove
    method: SELECT
    data_selector: array_remove
    params: {}
- name: array_remove_all
  endpoint:
    path: /array_remove_all
    method: SELECT
    data_selector: array_remove_all
    params: {}
- name: array_remove_n
  endpoint:
    path: /array_remove_n
    method: SELECT
    data_selector: array_remove_n
    params: {}
- name: array_repeat
  endpoint:
    path: /array_repeat
    method: SELECT
    data_selector: array_repeat
    params: {}
- name: array_replace
  endpoint:
    path: /array_replace
    method: SELECT
    data_selector: array_replace
    params: {}
- name: array_replace_all
  endpoint:
    path: /array_replace_all
    method: SELECT
    data_selector: array_replace_all
    params: {}
- name: array_replace_n
  endpoint:
    path: /array_replace_n
    method: SELECT
    data_selector: array_replace_n
    params: {}
- name: array_resize
  endpoint:
    path: /array_resize
    method: SELECT
    data_selector: array_resize
    params: {}
- name: array_reverse
  endpoint:
    path: /array_reverse
    method: SELECT
    data_selector: array_reverse
    params: {}
- name: array_slice
  endpoint:
    path: /array_slice
    method: SELECT
    data_selector: array_slice
    params: {}
- name: array_sort
  endpoint:
    path: /array_sort
    method: SELECT
    data_selector: array_sort
    params: {}
- name: array_to_string
  endpoint:
    path: /array_to_string
    method: SELECT
    data_selector: array_to_string
    params: {}
- name: array_union
  endpoint:
    path: /array_union
    method: SELECT
    data_selector: array_union
    params: {}
- name: cardinality
  endpoint:
    path: /cardinality
    method: SELECT
    data_selector: cardinality
    params: {}
- name: empty
  endpoint:
    path: /empty
    method: SELECT
    data_selector: empty
    params: {}
- name: flatten
  endpoint:
    path: /flatten
    method: SELECT
    data_selector: flatten
    params: {}
- name: generate_series
  endpoint:
    path: /generate_series
    method: SELECT
    data_selector: generate_series
    params: {}
- name: Write data
  endpoint:
    path: /influxdb3/core/write-data/
    method: GET
    data_selector: records
- name: Query data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
- name: Visualize data
  endpoint:
    path: /influxdb3/core/visualize-data/
    method: GET
    data_selector: records
- name: Administer InfluxDB
  endpoint:
    path: /influxdb3/core/admin/
    method: GET
    data_selector: records
- name: Reference
  endpoint:
    path: /influxdb3/core/reference/
    method: GET
    data_selector: records
- name: regexp_count
  endpoint:
    path: /functions/regexp_count
    method: GET
    data_selector: results
- name: regexp_like
  endpoint:
    path: /functions/regexp_like
    method: GET
    data_selector: results
- name: regexp_match
  endpoint:
    path: /functions/regexp_match
    method: GET
    data_selector: results
- name: regexp_replace
  endpoint:
    path: /functions/regexp_replace
    method: GET
    data_selector: results
- name: digest
  endpoint:
    path: /functions/digest
    method: GET
    data_selector: records
    params: {}
- name: md5
  endpoint:
    path: /functions/md5
    method: GET
    data_selector: records
    params: {}
- name: sha224
  endpoint:
    path: /functions/sha224
    method: GET
    data_selector: records
    params: {}
- name: sha256
  endpoint:
    path: /functions/sha256
    method: GET
    data_selector: records
    params: {}
- name: sha384
  endpoint:
    path: /functions/sha384
    method: GET
    data_selector: records
    params: {}
- name: sha512
  endpoint:
    path: /functions/sha512
    method: GET
    data_selector: records
    params: {}
- name: distinct_cache
  endpoint:
    path: /distinct_cache
    method: GET
    data_selector: records
- name: last_cache
  endpoint:
    path: /last_cache
    method: GET
    data_selector: records
- name: arrow_cast
  endpoint:
    path: /functions/arrow_cast
    method: GET
    data_selector: records
- name: arrow_typeof
  endpoint:
    path: /functions/arrow_typeof
    method: GET
    data_selector: records
- name: get_field
  endpoint:
    path: /functions/get_field
    method: GET
    data_selector: records
- name: interpolate
  endpoint:
    path: /functions/interpolate
    method: GET
    data_selector: records
- name: locf
  endpoint:
    path: /functions/locf
    method: GET
    data_selector: records
- name: version
  endpoint:
    path: /functions/version
    method: GET
    data_selector: records
- name: window_functions
  endpoint:
    path: /sql/window/functions
    method: GET
    data_selector: functions
    params: {}
- name: ranking_functions
  endpoint:
    path: /sql/ranking/functions
    method: GET
    data_selector: functions
    params: {}
- name: home
  endpoint:
    path: /home
    method: GET
    data_selector: '*'
- name: weather
  endpoint:
    path: /weather
    method: GET
    data_selector: '*'
- name: home
  endpoint:
    path: /home
    method: GET
    data_selector: '*'
- name: retention_policies
  endpoint:
    path: /SHOW RETENTION POLICIES
    method: GET
- name: measurements
  endpoint:
    path: /SHOW MEASUREMENTS
    method: GET
- name: field_keys
  endpoint:
    path: /SHOW FIELD KEYS
    method: GET
- name: tag_keys
  endpoint:
    path: /SHOW TAG KEYS
    method: GET
- name: tag_values
  endpoint:
    path: /SHOW TAG VALUES
    method: GET
- name: home_sensor_data
  endpoint:
    path: /home_sensor_data
    method: GET
    data_selector: records
    params: {}
- name: random_numbers
  endpoint:
    path: /random_numbers
    method: GET
    data_selector: records
    params: {}
- name: count
  endpoint:
    path: /count
    method: GET
    data_selector: records
    params: {}
- name: distinct
  endpoint:
    path: /distinct
    method: GET
    data_selector: records
    params: {}
- name: integral
  endpoint:
    path: /integral
    method: GET
    data_selector: records
    params: {}
- name: mean
  endpoint:
    path: /mean
    method: GET
    data_selector: records
    params: {}
- name: median
  endpoint:
    path: /median
    method: GET
    data_selector: records
    params: {}
- name: mode
  endpoint:
    path: /mode
    method: GET
    data_selector: records
    params: {}
- name: spread
  endpoint:
    path: /spread
    method: GET
    data_selector: records
    params: {}
- name: stddev
  endpoint:
    path: /stddev
    method: GET
    data_selector: records
    params: {}
- name: bottom
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/bottom
    method: GET
    data_selector: results
- name: first
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/first
    method: GET
    data_selector: results
- name: last
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/last
    method: GET
    data_selector: results
- name: max
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/max
    method: GET
    data_selector: results
- name: min
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/min
    method: GET
    data_selector: results
- name: percentile
  endpoint:
    path: /influxdb3/core/reference/influxql/functions/percentile
    method: GET
    data_selector: results
- name: ABS
  endpoint:
    path: /abs
    method: GET
    data_selector: records
- name: ACOS
  endpoint:
    path: /acos
    method: GET
    data_selector: records
- name: ASIN
  endpoint:
    path: /asin
    method: GET
    data_selector: records
- name: ATAN
  endpoint:
    path: /atan
    method: GET
    data_selector: records
- name: numbers
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: numbers
  endpoint:
    path: /numbers
    method: GET
    data_selector: records
    params: {}
- name: numbers
  endpoint:
    path: /your/endpoint/path
    method: SELECT
    data_selector: records
    params: {}
- name: numbers
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: numbers
  endpoint:
    path: /numbers
    method: GET
    data_selector: records
    params: {}
- name: bitcoin
  endpoint:
    path: /bitcoin
    method: GET
    data_selector: records
    params:
      time: '2023-05-01T00:00:00Z'
- name: home
  endpoint:
    path: /home
    method: GET
    data_selector: records
    params:
      room: Kitchen
      time: '2025-10-30T08:00:00Z'
- name: bitcoin
  endpoint:
    path: /bitcoin
    method: GET
    data_selector: mean
    params: {}
- name: admin_token
  endpoint:
    path: /api/v3/configure/token/admin
    method: POST
- name: health_check
  endpoint:
    path: /health
    method: GET
- name: write_data
  endpoint:
    path: /api/v3/write_lp
    method: POST
    params:
      db: sensors
      precision: auto
- name: query_data
  endpoint:
    path: /api/v3/query_sql
    method: GET
    params:
      db: sensors
      format: jsonl
- name: query
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: data
    params: {}
- name: health
  endpoint:
    path: /health
    method: GET
    data_selector: data
    params: {}
- name: ping
  endpoint:
    path: /api/v3/configure/database
    method: GET
    data_selector: databases
- name: configure_processing_engine_trigger
  endpoint:
    path: /api/v3/configure/processing_engine_trigger
    method: POST
    data_selector: string
- name: install_packages
  endpoint:
    path: /api/v3/configure/plugin_environment/install_packages
    method: POST
    data_selector: ''
    params: {}
- name: install_requirements
  endpoint:
    path: /api/v3/configure/plugin_environment/install_requirements
    method: POST
    data_selector: ''
    params: {}
- name: health_check
  endpoint:
    path: /health
    method: GET
    data_selector: data
- name: configure_table
  endpoint:
    path: /api/v3/configure/table
    method: POST
    data_selector: data
- name: query_sql
  endpoint:
    path: /api/v3/query_sql
    method: GET
    data_selector: data
    params: {}
- name: configure_token_admin
  endpoint:
    path: /api/v3/configure/token/admin
    method: POST
    data_selector: data
    params: {}
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
- name: home
  endpoint:
    path: /execute
    method: POST
    data_selector: records
- name: query
  endpoint:
    path: /execute
    method: POST
    data_selector: results
    params: {}
- name: home
  endpoint:
    path: /home
    method: POST
    data_selector: data
    params: {}
- name: sql_query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params:
      query_type: sql
- name: influxql_query
  endpoint:
    path: /query
    method: POST
    data_selector: results
    params:
      query_type: influxql
- name: home_data
  endpoint:
    path: /query
    method: POST
    data_selector: data
    params:
      sql_query: SELECT FIRST(temp) FROM home WHERE room = 'kitchen' AND time >= now()
        - 100d AND time <= now() - 10d GROUP BY time(2h)
- name: SQL Query
  endpoint:
    path: /
    method: POST
    data_selector: results
    params:
      namespace_name: DATABASE_NAME
      sql_query: sql
      query_type: sql
- name: InfluxQL Query
  endpoint:
    path: /
    method: POST
    data_selector: results
    params:
      namespace_name: DATABASE_NAME
      sql_query: influxql
      query_type: influxql
- name: FlightSQLClient
  endpoint:
    path: /flight/sql
    method: POST
    data_selector: data
    params: {}
- name: FlightInfo
  endpoint:
    path: /execute
    method: POST
    data_selector: endpoints
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
- name: temperature
  endpoint:
    path: /temperature
    method: POST
    data_selector: data
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: data
    params: {}
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: write
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: query
  endpoint:
    path: /query
    method: GET
    data_selector: results
- name: write
  endpoint:
    path: /write
    method: POST
- name: query
  endpoint:
    path: /query
    method: GET
- name: home
  endpoint:
    path: /
    method: POST
    data_selector: records
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
- name: write_file
  endpoint:
    path: /write_file
    method: POST
    data_selector: records
    params: {}
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: results
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: data
    params:
      database: '{{ dlt.secrets[''database_name''] }}'
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params:
      database: '{{ dlt.secrets[''database_name''] }}'
- name: home
  endpoint:
    path: /home
    method: GET
    data_selector: records
    params: {}
- name: v3 write_lp
  endpoint:
    path: /influxdb3/core/write-data/http-api/v3-write-lp
    method: POST
- name: v1 and v2 compatibility APIs
  endpoint:
    path: /influxdb3/core/write-data/http-api/compatibility-apis
    method: GET
- name: execute queries
  endpoint:
    path: /influxdb3/core/query-data/execute-queries
    method: GET
- name: query system data
  endpoint:
    path: /influxdb3/core/admin/query-system-data
    method: GET
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: points
    params: {}
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: query_data
  endpoint:
    path: /query
    method: GET
- name: write_data
  endpoint:
    path: /influxdb3/core/write-data/
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /influxdb3/core/query-data/
    method: GET
    data_selector: records
    params: {}
- name: write_point
  endpoint:
    path: /write
    method: POST
    data_selector: points
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: records
    params: {}
- name: query_data
  endpoint:
    path: /query
    method: GET
    data_selector: records
    params: {}
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: records
- name: write_data
  endpoint:
    path: /v3/write_lp
    method: POST
    data_selector: data
    params: {}
- name: query_data
  endpoint:
    path: /v3/query
    method: GET
    data_selector: results
    params: {}
- name: configure_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: db
    params: {}
- name: create_database
  endpoint:
    path: /api/v3/configure/database
    method: POST
    data_selector: ''
    params: {}
- name: token
  endpoint:
    path: /influxdb3/core/admin/tokens
    method: GET
- name: permissions
  endpoint:
    path: /influxdb3/core/admin/permissions
    method: GET
- name: home
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: data
    params:
      precision: auto
      accept_partial: 'false'
- name: home_actions
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: ''
    params:
      db: DATABASE_NAME
      precision: auto
      accept_partial: 'false'
- name: weather
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: ''
    params:
      db: DATABASE_NAME
- name: wind_data
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: ''
    params:
      db: DATABASE_NAME
- name: bitcoin
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: ''
    params:
      db: DATABASE_NAME
- name: random_numbers
  endpoint:
    path: /api/v3/write_lp
    method: POST
    data_selector: ''
    params:
      db: DATABASE_NAME
      precision: auto
      accept_partial: 'false'
- name: databases
  endpoint:
    path: /api/v2/databases
    method: GET
    data_selector: records
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: us-west-2-1
  endpoint:
    path: /us-west-2-1
    method: GET
- name: us-west-2-2
  endpoint:
    path: /us-west-2-2
    method: GET
- name: us-east-1-1
  endpoint:
    path: /us-east-1-1
    method: GET
- name: eu-central-1-1
  endpoint:
    path: /eu-central-1-1
    method: GET
- name: us-central1-1
  endpoint:
    path: /us-central1-1
    method: GET
- name: westeurope-1
  endpoint:
    path: /westeurope-1
    method: GET
- name: eastus-1
  endpoint:
    path: /eastus-1
    method: GET
- name: inputs.mem
  endpoint:
    path: /etc/telegraf.conf
    method: GET
- name: outputs.influxdb
  endpoint:
    path: /etc/telegraf.conf
    method: GET
- name: AppInstance
  endpoint:
    path: /api/v1/influxdb
    method: POST
    data_selector: spec
    params: {}
- name: AppInstance
  endpoint:
    path: /api/v1/app-instances
    method: POST
    data_selector: spec
    params: {}
- name: AppInstance
  endpoint:
    path: /influxdb
    method: POST
    data_selector: spec
    params: {}
- name: AppInstance
  endpoint:
    path: /AppInstance
    method: POST
- name: AppInstance
  endpoint:
    path: /influxdb/AppInstance
    method: POST
    data_selector: records
- name: ingesterStorage
  endpoint:
    path: /services/data/vXX.X/sobjects/IngesterStorage
    method: GET
    data_selector: records
    params: {}
- name: license
  endpoint:
    path: /license
    method: POST
    data_selector: status
    params: {}
- name: influxdb
  endpoint:
    path: /influxdb3-clustered
    method: GET
    data_selector: records
    params: {}
- name: testdb
  endpoint:
    path: /database/create
    method: POST
- name: write_test_data
  endpoint:
    path: /write
    method: POST
- name: query_test_data
  endpoint:
    path: /query
    method: GET
- name: router
  endpoint:
    path: /services/data/vXX.X/sobjects/Router
    method: GET
    data_selector: records
    params: {}
- name: write
  endpoint:
    path: /write
    method: POST
    data_selector: null
    params:
      db: InfluxDB database name
      precision: ns
- name: write_data
  endpoint:
    path: /write
    method: POST
    data_selector: line_protocol
    params:
      db: get-started
      precision: s
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: ''
    params:
      bucket: get-started
      precision: s
- name: line_protocol_records
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: write_protocol
  endpoint:
    path: /write
    method: POST
    data_selector: records
- name: home
  endpoint:
    path: /query
    method: GET
    data_selector: records
- name: query
  endpoint:
    path: /query
    method: POST
    data_selector: rows
    params:
      database: get-started
      sql: SELECT time, room, temp, hum, co FROM home WHERE time >= '2025-10-30T08:00:00Z'
        AND time <= '2025-10-30T20:00:00Z'
- name: query_results
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: write_v1
  endpoint:
    path: /write
    method: POST
- name: write_v2
  endpoint:
    path: /api/v2/write
    method: POST
- name: outputs.influxdb_v2
  endpoint:
    path: /
    method: POST
    data_selector: urls
    params: {}
- name: influxdb_v2
  endpoint:
    path: /
    method: POST
    data_selector: data
    params: {}
- name: home_sensor_data
  endpoint:
    method: POST
    data_selector: line protocol
- name: home
  endpoint:
    path: /write
    method: POST
    data_selector: points
- name: home
  endpoint:
    path: /write
    method: POST
    data_selector: points
- name: inputs.file
  endpoint:
    path: /path/to/example.csv
    method: GET
    data_selector: records
    params: {}
- name: csv_file
  endpoint:
    path: /path/to/example.csv
    method: GET
    data_selector: records
    params: {}
- name: csv_file_input
  endpoint:
    path: /path/to/example.csv
    method: GET
    data_selector: records
    params: {}
- name: influxdb_output
  endpoint:
    path: /write
    method: POST
    data_selector: records
    params:
      bucket: DATABASE_NAME
- name: write_data
  endpoint:
    path: /write
    method: POST
- name: write_data
  endpoint:
    path: /api/v2/write
    method: POST
    data_selector: data
    params:
      org: ignored
      bucket: DATABASE_NAME
      precision: s
- name: home
  endpoint:
    path: /home.lp
    method: POST
    data_selector: home
- name: garbage_collection_tuning
  endpoint:
    path: /influxdb3/clustered/reference/internals/storage-engine/#garbage-collection
    method: GET
    data_selector: garbage_collection_tuning
    params: {}
- name: garbage-collector
  endpoint:
    path: /influxdb/garbage-collector
    method: POST
    data_selector: env
    params:
      INFLUXDB_IOX_GC_OBJECTSTORE_CUTOFF: 6h
      INFLUXDB_IOX_GC_PARQUETFILE_CUTOFF: 6h
- name: query
  endpoint:
    path: /query
    method: GET
    data_selector: results
    params: {}
- name: query
  endpoint:
    path: /query
    method: GET
notes:
- Uses OAuth2 with refresh token  requires setup of connected app in api
- The latest tag for InfluxDB Docker images will point to InfluxDB 3 Core on February
  3, 2026.
- Authentication and authorization details can be found in the documentation.
- On February 3, 2026, the latest tag for InfluxDB Docker images will point to InfluxDB
  3 Core.
- InfluxDB Cloud Serverless is backed by InfluxDB 3, designed to handle high write
  and query loads.
- InfluxDB Cloud Serverless does not officially support Flux.
- Uses OAuth2 with refresh token  requires setup of connected app in InfluxDB
- Some objects may return nulls in deeply nested fields
- Requires setup of OAuth2 for API access
- No authentication required for accessing data
- To use this feature, OAuth 2.0 authentication must be configured.
- Kapacitor is an open source data processing framework that makes it easy to create
  alerts, run ETL jobs and detect anomalies.
- Uses HTTP API for data ingestion and querying
- InfluxDB 3 Core uses object storage to store time series data in Apache Parquet
  format.
- Performance on your local filesystem will likely be better, but object storage has
  the advantage of not running out of space and being accessible by other systems
  over the network.
- InfluxDB 3 Core runs on Linux, macOS, and Windows.
- Uses HTTP API for data writing and querying.
- InfluxDB 3 Core uses token-based authorization.
- Authorization is enabled by default when you start the server.
- InfluxDB 3 Core uses token-based authorization to authorize actions in the database.
- InfluxDB is a schema-on-write database, meaning you can start writing data and InfluxDB
  creates the logical database, tables, and their schemas automatically.
- Requires a database named `mydb` with a table named `foo`.
- Credentials should be stored as environment variables or in a .env file.
- Default precision for timestamps is lineprotocol.Nanosecond.
- Uses token-based authentication.
- Default timestamp precision is second.
- Tags in a table (measurement) are immutable
- A tag and a field cant have the same name within a table
- Authorization can use Bearer or Token schemes for v1 and v2 APIs.
- Telegraf 1.9.2 or greater is required.
- Store the InfluxDB 3 Core token in a secret store or environment variable to avoid
  exposing the raw token string.
- Store this in a secret store or environment variable to avoid exposing the raw token
  string.
- Store your authorization token as an environment variable to avoid exposing the
  raw token string.
- For InfluxDB 3 Core, set organization to an empty string.
- Telegraf is a plugin-based agent that collects metrics from different sources and
  writes them to specified destinations.
- Each row must include a time column.
- Data stored in a table should be homogenous.
- Uses OAuth2 for authentication
- Use gzip compression to speed up writes to InfluxDB 3 Core.
- Optimize writes to InfluxDB 3 Core
- The optimal batch size is 10,000 lines of line protocol or 10 MBs, whichever threshold
  is met first.
- Store the AUTH_TOKEN in a secret store or environment variable to avoid exposing
  the raw token string.
- Optimize performance and system overhead when writing data to InfluxDB 3 Core
- Optimal batch size is 10,000 lines of line protocol or 10 MBs.
- Store AUTH_TOKEN in a secret store or environment variable to avoid exposing the
  raw token string.
- The influxdb3 binary requires the adjacent python/ directory to function.
- Add the parent directory to your PATH; do not move the binary out of this directory.
- Use shared storage or file synchronization tools to keep plugins consistent.
- Plugin upload and update operations require admin tokens to prevent unauthorized
  code deployment
- Standard resource tokens cannot upload or modify plugin code
- --upload flag requires admin privileges
- update trigger command requires admin token
- Admin privileges required for plugin uploads
- Plugin upload and update operations require admin tokens to prevent unauthorized
  code deployment.
- This security model ensures only administrators can introduce or modify executable
  code in your database.
- Admin privileges required for plugin uploads.
- Use trigger arguments to pass configuration from a trigger to the plugin it runs.
- Arguments are passed to the plugin as a Dict[str, str] where the key is the argument
  name and the value is the argument value.
- Requires setup of InfluxDB 3 Core instance
- Triggers run synchronously by default.
- To allow multiple instances of the same trigger to run simultaneously, configure
  triggers to run asynchronously.
- For air-gapped deployments or environments with strict security requirements, you
  can disable Python package installation while maintaining Processing Engine functionality.
- Each plugin automatically has access to the shared API through the influxdb3_local
  object. You dont need to import any libraries.
- Use the trigger-specific namespace to keep plugin state isolated.
- Use TTL appropriately based on how frequently your data changes.
- Some objects like Contact may return nulls in deeply nested fields
- To use TOML configuration files, you must set the PLUGIN_DIR environment variable
  in the InfluxDB 3 Core host environment.
- This is required in addition to the --plugin-dir flag when starting InfluxDB 3 Core.
- The plugin automatically generates write reports in the `write_reports` measurement.
- Table name for which to double the row count in write reports (for testing/demonstration)
  is 'double_count_table'.
- The plugin automatically skips the write_reports table to avoid infinite recursion
- Logs are stored in the _internal database in the system.processing_engine_logs table.
- Uses InfluxDB 3 for downsampling data
- To use a TOML configuration file, set the PLUGIN_DIR environment variable and specify
  the config_file_path in the trigger arguments.
- No additional Python packages required for this plugin.
- Requires InfluxDB 3 Core with Processing Engine enabled.
- Uses environment variable for InfluxDB 3 API token
- To use TOML configuration files, you must set the PLUGIN_DIR environment variable
  in the InfluxDB 3 host environment.
- Use environment variables instead when possible
- Requires existing schema in the database.
- The InfluxDB to Iceberg Plugin enables data transfer from InfluxDB 3 to Apache Iceberg
  tables.
- Base64-encoded catalog config must be provided in the request.
- Processes data in 12-hour batches for the specified week
- Transfers only temp_celsius and humidity fields
- The plugin supports both scheduled batch transfers of historical data and on-demand
  transfers via HTTP API.
- Uses InfluxDB 3 for anomaly detection setup
- Requires properly defined table schema in the database
- Requires setup of connected app in InfluxDB 3
- Schema must exist for processing
- The MAD-Based Anomaly Detection Plugin provides real-time anomaly detection for
  time series data in InfluxDB 3 using Median Absolute Deviation (MAD).
- Plugin maintains a 20-point window of recent temperature values
- Computes median and MAD from this window
- The plugin provides real-time anomaly detection for time series data.
- Configurable thresholds for count-based and duration-based alerts.
- Plugin assumes that the table schema is already defined in the database.
- Returns an error if the schema doesnt exist or doesnt contain the expected columns.
- Enable alerts on validation failure
- Verify JSON request body format matches expected schema
- If the schema doesnt exist or doesnt contain the expected columns, an error will
  be returned.
- Ensure detector_params is valid Base64-encoded JSON
- False positive notifications may require adjusting min_consensus and min_condition_duration
- Verify sufficient data points in the specified window
- Requires setup of connected app in influxdb3
- No required parameters - all system metrics are collected by default with sensible
  defaults.
- Monitor specific metrics with custom hostname
- Customers with an annual or support contract can contact InfluxData Support.
- All system metrics are collected by default with sensible defaults.
- Uses InfluxDB 3 for monitoring system metrics
- Some disk I/O metrics may require elevated permissions.
- The plugin will continue collecting other metrics even if some require elevated
  permissions.
- If the plugin causes high CPU usage, consider increasing the trigger interval.
- Monitor system performance every 30 seconds.
- Requires existing schema
- Returns an error if the schema doesnt exist or doesnt contain the expected columns
- Use the InfluxDB v3 HTTP query API to query data with SQL or InfluxQL.
- Authorization token must have read permissions on the database.
- 'Use Encryption: Enable for HTTPS connections'
- Ensure the token was copied correctly without extra spaces or characters
- Uses SQL for querying data.
- Gap-filling SQL queries handle missing data in time series data by filling in gaps
  with interpolated values or by carrying forward the last available observation.
- InfluxDB 3 supports parameters in WHERE clause predicate expressions.
- InfluxQL was designed around the InfluxDB v1 data model.
- Use of FROM clause is strongly recommended for performance.
- Querying data without time bounds can return an unexpected amount of data.
- Requires setup of connected app in InfluxDB
- Grafana requires HTTP/2 for SQL queries.
- Admin token is required for authentication.
- The Microsoft Power BI Connector for InfluxDB is currently in BETA
- Custom connector requires Windows operating system
- Requires InfluxDB 3 Core running instance with data to query
- Use a LIMIT clause to restrict the number of rows returned.
- Always include time range filters in your queries to limit data volume.
- Uses HTTP API for data ingestion
- InfluxDB 3 provides HTTP API for data ingestion and querying.
- The minimum practical retention period is 1 hour (`1h`)
- 'Maximum number of databases: 5'
- 'Maximum number of tables across all databases: 2000'
- 'Maximum number of columns per table: 500'
- By default, data does not expire.
- The minimum practical retention period is 1 hour (`1h`).
- 'Maximum number of databases: 5.'
- 'Maximum number of tables across all databases: 2000.'
- 'Maximum number of columns per table: 500.'
- Retention periods are immutable in Core
- Retention periods can only be set when creating a database and cannot be changed
  afterward.
- Retention periods are immutable in Core.
- Deleting a database is a destructive action. Once a database is deleted, data stored
  in that database cannot be recovered.
- Tables are synonymous with measurements in InfluxDB 3 Core.
- They are typically created automatically when you write line protocol data, but
  you can also manually create them to define custom schemas or apply settings before
  writing data.
- Requires proper OAuth configuration for access.
- Tables must include at least one tag column.
- Field columns are optional and can be added later when you write data.
- Username is ignored, but required for the request
- 'InfluxDB 3 Core supports two types of admin tokens: Operator token and Named admin
  token.'
- An InfluxDB 3 Core instance can have one operator token and unlimited named admin
  tokens.
- Regenerating the operator token invalidates the previous token.
- Always store your operator token securely and consider implementing proper secret
  management practices.
- Token file permissions should be restricted 0600 to protect the token.
- Use a preconfigured admin token file for automated deployments.
- LVCs are flushed when the server stops.
- Historical data is loaded into the cache on creation.
- Use the --select option to query specific columns from the last_caches system table.
- Results are sorted in ascending order based on the provided columns.
- DVCs are stored in memory; the larger the cache, the more memory your InfluxDB 3
  node requires to maintain it.
- Distinct Value Caches are flushed when the server stops.
- InfluxDB 3 requires setup for authentication.
- Ensure proper database and token management.
- MinIO uses the S3-compatible API to interact with your MinIO server or cluster.
- Backup and restore procedures dont apply to memory-based object stores
- Files in <node_id>/table-snapshots/ are intentionally excluded from backup
- InfluxDB 3.6 is now available for both Core and Enterprise.
- Key enhancements include the beta launch of Ask AI in InfluxDB 3 Explorer.
- Configure thread allocation, memory settings, and other parameters to optimize InfluxDB
  3 Core performance based on your workload characteristics.
- 'Start with monitoring: Understand your current bottlenecks before tuning'
- 'Change one parameter at a time: Isolate the impact of each change'
- 'Test with production-like workloads: Use realistic data and query patterns'
- 'Document your configuration: Keep track of what works for your workload'
- 'Plan for growth: Leave headroom for traffic increases'
- 'Regular review: Periodically reassess as workloads evolve'
- Increase execution memory pool to 90%
- Enable Parquet caching of 4GB
- Reduce WAL flush interval from 1s to 100ms
- Long query execution times
- High memory usage during queries
- Query timeouts
- 'Increase execution memory pool: --exec-mem-pool-bytes=90%'
- 'Enable Parquet caching: --parquet-mem-cache-size=4GB'
- Optimize query patterns (smaller time ranges, fewer fields)
- Monitor CPU per thread
- Track memory usage
- Check IO wait
- Control memory pressure from write buffers
- Symptoms of slow query performance include long query execution times and high memory
  usage during queries
- Solutions for slow queries include increasing execution memory pool, enabling Parquet
  caching, and optimizing query patterns
- On February 3, 2026, the latest tag for InfluxDB Docker images will point to InfluxDB
  3 Core. To avoid unexpected upgrades, use specific version tags in your Docker deployments.
- Before and after upgrading, verify the InfluxDB 3 Core version running on your instance.
- Use environment variables to configure the InfluxDB 3 MCP server and connect it
  to your InfluxDB 3 Core server.
- Disables authentication for all server actions.
- Set IO threads (global option before serve)
- Default preemptive-cache-age is 3d
- Default parquet-mem-cache-size is 20%
- Default parquet-mem-cache-prune-percentage is 0.1
- Default parquet-mem-cache-prune-interval is 1s
- Default parquet-mem-cache-query-path-duration is 5h
- Default table-index-cache-max-entries is 1000
- Default table-index-cache-concurrency-limit is 8
- Default last-cache-eviction-interval is 10s
- Default distinct-cache-eviction-interval is 10s
- Default query-file-limit is 432
- Defaults for various configurations are provided.
- Default DataFusion threads are all available cores minus IO threads
- Default gen1-lookback-duration is 24h
- Default retention-check-interval is 30m
- Default delete-grace-period is 24h
- Default hard-delete-default-duration is 90d
- Default telemetry-disable-upload is false
- Quick-start mode is designed for development and testing environments.
- For production deployments, use explicit configuration with the serve subcommand
  and specify all required parameters.
- 'Configuration precedence: CLI flags > environment variables > auto-generated defaults.'
- Valid database names are alphanumeric and start with a letter or number. Dashes
  (-) and underscores (_) are allowed.
- Minimum for data retention is 1 hour (1h).
- Setting a retention period to 0<unit> marks all data for immediate deletion at query
  time.
- Tokens must be included explicitly unless set via INFLUXDB3_AUTH_TOKEN
- Table and column names must already exist or be recognized by the engine
- The operator token has full administrative privileges and doesnt expire.
- Offline admin tokens are designed for automated deployments.
- Authentication token is required
- Authentication token is required for operations.
- Some configuration options must be specified before the serve command.
- To avoid unexpected upgrades, use specific version tags in your Docker deployments.
- Output format can be pretty, json, jsonl, csv, or parquet
- 'Required option: --database <DATABASE_NAME>'
- Requires authentication token
- Output format can be set to pretty, json, jsonl, csv, or parquet
- Requires setup of database name and authentication token
- InfluxDB 3 Core does not support updating database retention periods.
- Requires admin token for uploading local plugin files.
- Requires admin token for uploading local files
- Requires admin token for uploading local plugin files
- Table names are case-sensitive and subject to naming restrictions.
- Line protocol is whitespace-sensitive.
- Line protocol does not support the newline character in tag or field values.
- Refer to the InfluxDB 3 API documentation for specific usage and best practices.
- Unquoted identifiers are not case-sensitive and match any measurement, tag key,
  or field key with the same characters.
- We recommend always double-quoting identifiers, regardless of case-sensitivity.
- Data types define the type of values that can be stored in table columns.
- Names and identifiers in SQL are case-insensitive by default.
- If a column exists on both sides of the join and is used in the SELECT, ON, WHERE,
  HAVING, GROUP BY, or ORDER BY clause, you must use a fully-qualified reference.
- Unlike InfluxQL, SQL supports OR in the WHERE clause to specify multiple conditions,
  including time ranges.
- Single quotes are required for string literals in the WHERE clause.
- If your query includes a GROUP BY clause, the ORDER BY clause must appear after
  the GROUP BY clause.
- InfluxDB 3 provides a comprehensive set of APIs for data ingestion and querying.
- Default InfluxDB Core URL is localhost:8181
- Subqueries can be used in SELECT, FROM, WHERE, and HAVING clauses.
- Use specific version tags in your Docker deployments to avoid unexpected upgrades.
- Use the InfluxDB HTTP API for data ingestion
- Uses InfluxDB HTTP API for data ingestion
- Refer to specific API version documentation for detailed usage
- Supports various string functions for SQL operations.
- The 'latest' tag for InfluxDB Docker images will point to InfluxDB 3 Core on February
  3, 2026.
- The default time range is the Unix epoch (`1970-01-01T00:00:00Z`) to *now*.
- InfluxQL does not support querying multiple time ranges
- InfluxQL does not support grouping data by fields.
- The order that tags are listed in the GROUP BY clause does not affect how data is
  grouped.
- GROUP BY time() intervals use preset round-number time boundaries that are independent
  of time conditions in the WHERE clause.
- Queries ignore fill() if no data exists in the queried time range.
- fill(previous) doesnt fill null values if there is no previous value in the queried
  time range.
- fill(linear) doesnt fill null values if there are no values before or after the
  null value in the queried time range.
- 'If the `ORDER BY` clause is not included, the default behavior is to sort data
  by time in ascending order: `ORDER BY time ASC`.'
- InfluxQL is being rearchitected to work with the InfluxDB 3 storage engine.
- InfluxQL does not support a HAVING clause, however subqueries offer similar functionality.
- Apply time bounds to the outer query to improve performance.
- TOP() returns the field value with the earliest timestamp if theres a tie between
  two or more values for the greatest value.
- TOP() maintains original timestamps when grouping by time.
- Must use aggregate or selector functions when grouping by time
- Supported only in the GROUP BY clause.
- Cannot query multiple time ranges
- Querying future data with a GROUP BY time() clause
- Cannot use parameters for durations
- Regular expression comparisons are more computationally intensive than exact string
  comparisons.
- Queries with regular expressions are not as performant as those without.
- InfluxQL in InfluxDB 3 Core does not currently support the SLIMIT clause
- InfluxQL in InfluxDB 3 Core does not currently support the SOFFSET clause
- Flight SQL requires HTTP/2
- Uses Bearer token for authorization.
- INFLUX_TOKEN should be set as an environment variable.
- Requires a database token with read permission to the database
- InfluxDB 3 client libraries provide configurable batch writing of data to InfluxDB
  HTTP APIs.
- The token must have read and write permissions on the specified database.
- Token must have read/write permissions on the specified database.
- The client library supports both SQL and InfluxQL for querying.
- Batch writing mode is available for efficient data operations.
- Default batch_size is 1000
- Default flush_interval is 1000
- Default jitter_interval is 0
- Default retry_interval is 5000
- Default max_retries is 5
- Default max_retry_delay is 125000
- Default max_retry_time is 180000
- Default exponential_base is 2
- Default max_close_wait is 300000
- Default write_scheduler is ThreadPoolScheduler(max_workers=1)
- Uses token-based authentication for database access.
- Database token is required with read/write permissions.
- Use appropriate precision for timestamps while writing data.
- Execution is synchronous.
- Default batch size is 1000
- Default flush interval is 1000
- InfluxDB 3 supports compatibility endpoints for writing data using InfluxDB v2 and
  v1 tools.
- The /api/v2/query API endpoint cant query data stored in InfluxDB 3 Core.
- InfluxDB database token is required for authentication
- ORG_ID is ignored by InfluxDB but required by the client library
- Uses environment variables for configuration
- Uses HTTP API for data ingestion and querying.
- For security reasons, it is recommended to set an environment variable to store
  your token.
- InfluxDB 3 requires authentication for data access.
- Setting a retention period to 0<unit> marks all data for immediate deletion.
- Database names must follow specific restrictions and conventions
- Table names in InfluxDB 3 Core follow line protocol measurement naming rules
- Telemetry data is transmitted once per hour.
- When telemetry is disabled, no usage data is collected or transmitted.
- The minimum retention period is one hour.
- The precision configuration setting determines the timestamp precision retained
  for input data points.
- Valid precisions are `ns`, `us` or `s`, `ms`, and `s`.
- Token management is essential for authentication.
- Ensure proper token scopes are assigned.
- InfluxDB Clustered requires authentication using token types.
- Avoid using the `influx` CLI with InfluxDB Clustered.
- InfluxDB Cloud Serverless requires authentication using API tokens.
- Uses environment variables for sensitive information
- InfluxDB Clustered is a commercial product offered by InfluxData.
- Contact InfluxData Sales to obtain a license before installing InfluxDB Clustered.
- InfluxDB Clustered requires kubectl 1.27 or higher.
- The recommended minimum size of the local storage is 2 GiB.
- InfluxDB Clustered deployments are managed using Kubernetes and configured using
  a YAML configuration file.
- We recommend editing the `AppInstance` resource directly as the primary method for
  configuring and managing your InfluxDB cluster.
- Docker latest tag changing to InfluxDB 3 Core on February 3, 2026. Use specific
  version tags to avoid unexpected upgrades.
- InfluxDB Clustered software is in a secure container registry.
- Namespace is referred to as influxdb.
- InfluxDB cluster hostname is required for configuration.
- PostgreSQL-style DSN is needed to access the PostgreSQL-compatible database.
- Copy the provided example-customer.yml file to create a new configuration file specific
  to your InfluxDB cluster.
- Use Visual Studio Code (VS Code) to edit your configuration file.
- For simplicity, we assume this namespace is influxdb, however you may use any name
  you like.
- Due to the additional complexity and maintenance requirements, using kubectl apply
  isnt recommended for air-gapped environments.
- Helm deploys the kubit operator - The Helm chart includes the kubit operator, which
  needs its images mirrored to your private registry.
- The garbage collector cannot be scaled horizontally.
- 'Avoid wide schemas: A wide schema is one with a large number of columns (tags and
  fields).'
- 'Avoid sparse schemas: A sparse schema is one where, for many rows, columns contain
  null values.'
- 'Keep table schemas homogenous: A homogenous table schema is one where every row
  has values for all tags and fields.'
- InfluxDB Clustered provides backwards-compatible HTTP write APIs for writing data
  to your cluster.
- Set up TLS in your InfluxDB cluster to ensure both incoming and outgoing data is
  encrypted and secure.
- If using self-signed certs, provide a custom certificate authority (CA) bundle.
- Some object store providers allow unsecure connections when accessing the object
  store.
- If currently using an unsecure connection to your Catalog store database, update
  your Catalog store data source name (DSN) to remove the sslmode=disable query parameter.
- Supports OAuth 2.0 Device Authorization Flow
- Token strings are returned only on token creation. We recommend storing database
  tokens in a secure secret store.
- Some queries may return nulls for non-existing data points
- Uses line protocol for writing data to InfluxDB.
- Timestamp precision must be specified when writing data.
- 'Authorization: Bearer and Authorization: Token are equivalent.'
- Uses Bearer token for authentication.
- The default timestamp precision is ns.
- Writes line protocol to InfluxDB using the Java client library.
- Use 'INFLUX_TOKEN' environment variable for authentication.
- Telegraf is a plugin-based agent with plugins that are enabled and configured in
  your Telegraf configuration file (`telegraf.conf`). Each Telegraf configuration
  must have at least one input plugin and one output plugin.
- Use the outputs.influxdb_v2 plugin to write metrics collected by Telegraf to InfluxDB
  Clustered.
- Set organization to an empty string.
- Requires a database token with write permissions on the target database
- Timestamp precision is in seconds
- Default precision is lineprotocol.Nanosecond
- Uses API token for authentication
- Default precision is nanosecond ('ns')
- Default precision is nanosecond ('ns').
- If using Docker to install and run InfluxDB, the latest tag will point to InfluxDB
  3 Core. To avoid unexpected upgrades, use specific version tags in your Docker deployments.
- CSV files must be accessible by the Telegraf agent.
- INFLUX_TOKEN is an environment variable you created for your database WRITE token
- Restart the Telegraf agent to apply the configuration change and write the CSV data
  to InfluxDB.
- Avoid wide schemas
- Avoid sparse schemas
- Table schemas should be homogenous
- Use the best data type for your data
- Keep table names, tags, and fields simple
- Avoid keywords and special characters
- Use gzip compression to speed up writes to InfluxDB.
- Write data in batches to minimize network overhead when writing data to InfluxDB.
- By default, InfluxDB writes data in nanosecond precision. However if your data isnt
  collected in nanoseconds, there is no need to write at that precision.
- Store token in a secret store or environment variable to avoid exposing the raw
  token string.
- Data ingested into InfluxDB must conform to the retention period of the database
  in which it is stored.
- Points with timestamps outside of the retention period are no longer queryable.
- To reduce operational overhead and cost, manage the lifecycle of ingested data.
- Once data falls outside of a databases retention period, the garbage collection
  service can remove all artifacts associated with the data from the Catalog store
  and Object store.
- The minimum garbage collector (GC) object store and Parquet file cutoff time is
  three hours (`3h`).
- We recommend setting these options to a value aligned to your organizations backup
  and recovery strategy.
- Use appropriate retention periods to reduce operational cost.
- Tune garbage collection to remove artifacts associated with data timely.
- Use appropriate retention periods
- Tune garbage collection
- Writes are synchronousthe response status indicates the final status of the write
  and all ingested data is queryable.
- InfluxDB rejects points that have syntax errors or schema conflicts.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Check your credentials'
- '403 Forbidden: Insufficient permissions'
- '404 Not Found: Check the endpoint URL'
- 'Error writing data to InfluxDB: Check token validity.'
- 'Failed writing batch: config: None, data: ... due: InfluxDBError.'
- '400 Bad Request: partial write of line protocol occurred'
- '400 Bad Request: parsing failed for write_lp endpoint'
- '401 Unauthorized: Recheck token validity'
- '401 Unauthorized: Recheck token or credentials'
- '204 "Success": If InfluxDB ingested the data'
- '400 "Bad request": error details about rejected points, up to 100 points: `line`
  contains the first rejected line, `message` describes rejections'
- '401 "Unauthorized": If the `Authorization` header is missing or malformed or if
  the [token] doesnt have permission to write to the database.'
- '404 "Not found": If a requested resource (for example, organization or database)
  wasnt found'
- '500 "Internal server error": Default status for an error'
- '503 "Service unavailable": If the server is temporarily unavailable to accept writes.'
- '400 Bad Request: Check if required parameters are provided.'
- 'Error threshold format not recognized: Use proper threshold format with level prefixes'
- Schema doesnt exist or doesnt contain the expected columns
- 'Invalid measurement: Ensure the measurement name is correct'
- 'Threshold format error: Check the MAD threshold format'
- 'Invalid MAD thresholds format: Check threshold format is correct'
- 'No notifications being sent: Verify the Notification Sender Plugin is installed
  and running'
- 'No notifications triggered: Verify notification channel configuration.'
- 'Too many notifications: Adjust state_change_window and state_change_count for stability
  filtering.'
- 'Authentication errors: Set INFLUXDB3_AUTH_TOKEN environment variable.'
- '401 Unauthorized: Check authentication credentials'
- '429 Too Many Requests: Rate limit exceeded'
- 'ERROR: No module named ''psutil'''
- Consider running InfluxDB 3 with appropriate permissions if disk I/O metrics are
  critical.
- Check that the trigger is active.
- 'No alerts triggered: Verify threshold values are appropriate for your data ranges'
- 'False positive alerts: Increase trigger_count to require more consecutive failures'
- 'Missing deadman alerts: Verify deadman_check=true is set in configuration'
- 'Authentication issues: Set INFLUXDB3_AUTH_TOKEN environment variable'
- 'error: database name required'
- 'error parsing query: found , expected identifier at '
- 'error parsing query: mixing aggregate and non-aggregate queries is not supported'
- 'invalid operation: time and *influxql.VarRef are not compatible'
- '401 Unauthorized: Check admin token and permissions.'
- 'HTTP/2 Support: Ensure proxy configuration supports HTTP/2.'
- '401 Unauthorized: Check that you entered the token in the Password field.'
- 'Connection errors: Verify your InfluxDB 3 Core instance is accessible.'
- 'Maximum number of databases: 5'
- '401 Unauthorized: Check your OAuth credentials.'
- '429 Too Many Requests: Rate limit exceeded.'
- '404: Table not found'
- '201: Success. Last cache created.'
- '400: Bad request.'
- '401: Unauthorized.'
- '404: Cache not found.'
- '409: Cache already exists.'
- '200: Success. The last cache has been deleted.'
- Error handling behavior can be set to log, retry, or disable
- Unsupported SQL types include UUID, BLOB, CLOB, BINARY, VARBINARY, REGCLASS, NVARCHAR,
  CUSTOM, ARRAY, ENUM, SET, DATETIME, BYTEA.
- Selector functions may return fewer points than expected.
- '401 Unauthorized: Unauthorized access.'
- '403 Access denied: Access denied.'
- '404 Database not found: Database not found.'
- '405 Method not allowed: Method not allowed.'
- '422 Unprocessable entity: Unprocessable entity.'
- 400 Bad request.
- 401 Unauthorized access.
- 404 Database not found.
- 400 Bad request
- 401 Unauthorized access
- '400 Bad request: Bad request.'
- '401 Unauthorized: Check token validity and permissions.'
- '429 Too Many Requests: Implement exponential backoff strategy.'
- '401 Unauthorized: Check authentication token.'
- 'failed to pull image: failed to resolve reference'
- 'UNAUTHORIZED: authentication failed'
- '429 Too Many Requests: Throttle API calls or reduce frequency'
- '500 Internal Server Error: Check server logs for more details'
- '204 No Content: Successful write'
- Other error status code and failure message
- '401 Unauthorized: Recheck token expiration or validity'
- '204 No Content: Successful write.'
- 'Error status code and failure message: Check the request parameters.'
- '429 Too Many Requests: Throttle API calls'
- '401 Unauthorized: Check database token permissions'
- 'InfluxDBError: Error writing data to InfluxDB'
- '400 Bad Request: Check data format and precision'
- '401 Unauthorized: Recheck API token or permissions'
- 'GC_OBJECTSTORE_CUTOFF: The age at which Parquet files become eligible for deletion
  from Object storage.'
- 'GC_PARQUETFILE_CUTOFF: How long to retain rows in the Catalog store that reference
  Parquet files marked for deletion.'
- '204 "No Content": InfluxDB ingested all of the data in the batch'
- '400 "Bad request": Some or all request data isnt allowed (for example, is malformed
  or falls outside of the databases retention period)'
- '401 "Unauthorized": The `Authorization` request header is missing or malformed
  or the token doesnt have permission to write to the database'
- '404 "Not found": A requested resource wasnt found'
- '422 "Unprocessable Entity": The data isnt allowed (for example, falls outside
  of the databases retention period)'
- '503 "Service unavailable": The server is temporarily unavailable to accept writes'
- '400 "Bad request": Some or all request data isnt allowed (for example, is malformed
  or falls outside of the databases retention period)the response body indicates
  whether a partial write has occurred or if all data has been rejected'
- '503 "Service unavailable": The server is temporarily unavailable to accept writes.'
- '400 "Bad request": Some or all request data isnt allowed'
- '401 "Unauthorized": The Authorization request header is missing or malformed'
- '422 "Unprocessable Entity": The data isnt allowed'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Authentication and authorization
  - influxdb3_auth_token
  - OAuthToken
  - admin token
  - BearerAuthentication
  - TokenAuthentication
  - BasicAuthentication
  - QuerystringAuthentication
  - Authentication
  - Authorization
  - Database token
  - Management token
  - Keycloak
  - Auth0
  - Microsoft Entra ID
client:
  base_url: http://localhost:8181
source_metadata: null
