resources:
- name: MemoryAllocator
  endpoint:
    path: /memory_allocator
    method: GET
    data_selector: records
- name: HierarchicalAllocator
  endpoint:
    path: /hierarchical_allocator
    method: GET
    data_selector: records
- name: MemoryManager
  endpoint:
    path: /memory_manager
    method: GET
    data_selector: records
- name: Exporting to ExecuTorch
  endpoint:
    path: /export-to-executorch
    method: GET
- name: ExecuTorch Model
  endpoint:
    path: /executorch/model
    method: GET
    data_selector: model_data
    params: {}
- name: Module
  endpoint:
    path: /path/to/model.pte
    method: GET
    data_selector: ''
    params: {}
- name: executorch
  endpoint:
    path: /integrating-and-running-executorch-on-apple-platforms
    method: GET
    data_selector: logs
    params: {}
- name: MobileNet v3
  endpoint:
    path: /models/mobilenetv3
    method: GET
    data_selector: models
    params: {}
- name: DeepLab v3
  endpoint:
    path: /hub/pytorch_vision_deeplabv3_resnet101/
    method: GET
    data_selector: model
- name: backend_api
  endpoint:
    path: /backend_api.py
    method: GET
    data_selector: LoweredBackendModule
    params: {}
- name: export
  endpoint:
    path: /export
    method: GET
    data_selector: ExportedProgram
    params: {}
- name: Export to ExecuTorch API Reference
  endpoint:
    path: /export-to-executorch-api-reference
    method: GET
- name: ExecuTorch Runtime API Reference
  endpoint:
    path: /executorch-runtime-api-reference
    method: GET
- name: ExecuTorch Runtime Python API Reference
  endpoint:
    path: /executorch-runtime-python-api-reference
    method: GET
- name: vulkan_delegate
  endpoint:
    path: /native-delegates-executorch-vulkan-delegate
    method: GET
    data_selector: results
    params: {}
- name: SoftmaxModule
  endpoint:
    path: /examples/arm/aot_arm_compiler
    method: POST
    data_selector: model_name
    params:
      model_name: softmax
- name: AddModule
  endpoint:
    path: /examples/arm/aot_arm_compiler
    method: POST
    data_selector: model_name
    params:
      model_name: add
- name: MobileNetV2
  endpoint:
    path: /examples/models/mobilenet_v2
    method: GET
    data_selector: model
    params: {}
- name: softmax
  endpoint:
    path: /examples/arm/aot_arm_compiler
    method: GET
    data_selector: output
    params:
      model_name: softmax
- name: add
  endpoint:
    path: /examples/arm/aot_arm_compiler
    method: GET
    data_selector: output
    params:
      model_name: add
      delegate: true
- name: quantized_ops_aot_lib
  endpoint:
    path: /executorch/quantized_ops_aot_lib
    method: GET
    data_selector: records
    params: {}
- name: bare_metal_executable
  endpoint:
    path: /executorch/bare_metal_executable
    method: GET
    data_selector: records
    params: {}
- name: Core ML Delegate
  endpoint:
    path: /examples/apple/coreml/scripts/export
    method: POST
    data_selector: none
    params:
      model_name: mv3
- name: MobileNet V3
  endpoint:
    path: /examples/apple/mps/scripts/mps_example
    method: POST
    data_selector: model_name
    params:
      model_name: mv3
- name: MPS backend
  endpoint:
    path: /building-and-running-executorch-with-mps-backend
    method: GET
    data_selector: results
    params: {}
- name: model_export
  endpoint:
    path: /export-to-executorch
    method: POST
    data_selector: model
    params: {}
- name: model_run
  endpoint:
    path: /run-model
    method: POST
    data_selector: run
    params: {}
- name: model
  endpoint:
    path: /build/execuTorch
    method: POST
    data_selector: response
    params: {}
- name: qnn_executor_runner
  endpoint:
    path: /build-android/examples/qualcomm/qnn_executor_runner
    method: GET
- name: Program
  endpoint:
    path: /executorch/runtime/Program
    method: GET
    data_selector: programs
- name: Runtime
  endpoint:
    path: /executorch/runtime/Runtime
    method: GET
    data_selector: runtime
- name: Method
  endpoint:
    path: /executorch/runtime/Method
    method: GET
    data_selector: methods
- name: Export IR
  endpoint:
    path: /export/ir
    method: GET
    data_selector: exported_ir
    params: {}
- name: ATen Dialect
  endpoint:
    path: /export/aten
    method: GET
    data_selector: aten_dialect
    params: {}
- name: Edge Dialect
  endpoint:
    path: /export/edge
    method: GET
    data_selector: edge_dialect
    params: {}
- name: Backend Dialect
  endpoint:
    path: /export/backend
    method: GET
    data_selector: backend_dialect
    params: {}
- name: execuTorch
  endpoint:
    path: /executorch
    method: GET
    data_selector: records
- name: Custom Compiler Passes
  endpoint:
    path: /custom/compiler/passes
    method: GET
- name: Partitioners
  endpoint:
    path: /custom/partitioners
    method: GET
- name: PassManager
  endpoint:
    path: /executorch/exir/pass_manager
    method: GET
    data_selector: passes
    params: {}
- name: SubgraphMatcher
  endpoint:
    path: /torch/fx/passes/utils/matcher_utils
    method: GET
    data_selector: match
    params: {}
- name: Custom Compiler Passes
  endpoint:
    path: /custom-compiler-passes
    method: GET
    data_selector: passes
- name: Partitioners
  endpoint:
    path: /partitioners
    method: GET
    data_selector: partitioners
- name: Custom Compiler Passes
  endpoint:
    path: /executorch/docs/custom-compiler-passes
    method: GET
    data_selector: records
- name: Partitioners
  endpoint:
    path: /executorch/docs/partitioners
    method: GET
    data_selector: records
- name: source_partition
  endpoint:
    path: /get_source_partitions
    method: GET
    data_selector: partitions
- name: kernel_registration
  endpoint:
    path: /kernel_registration
    method: GET
    data_selector: kernels
- name: kernel_registration
  endpoint:
    path: /kernel_registration
    method: GET
    data_selector: records
- name: custom_linear
  endpoint:
    path: /custom_linear
    method: GET
    data_selector: records
- name: kernel_library_selective_build
  endpoint:
    path: /kernel-library-selective-build
    method: GET
    data_selector: op_info
- name: Bundled Program
  endpoint:
    path: /executorch/devtools/bundled_program
    method: POST
    data_selector: results
- name: method_test_suites
  endpoint:
    path: /
    method: POST
    data_selector: method_test_suites
    params: {}
- name: generate_etrecord
  endpoint:
    path: /executorch/devtools/etrecord/_etrecord/generate_etrecord
    method: POST
    data_selector: None
    params: {}
- name: ETDump
  endpoint:
    path: /executorch/devtools/etdump
    method: POST
    data_selector: etdump_data
    params: {}
- name: Inspector
  endpoint:
    path: /inspector
    method: GET
    data_selector: records
- name: inspector
  endpoint:
    path: /inspector
    method: GET
    data_selector: events
    params: {}
- name: memory_planning_inspection
  endpoint:
    path: /memory_planning_inspection
    method: GET
    data_selector: memory_allocation_info
notes:
- ExecuTorch defines a streamlined workflow to prepare (export, transformation, and
  compilation) and execute a PyTorch program.
- ExecuTorch runtime is written in C++ with minimal dependencies for portability and
  execution efficiency.
- Tested on Linux (CentOS 8+, Ubuntu 20.04.6 LTS+, RHEL 8+), macOS (Big Sur (11.0)+),
  and Windows (WSL).
- Requires `conda` or another virtual environment manager.
- When fetching a new version of the upstream repo (via git fetch or git pull) it
  is a good idea to clean the old build artifacts.
- The build system does not currently adapt well to changes in build dependencies.
- ExecuTorch uses CMake as its primary build system.
- This must be linked into your application with a flag like -Wl,-force_load or -Wl,--whole-archive.
- Uses CMake as its primary build system.
- To optimize the release build for size, use CMAKE_FLAGS="$CMAKE_FLAGS -DOPTIMIZE_SIZE=ON".
- ExecuTorch provides granular control over memory management and execution.
- The Module APIs provide a simplified interface for running ExecuTorch models in
  C++.
- For optimal performance, always link against the Release version of the deliverables.
- To access logs, link against the Debug build of the ExecuTorch runtime, i.e., the
  `executorch_debug` framework. For optimal performance, always link against the Release
  version of the deliverables.
- 'Uses MobileNet v3 model to process live camera images leveraging three different
  backends: XNNPACK, Core ML, and MPS.'
- This demo app and tutorial has only been validated with arm64-v8a ABI.
- The backend delegate is usually either provided by ExecuTorch or vendors.
- Backend delegation is an entry point for backends to process and execute PyTorch
  programs.
- The resultant model will only be partially delegated to the Vulkan backend.
- Operator support for LLaMA models is currently in active development.
- ExecuTorch backend delegate is under active development.
- Ensure you're on a supported platform with x86_64 or aarch64 architecture.
- This ExecuTorch backend delegate is under active development.
- You may encounter some rough edges and features which may be documented or planned
  but not implemented.
- Ensure you have completed ExecuTorch cmake build setup
- You need to generate a new executor-runner binary for each model change
- Core ML delegate uses Core ML APIs to enable running neural networks via Appleâ€™s
  hardware acceleration.
- A mac system for building is required.
- macOS >= 13.0, iOS >= 16.0, iPadOS >= 16.0, and tvOS >= 16.0 are required for running
  the model.
- Core ML backend uses coremltools to lower Edge dialect to Core ML format.
- Requires macOS 12 or later for tracing and macOS 12.4 or later for runtime.
- Xcode version must be 14.1 or later.
- Requires macOS 12 or later for tracing models
- Requires iOS 15.4 or later for runtime
- The Linux host operating system that QNN Backend is verified with is Ubuntu 22.04
  LTS x64.
- This example is verified with SM8550 and SM8450.
- Qualcomm AI Engine Direct is also referred to as QNN in the source and documentation.
- Currently, this ExecuTorch Backend can delegate AI computations to Hexagon processors
  through Qualcomm AI Engine Direct APIs.
- Qualcomm AI Engine Direct is designed to provide unified, low-level APIs for AI
  development.
- The Linux host operating system that QNN Backend is verified with is Ubuntu 22.04
  LTS x64
- make sure you have write-permission on below path.
- The <device_serial> can be found by adb devices command.
- The linux part of this tutorial has been designed and tested on Ubuntu 22.04 LTS,
  and requires glibc 2.34.
- Before proceeding forward to the next section users should be able to successfully
  flash the dsp_mu_polling_cm33 sample application.
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- ExecuTorch enables on-device inference capabilities across mobile and edge devices.
- Backend dialect is an optional stage, only needed if we want to introduce backend-awareness
  into the graph.
- This dialect allows the introduction of operators that do not conform to the schema
  defined in the canonical ATen operator set.
- Uses an interpreter-based approach for executing nodes and recreating graphs
- Quantization is useful for edge devices including wearables, embedded devices and
  microcontrollers.
- ExecuTorch only supports out-style operators, where the caller provides the output
  Tensor or Tensor list in the final position with the name 'out'.
- ExecuTorch supports custom kernels for optimized performance.
- ExecuTorch users are asked to provide a custom kernel library with C++ implementations
  and a YAML file associated with the library.
- During the development phase, itâ€™s recommended to use the C++ API since itâ€™s low-cost
  to use and flexible to change.
- Once the application moves to the production phase, it is recommended to use the
  Yaml entry API.
- Selective build can reduce the binary size of the ExecuTorch runtime without compromising
  support for a target model.
- The Vulkan delegate is currently under active development, and its components are
  subject to change.
- BundledProgram is not necessarily a core part of the program and not needed for
  its execution.
- All method names in bundled config should be found in program.execution_plan
- Users should do a deepcopy of the output of to_edge() and pass in the deepcopy to
  the generate_etrecord API.
- Users canâ€™t parse ETDump directly; instead, they should pass it into the Inspector
  API.
- After the inference iterations, users can dump out the ETDump buffer.
- ExecuTorch supports the extraction of model level outputs via ETDump.
- 'Currently supports two levels of debugging: Program level outputs and Intermediate
  outputs.'
- User should add this code after they call to_executorch(), and it will write memory
  allocation information stored on the nodes to the file path 'memory_profile.json'.
  The file is compatible with the Chrome trace viewer.
- Delegate backends are a prominent component of on-device models due to their flexibility
  in defining behavior.
errors:
- 'If you encountered any bugs or issues following this tutorial please file a bug/issue
  with tag #coreml.'
- 'MODEL_NOT_FOUND: Ensure the model name is correct'
- 'BUILD_FAILED: Check dependencies for building the MPS backend'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- All method names in bundled config should be found in program.execution_plan, but
  {'MISSING_METHOD_NAME'} does not include.
auth_info:
  mentioned_objects: []
client:
  base_url: https://pytorch.org/executorch
  headers:
    Accept: application/json
source_metadata: null
