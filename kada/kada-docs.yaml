resources:
- name: Database
  endpoint:
    path: /data/assets/database
    method: GET
    data_selector: records
- name: Schema
  endpoint:
    path: /data/assets/schema
    method: GET
    data_selector: records
- name: Table
  endpoint:
    path: /data/assets/table
    method: GET
    data_selector: records
- name: Column
  endpoint:
    path: /data/assets/column
    method: GET
    data_selector: records
- name: Code
  endpoint:
    path: /data/assets/code
    method: GET
    data_selector: records
- name: Procedure
  endpoint:
    path: /data/assets/procedure
    method: GET
    data_selector: records
- name: Macro
  endpoint:
    path: /data/assets/macro
    method: GET
    data_selector: records
- name: Tool
  endpoint:
    path: /analytical/assets/tool
    method: GET
    data_selector: records
- name: Workspace
  endpoint:
    path: /analytical/assets/workspace
    method: GET
    data_selector: records
- name: Report
  endpoint:
    path: /analytical/assets/report
    method: GET
    data_selector: records
- name: Sheet
  endpoint:
    path: /analytical/assets/sheet
    method: GET
    data_selector: records
- name: Content App
  endpoint:
    path: /analytical/assets/content_app
    method: GET
    data_selector: records
- name: Pipeline
  endpoint:
    path: /analytical/assets/pipeline
    method: GET
    data_selector: records
- name: ML Model
  endpoint:
    path: /analytical/assets/ml_model
    method: GET
    data_selector: records
- name: Dataset
  endpoint:
    path: /analytical/assets/dataset
    method: GET
    data_selector: records
- name: Dataset Table
  endpoint:
    path: /analytical/assets/dataset_table
    method: GET
    data_selector: records
- name: Dataset Field
  endpoint:
    path: /analytical/assets/dataset_field
    method: GET
    data_selector: records
- name: File
  endpoint:
    path: /analytical/assets/file
    method: GET
    data_selector: records
- name: Customer
  endpoint:
    path: /customer
    method: GET
    data_selector: records
    params: {}
- name: Transactions
  endpoint:
    path: /transactions
    method: GET
    data_selector: records
    params: {}
- name: Financial
  endpoint:
    path: /financial
    method: GET
    data_selector: records
    params: {}
- name: Risk Data
  endpoint:
    path: /risk_data
    method: GET
    data_selector: records
    params: {}
- name: Compliance and Regulatory Data
  endpoint:
    path: /compliance
    method: GET
    data_selector: records
    params: {}
- name: Marketing and Sales Data
  endpoint:
    path: /marketing_sales
    method: GET
    data_selector: records
    params: {}
- name: Operational Data
  endpoint:
    path: /operational
    method: GET
    data_selector: records
    params: {}
- name: Vendor and Third-Party Data
  endpoint:
    path: /vendor_data
    method: GET
    data_selector: records
    params: {}
- name: Security and Access Data
  endpoint:
    path: /security_access
    method: GET
    data_selector: records
    params: {}
- name: Digital & Web
  endpoint:
    path: /digital_web
    method: GET
    data_selector: records
    params: {}
- name: Human Resources
  endpoint:
    path: /human_resources
    method: GET
    data_selector: records
    params: {}
- name: Customer & Corporate relations
  endpoint:
    path: /customer_corporate_relations
    method: GET
    data_selector: records
    params: {}
- name: roles
  endpoint:
    path: /roles
    method: GET
    data_selector: roles
    params: {}
- name: data_profile
  endpoint:
    path: /api/dataProfile
    method: GET
    data_selector: data
    params: {}
- name: Usage Insights
  endpoint:
    path: /usage/insights
    method: GET
    data_selector: data
    params: {}
- name: Complexity Insights
  endpoint:
    path: /complexity/insights
    method: GET
    data_selector: data
    params: {}
- name: Governance Insights
  endpoint:
    path: /governance/insights
    method: GET
    data_selector: data
    params: {}
- name: Data Quality Dashboard
  endpoint:
    path: /data-quality/dashboard
    method: GET
    data_selector: data
    params: {}
- name: Issue Dashboard
  endpoint:
    path: /issue/dashboard
    method: GET
    data_selector: data
    params: {}
- name: Owner Dashboard
  endpoint:
    path: /owner/dashboard
    method: GET
    data_selector: data
    params: {}
- name: Steward Dashboard
  endpoint:
    path: /steward/dashboard
    method: GET
    data_selector: data
    params: {}
- name: Data Load Dashboard
  endpoint:
    path: /data-load/dashboard
    method: GET
    data_selector: data
    params: {}
- name: Search Views
  endpoint:
    path: /search-views
    method: GET
    data_selector: views
    params: {}
- name: impact_assessment
  endpoint:
    path: /impact_assessment
    method: POST
    data_selector: results
- name: impacted_assessment_source_to_target_report
  endpoint:
    path: /reports/impacted_assessment_source_to_target
    method: GET
    data_selector: records
- name: impacted_assets_export
  endpoint:
    path: /exports/impacted_assets
    method: GET
    data_selector: records
- name: impacted_users_export
  endpoint:
    path: /exports/impacted_users
    method: GET
    data_selector: records
- name: impacted_asset_by_user_export
  endpoint:
    path: /exports/impacted_asset_by_user
    method: GET
    data_selector: records
- name: impacted_asset_lineage_export
  endpoint:
    path: /exports/impacted_asset_lineage
    method: GET
    data_selector: records
- name: governance_widget
  endpoint:
    path: /embedded/governance/widget
    method: GET
    data_selector: widget_info
- name: Power BI Governance Link
  endpoint:
    path: /home/adding-the-embedded-governance-link-into-a-power-b
    method: GET
- name: data_usage_reports
  endpoint:
    path: /data_usage_reports
    method: GET
    data_selector: reports
    params: {}
- name: content_usage_reports
  endpoint:
    path: /content_usage_reports
    method: GET
    data_selector: reports
    params: {}
- name: user_usage_reports
  endpoint:
    path: /user_usage_reports
    method: GET
    data_selector: reports
    params: {}
- name: team_usage_reports
  endpoint:
    path: /team_usage_reports
    method: GET
    data_selector: reports
    params: {}
- name: governance_reports
  endpoint:
    path: /governance_reports
    method: GET
    data_selector: reports
    params: {}
- name: risks_reports
  endpoint:
    path: /risks_reports
    method: GET
    data_selector: reports
    params: {}
- name: cost_reports
  endpoint:
    path: /cost_reports
    method: GET
    data_selector: reports
    params: {}
- name: platform_administration_reports
  endpoint:
    path: /platform_administration_reports
    method: GET
    data_selector: reports
    params: {}
- name: cascade_update
  endpoint:
    path: /bulk-update-via-data-cart
    method: POST
- name: issues
  endpoint:
    path: /rest/api/2/issue
    method: GET
    data_selector: issues
- name: K Data Catalog
  endpoint:
    path: /applications/KDataCatalog
    method: POST
    data_selector: application
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Athena Metadata
  endpoint:
    path: /services/data/v3.0/sobjects/AthenaMetadata
    method: GET
    data_selector: records
- name: pg_catalog
  endpoint:
    path: /pg_catalog
    method: GET
    data_selector: records
- name: pg_tables
  endpoint:
    path: /pg_tables
    method: GET
    data_selector: records
- name: SNOWFLAKE
  endpoint:
    path: /services/data/v3.1/snowflake
    method: GET
    data_selector: records
- name: metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/Metadata
    method: POST
    data_selector: records
    params: {}
- name: glue_tables
  endpoint:
    path: /get_tables
    method: GET
    data_selector: tables
    params: {}
- name: glue_databases
  endpoint:
    path: /get_databases
    method: GET
    data_selector: databases
    params: {}
- name: kada_bigquery_extractor_config
  endpoint:
    path: /services/data/v3.0/sobjects/kada_bigquery_extractor_config
    method: POST
    data_selector: records
- name: mysql_source
  endpoint:
    path: /services/data/vXX.X/sobjects/MySQLSource
    method: POST
    data_selector: records
- name: Cognos Metadata
  endpoint:
    path: /services/data/v11.1.7/metadata
    method: GET
    data_selector: metadata
- name: metadata
  endpoint:
    path: /metadata
    method: GET
- name: metadata
  endpoint:
    path: /api/v3/metadata
    method: GET
    data_selector: records
    params: {}
- name: collector
  endpoint:
    path: /path/to/collector
    method: POST
    data_selector: data
    params: {}
- name: extractor_config
  endpoint:
    path: /kada_dbt_extractor_config.json
    method: GET
    data_selector: config
    params: {}
- name: metadata
  endpoint:
    path: /api/v1/metadata
    method: POST
    data_selector: records
    params: {}
- name: gpcc_queries_history
  endpoint:
    path: /gpcc_queries_history
    method: GET
    data_selector: records
- name: kada_hevo_extractor_config
  endpoint:
    path: /tmp/output
    method: POST
    data_selector: output_path
    params: {}
- name: Hightouch Metadata
  endpoint:
    path: /api/hightouch/metadata
    method: POST
    data_selector: data
    params:
      incremental: updated_at
- name: metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/Metadata
    method: POST
    data_selector: records
    params: {}
- name: Informatica Metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/Metadata
    method: POST
    data_selector: data
    params: {}
- name: metadata
  endpoint:
    path: /metadata
    method: POST
    data_selector: records
- name: mysql_metadata
  endpoint:
    path: /mysql/metadata
    method: POST
    data_selector: metadata
- name: metadata
  endpoint:
    path: /path/to/metadata/endpoint
    method: POST
    data_selector: data
    params: {}
- name: metadata
  endpoint:
    path: /services/data/metadata
    method: POST
    data_selector: metadata_records
- name: MV_KADA_TABLES
  endpoint:
    path: /services/data/v3.0/sobjects/MV_KADA_TABLES
    method: GET
    data_selector: records
- name: MV_KADA_DB_LOG
  endpoint:
    path: /services/data/v3.0/sobjects/MV_KADA_DB_LOG
    method: GET
    data_selector: records
- name: V_KADA_OACS_LOGICAL
  endpoint:
    path: /services/data/v3.0/sobjects/V_KADA_OACS_LOGICAL
    method: GET
    data_selector: records
- name: V_KADA_OACS_PHYSICAL
  endpoint:
    path: /services/data/v3.0/sobjects/V_KADA_OACS_PHYSICAL
    method: GET
    data_selector: records
- name: output_path
  endpoint:
    path: /tmp/output
- name: Postgres
  endpoint:
    path: /home/postgres-via-collector-method
    method: GET
- name: pg_catalog
  endpoint:
    path: /pg_catalog
    method: GET
    data_selector: records
- name: databases
  endpoint:
    path: /databases
    method: GET
    data_selector: records
- name: metadata
  endpoint:
    path: /metadata
    method: GET
    data_selector: records
    params: {}
- name: metadata
  endpoint:
    path: /metadata
    method: POST
    data_selector: records
- name: pg_catalog
  endpoint:
    path: /pg_catalog
    method: GET
- name: system_tables
  endpoint:
    path: /system_tables
    method: GET
- name: databases
  endpoint:
    path: /databases
    method: GET
- name: pii_object
  endpoint:
    path: /pii_objects
    method: POST
- name: account_usage
  endpoint:
    path: /services/data/v3.4.0/account_usage
    method: GET
    data_selector: records
    params: {}
- name: catalog_metadata
  endpoint:
    path: /services/data/v3.3.0/sobjects/catalog_metadata
    method: GET
    data_selector: records
- name: account_usage
  endpoint:
    path: /services/data/vXX.X/sobjects/account_usage
    method: GET
    data_selector: records
- name: account_usage
  endpoint:
    path: /account_usage
    method: GET
    data_selector: data
    params: {}
- name: account_usage_history
  endpoint:
    path: /account_usage/history
    method: GET
- name: account_usage_views
  endpoint:
    path: /account_usage/views
    method: GET
- name: account_usage_tables
  endpoint:
    path: /account_usage/tables
    method: GET
- name: account_usage_columns
  endpoint:
    path: /account_usage/columns
    method: GET
- name: account_usage_copy_history
  endpoint:
    path: /account_usage/copy_history
    method: GET
- name: account_usage_grants_to_roles
  endpoint:
    path: /account_usage/grants_to_roles
    method: GET
- name: account_usage_grants_to_users
  endpoint:
    path: /account_usage/grants_to_users
    method: GET
- name: account_usage
  endpoint:
    path: /account_usage
    method: GET
    data_selector: data
    params: {}
- name: snowflake_metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/SnowflakeMetadata
    method: GET
    data_selector: records
    params: {}
- name: metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/Metadata
    method: GET
    data_selector: records
- name: metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/Metadata
    method: GET
    data_selector: records
    params: {}
- name: metadata
  endpoint:
    path: /path/to/metadata/endpoint
    method: GET
    data_selector: records
    params: {}
- name: ssis
  endpoint:
    path: /home/ssis-collector-method
    method: GET
- name: ssis_logging
  endpoint:
    path: /ssis_logging
    method: GET
    data_selector: records
- name: SSRS
  endpoint:
    path: /ssrs/v3.0.0
    method: GET
- name: metadata
  endpoint:
    path: /services/metadata
    method: POST
    data_selector: data
    params: {}
- name: kada_site_content
  endpoint:
    path: /kada_site_content
    method: GET
    data_selector: records
- name: kada_ts_events
  endpoint:
    path: /kada_ts_events
    method: GET
    data_selector: records
- name: Tableau Server Data
  endpoint:
    path: /api/v3.3/sites
    method: GET
    data_selector: data
    params: {}
- name: kada_tableau_extractor
  endpoint:
    path: /services/data/v3.3/sobjects/Tableau
    method: GET
    data_selector: records
- name: Tableau Server
  endpoint:
    path: /services/data/v3.2/collectors
    method: GET
    data_selector: records
- name: extracts
  endpoint:
    path: /extracts
    method: POST
    data_selector: files
- name: kada_teradata_extractor_config
  endpoint:
    path: /path/to/teradata/endpoint
    method: POST
    data_selector: config
    params: {}
- name: metadata
  endpoint:
    path: /path/to/metadata
    method: POST
    data_selector: records
- name: metadata_list
  endpoint:
    path: tspublic/v1/metadata/list
    method: GET
    data_selector: records
- name: metadata_details
  endpoint:
    path: tspublic/v1/metadata/details
    method: GET
    data_selector: records
- name: session_orgs
  endpoint:
    path: tspublic/v1/session/orgs
    method: GET
    data_selector: records
- name: session_org
  endpoint:
    path: tspublic/v1/session/org
    method: GET
    data_selector: records
- name: session_login
  endpoint:
    path: tspublic/v1/session/login
    method: POST
    data_selector: records
- name: assets
  endpoint:
    path: /api/assets
    method: GET
- name: reports
  endpoint:
    path: /api/reports
    method: GET
- name: metadata_list
  endpoint:
    path: /tspublic/v1/metadata/list
    method: GET
    data_selector: results
    params: {}
- name: metadata_details
  endpoint:
    path: /tspublic/v1/metadata/details
    method: GET
    data_selector: results
    params: {}
- name: session_orgs
  endpoint:
    path: /tspublic/v1/session/orgs
    method: GET
    data_selector: orgs
    params: {}
- name: session_org
  endpoint:
    path: /tspublic/v1/session/org
    method: GET
    data_selector: org
    params: {}
- name: session_login
  endpoint:
    path: /tspublic/v1/session/login
    method: POST
    data_selector: session
    params: {}
- name: metadata_list
  endpoint:
    path: /tspublic/v1/metadata/list
    method: GET
    data_selector: records
- name: metadata_details
  endpoint:
    path: /tspublic/v1/metadata/details
    method: GET
    data_selector: records
- name: session_orgs
  endpoint:
    path: /tspublic/v1/session/orgs
    method: GET
    data_selector: records
- name: session_org
  endpoint:
    path: /tspublic/v1/session/org
    method: GET
    data_selector: records
- name: session_login
  endpoint:
    path: /tspublic/v1/session/login
    method: POST
    data_selector: records
- name: metadata_interface
  endpoint:
    path: /META_YYYYMMDDHHMMSS.csv
    method: GET
    data_selector: records
- name: metrics_interface
  endpoint:
    path: /METRICS_YYYYMMDDHHMMSS.csv
    method: GET
    data_selector: records
- name: linkage_interface
  endpoint:
    path: /LINKAGES_YYYYMMDDHHMMSS.csv
    method: GET
    data_selector: records
- name: INFORMATION_SCHEMA.ROUTINES
  endpoint:
    path: /INFORMATION_SCHEMA.ROUTINES
    method: SELECT
- name: INFORMATION_SCHEMA.VIEWS
  endpoint:
    path: /INFORMATION_SCHEMA.VIEWS
    method: SELECT
- name: INFORMATION_SCHEMA.TABLE_CONSTRAINTS
  endpoint:
    path: /INFORMATION_SCHEMA.TABLE_CONSTRAINTS
    method: SELECT
- name: INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE
  endpoint:
    path: /INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE
    method: SELECT
- name: INFORMATION_SCHEMA.TABLES
  endpoint:
    path: /INFORMATION_SCHEMA.TABLES
    method: SELECT
- name: INFORMATION_SCHEMA.COLUMNS
  endpoint:
    path: /INFORMATION_SCHEMA.COLUMNS
    method: SELECT
- name: sys.foreign_key_columns
  endpoint:
    path: /sys.foreign_key_columns
    method: SELECT
- name: sys.objects
  endpoint:
    path: /sys.objects
    method: SELECT
- name: sys.tables
  endpoint:
    path: /sys.tables
    method: SELECT
- name: sys.schemas
  endpoint:
    path: /sys.schemas
    method: SELECT
- name: sys.columns
  endpoint:
    path: /sys.columns
    method: SELECT
- name: sys.databases
  endpoint:
    path: /sys.databases
    method: SELECT
- name: workgroups
  endpoint:
    path: /workgroups
    method: GET
    data_selector: workgroups
- name: query_executions
  endpoint:
    path: /query/executions
    method: GET
    data_selector: query_executions
- name: databases
  endpoint:
    path: /databases
    method: GET
    data_selector: databases
- name: table_metadata
  endpoint:
    path: /tables
    method: GET
    data_selector: tables
- name: runs
  endpoint:
    path: /runs
    method: GET
    data_selector: data
- name: jobs
  endpoint:
    path: /jobs
    method: GET
    data_selector: data
- name: manifest
  endpoint:
    path: /path/to/manifest.json
    method: GET
    data_selector: records
- name: catalog
  endpoint:
    path: /path/to/catalog.json
    method: GET
    data_selector: records
- name: run_results
  endpoint:
    path: /path/to/run_results.json
    method: GET
    data_selector: records
- name: store_kada_validation_result
  endpoint:
    path: /kada_ge_store_plugin/kada_store_validation
    method: POST
- name: pg_catalog
  endpoint:
    path: /pg_catalog
    method: GET
    data_selector: records
- name: INFORMATION_SCHEMA
  endpoint:
    path: /INFORMATION_SCHEMA
    method: GET
    data_selector: tables
    params: {}
- name: account_usage_history
  endpoint:
    path: /account_usage/history
    method: GET
    data_selector: data
    params: {}
- name: account_usage_views
  endpoint:
    path: /account_usage/views
    method: GET
    data_selector: data
    params: {}
- name: account_usage_tables
  endpoint:
    path: /account_usage/tables
    method: GET
    data_selector: data
    params: {}
- name: account_usage_columns
  endpoint:
    path: /account_usage/columns
    method: GET
    data_selector: data
    params: {}
- name: account_usage_copy_history
  endpoint:
    path: /account_usage/copy_history
    method: GET
    data_selector: data
    params: {}
- name: account_usage_grants_to_roles
  endpoint:
    path: /account_usage/grants_to_roles
    method: GET
    data_selector: data
    params: {}
- name: account_usage_grants_to_users
  endpoint:
    path: /account_usage/grants_to_users
    method: GET
    data_selector: data
    params: {}
- name: account_usage_schemata
  endpoint:
    path: /account_usage/schemata
    method: GET
    data_selector: data
    params: {}
- name: account_usage_databases
  endpoint:
    path: /account_usage/databases
    method: GET
    data_selector: data
    params: {}
- name: account_usage_policy_references
  endpoint:
    path: /account_usage/policy_references
    method: GET
    data_selector: data
    params: {}
- name: SQL Server (via Collector method)
  endpoint:
    path: /home/sql-server-via-collector-method-1
    method: GET
- name: metadata
  endpoint:
    path: /sqlserver/metadata
    method: GET
    data_selector: records
- name: kada_ts_events
  endpoint:
    path: /path/to/kada_ts_events
    method: GET
    data_selector: records
    params: {}
- name: kada_site_content
  endpoint:
    path: /path/to/kada_site_content
    method: GET
    data_selector: records
    params: {}
- name: Tableau Server
  endpoint:
    path: /home/tableau-via-collector-method
    method: GET
- name: metadata
  endpoint:
    path: /api/metadata
    method: GET
    data_selector: records
- name: lineage
  endpoint:
    path: /api/lineage
    method: GET
    data_selector: records
- name: usage
  endpoint:
    path: /api/usage
    method: GET
    data_selector: records
- name: metadata_list
  endpoint:
    path: /tspublic/v1/metadata/list
    method: GET
    data_selector: records
- name: metadata_details
  endpoint:
    path: /tspublic/v1/metadata/details
    method: GET
    data_selector: records
- name: session_orgs
  endpoint:
    path: /tspublic/v1/session/orgs
    method: GET
    data_selector: orgs
- name: session_org
  endpoint:
    path: /tspublic/v1/session/org
    method: GET
    data_selector: org
- name: session_login
  endpoint:
    path: /tspublic/v1/session/login
    method: POST
    data_selector: session
- name: project
  endpoint:
    path: /rest/api/2/project
    method: GET
- name: customer
  endpoint:
    path: /api/tables
    method: GET
    data_selector: records
    params:
      name: customer
- name: classification
  endpoint:
    path: /api/collections
    method: GET
    data_selector: records
    params:
      name: Classification
- name: collection_instance
  endpoint:
    path: /api/collectioninstances
    method: POST
    data_selector: records
- name: reports_and_datasets
  endpoint:
    path: /search/select
    method: POST
    data_selector: response.docs
    params:
      q: '*'
      fq: object_type_txt:(REPORT DATASET) AND category_kc_msrt:(GOLD)
      fl: name_srt,location_srt,id
      defType: edismax
      wt: json
      sort: name_srt asc
      start: 0
      rows: 30
- name: issue
  endpoint:
    path: /issues
    method: POST
    data_selector: ''
    params: {}
- name: link_issue
  endpoint:
    path: /issues/<issue.id>/related
    method: PUT
    data_selector: ''
    params: {}
- name: email_configuration
  endpoint:
    path: /configure/email
    method: POST
    data_selector: response
    params:
      smtp_server: smtp.office365.com
      smtp_port: 587
      username: kada-noreply@kada.ai
      password: ''
      sender_email: kada-noreply@kada.ai
      ssl_enabled: true
- name: metadata_enrichment
  endpoint:
    path: /api/metadata/enrichment
    method: POST
- name: job_status
  endpoint:
    path: /api/jobexecutions/<job id>
    method: GET
- name: Customer model
  endpoint:
    path: /tmp/extract.csv
    method: COPY
    data_selector: records
notes:
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects may return nulls in deeply nested fields
- Some objects like Contact may return nulls in deeply nested fields
- Only Data Governance Managers and K Admin users have the ability to configure a
  collection and create a new instance.
- Each role in K has a different level of permissions granted to it.
- Some permissions can be customised by your Administrator.
- Roles higher up the hierarchy override roles that are below.
- Role restrictions defined by you will not be inherited.
- Requires setup of connected app in K
- Any updates to the Data Profile page is automatically sent to users that have used
  the data within the last 30 days via their homepage feed or K Daily Briefing.
- 'Help add relevant collections and system tags: This will help others understand
  more about this data item.'
- Usage is tracked by unique query signatures by user, by db_session, by hour interval.
- K helps you catalog and identify changes to data items through automated change
  detection.
- Dashboard data is refreshed daily by default.
- Insights and Dashboards are only accessible to Data Managers and Data Governance
  Users.
- Be careful to not link Accounts that do not belong to you. You do not want to accidentally
  claim someone else's Account!
- Once an account is linked, only Admin Users have the ability to remove an incorrectly
  linked account via the Merge page
- The default view is Data Discovery.
- Currently only exact match is supported. You cannot use wildcard in the filter search.
- For a large impact assessment, this report may take some time to download.
- Code search extracts are only available for 7 days.
- Lists can help you organise your data ecosystem.
- In an upcoming release, we will be improving the functionality of Data Lists.
- Available for Advance and Enterprise customers.
- The report must be published FIRST and then profiled in K. This process may take
  up to 24 hours after the publish.
- Column level lineage has limitations due to the variety and complexity of SQL dialects.
- The column level lineage parse is currently available as a manually triggered process
  to supplement lineage captured during daily processes.
- 'Bulk Action is only available to the following roles: Data Governance, Data Managers,
  K Admin users'
- Uses OAuth2 with refresh token
- The bulk edit via Excel function is currently limited to Table, Column, Report and
  Sheet objects and is only available to K Administrator
- Always check and delete any blank rows. Blank rows that have been previously edited
  and content deleted will be treated as an ‘activated' row.
- Uses OAuth2 with refresh token — requires setup of connected app in Jira
- Be mindful of Jira rate limits
- This process is currently a BETA feature and is run manually.
- Initial run will process all queries that are currently used in lineage and may
  take some time to complete (~50K queries per hour).
- Subsequent runs will be significantly shorter as only new queries are processed.
- While this feature is in BETA we recommend running the initial run out of business
  hours.
- K processes lineage across numerous sources however there are some scenarios where
  lineage and the grain of lineage is limited.
- Nested SELECT * from a CTE not supported
- SELECT * with Unions not supported
- Insert into statements currently skipped in processing
- Copy grants in statements currently causes statement to be skipped in processing
- Code that is not currently wrapped in a CREATE statement e.g. DBT code from manifest
  (represented as a select statement)
- You will receive an ‘Account already exists’ warning message if K detects that you
  have multiple accounts.
- If you receive the above warning message, click on ‘Add to existing account’ and
  enter your SSO password.
- Your old K local account will now be connected to your SSO account, this includes
  retaining your previous K history and actions (e.g. lists, knowledge contribution
  and follows).
- Usage or load frequency definitions are provided.
- K platform does NOT need internet access.
- Kubernetes service needs to have internet access to download the K images from the
  KADA repository.
- K is deployed into a node pool of at least 3 nodes.
- Each node requires a minimum of 4 vCPU (intel / amd) and 16gb Memory.
- Each node should be attached storage minimum of 32gb.
- The Postgres PV StorageClass can be upgraded to a high IOPS tier (3000 IOPS+) when
  loading larger customers.
- A landing area is ideally hosted on either AWS s3 or Azure Blob.
- The Kubernetes node pool must be deployed into a single zone / subnet with a minimum
  of /25 CIDR (128 IP addresses).
- K does not need internet access but some features will become disabled.
- Configuring role permissions involves setting up specific access for users in the
  K platform.
- This function is restricted to K Administrators
- 'Enforcing Assignments: Turn on Assignment required in the K application settings
  in Active Directory / Entra ID.'
- This function is restricted to K Administrators and Okta Administrators
- Uses OpenID Connect (Oauth 2.0) for authentication
- Removing Local account access after SSO is enabled
- Bulk setup of users and teams
- Requires K administrator access and access to your K instance storage container.
- Configuring KADA in your Microsoft App Page
- You must request access to the Open AI services prior to completing the below steps.
- HOST is in the format of the alias name or canonical host name. It must be lowercase
  e.g. if I intend to access K via `https://prod.kada.ai` , then the host value is
  `prod.kada.ai`
- 'FERNET_KEYS should be generated using one of these methods: Python or Unix shell'
- SSL_SECRET_NAME is the Kubernetes secret name you installed the SSL Certificate
  as
- Avoid special characters in the values above if possible.
- HOST is in the format of the alias name or canonical host name. It must be lowercase
  e.g. if I intend to access K via `https://prod.kada.ai`, then the host value is
  `prod.kada.ai`.
- 'FERNET_KEYS should be generated using one of these methods: Python or Unix shell.'
- KADA uses Keycloak to manage users in platform.
- KADA uses object store as a landing zone to process metadata and log files.
- We currently support AWS s3, Azure Blob or local attached Kubernetes PVs.
- 'WIP: Deploying to your own Kubernetes Service'
- To get setup with K.ai you will need Administrator access
- 'Authentication plugin Error: Cannot connect to database'
- Solve by setting environment variable
- 'Unix: export LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN=1'
- 'Windows: set LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN=1'
- Using a collector requires you to manage deploying and orchestrating the extract
  code.
- Managing a high water mark so the extract only pull the latest metadata.
- Storing and pushing the extracts to your K instance.
- Collector Integration General Notes
- Collector requires a set of parameters to connect to and extract metadata from Athena
- The service user/account/role will need permissions to access all workgroups
- Always recommend using the latest Collector Method to ensure access to the latest
  features.
- 'Collector requires deployment on server with minimum CPU: 2 vCPU, Memory: 8GB,
  Storage: 30GB'
- User must have access to pg_catalog tables for extraction
- Not all sources and destinations are included in the metadata extraction. Improvements
  are planned to provide wider coverage
- Queries, macros and procedures must include fully qualified names in order to be
  correctly parsed.
- Extended Events Setup is in pilot for Azure SQL
- Ensure your IAM role has permissions for Glue APIs.
- Collector requires a set of parameters to connect to and extract metadata from BigQuery
- Ensure to handle external arguments when running the extractor
- We always recommend using the latest Collector Method to ensure that you can access
  the latest features.
- The ClickHouse collector only extracts metadata and does not extract or process
  query usage on the database.
- Integration with Cognos requires Cognos Analytics APIs from version 11.1.7 onwards.
- Collector currently only supports a SQLServer version 2016 or higher Audit Database.
- This collector only supports DB2 for LUW (Linux, Unix, Windows)
- DB2 for z/OS and DB2 for ISeries (aka AS400) currently is not supported
- The DB2 collector currently only supports meta_only=true, do not set this to false.
- DBT extractor produces manifest, catalog and run_result json files instead of csv
  files.
- The configuration file is simply the keyword arguments in JSON format.
- Unlike the other collectors, the DBT extractor produces manifest, catalog and run_result
  json files instead of csv files.
- This only works for DBT Cloud not DBT Core.
- This only works for DBT Cloud not DBT Core
- Uses Collector method for extracting metadata.
- Requires access to Greenplum database.
- Not all sources and destinations are included in the metadata extraction. Improvements
  are planned to provide wider coverage.
- Collector requires a set of parameters to connect to and extract metadata from Hevo.
- A high water mark file is created in the same directory as the execution.
- Supports Data Integration Objects Only, specifically Taskflows, Workflows, Mapping
  Tasks
- Requires support of 20 requests per second if rate limiting to the API has been
  applied
- We do not support gathering Metadata from Advance Filtering within Extended Objects
- Collector requires a server with minimum 2 vCPU and 8GB memory.
- Informatica repository must be hosted in Oracle.
- Collector method requires setup of connected app in Informatica
- Ensure proper access to Informatica Repository and Server
- Requires access to Informatica 9.1+ with repository hosted in Oracle.
- Supported MySQL versions include 5.7x & 8.0x
- The collector only extracts metadata and does not extract or process query usage
  on the database.
- Supported Oracle Databases include 11g and 12c+
- Uses a collector for metadata extraction
- Requires deployment and orchestration of extract code
- You may require an ODBC package for the OS to be installed as well as an oracle
  client library package if do you not have one already.
- Check the MAX_STRING_SIZE option in the database.
- Uses Python 3.6 - 3.10
- Create an Oracle user with read access to the required tables
- Uses Collector method for data extraction.
- Requires managing high water mark for incremental loading.
- When using a Collector you will push metadata to a K landing directory.
- If you want prefer file managed hwm, you can edit the location of the hwn by following
  these instructions.
- Collector method requires management of high water mark
- Requires creating specific materialized views for extraction
- Power BI API is currently rate limited. This may impact extracts where you have
  200+ workspaces or groups. Some Power BI reports may fail to extract correctly and
  may their details may not be profiled.
- Power BI datasets using large dataset storage format cannot be profiled
- Power BI reports created in web browser (not uploaded from desktop) cannot be profiled
  for detailed lineage. Dataset lineage will be available.
- Power BI dashboards in Apps are not able to be linked to their corresponding Power
  BI dashboard in a Report
- Power BI currently only supports lineage to Snowflake, SQL Server and Oracle
- Power BI reports that use inbuilt fields cannot be profiled for detailed lineage
- Power BI reports created in web browser missing dataset lineage
- Power BI access does not consider direct grants to users. Access is only detected
  for users via AD groups.
- Lineage limitations in regards to Dataset Fields to Pages, lineage for this is dependant
  on the ability to export the PowerBI Report to analyse the pbix file.
- Lineage limitations in regards to Dataset Fields to Pages, lineage for this is dependant
  on the ability to export the PowerBI Report to analyse the pbix file. If we are
  unable to download the pbix file, this lineage will be missing for that report.
- A high water mark file is created in the same directory as the execution called
  powerbi_hwm.txt.
- Requires setup of a Service Principal with access to Power BI
- Ensure SELECT access is granted to the user for all tables within the database
- Requires Superuser access to Redshift.
- Using a collector requires management of deploying and orchestrating the extract
  code.
- The collector requires a set of parameters to connect to and extract metadata from
  Redshift.
- The Scanner has a number of known limitations
- Significant improvements to the scanner is planned for Q4 24 and Q1 25.
- Tasks are not currently included in the metadata extracted from Snowflake. Currently
  under review.
- Access roles only consider grants to users. Access roles do not consider grants
  to other roles.
- Requires access to multiple account usage tables in Snowflake
- Ensure proper user permissions and role setup in Snowflake
- Requires setup of Snowflake user with specific access.
- Ensure you manage a high water mark for metadata extraction.
- Collector Method requires managing extracts and high water mark.
- Python 3.6 - 3.9 is required.
- Snowflake integration uses username/password. Using keys will be supported in an
  upcoming release.
- Snowflake integration uses username/password. Using keys will be supported in an
  upcoming release
- Only DQ Tests that have run are included in the load, if you have defined a DQ Function
  in Snowflake but never run it, it won’t be picked up
- Collector requires management of a high water mark
- Requires setup of roles and permissions in Snowflake
- A SQLServer Admin will need to setup an extended events process to capture Query
  Execution in SQLServer.
- Collector requires access to SQLServer Database
- 'The following catalog item types are currently NOT supported: Linked Reports, Files,
  Power BI Desktop Files, Report Models'
- Parameter resolution is not supported.
- SSAS query syntax is not supported
- 'Some TSQL syntax is not supported. These are mostly statements that contain not
  standard ANSI SQL constructs. Examples include: Variables (DECLARE), Flow control
  (IF BEGIN .. )'
- Uses a collector to push metadata to a K landing directory.
- The collector requires setup for SQL Server connection.
- Collector requires a set of parameters to connect to and extract metadata from SSRS.
- SSRS database must be called ReportServer$RS.
- Uses PAT Authentication (Personal Access Tokens)
- As of 3.2.0 the collector now supports PAT Authentication and Tableau Cloud
- Collector requires PAT for authentication
- User needs 'Site Administrator Creator' or 'Server/Site Administrator' role
- Collector no longer supports Tableau Server Version lower than 3.5 which does not
  have the Metadata API Endpoint.
- The Metadata API Endpoint MUST BE ENABLED for the collector to work.
- Collector requires Tableau Metadata API to be enabled.
- API user must have Server/Site Administrator role.
- The user cannot be a SSO user. This is a Tableau limitation.
- Roles are dependent on both Licensing and Server version.
- An API user needs to be created to access Tableau API.
- The user cannot be a SSO user due to Tableau limitations.
- Use a SAS token for Azure or Access key and Secret for AWS to push data.
- Collector requires managing high water mark for latest metadata extraction.
- PDCR enabled may affect table access.
- Collector method requires managing deployment and orchestration of extract code.
- Access to Teradata is needed, including creating a KADA_USER account with specific
  table access.
- Uses a collector for data extraction.
- Setup requires Teradata permissions.
- User account must be a local account (i.e. not via SSO authentication)
- Only true is supported for metadata extraction
- This collector is a metadata only collector and no usage information can be extracted
  at this stage.
- Ensure that the meta_only property value is set to true.
- 'Daily Load: Extract for new or updated objects since last extract. OR Full snapshot.'
- 'Historical Load: Full snapshot object metadata.'
- Extended Events Setup is in pilot for Azure SQL.
- Requires permissions to access INFORMATION_SCHEMA views and execute queries in Athena.
- Athena results bucket must allow Read Write Listing access.
- Sensitive Scanner does not currently support Big Query.
- Add Read Only Access to All Projects
- API supports pagination through cursor-based method.
- Use an orchestration tool like Airflow to align the filenames and push the docs
  (manifest, catalog, run_results) to the landing folder
- The inclusion of the project_id in the filename is to support multiple dbt projects.
- Inferred data connectors for databases are currently supported.
- SQL run time data connectors for databases will be added in upcoming sprints.
- Spark data connectors for files are not supported.
- The Kada plugin has been tested with GX versions 0.15.41 - 0.18.19 and Python 3.8
  - 3.11
- If you have another action that stores the results already and add this action,
  GX will simply just push the validations in both locations.
- Power BI datasets using large dataset storage format cannot be profiled.
- Power BI reports that use report measures cannot be profiled for detailed lineage.
- Generally all users should have access to the pg_catalog tables on Database creation
  for Postgres.
- In the event the user doesn’t have access, explicit grants will need to be done
  per new Database in Postgres to the <kada user>.
- Streams & Tasks are not currently included in the metadata extracted from Snowflake.
- K does NOT currently support the out of the box DMFs (SNOWFLAKE.CORE).
- K only uses user defined DMFs because K measures DQ results as a % of rows that
  meet the test condition.
- K expects the DMF result to be a value between 0 to 100.
- K does NOT SUPPORT the out of the box DMFs (in the CORE schema)
- K only uses user defined DMFs because K measures DQ results as a % of rows that
  meet the test condition
- K expects the DMF result to be a value between 0 to 100
- All DMFs will be detached from the table when the table is dropped/recreated
- A workaround is to attached the DMFs using a scheduled task
- Requires logging to be enabled
- Queries, macros and procedures must include fully qualified names in order to be
  correctly parsed
- PAT tokens expire after 1 year. You will need to refresh the token in K after it
  expires.
- Tableau Metadata API must be enabled
- Tableau Repository must be accessible
- Changing settings will require a restart of the tableau server
- Manual Upload Method available
- Use the appropriate API documentation for more details on integration.
- The API Key is linked to an individual and will be recorded as the Issue creator
  for all Jira’s created via the K issue integration.
- Mapping allows K to resolve sources that might be named differently across different
  tools.
- This step is required for dbt. We recommend option 1 for all other Tools.
- The access_token will be used to authenticate API calls to KADA.
- API calls require an Authorisation header Bearer token.
- Merging duplicate database sources
- This feature is currently restricted to KADA Admins
- Your K instance will be deployed on Azure or AWS
- There is no guarantee that the job will load all data provided in the payload.
- Ensure all participants have the correct K roles assigned
- Avoid unnecessary batch jobs running during a workshop time to improve upload speeds
- Recommended tasks for Enterprise deployment checklist
- Some tasks are optional or specific to certain deployment types
- v1.2.0 includes support for Snowflake keypair
- The landing directory is the lz…
- Do not include kada-data in the landing directory path
- Kada API requires OAuth2 authentication.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- Row number is incremental across all tabs e.g. If Tables tab (2nd tab after instructions
  tab) has 500 records, and Report (3rd tab) has 100 records, the error of row 550
  is row 50 in the report tab.
- '400 Bad Request: Check the request parameters'
- '401 Unauthorized: Recheck OAuth credentials'
- '429 Too Many Requests: Reduce the request rate'
- Nested query where a select * is used
- Create statement containing UNION ALL uses table aliases in the SELECT statements
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- 'Access Denied: Ensure IAM policies are correctly set for the user/role'
- 'Query Execution Failed: Verify queries and permissions'
- '401 Unauthorized: Recheck API token or permissions'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- '401 Unauthorized: Recheck user credentials or permissions.'
- '401 Unauthorized: Recheck API key or permissions'
- '401 Unauthorized: Check your username and password.'
- '500 Internal Server Error: Check the Informatica server status.'
- '401 Unauthorized: Check credentials and permissions'
- '403 Forbidden: Ensure user has access to required resources'
- '401 Unauthorized: Recheck user credentials'
- 'ORA-00910: specified length is too long for its datatype'
- FAILED scan status if unable to scan tables with case sensitive names
- FAILED scan status if unable to scan tables with special characters
- FAILED scan status if unable to scan tables that are named after keywords
- FAILED scan status if unable to scan tables that contain a period (.) in the name
- FAILED scan status if the table contains a column that causes a data retrieval error
- FAILED scan status if the view has issues executing the underlying Stored Proc or
  SQL
- '403 Forbidden: Check your permissions'
- '401 Unauthorized: Recheck your credentials'
- '403 Forbidden: Ensure proxy settings are correct when using private link.'
- 'Invalid credentials: Check username and password.'
- '403 Forbidden: Ensure correct proxy settings if using private link.'
- '403 Forbidden: Check permissions and proxy settings'
- '401 Unauthorized: Verify user credentials'
- '401 Unauthorized: Check PAT token and user roles'
- '401 Unauthorized: Recheck API user credentials and their roles.'
- '401 Unauthorized: Recheck API user credentials'
- '401 Unauthorized: Recheck Teradata username or password'
- 'Connection Error: Verify Teradata server address and port'
- '401 Unauthorized: Check username and password.'
- 'ConnectionError: Verify server address.'
- '401 Unauthorized: Recheck user credentials or permissions'
- '401 Unauthorized: Recheck username and password'
- '403 Forbidden: Check user permissions'
- '500 Internal Server Error: Retry the request or check the server status'
- '401 Unauthorized: Local user must be authenticated with Username and Password not
  SSO'
- '404 Not Found: Check the endpoint path.'
- '401 Unauthorized: Verify your API key.'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - CATALOG_READ_ONLY
  - kada_user
  - CATALOG_METADATA
client:
  base_url: https://www.kada.ai/
  auth:
    type: oauth2
    flow: refresh_token
  headers:
    Accept: application/json
source_metadata: null
