resources:
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: models
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: components.llm
    params: {}
- name: translation
  endpoint:
    path: /api/translation
    method: POST
    data_selector: translations
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: models
    params: {}
- name: translation
  endpoint:
    path: /api/translation
    method: GET
    data_selector: translations
    params: {}
- name: examples
  endpoint:
    path: raw_examples.yml
- name: summarization
  endpoint:
    path: /path/to/summarization
    method: POST
    data_selector: summary
    params: {}
- name: entity_linking
  endpoint:
    path: /path/to/entity_linking
    method: POST
    data_selector: entities
    params: {}
- name: relation_extraction
  endpoint:
    path: /path/to/relation_extraction
    method: POST
    data_selector: relations
    params: {}
- name: ner_task_v3
  endpoint:
    path: /components/llm/task
    method: GET
    data_selector: ''
    params: {}
- name: textcat
  endpoint:
    path: /components/llm/task
    method: GET
    data_selector: records
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: components.llm.task
    params: {}
- name: LLMWrapper
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: components
    params: {}
- name: translation
  endpoint:
    path: /translation
    method: POST
    data_selector: responses
- name: examples
  endpoint:
    path: raw_examples.yml
- name: summarization_task
  endpoint:
    path: /api/summarization
    method: POST
    data_selector: summary
- name: entity_linking_task
  endpoint:
    path: /api/entity_linking
    method: POST
    data_selector: entities
- name: relation_extraction_task
  endpoint:
    path: /api/relation_extraction
    method: POST
    data_selector: relations
- name: trained_pipelines
  endpoint:
    path: /models
    method: GET
    data_selector: pipelines
    params: {}
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: models
    params: {}
- name: translation
  endpoint:
    path: /api/translation
    method: GET
    data_selector: translations
    params:
      target_lang: Spanish
- name: llm_reply
  endpoint: {}
- name: components.llm.task
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: records
- name: DocBin
  endpoint:
    path: /api/docbin
    method: GET
    data_selector: records
- name: Example
  endpoint:
    path: /api/example
    method: GET
    data_selector: records
- name: Language
  endpoint:
    path: /api/language
    method: GET
    data_selector: records
- name: Lexeme
  endpoint:
    path: /api/lexeme
    method: GET
    data_selector: records
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: records
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: records
- name: textcat_v3
  endpoint:
    path: /components/llm/task
    method: GET
    data_selector: labels
    params: {}
- name: textcat_v2
  endpoint:
    path: /components/llm/task
    method: GET
    data_selector: labels
    params: {}
- name: textcat_v1
  endpoint:
    path: /components/llm/task
    method: GET
    data_selector: labels
    params: {}
- name: language_support
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
    params: {}
- name: pipeline
  endpoint:
    path: /usage/models
    method: GET
    data_selector: pipelines
    params: {}
- name: language_support
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
    params: {}
- name: models
  endpoint:
    path: /v1/models
    method: GET
    data_selector: data
- name: models
  endpoint:
    path: /usage/models
    method: GET
    data_selector: models
    params: {}
- name: languages
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
    params: {}
- name: initialize_tokenizer
  endpoint:
    path: /initialize/tokenizer
    method: GET
    data_selector: pkuseg_model
    params: {}
- name: Chinese
  endpoint:
    path: /usage/models/chinese
    method: GET
    data_selector: models
    params: {}
- name: trained_pipelines
  endpoint:
    path: /models
    method: GET
    data_selector: pipelines
    params: {}
- name: pkuseg_model
  endpoint:
    path: /path/to/model
    method: GET
    data_selector: model
    params: {}
- name: pkuseg_user_dict
  endpoint:
    path: default
    method: GET
    data_selector: user_dict
    params: {}
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: records
- name: Language
  endpoint:
    path: /api/language
    method: GET
    data_selector: records
- name: Vocab
  endpoint:
    path: /api/vocab
    method: GET
    data_selector: records
- name: Korean language support
  endpoint:
    path: /usage/linguistic-features#tokenization
    method: GET
    data_selector: tokenizer
    params: {}
- name: language_support
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
    params: {}
- name: multi_language
  endpoint:
    path: /models/xx
    method: GET
    data_selector: pipelines
- name: chinese
  endpoint:
    path: /models/zh
    method: GET
    data_selector: pipelines
- name: pkuseg_model
  endpoint:
    path: /path/to/model
    method: GET
    data_selector: records
- name: pkuseg_user_dict
  endpoint:
    path: default
    method: GET
    data_selector: records
- name: trained_pipeline
  endpoint:
    path: /usage/models
    method: GET
    data_selector: pipelines
- name: language_support
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
- name: en_core_web_sm
  endpoint:
    path: /download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl
    method: GET
    data_selector: pipeline
- name: nlp_tokenizer
  endpoint:
    path: /path/to/model
    method: GET
    data_selector: records
    params: {}
- name: language_models
  endpoint:
    path: /usage/models
    method: GET
    data_selector: models
- name: initialize
  endpoint:
    path: /usage/training#config
    method: GET
    data_selector: records
    params: {}
- name: Chinese
  endpoint:
    path: /api/chinese
    method: GET
- name: pkuseg_model
  endpoint:
    path: /path/to/model
    method: GET
    data_selector: model
- name: pkuseg_user_dict
  endpoint:
    path: default
    method: GET
    data_selector: user_dict
- name: language_support
  endpoint:
    path: /usage/languages
    method: GET
    data_selector: languages
    params: {}
- name: multi_language_support
  endpoint:
    path: /usage/multi-language
    method: GET
    data_selector: multi_language
    params: {}
- name: chinese_language_support
  endpoint:
    path: /usage/chinese
    method: GET
    data_selector: chinese
    params: {}
- name: en_core_web_sm
  endpoint:
    path: /download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl
    method: GET
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: sentences
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: similarity
    params: {}
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: vector
    params: {}
- name: Lexeme
  endpoint:
    path: /api/lexeme
    method: GET
    data_selector: similarity
    params: {}
- name: rule_based_matching
  endpoint:
    path: /usage/rule-based-matching
    method: GET
    data_selector: content
    params: {}
- name: sl_core_news_sm
  endpoint:
    path: /models/sl#sl_core_news_sm
    method: GET
    data_selector: records
- name: sl_core_news_md
  endpoint:
    path: /models/sl#sl_core_news_md
    method: GET
    data_selector: records
- name: sl_core_news_lg
  endpoint:
    path: /models/sl#sl_core_news_lg
    method: GET
    data_selector: records
- name: sl_core_news_trf
  endpoint:
    path: /models/sl#sl_core_news_trf
    method: GET
    data_selector: records
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: annotations
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: attributes
    params: {}
- name: MorphAnalysis
  endpoint:
    path: /api/morphology
    method: GET
    data_selector: morphological_features
    params: {}
- name: part_of_speech_tagging
  endpoint:
    path: /usage/linguistic-features#pos-tagging
    method: GET
    data_selector: annotations
    params: {}
- name: morphology
  endpoint:
    path: /usage/linguistic-features#morphology
    method: GET
    data_selector: annotations
    params: {}
- name: lemmatization
  endpoint:
    path: /usage/linguistic-features#lemmatization
    method: GET
    data_selector: annotations
    params: {}
- name: dependency_parsing
  endpoint:
    path: /usage/linguistic-features#dependency-parse
    method: GET
    data_selector: annotations
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: components
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
- name: tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: Doc
    params: {}
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: Token.tag
    params: {}
- name: parser
  endpoint:
    path: /api/dependencyparser
    method: GET
    data_selector: Token.head
    params: {}
- name: ner
  endpoint:
    path: /api/entityrecognizer
    method: GET
    data_selector: Doc.ents
    params: {}
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
    data_selector: Token.lemma
    params: {}
- name: textcat
  endpoint:
    path: /api/textcategorizer
    method: GET
    data_selector: Doc.cats
    params: {}
- name: custom
  endpoint:
    path: /api/custom-components
    method: GET
    data_selector: Doc._.xxx
    params: {}
- name: tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: Doc
    params: {}
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: Token.tag
    params: {}
- name: parser
  endpoint:
    path: /api/parser
    method: GET
    data_selector: Token.head
    params: {}
- name: ner
  endpoint:
    path: /api/ner
    method: GET
    data_selector: Doc.ents
    params: {}
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
    data_selector: Token.lemma
    params: {}
- name: matcher
  endpoint:
    path: /api/matcher
    method: GET
    data_selector: records
    params: {}
- name: phrasematcher
  endpoint:
    path: /api/phrasematcher
    method: GET
    data_selector: records
    params: {}
- name: matcher
  endpoint:
    path: /api/matcher
    method: GET
    data_selector: records
    params: {}
- name: phrasematcher
  endpoint:
    path: /api/phrasematcher
    method: GET
    data_selector: records
    params: {}
- name: pipeline_component
  endpoint:
    path: /api/pipeline_component
    method: GET
    data_selector: components
    params: {}
- name: matcher
  endpoint:
    path: /api/matcher
    method: GET
    data_selector: records
    params: {}
- name: phrasematcher
  endpoint:
    path: /api/phrasematcher
    method: GET
    data_selector: records
    params: {}
- name: tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: Doc
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: Token.tag
- name: parser
  endpoint:
    path: /api/dependencyparser
    method: GET
    data_selector: Token.head
- name: ner
  endpoint:
    path: /api/entityrecognizer
    method: GET
    data_selector: Doc.ents
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
    data_selector: Token.lemma
- name: textcat
  endpoint:
    path: /api/textcategorizer
    method: GET
    data_selector: Doc.cats
- name: custom
  endpoint:
    path: /usage/processing-pipelines#custom-components
    method: GET
    data_selector: Doc._.xxx
- name: DependencyMatcherOperators
  endpoint:
    path: /api/dependency_matcher/operators
    method: GET
    data_selector: operators
    params: {}
- name: EntityPatterns
  endpoint:
    path: /api/entity_patterns
    method: GET
    data_selector: patterns
    params: {}
- name: UsingEntityRuler
  endpoint:
    path: /api/entity_ruler/usage
    method: GET
    data_selector: usage
    params: {}
- name: UsingSpanRuler
  endpoint:
    path: /api/span_ruler/usage
    method: GET
    data_selector: usage
    params: {}
- name: PatternValidation
  endpoint:
    path: /api/pattern_validation
    method: GET
    data_selector: validation
    params: {}
- name: tagger
  endpoint:
    path: /api/language#tagger
    method: GET
    data_selector: assigns
    params: {}
- name: entity_linker
  endpoint:
    path: /api/language#entity_linker
    method: GET
    data_selector: assigns
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: pipeline
- name: components
  endpoint:
    path: /usage/processing-pipelines#components
    method: GET
    data_selector: components
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: entities
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: attributes
    params: {}
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: attributes
    params: {}
- name: pipeline
  endpoint:
    path: /models
    method: GET
    data_selector: models
    params: {}
- name: llm
  endpoint:
    path: /api/large-language-models
    method: GET
    data_selector: components
    params: {}
- name: tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: Doc
    params: {}
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: Token.tag
    params: {}
- name: parser
  endpoint:
    path: /api/parser
    method: GET
    data_selector: Token.head, Token.dep, Doc.sents, Doc.noun_chunks
    params: {}
- name: ner
  endpoint:
    path: /api/entityrecognizer
    method: GET
    data_selector: Doc.ents, Token.ent_iob, Token.ent_type
    params: {}
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
    data_selector: Token.lemma
    params: {}
- name: processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: components
- name: training_data
  endpoint:
    path: /usage/training
    method: GET
    data_selector: examples
    params: {}
- name: evaluation_data
  endpoint:
    path: /usage/evaluation
    method: GET
    data_selector: examples
    params: {}
- name: language_processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: components
- name: models
  endpoint:
    path: /usage/models
    method: GET
    data_selector: models
- name: train
  endpoint:
    path: /api/train
    method: POST
    data_selector: data
    params: {}
- name: dev
  endpoint:
    path: /api/dev
    method: GET
    data_selector: data
    params: {}
- name: processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: components
- name: training_data
  endpoint:
    path: /api/training
    method: POST
    data_selector: data
- name: evaluation_data
  endpoint:
    path: /api/evaluation
    method: POST
    data_selector: data
- name: pipeline
  endpoint:
    path: /api/language#initialize
    method: GET
- name: processing_pipeline
  endpoint:
    path: /usage/processing-pipelines
    method: GET
    data_selector: components
- name: training
  endpoint:
    path: /api/train
    method: POST
    data_selector: training_data
    params: {}
- name: evaluation
  endpoint:
    path: /api/evaluate
    method: GET
    data_selector: evaluation_data
    params: {}
- name: acronyms
  endpoint:
    path: /components/acronyms
    method: GET
    data_selector: data
    params: {}
- name: trainable_component
  endpoint:
    path: /components/trainable_component
    method: GET
    data_selector: data
    params: {}
- name: custom_extension
  endpoint:
    path: /api/doc#set_extension
    method: POST
    data_selector: records
- name: similarity_component
  endpoint:
    path: /api/language#add_pipe
    method: POST
    data_selector: records
- name: Loss
  endpoint:
    path: /loss
    method: GET
    data_selector: training_loss
- name: Precision
  endpoint:
    path: /precision
    method: GET
    data_selector: precision
- name: Recall
  endpoint:
    path: /recall
    method: GET
    data_selector: recall
- name: F-Score
  endpoint:
    path: /fscore
    method: GET
    data_selector: f_score
- name: UAS
  endpoint:
    path: /uas
    method: GET
    data_selector: uas
- name: LAS
  endpoint:
    path: /las
    method: GET
    data_selector: las
- name: Speed
  endpoint:
    path: /speed
    method: GET
    data_selector: speed
- name: tagger
  endpoint:
    path: /components/tagger
    method: GET
    data_selector: model
    params: {}
- name: corpora.train
  endpoint:
    path: /corpora/train
    method: GET
    data_selector: corpora
    params: {}
- name: corpora.dev
  endpoint:
    path: /corpora/dev
    method: GET
    data_selector: corpora
    params: {}
- name: text_classifier
  endpoint:
    path: /usage/large-language-models#example-1
    method: GET
    data_selector: records
- name: ner
  endpoint:
    path: /usage/large-language-models#example-2
    method: GET
    data_selector: records
- name: train_corpus
  endpoint:
    path: ${paths.train}
    method: GET
    data_selector: examples
    params: {}
- name: dev_corpus
  endpoint:
    path: ${paths.dev}
    method: GET
    data_selector: examples
    params: {}
- name: Text Classification
  endpoint:
    path: /v1/engines/gpt-3.5-turbo/completions
    method: POST
    data_selector: choices
    params: {}
- name: train
  endpoint:
    path: /corpora.train
    method: GET
    data_selector: corpus
    params:
      source: s3://your_bucket/path/data.csv
- name: dev
  endpoint:
    path: /corpora.dev
    method: GET
    data_selector: corpus
    params:
      limit: 100
- name: tagger
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.tagger
- name: textcat
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.textcat
- name: tok2vec
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.tok2vec
- name: training_data
  endpoint:
    path: /usage/training
    method: GET
    data_selector: training
- name: evaluation_data
  endpoint:
    path: /usage/evaluation
    method: GET
    data_selector: evaluation
- name: model.create_instance_tensor
  endpoint:
    params: {}
- name: rel_instance_generator.v1
  endpoint:
    params:
      max_length: 100
- name: training_data
  endpoint:
    path: /usage/training
    method: GET
    data_selector: examples
- name: evaluation_data
  endpoint:
    path: /usage/evaluation
    method: GET
    data_selector: examples
- name: relation_extractor
  endpoint:
    path: /services/data/vXX.X/sobjects/relation_extractor
    method: GET
    data_selector: records
    params: {}
- name: training_data
  endpoint:
    path: /api/training
    method: POST
    data_selector: records
- name: project_templates
  endpoint:
    path: /projects
    method: GET
    data_selector: templates
    params: {}
- name: training_data
  endpoint:
    path: assets/training.spacy
    data_selector: data
    params:
      checksum: 63373dd656daa1fd3043ce166a59474c
- name: development_data
  endpoint:
    path: assets/development.spacy
    data_selector: data
    params:
      checksum: 5113dc04e03f079525edd8df3f4f39e3
- name: training_pipelines
  endpoint:
    path: /training/pipelines
    method: GET
    data_selector: pipelines
    params: {}
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
    params: {}
- name: train
  endpoint:
    path: /train
    method: POST
    data_selector: outputs
    params: {}
- name: test
  endpoint:
    path: /test
    method: POST
    data_selector: outputs
    params: {}
- name: training_data
  endpoint:
    path: /api/training/data
    method: GET
    data_selector: records
- name: project
  endpoint:
    path: /api/projects
    method: GET
    data_selector: data
- name: project
  endpoint:
    path: /api/project
    method: GET
    data_selector: directories
    params: {}
- name: train_corpus
  endpoint:
    path: /corpora/train
    method: GET
    data_selector: records
- name: dev_corpus
  endpoint:
    path: /corpora/dev
    method: GET
    data_selector: records
- name: corpora.train
  endpoint:
    path: ${paths.train}
    method: GET
    data_selector: records
    params: {}
- name: corpora.dev
  endpoint:
    path: ${paths.dev}
    method: GET
    data_selector: records
    params: {}
- name: remote_storage
  endpoint:
    path: /api/cli#project-push
    method: POST
    data_selector: outputs
    params: {}
- name: DVC
  endpoint:
    path: /api/cli#project-dvc
    method: POST
    data_selector: workflow
    params: {}
- name: Prodigy
  endpoint:
    path: /api/cli#project-prodigy
    method: POST
    data_selector: annotations
    params: {}
- name: Streamlit
  endpoint:
    path: /api/cli#project-streamlit
    method: POST
    data_selector: visualization
    params: {}
- name: FastAPI
  endpoint:
    path: /api/cli#project-fastapi
    method: POST
    data_selector: API
    params: {}
- name: Weights & Biases
  endpoint:
    path: /api/cli#project-wandb
    method: POST
    data_selector: logging
    params: {}
- name: Hugging Face Hub
  endpoint:
    path: /api/cli#project-huggingface_hub
    method: POST
    data_selector: model_upload
    params: {}
- name: push_to_hub
  endpoint:
    path: /api/upload
    method: POST
    data_selector: packages
    params: {}
- name: train_corpus
  endpoint:
    path: /api/data-formats#config-corpora
    method: GET
    data_selector: corpora.train
    params: {}
- name: dev_corpus
  endpoint:
    path: /api/data-formats#config-corpora
    method: GET
    data_selector: corpora.dev
    params: {}
- name: training
  endpoint:
    path: /api/data-formats#config-training
    method: GET
    data_selector: training
    params: {}
- name: tagger
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.tagger
    params: {}
- name: textcat
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.textcat
    params: {}
- name: tok2vec
  endpoint:
    path: /usage/layers-architectures
    method: GET
    data_selector: components.tok2vec
    params: {}
- name: dependency_visualizer
  endpoint:
    path: /api/top-level#displacy.serve
    method: GET
    data_selector: Doc
    params: {}
- name: entity_visualizer
  endpoint:
    path: /api/top-level#displacy.serve
    method: GET
    data_selector: Doc
    params: {}
- name: span_visualizer
  endpoint:
    path: /api/top-level#displacy.serve
    method: GET
    data_selector: Doc
    params: {}
- name: training.score_weights
  endpoint:
    params:
      rel_micro_p: 0.0
      rel_micro_r: 0.0
      rel_micro_f: 1.0
- name: model_architectures
  endpoint:
    path: /api/architectures
    method: GET
    data_selector: architectures
    params: {}
- name: api_usage
  endpoint:
    path: /api/usage
    method: GET
    data_selector: usage
    params: {}
- name: relation_extractor
- name: relation_extractor
  endpoint:
    path: /factory/annotations
    method: GET
    data_selector: default_score_weights
    params: {}
- name: project_templates
  endpoint:
    path: /api/projects
    method: GET
    data_selector: templates
    params: {}
- name: TransformerModel
  endpoint:
    path: /api/architectures#TransformerModel
    method: GET
    data_selector: models
- name: TransformerListener
  endpoint:
    path: /api/architectures#TransformerListener
    method: GET
    data_selector: models
- name: Tok2VecTransformer
  endpoint:
    path: /api/architectures#Tok2VecTransformer
    method: GET
    data_selector: models
- name: AlbertTransformer
  endpoint:
    path: /api/curated-transformers#AlbertTransformer
    method: GET
    data_selector: models
- name: BertTransformer
  endpoint:
    path: /api/curated-transformers#BertTransformer
    method: GET
    data_selector: models
- name: CamembertTransformer
  endpoint:
    path: /api/curated-transformers#CamembertTransformer
    method: GET
    data_selector: models
- name: RobertaTransformer
  endpoint:
    path: /api/curated-transformers#RobertaTransformer
    method: GET
    data_selector: models
- name: XlmrTransformer
  endpoint:
    path: /api/curated-transformers#XlmrTransformer
    method: GET
    data_selector: models
- name: training_data
  endpoint:
    path: assets/training.spacy
    method: GET
    data_selector: assets
    params:
      url: https://example.com/data.spacy
      checksum: 63373dd656daa1fd3043ce166a59474c
- name: development_data
  endpoint:
    path: assets/development.spacy
    method: GET
    data_selector: assets
    params:
      url: gs://your-bucket/corpora
      checksum: 5113dc04e03f079525edd8df3f4f39e3
- name: TextCatReduce
  endpoint:
    path: /api/models/spacy.TextCatReduce.v1
    method: GET
- name: training_data
  endpoint:
    path: /assets/training.spacy
    method: GET
    data_selector: assets
    params: {}
- name: development_data
  endpoint:
    path: /assets/development.spacy
    method: GET
    data_selector: assets
    params: {}
- name: project
  endpoint:
    path: /api/cli#project
    method: GET
    data_selector: directories
- name: custom_scripts
  endpoint:
    path: /api/cli#custom
    method: GET
    data_selector: commands
- name: push_to_hub
  endpoint:
    path: /api/v1/push
    method: POST
- name: nlp
  endpoint:
    path: /api/language
    method: GET
    data_selector: nlp
- name: convert
  endpoint:
    path: /api/convert
    method: POST
    data_selector: data
    params: {}
- name: debug
  endpoint:
    path: /api/debug
    method: POST
    data_selector: data
    params: {}
- name: debug_data
  endpoint:
    path: /spacy/debug/data
    method: GET
    data_selector: stats
    params:
      config_path: ./config.cfg
      code: null
      ignore-warnings: false
      verbose: true
      no-format: false
- name: displacy
  endpoint:
    path: /api/top-level#displacy
    method: GET
    data_selector: none
    params: {}
- name: debug_diff_config
  endpoint:
    path: /debug/diff-config
    method: GET
- name: debug_profile
  endpoint:
    path: /debug/profile
    method: GET
- name: debug_model
  endpoint:
    path: /debug/model
    method: GET
- name: debug_pieces
  endpoint:
    path: /debug/pieces
    method: GET
- name: train
  endpoint:
    path: /train
    method: POST
- name: TransformerModel
  endpoint:
    path: /api/architectures/TransformerModel
    method: GET
    data_selector: model
- name: TransformerListener
  endpoint:
    path: /api/architectures/TransformerListener
    method: GET
    data_selector: model
- name: Tok2VecTransformer
  endpoint:
    path: /api/architectures/Tok2VecTransformer
    method: GET
    data_selector: model
- name: train
  endpoint:
    path: /train
    method: POST
- name: pretrain
  endpoint:
    path: /pretrain
    method: POST
- name: evaluate
  endpoint:
    path: /evaluate
    method: POST
- name: benchmark
  endpoint:
    path: /benchmark
    method: POST
- name: huggingface-hub push
  endpoint:
    path: /huggingface-hub/push
    method: POST
    data_selector: uploads
    params: {}
- name: ScalarWeightingListener
  endpoint:
    path: /spacy-curated-transformers/ScalarWeightingListener/v1
    method: GET
    data_selector: outputs
- name: BertWordpieceEncoder
  endpoint:
    path: /spacy-curated-transformers/BertWordpieceEncoder/v1
    method: GET
    data_selector: outputs
- name: ByteBpeEncoder
  endpoint:
    path: /spacy-curated-transformers/ByteBpeEncoder/v1
    method: GET
    data_selector: outputs
- name: CamembertSentencepieceEncoder
  endpoint:
    path: /spacy-curated-transformers/CamembertSentencepieceEncoder/v1
    method: GET
    data_selector: outputs
- name: CharEncoder
  endpoint:
    path: /spacy-curated-transformers/CharEncoder/v1
    method: GET
    data_selector: outputs
- name: SentencepieceEncoder
  endpoint:
    path: /spacy-curated-transformers/SentencepieceEncoder/v1
    method: GET
    data_selector: outputs
- name: WordpieceEncoder
  endpoint:
    path: /spacy-curated-transformers/WordpieceEncoder/v1
    method: GET
    data_selector: outputs
- name: XlmrSentencepieceEncoder
  endpoint:
    path: /spacy-curated-transformers/XlmrSentencepieceEncoder/v1
    method: GET
    data_selector: outputs
- name: TextCatReduce
  endpoint:
    path: /api/architectures#TextCatReduce
    method: GET
- name: SpanCategorizer
  endpoint:
    path: /api/architectures#SpanCategorizer
    method: GET
- name: read_json
  endpoint:
    path: /readers/srsly.read_json.v1
    method: GET
    data_selector: records
- name: read_jsonl
  endpoint:
    path: /readers/srsly.read_jsonl.v1
    method: GET
    data_selector: records
- name: read_yaml
  endpoint:
    path: /readers/srsly.read_yaml.v1
    method: GET
    data_selector: records
- name: read_msgpack
  endpoint:
    path: /readers/srsly.read_msgpack.v1
    method: GET
    data_selector: records
- name: load_pipeline
  endpoint:
    path: /api/load
    method: GET
    data_selector: pipeline
- name: blank_pipeline
  endpoint:
    path: /api/blank
    method: GET
    data_selector: pipeline
- name: download
  endpoint:
    path: /api/cli/download
    method: GET
- name: info
  endpoint:
    path: /api/cli/info
    method: GET
- name: validate
  endpoint:
    path: /api/cli/validate
    method: GET
- name: init
  endpoint:
    path: /api/cli/init
    method: GET
- name: Named Entity Recognition data structure
  endpoint:
    path: /api/structure-ent
    method: GET
    data_selector: ents
    params: {}
- name: Span Classification data structure
  endpoint:
    path: /api/structure-span
    method: GET
    data_selector: spans
    params: {}
- name: convert
  endpoint:
    path: /api/cli/convert
    method: POST
    data_selector: results
    params: {}
- name: debug
  endpoint:
    path: /api/cli/debug
    method: POST
    data_selector: results
    params: {}
- name: JsonlCorpus
  endpoint:
    path: /api/corpus#jsonlcorpus
    method: GET
    data_selector: records
    params: {}
- name: Batchers
  endpoint:
    path: /api/corpus#batchers
    method: GET
    data_selector: records
    params: {}
- name: Augmenters
  endpoint:
    path: /api/corpus#augmenters
    method: GET
    data_selector: records
    params: {}
- name: Callbacks
  endpoint:
    path: /api/corpus#callbacks
    method: GET
    data_selector: records
    params: {}
- name: Training data and alignment
  endpoint:
    path: /api/corpus#gold
    method: GET
    data_selector: records
    params: {}
- name: push
  endpoint:
    path: /api/push
    method: POST
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: records
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: records
    params: {}
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: records
    params: {}
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: attributes
    params: {}
- name: DocBin
  endpoint:
    path: /api/docbin
    method: GET
    data_selector: records
    params: {}
- name: Example
  endpoint:
    path: /api/example
    method: GET
    data_selector: records
    params: {}
- name: Alignment
  endpoint:
    path: /api/alignment
    method: GET
    data_selector: records
    params: {}
- name: orth_variants
  endpoint:
    path: corpus/en_orth_variants.json
    method: GET
    data_selector: records
    params: {}
- name: ner
  endpoint:
    path: corpus/labels/ner.json
    method: GET
    data_selector: records
    params: {}
- name: train
  endpoint:
    path: corpus/train.spacy
    method: GET
    data_selector: records
    params: {}
- name: pretrain
  endpoint:
    path: corpus/raw_text.jsonl
    method: GET
    data_selector: records
    params: {}
- name: spacy.load
  endpoint:
    path: /api/top-level/spacy.load
    method: GET
    data_selector: records
- name: spacy.blank
  endpoint:
    path: /api/top-level/spacy.blank
    method: GET
    data_selector: records
- name: spacy.info
  endpoint:
    path: /api/top-level/spacy.info
    method: GET
    data_selector: records
- name: spacy.explain
  endpoint:
    path: /api/top-level/spacy.explain
    method: GET
    data_selector: records
- name: spacy.prefer_gpu
  endpoint:
    path: /api/top-level/spacy.prefer_gpu
    method: GET
    data_selector: records
- name: spacy.require_gpu
  endpoint:
    path: /api/top-level/spacy.require_gpu
    method: GET
    data_selector: records
- name: spacy.require_cpu
  endpoint:
    path: /api/top-level/spacy.require_cpu
    method: GET
    data_selector: records
- name: JsonlCorpus
  endpoint:
    path: /api/corpus#jsonlcorpus
    method: GET
    data_selector: records
    params: {}
- name: Batchers
  endpoint:
    path: /api/training/batchers
    method: GET
    data_selector: records
    params: {}
- name: Augmenters
  endpoint:
    path: /api/training/augment
    method: GET
    data_selector: records
    params: {}
- name: Callbacks
  endpoint:
    path: /api/training/callbacks
    method: GET
    data_selector: records
    params: {}
- name: Training data and alignment
  endpoint:
    path: /api/training
    method: GET
    data_selector: records
    params: {}
- name: update
  endpoint:
    path: /api/language/update
    method: POST
    data_selector: losses
- name: rehearse
  endpoint:
    path: /api/language/rehearse
    method: POST
    data_selector: losses
- name: evaluate
  endpoint:
    path: /api/language/evaluate
    method: POST
    data_selector: scores
- name: use_params
  endpoint:
    path: /api/language/use_params
    method: POST
    data_selector: params
- name: add_pipe
  endpoint:
    path: /api/language/add_pipe
    method: POST
    data_selector: component
- name: create_pipe
  endpoint:
    path: /api/language/create_pipe
    method: POST
    data_selector: component
- name: has_factory
  endpoint:
    path: /api/language/has_factory
    method: GET
    data_selector: exists
- name: has_pipe
  endpoint:
    path: /api/language/has_pipe
    method: GET
    data_selector: exists
- name: get_pipe
  endpoint:
    path: /api/language/get_pipe
    method: GET
    data_selector: component
- name: replace_pipe
  endpoint:
    path: /api/language/replace_pipe
    method: POST
    data_selector: component
- name: rename_pipe
  endpoint:
    path: /api/language/rename_pipe
    method: POST
    data_selector: new_name
- name: remove_pipe
  endpoint:
    path: /api/language/remove_pipe
    method: DELETE
    data_selector: removed_component
- name: disable_pipe
  endpoint:
    path: /api/language/disable_pipe
    method: POST
    data_selector: disabled
- name: enable_pipe
  endpoint:
    path: /api/language/enable_pipe
    method: POST
    data_selector: enabled
- name: select_pipes
  endpoint:
    path: /api/language/select_pipes
    method: POST
    data_selector: disabled_pipes
- name: get_factory_meta
  endpoint:
    path: /api/language/get_factory_meta
    method: GET
    data_selector: factory_meta
- name: get_pipe_meta
  endpoint:
    path: /api/language/get_pipe_meta
    method: GET
    data_selector: factory_meta
- name: analyze_pipes
  endpoint:
    path: /api/language/analyze_pipes
    method: GET
    data_selector: analysis
- name: Lexeme
  endpoint:
    path: /api/lexeme
    method: GET
    data_selector: records
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: records
- name: iob_to_biluo
  endpoint:
    path: /util/iob_to_biluo
    method: GET
    data_selector: returns
    params: {}
- name: get_lang_class
  endpoint:
    path: /util/get_lang_class
    method: GET
    data_selector: returns
    params: {}
- name: lang_class_is_loaded
  endpoint:
    path: /util/lang_class_is_loaded
    method: GET
    data_selector: returns
    params: {}
- name: load_model
  endpoint:
    path: /util/load_model
    method: GET
    data_selector: returns
    params: {}
- name: load_model_from_init_py
  endpoint:
    path: /util/load_model_from_init_py
    method: GET
    data_selector: returns
    params: {}
- name: load_config
  endpoint:
    path: /util/load_config
    method: GET
    data_selector: returns
    params: {}
- name: load_meta
  endpoint:
    path: /util/load_meta
    method: GET
    data_selector: returns
    params: {}
- name: get_installed_models
  endpoint:
    path: /util/get_installed_models
    method: GET
    data_selector: returns
    params: {}
- name: is_package
  endpoint:
    path: /util/is_package
    method: GET
    data_selector: returns
    params: {}
- name: get_package_path
  endpoint:
    path: /util/get_package_path
    method: GET
    data_selector: returns
    params: {}
- name: vector
  endpoint:
    path: /vector
    method: GET
    data_selector: numpy.ndarray[ndim=1, dtype=float32]
- name: vector_norm
  endpoint:
    path: /vector_norm
    method: GET
    data_selector: float
- name: sent
  endpoint:
    path: /sent
    method: GET
    data_selector: '[Span]'
- name: sents
  endpoint:
    path: /sents
    method: GET
    data_selector: Iterable[[Span]]
- name: SpanGroup
  endpoint:
    path: /api/spangroup
    method: GET
    data_selector: records
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
    data_selector: records
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: records
    params: {}
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: records
    params: {}
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: token
- name: attribute_ruler
  endpoint:
    path: /api/attributeruler
    method: GET
    data_selector: patterns
    params: {}
- name: from_array
  endpoint:
    path: /from_array
    method: POST
    data_selector: attributes
- name: from_docs
  endpoint:
    path: /from_docs
    method: POST
    data_selector: docs
- name: to_disk
  endpoint:
    path: /to_disk
    method: POST
    data_selector: path
- name: from_disk
  endpoint:
    path: /from_disk
    method: POST
    data_selector: path
- name: to_bytes
  endpoint:
    path: /to_bytes
    method: POST
- name: from_bytes
  endpoint:
    path: /from_bytes
    method: POST
    data_selector: data
- name: to_json
  endpoint:
    path: /to_json
    method: POST
- name: from_json
  endpoint:
    path: /from_json
    method: POST
    data_selector: doc_json
- name: coreference_resolver
  endpoint:
    path: /api/coref
    method: GET
    data_selector: Doc.spans
    params: {}
- name: DocBin
  endpoint:
    path: /api/docbin
    method: GET
    data_selector: records
    params: {}
- name: curated_transformer
  endpoint:
    path: /api/curated_transformer
    method: GET
    data_selector: outputs
    params: {}
- name: Example
  endpoint:
    path: /api/example
    method: GET
    data_selector: records
    params: {}
- name: Alignment
  endpoint:
    path: /api/alignment
    method: GET
    data_selector: records
    params: {}
- name: curated_transformer
  endpoint:
    path: /api/curated-transformer
    method: GET
    data_selector: output
    params: {}
- name: last_hidden_layer_output
  endpoint:
    path: /last_hidden_layer_output
    method: GET
    data_selector: output
- name: all_hidden_layer_states
  endpoint:
    path: /all_hidden_layer_states
    method: GET
    data_selector: outputs
- name: num_outputs
  endpoint:
    path: /num_outputs
    method: GET
    data_selector: num_outputs
- name: span_getters
  endpoint:
    path: /span_getters
    method: GET
    data_selector: spans
- name: WithStridedSpans
  endpoint:
    path: /with_strided_spans
    method: GET
    data_selector: strided_spans
- name: parser
  endpoint:
    path: /api/dependencyparser
    method: GET
    data_selector: ''
    params: {}
- name: update
  endpoint:
    path: /api/language/update
    method: POST
    data_selector: losses
- name: rehearse
  endpoint:
    path: /api/language/rehearse
    method: POST
    data_selector: losses
- name: evaluate
  endpoint:
    path: /api/language/evaluate
    method: POST
    data_selector: scores
- name: use_params
  endpoint:
    path: /api/language/use_params
    method: POST
    data_selector: params
- name: add_pipe
  endpoint:
    path: /api/language/add_pipe
    method: POST
    data_selector: pipeline_component
- name: create_pipe
  endpoint:
    path: /api/language/create_pipe
    method: POST
    data_selector: pipeline_component
- name: get_pipe
  endpoint:
    path: /api/language/get_pipe
    method: GET
    data_selector: pipeline_component
- name: remove_pipe
  endpoint:
    path: /api/language/remove_pipe
    method: DELETE
    data_selector: removed_component
- name: disable_pipe
  endpoint:
    path: /api/language/disable_pipe
    method: POST
    data_selector: disabled_component
- name: enable_pipe
  endpoint:
    path: /api/language/enable_pipe
    method: POST
    data_selector: enabled_component
- name: select_pipes
  endpoint:
    path: /api/language/select_pipes
    method: POST
    data_selector: selected_pipes
- name: get_factory_meta
  endpoint:
    path: /api/language/get_factory_meta
    method: GET
    data_selector: factory_meta
- name: get_pipe_meta
  endpoint:
    path: /api/language/get_pipe_meta
    method: GET
    data_selector: pipe_meta
- name: analyze_pipes
  endpoint:
    path: /api/language/analyze_pipes
    method: GET
    data_selector: analysis
- name: labels
  endpoint:
    path: /api/dependencyparser/labels
    method: GET
    data_selector: labels
- name: label_data
  endpoint:
    path: /api/dependencyparser/label_data
    method: GET
    data_selector: label_data
- name: edit_tree_lemmatizer
  endpoint:
    path: /api/edittreelemmatizer
    method: GET
    data_selector: Token.lemma
- name: Lexeme
  endpoint:
    path: /api/lexeme
    method: GET
- name: entity_linker
  endpoint:
    path: /api/entity_linker
    method: GET
    data_selector: records
    params: {}
- name: Span
  endpoint:
    path: /api/span
    method: GET
    data_selector: records
- name: serialization_fields
  endpoint:
    path: /api/entitylinker#serialization-fields
    method: GET
    data_selector: fields
    params: {}
- name: Span.vector
  endpoint:
    path: /api/span/vector
    method: GET
    data_selector: returns
- name: Span.vector_norm
  endpoint:
    path: /api/span/vector_norm
    method: GET
    data_selector: returns
- name: Span.sent
  endpoint:
    path: /api/span/sent
    method: GET
    data_selector: returns
- name: Span.sents
  endpoint:
    path: /api/span/sents
    method: GET
    data_selector: returns
- name: EntityRecognizer
  endpoint:
    path: /api/entityrecognizer
    method: GET
    data_selector: records
- name: SpanGroup
  endpoint:
    path: /api/spangroup
    method: GET
- name: entity_ruler
  endpoint:
    path: /api/entityruler
    method: GET
    data_selector: records
- name: Token
  endpoint:
    path: /api/token
    method: GET
    data_selector: records
    params: {}
- name: patterns
  endpoint:
    path: /api/entityruler/patterns
    method: GET
    data_selector: patterns
- name: attributes
  endpoint:
    path: /api/entityruler/attributes
    method: GET
    data_selector: attributes
- name: attribute_ruler
  endpoint:
    path: /api/attributeruler
    method: GET
- name: attribute_ruler
  endpoint:
    path: /api/attributeruler
    method: GET
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
- name: coref
  endpoint:
    path: /api/coref
    method: POST
    data_selector: clusters
- name: curated_transformer
  endpoint:
    path: /api/curated_transformers
    method: GET
    data_selector: model_outputs
    params: {}
- name: lemmatizer.pipe
  endpoint:
    path: /api/lemmatizer/pipe
    method: GET
    data_selector: documents
- name: lemmatizer.initialize
  endpoint:
    path: /api/lemmatizer/initialize
    method: GET
    data_selector: initialization
- name: lemmatizer.lookup_lemmatize
  endpoint:
    path: /api/lemmatizer/lookup_lemmatize
    method: GET
    data_selector: lemmas
- name: lemmatizer.rule_lemmatize
  endpoint:
    path: /api/lemmatizer/rule_lemmatize
    method: GET
    data_selector: lemmas
- name: lemmatizer.is_base_form
  endpoint:
    path: /api/lemmatizer/is_base_form
    method: GET
    data_selector: base_form_check
- name: lemmatizer.get_lookups_config
  endpoint:
    path: /api/lemmatizer/get_lookups_config
    method: GET
    data_selector: lookups
- name: lemmatizer.to_disk
  endpoint:
    path: /api/lemmatizer/to_disk
    method: GET
    data_selector: disk
- name: lemmatizer.from_disk
  endpoint:
    path: /api/lemmatizer/from_disk
    method: GET
    data_selector: disk
- name: lemmatizer.to_bytes
  endpoint:
    path: /api/lemmatizer/to_bytes
    method: GET
    data_selector: bytes
- name: lemmatizer.from_bytes
  endpoint:
    path: /api/lemmatizer/from_bytes
    method: GET
    data_selector: bytes
- name: last_hidden_layer_state
  endpoint:
    path: /doctransformeroutput/lasthiddenlayerstate
    method: GET
    data_selector: returns
- name: all_hidden_layer_states
  endpoint:
    path: /doctransformeroutput/allhiddenlayerstates
    method: GET
    data_selector: returns
- name: num_outputs
  endpoint:
    path: /doctransformeroutput/numoutputs
    method: GET
    data_selector: returns
- name: DependencyParser
  endpoint:
    path: /api/dependencyparser
    method: GET
    data_selector: parser
    params: {}
- name: SentenceRecognizer
  endpoint:
    path: /api/sentencerecognizer
    method: GET
    data_selector: records
- name: assigned_attributes
  endpoint:
    path: /api/edittreelemmatizer/assigned_attributes
    method: GET
    data_selector: records
- name: config
  endpoint:
    path: /api/edittreelemmatizer/config
    method: GET
    data_selector: records
- name: create_optimizer
  endpoint:
    path: /create_optimizer
    method: GET
- name: use_params
  endpoint:
    path: /use_params
    method: GET
- name: to_disk
  endpoint:
    path: /to_disk
    method: GET
- name: from_disk
  endpoint:
    path: /from_disk
    method: GET
- name: to_bytes
  endpoint:
    path: /to_bytes
    method: GET
- name: from_bytes
  endpoint:
    path: /from_bytes
    method: GET
- name: entity_linker
  endpoint:
    path: /api/entitylinker
    method: GET
- name: sentencizer
  endpoint:
    path: /api/sentencizer
    method: GET
- name: serialization_fields
  endpoint:
    path: /api/entitylinker#serialization-fields
    method: GET
    data_selector: fields
    params: {}
- name: SpanCategorizer
  endpoint:
    path: /api/spancategorizer
    method: GET
    data_selector: records
    params: {}
- name: EntityRecognizer
  endpoint:
    path: /api/entityrecognizer
    method: GET
    data_selector: records
- name: spancat
  endpoint:
    path: /api/spancat
    method: GET
    data_selector: records
    params: {}
- name: entity_ruler
  endpoint:
    path: /api/entityruler
    method: GET
    data_selector: data
    params: {}
- name: span_finder
  endpoint:
    path: /api/spanfinder
    method: GET
    data_selector: spans
- name: patterns
  endpoint:
    path: /path/to/patterns.jsonl
    method: GET
    data_selector: patterns
    params: {}
- name: entity_ruler
  endpoint:
    path: /path/to/entity_ruler
    method: GET
    data_selector: config
    params: {}
- name: span_resolver
  endpoint:
    path: /api/span-resolver
    method: GET
    data_selector: predictions
    params: {}
- name: patterns
  endpoint:
    path: /api/entityruler/patterns
    method: GET
    data_selector: patterns
- name: span_ruler
  endpoint:
    path: /api/spanruler
    method: GET
    data_selector: matches
    params: {}
- name: lemmatizer
  endpoint:
    path: /api/lemmatizer
    method: GET
    data_selector: records
- name: patterns
  endpoint:
    path: /api/spanruler/patterns
    method: GET
    data_selector: patterns
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: records
- name: morphologizer
  endpoint:
    path: /api/morphologizer
    method: GET
    data_selector: records
- name: labels
  endpoint:
    path: /api/morphologizer#labels
    method: GET
    data_selector: labels
    params: {}
- name: label_data
  endpoint:
    path: /api/morphologizer#label_data
    method: GET
    data_selector: label_data
    params: {}
- name: serialization_fields
  endpoint:
    path: /api/morphologizer#serialization-fields
    method: GET
    data_selector: serialization_fields
    params: {}
- name: senter
  endpoint:
    path: /api/sentencerecognizer
    method: GET
- name: predict
  endpoint:
    path: /api/tagger#predict
    method: POST
- name: set_annotations
  endpoint:
    path: /api/tagger#set_annotations
    method: POST
- name: update
  endpoint:
    path: /api/tagger#update
    method: POST
- name: rehearse
  endpoint:
    path: /api/tagger#rehearse
    method: POST
- name: get_loss
  endpoint:
    path: /api/tagger#get_loss
    method: POST
- name: create_optimizer
  endpoint:
    path: /api/tagger#create_optimizer
    method: POST
- name: use_params
  endpoint:
    path: /api/tagger#use_params
    method: POST
- name: add_label
  endpoint:
    path: /api/tagger#add_label
    method: POST
- name: to_disk
  endpoint:
    path: /api/tagger#to_disk
    method: POST
- name: from_disk
  endpoint:
    path: /api/tagger#from_disk
    method: POST
- name: to_bytes
  endpoint:
    path: /api/tagger#to_bytes
    method: POST
- name: from_bytes
  endpoint:
    path: /api/tagger#from_bytes
    method: POST
- name: labels
  endpoint:
    path: /api/tagger#labels
    method: GET
- name: label_data
  endpoint:
    path: /api/tagger#label_data
    method: GET
- name: textcat
  endpoint:
    path: /api/textcat
    method: GET
    data_selector: records
- name: textcat_multilabel
  endpoint:
    path: /api/textcat_multilabel
    method: GET
    data_selector: records
- name: create_optimizer
  endpoint:
    path: /create_optimizer
    method: POST
- name: use_params
  endpoint:
    path: /use_params
    method: POST
- name: to_disk
  endpoint:
    path: /to_disk
    method: POST
- name: from_disk
  endpoint:
    path: /from_disk
    method: POST
- name: to_bytes
  endpoint:
    path: /to_bytes
    method: POST
- name: from_bytes
  endpoint:
    path: /from_bytes
    method: POST
- name: labels
  endpoint:
    data_selector: Tuple[str, ]
- name: label_data
  endpoint:
    data_selector: Tuple[str, ]
- name: serialization_fields
  endpoint:
    data_selector: string names of serialization fields
- name: Sentencizer
  endpoint:
    path: /api/sentencizer
    method: GET
- name: tok2vec
  endpoint:
    path: /api/tok2vec
    method: GET
    data_selector: model
- name: Tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: tokenizer
- name: spancat
  endpoint:
    path: /api/spancategorizer
    method: POST
    data_selector: records
- name: spancat_singlelabel
  endpoint:
    path: /api/spancategorizer/singlelabel
    method: POST
    data_selector: records
- name: TrainablePipe
  endpoint:
    path: /api/pipe
    method: GET
    data_selector: TrainablePipe
    params: {}
- name: spancat_suggester
  endpoint:
    path: /components/spancat/suggester
    method: GET
- name: span_finder
  endpoint:
    path: /api/spanfinder
    method: GET
    data_selector: records
- name: transformer
  endpoint:
    path: /api/transformer
    method: GET
    data_selector: Doc._.trf_data
    params: {}
- name: transformer
  endpoint:
    path: /api/transformer
    method: GET
    data_selector: transformer_data
- name: span_resolver
  endpoint:
    path: /api/span-resolver
    method: GET
    data_selector: predictions
    params: {}
- name: model_output
  endpoint:
    path: /transformers/model_output
    method: GET
    data_selector: model_output
    params: {}
- name: tensors
  endpoint:
    path: /transformers/tensors
    method: GET
    data_selector: tensors
    params: {}
- name: align
  endpoint:
    path: /transformers/align
    method: GET
    data_selector: align
    params: {}
- name: width
  endpoint:
    path: /transformers/width
    method: GET
    data_selector: width
    params: {}
- name: span_ruler
  endpoint:
    path: /api/spanruler
    method: GET
    data_selector: records
- name: FullTransformerBatch
  endpoint:
    path: /api/transformer/full_transformer_batch
    method: GET
    data_selector: model_output
    params: {}
- name: TransformerData
  endpoint:
    path: /api/transformer/transformer_data
    method: GET
    data_selector: tensors
    params: {}
- name: patterns
  endpoint:
    path: /api/spanruler/patterns
    method: GET
    data_selector: patterns
    params: {}
- name: tagger
  endpoint:
    path: /api/tagger
    method: GET
    data_selector: model
    params: {}
- name: merge_noun_chunks
  endpoint:
    path: /api/pipeline-functions/merge_noun_chunks
    method: GET
    data_selector: Doc
    params: {}
- name: merge_entities
  endpoint:
    path: /api/pipeline-functions/merge_entities
    method: GET
    data_selector: Doc
    params: {}
- name: merge_subtokens
  endpoint:
    path: /api/pipeline-functions/merge_subtokens
    method: GET
    data_selector: Doc
    params: {}
- name: token_splitter
  endpoint:
    path: /api/pipeline-functions/token_splitter
    method: GET
    data_selector: Doc
    params: {}
- name: doc_cleaner
  endpoint:
    path: /api/pipeline-functions/doc_cleaner
    method: GET
    data_selector: Doc
    params: {}
- name: span_cleaner
  endpoint:
    path: /api/pipeline-functions/span_cleaner
    method: GET
    data_selector: Doc
    params: {}
- name: matcher
  endpoint:
    path: /api/matcher
    method: GET
    data_selector: patterns
    params: {}
- name: predict
  endpoint:
    path: /api/tagger/predict
    method: POST
    data_selector: scores
- name: set_annotations
  endpoint:
    path: /api/tagger/set_annotations
    method: POST
    data_selector: result
- name: update
  endpoint:
    path: /api/tagger/update
    method: POST
    data_selector: losses
- name: rehearse
  endpoint:
    path: /api/tagger/rehearse
    method: POST
    data_selector: losses
- name: get_loss
  endpoint:
    path: /api/tagger/get_loss
    method: POST
    data_selector: result
- name: create_optimizer
  endpoint:
    path: /api/tagger/create_optimizer
    method: POST
    data_selector: optimizer
- name: use_params
  endpoint:
    path: /api/tagger/use_params
    method: POST
    data_selector: result
- name: add_label
  endpoint:
    path: /api/tagger/add_label
    method: POST
    data_selector: result
- name: to_disk
  endpoint:
    path: /api/tagger/to_disk
    method: POST
    data_selector: result
- name: from_disk
  endpoint:
    path: /api/tagger/from_disk
    method: POST
    data_selector: result
- name: to_bytes
  endpoint:
    path: /api/tagger/to_bytes
    method: POST
    data_selector: bytes_data
- name: from_bytes
  endpoint:
    path: /api/tagger/from_bytes
    method: POST
    data_selector: tagger
- name: labels
  endpoint:
    path: /api/tagger/labels
    method: GET
    data_selector: labels
- name: label_data
  endpoint:
    path: /api/tagger/label_data
    method: GET
    data_selector: label_data
- name: corpus
  endpoint:
    path: /api/corpus
    method: GET
    data_selector: records
    params: {}
- name: jsonl_corpus
  endpoint:
    path: /api/jsonlcorpus
    method: GET
    data_selector: records
    params: {}
- name: textcat
  endpoint:
    path: /api/textcat
    method: GET
    data_selector: predictions
- name: textcat_multilabel
  endpoint:
    path: /api/textcat_multilabel
    method: GET
    data_selector: predictions
- name: tok2vec
  endpoint:
    path: /api/tok2vec
    method: GET
    data_selector: records
- name: InMemoryLookupKB
  endpoint:
    path: /api/inmemorylookupkb
    method: GET
    data_selector: records
    params: {}
- name: tokenizer
  endpoint:
    path: /api/tokenizer
    method: GET
    data_selector: tokenization_data
    params: {}
- name: KnowledgeBase
  endpoint:
    path: /api/kb
    method: GET
    data_selector: records
- name: TrainablePipe
  endpoint:
    path: /api/pipe
    method: GET
    data_selector: records
- name: Lookups
  endpoint:
    path: /api/lookups
    method: GET
    data_selector: tables
    params: {}
- name: Morphology
  endpoint:
    path: /api/morphology
    method: GET
    data_selector: records
- name: MorphAnalysis
  endpoint:
    path: /api/morphology#morphanalysis
    method: GET
    data_selector: records
- name: transformer
  endpoint:
    path: /api/transformer
    method: GET
    data_selector: records
- name: Scorer
  endpoint:
    path: /api/scorer
    method: GET
    data_selector: scores
- name: transformer
  endpoint:
    path: /api/transformer
    method: GET
    data_selector: records
- name: StringStore
  endpoint:
    path: /api/stringstore
    method: GET
    data_selector: records
    params: {}
- name: Vectors
  endpoint:
    path: /api/vectors
    method: GET
    data_selector: data
- name: model_output
  endpoint:
    path: /transformers/model_output
    method: GET
    data_selector: model_output
    params: {}
- name: tensors
  endpoint:
    path: /transformers/tensors
    method: GET
    data_selector: tensors
    params: {}
- name: align
  endpoint:
    path: /transformers/align
    method: GET
    data_selector: align
    params: {}
- name: width
  endpoint:
    path: /transformers/width
    method: GET
    data_selector: width
    params: {}
- name: FullTransformerBatch
  endpoint:
    path: /api/transformer#fulltransformerbatch
    method: GET
    data_selector: records
    params: {}
- name: TransformerData
  endpoint:
    path: /api/transformer#transformerdata
    method: GET
    data_selector: records
    params: {}
- name: Vocab
  endpoint:
    path: /api/vocab
    method: GET
    data_selector: records
- name: merge_noun_chunks
  endpoint:
    path: /api/pipeline-functions/merge_noun_chunks
    method: GET
    data_selector: Doc
    params: {}
- name: merge_entities
  endpoint:
    path: /api/pipeline-functions/merge_entities
    method: GET
    data_selector: Doc
    params: {}
- name: merge_subtokens
  endpoint:
    path: /api/pipeline-functions/merge_subtokens
    method: GET
    data_selector: Doc
    params: {}
- name: token_splitter
  endpoint:
    path: /api/pipeline-functions/token_splitter
    method: GET
    data_selector: Doc
    params: {}
- name: doc_cleaner
  endpoint:
    path: /api/pipeline-functions/doc_cleaner
    method: GET
    data_selector: Doc
    params: {}
- name: span_cleaner
  endpoint:
    path: /api/pipeline-functions/span_cleaner
    method: GET
    data_selector: Doc
    params: {}
- name: DependencyMatcher
  endpoint:
    path: /api/dependencymatcher
    method: GET
    data_selector: matches
    params: {}
- name: TextCatEnsemble
  endpoint:
    path: /api/architectures#TextCatEnsemble
    method: GET
    data_selector: architecture
    params: {}
- name: TextCatCNN
  endpoint:
    path: /api/architectures#TextCatCNN
    method: GET
    data_selector: architecture
    params: {}
- name: TextCatBOW
  endpoint:
    path: /api/architectures#TextCatBOW
    method: GET
    data_selector: architecture
    params: {}
- name: TransitionBasedParser
  endpoint:
    path: /api/architectures#TransitionBasedParser
    method: GET
    data_selector: architecture
    params: {}
- name: StaticVectors
  endpoint:
    path: /api/architectures#StaticVectors
    method: GET
    data_selector: architecture
    params: {}
- name: ConsoleLogger
  endpoint:
    path: /api/loggers#ConsoleLogger
    method: GET
    data_selector: logger
    params: {}
- name: DependencyMatcher.remove
  endpoint:
    path: /api/dependencymatcher.mdx/remove
    method: GET
    data_selector: data
    params: {}
- name: PhraseMatcher
  endpoint:
    path: /api/phrasematcher
    method: GET
    data_selector: matches
- name: BaseVectors
  endpoint:
    path: /api/basevectors
    method: GET
    data_selector: records
- name: corpus
  endpoint:
    path: /api/corpus
    method: GET
    data_selector: records
    params: {}
- name: jsonl_corpus
  endpoint:
    path: /api/jsonlcorpus
    method: GET
    data_selector: records
    params: {}
- name: plain_text_documents
  endpoint:
    path: ./data/docs.txt
    method: GET
    data_selector: documents
- name: InMemoryLookupKB
  endpoint:
    path: /api/inmemorylookupkb
    method: GET
    data_selector: records
    params: {}
- name: KnowledgeBase
  endpoint:
    path: /api/kb
    method: GET
    data_selector: entities
    params: {}
- name: lookups
  endpoint:
    path: /api/lookups
    method: GET
    data_selector: tables
- name: Morphology
  endpoint:
    path: /api/morphology
    method: GET
    data_selector: features
    params: {}
- name: MorphAnalysis
  endpoint:
    path: /api/morphology#morphanalysis
    method: GET
    data_selector: analysis
    params: {}
- name: Scorer
  endpoint:
    path: /api/scorer
    method: GET
    data_selector: scores
    params: {}
- name: StringStore
  endpoint:
    path: /api/stringstore
    method: GET
    data_selector: records
- name: Vectors
  endpoint:
    path: /api/vectors
    method: GET
    data_selector: data
    params: {}
- name: Vocab
  endpoint:
    path: /api/vocab
    method: GET
    data_selector: records
- name: Doc
  endpoint:
    path: /api/doc
    method: GET
- name: Token
  endpoint:
    path: /api/token
    method: GET
- name: Span
  endpoint:
    path: /api/span
    method: GET
- name: Lexeme
  endpoint:
    path: /api/lexeme
    method: GET
- name: Vocab
  endpoint:
    path: /api/vocab
    method: GET
- name: StringStore
  endpoint:
    path: /api/stringstore
    method: GET
- name: Tok2Vec
  endpoint:
    path: /api/architectures/Tok2Vec
    method: GET
    data_selector: model
- name: MaxoutWindowEncoder
  endpoint:
    path: /api/architectures/MaxoutWindowEncoder
    method: GET
    data_selector: model
- name: MishWindowEncoder
  endpoint:
    path: /api/architectures/MishWindowEncoder
    method: GET
    data_selector: model
- name: TextCatEnsemble
  endpoint:
    path: /api/architectures#TextCatEnsemble
    method: GET
    data_selector: records
    params: {}
- name: TextCatCNN
  endpoint:
    path: /api/architectures#TextCatCNN
    method: GET
    data_selector: records
    params: {}
- name: TextCatBOW
  endpoint:
    path: /api/architectures#TextCatBOW
    method: GET
    data_selector: records
    params: {}
- name: TransitionBasedParser
  endpoint:
    path: /api/architectures#TransitionBasedParser
    method: GET
    data_selector: records
    params: {}
- name: StaticVectors
  endpoint:
    path: /api/architectures#StaticVectors
    method: GET
    data_selector: records
    params: {}
- name: ConsoleLogger
  endpoint:
    path: /api/legacy#ConsoleLogger
    method: GET
    data_selector: records
    params: {}
notes:
- Integrates LLMs into structured NLP pipelines
- Fast prototyping and prompting, no training data required
- Supports zero- and few-shot prompting.
- Requires pre-extracted entities for relation extraction.
- Supports both zero-shot and few-shot prompting.
- spaCy is designed to help you do real work  to build real products, or gather real
  insights.
- It's easy to install, and its API is simple and productive.
- Integrates Large Language Models into structured NLP pipelines
- Fast prototyping and prompting, turning unstructured responses into robust outputs
- spaCy is compatible with 64-bit CPython 3.7+ and runs on Unix/Linux, macOS/OS X
  and Windows.
- It is recommended to install packages in a virtual environment.
- Supports both zero-shot and few-shot prompting
- 'No compatible model found: No compatible package found for [lang] (spaCy vX.X.X).'
- 'Import error: No module named spacy: Import Error: No module named spacy'
- 'Import error: No module named [name]: ImportError: No module named ''en_core_web_sm'''
- 'Command not found: spacy: command not found: spacy'
- '''module'' object has no attribute ''load'': AttributeError: ''module'' object
  has no attribute ''load'''
- 'NER model doesn''t recognize other entities anymore after training: If your training
  data only contained new entities and you didnt mix in any examples the model previously
  recognized, it can cause the model to forget what it had previously learned.'
- 'Unhashable type: ''list'': TypeError: unhashable type: ''list'''
- Pipelines follow the naming convention of [lang]_[name].
- Different model versions indicate compatibility with spaCy versions.
- The processing pipeline can include multiple components.
- If upgrading to spaCy v3.x, download new pipeline packages and retrain any custom
  pipelines.
- Ensure that the proper API keys are set as environment variables.
- PKUSeg model can be loaded from a file during initialization
- Default word segmenter switched from Jieba to character segmentation in v3.0
- When using pip it is generally recommended to install packages in a virtual environment
  to avoid modifying system state.
- Uses OAuth2 with refresh token  requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- In v3.0, the default word segmenter has switched from Jieba to character segmentation.
- Trained pipelines are designed to be efficient and configurable.
- Multiple components can share a common token-to-vector model.
- The central data structures in spaCy are the Language class, Vocab, and Doc object.
- The default MeCab-based Korean tokenizer requires mecab-ko, mecab-ko-dic, and natto-py.
- For some Korean datasets and tasks, the rule-based tokenizer is better-suited than
  MeCab.
- As of spaCy v3.0, shortcut links like 'en' that create symlinks are deprecated.
- If lemmatization rules are available for your language, make sure to install spaCy
  with the lookups option.
- Supports multiple languages including Chinese and multi-language.
- Requires installation of specific packages for certain languages.
- If youre upgrading to spaCy v3.x, you need to download the new pipeline packages.
- If youve trained your own pipelines, you need to retrain them after updating spaCy.
- Uses Python packaging for handling spaCy pipelines.
- Shortcut links like 'en' are deprecated in v3.0.
- The default word segmenter has switched from Jieba to character segmentation.
- pkuseg segmenter depends on a model that can be loaded from a file.
- Uses pkuseg model for Chinese language processing
- This release drops support for Python 3.6.
- Uses PKUSeg for better segmentation for Chinese OntoNotes.
- spaCy v3.6 adds the new SpanFinder component to the core spaCy library and new trained
  pipelines for Slovenian.
- The models provided by pkuseg include data restricted to research use.
- For research use, pkuseg provides models for several different domains.
- New features and improvements introduced in v3.5
- Includes fuzzy matching and entity linking generalization
- As of spaCy v3.0, shortcut links like 'en' that create potentially brittle symlinks
  in your spaCy installation are deprecated.
- Trained pipelines can be installed from a download URL or a local directory.
- If upgrading to spaCy v3.x, new pipeline packages need to be downloaded.
- Uses SudachiPy with split mode A by default
- For Japanese, split mode can be configured to A, B or C
- For Korean, trained pipelines use the rule-based tokenizer
- Tokenization is non-destructive, allowing reconstruction of the original input.
- Keep in mind that your models results may be less accurate if the tokenization
  during training differs from the tokenization at runtime.
- To download a trained pipeline directly using pip, point pip install to the URL
  or local path of the wheel file or archive.
- You can also get URLs directly from the command line by using spacy info with the
  --url flag.
- The SentenceRecognizer is a simple statistical component that only provides sentence
  boundaries.
- spaCy is a free, open-source library for advanced Natural Language Processing (NLP)
  in Python.
- spaCy is designed specifically for production use and helps you build applications
  that process and understand large volumes of text.
- Uses larger pipeline packages for better vector representation
- Uses dependency parse to determine sentence boundaries.
- Training data should always be representative of the data we want to process.
- If you want to train a model from scratch, you usually need at least a few hundred
  examples for both training and evaluation.
- For complex tasks, its usually better to train a statistical entity recognition
  model.
- Rule-based systems are a good choice if theres a finite number of examples.
- spaCy v3.6 adds the new SpanFinder component to the core spaCy library.
- New trained pipelines for Slovenian.
- spaCy v3.5 introduces three new CLI commands, apply, benchmark and find-threshold,
  adds fuzzy matching, provides improvements to our entity linking functionality,
  and includes a range of language updates and bug fixes.
- Rule-based systems are a good choice if theres a more or less finite number of
  examples.
- 'Processing raw text intelligently is difficult: most words are rare.'
- spaCy can parse and tag a given Doc after tokenization.
- Uses linguistic knowledge to process raw text.
- Requires setup of connected app in spaCy
- Some linguistic features may vary across languages
- Disabling the parser will make spaCy load and run much faster.
- Uses custom pipeline components for entity recognition.
- Components in the pipeline can be swapped or removed without affecting others.
- The tokenizer is special and isn't part of the regular pipeline.
- The recall for the senter is typically slightly lower than for the parser.
- The pipeline used by the trained pipelines typically includes a tagger, a lemmatizer,
  a parser and an entity recognizer.
- Components can be swapped or removed without affecting others.
- Uses a dependency parser for sentence segmentation.
- Trained pipelines are required for accurate predictions.
- The processing pipeline typically includes components like tagger, parser, and entity
  recognizer.
- Statistical models require training data.
- Fuzzy matching allows for typos and alternate spellings.
- As of v3.0, the disable keyword argument specifies components to load but disable,
  instead of components to not load at all.
- The processing pipeline typically includes a tagger, a lemmatizer, a parser and
  an entity recognizer.
- You can use the nlp.select_pipes context manager to temporarily disable certain
  components for a given block.
- Rule-based matcher allows for complex token patterns.
- Pipeline components can be disabled or excluded to improve loading and inference
  speed.
- As of v3.0, the `disable` keyword argument specifies components to load but disable,
  instead of components to not load at all.
- Uses rule-based matcher for token patterns.
- The matcher can validate patterns against a JSON schema with the option validate=True.
- Pipeline components that are independent can also be reused across pipelines.
- spaCy supports transfer learning workflows that can improve pipeline efficiency.
- Transformers require a GPU to run effectively.
- Pipelines with word vectors can also use the vectors as features for the statistical
  models, which can improve the accuracy of your components.
- Pretraining uses the same config file as the regular training, which helps keep
  the settings and hyperparameters consistent.
- The pipeline used by trained pipelines typically includes a tagger, a lemmatizer,
  a parser, and an entity recognizer.
- spaCy v3.0 introduces a config.cfg, which includes more detailed settings for the
  pipeline, its components, and the training process.
- This package is still experimental and it is possible that changes made to the interface
  will be breaking in minor version updates.
- The pipeline used by the trained pipelines typically include a tagger, a lemmatizer,
  a parser and an entity recognizer.
- Evaluation data is necessary to check how well the model is generalizing.
- The processing pipeline includes components like tokenizer, tagger, parser, and
  entity recognizer.
- Components can be disabled or excluded to improve performance.
- Training data should be representative of the data we want to process.
- Evaluation data is needed to know how well the model is generalizing.
- Training is an iterative process.
- Model performance should be evaluated on unseen data.
- Uses OAuth2 for authentication.
- Initialization settings are only loaded and used when nlp.initialize is called.
- Pipeline specifies components and their settings in the config
- Order of components may affect processing
- Training requires a configuration file with all settings and hyperparameters.
- Component factories are callables that take settings and return a pipeline component
  function.
- Uses variable interpolation for configuration settings.
- Training data must be prepared in formats compatible with spaCy.
- Extensions can claim their own ._ namespace and exist as standalone packages.
- spaCy supports a number of transfer and multi-task learning workflows that can often
  help improve your pipelines efficiency or accuracy.
- You cannot add word vector features once the model has already been trained.
- Pretraining uses the same config file as the regular training.
- Pretraining requires relatively little training data (fewer than 5,000 sentences).
- Uses spaCy for model training and data management
- Training data examples should be representative of the data we want to process.
- Evaluation data is needed to assess model performance.
- If the development data has raw text, some of the gold-standard entities might not
  align to the predicted tokenization. These tokenization errors are excluded from
  the NER evaluation.
- Interacting with LLMs is costly; caching implemented to avoid reprocessing.
- If the train corpus is streamed, the train corpus should be streamed and batched,
  not shuffled.
- When adding data augmentation, keep in mind that it typically only makes sense to
  apply it to the training corpus, not the development data.
- spaCy components can be powered by custom neural networks.
- Model architectures can be swapped by updating the config.
- Training configs should always be complete and without hidden defaults, to keep
  your experiments reproducible.
- Run this example use-case by using our project template. It includes all the code
  to create the ML model and the pipeline component from scratch.
- Use a custom extension attribute `doc._.rel` to store relation data.
- Example use-case includes all code to create the ML model and the pipeline component
  from scratch.
- spaCy projects let you manage and share end-to-end spaCy workflows for different
  use cases and domains.
- Assets can be marked as extra - by default, these assets are not downloaded.
- Be aware that non-frozen annotating components with statistical models will run
  twice on each batch, once to update the model and once to apply the now-updated
  model to the predicted docs.
- Training configs should always be complete and without hidden defaults.
- Training data for NLP projects comes in many different formats.
- When converting training data for use in spaCy, the main thing is to create Doc
  objects just like the results you want as output from the pipeline.
- Loss represents the amount of work left for the optimizer.
- Precision indicates the percentage of predicted annotations that were correct.
- Recall shows the percentage of reference annotations recovered.
- F-Score is the harmonic mean of precision and recall.
- UAS / LAS refers to the percentage of correct arcs for the dependency parser.
- Speed measures prediction speed in words per second (WPS).
- The system supports multiple outputs for the same file and the same context.
- If the development data has raw text, some of the gold-standard entities might not
  align to the predicted tokenization.
- These tokenization errors are excluded from the NER evaluation.
- If your tokenization makes it impossible for the model to predict 50% of your entities,
  your NER F-score might still look good.
- Uses Pickle for serialization.
- Custom components must be imported before loading the pipeline.
- Train corpus can be streamed if too large to load into memory.
- Dev corpus should always be finite and fit in memory during evaluation.
- Exiting the memory-zone invalidates all Doc, Token, Span and Lexeme objects that
  were created within it.
- The memory zone solves the problem of keeping track of which Doc objects are referring
  to data in the shared Vocab cache.
- Model architecture functions accept sublayers as arguments.
- displaCy can visualize dependencies and entities in a text.
- The visualizations can be rendered in Jupyter notebooks.
- Run this example use-case by using our project template.
- The project applies the relation extraction component to identify biomolecular interactions.
- Assets can be downloaded using HTTP, HTTPS, FTP, SSH, and cloud storage protocols
  like GCS and S3.
- The main data format used in spaCy v3.0 is a binary format created by serializing
  a DocBin.
- spaCys CLI provides a range of helpful commands for downloading and training pipelines,
  converting data and debugging your config, data and installation.
- spaCy supports built-in serialization methods and the Pickle protocol.
- The command will create all objects in the tree and validate them.
- Managing Memory for persistent services
- spaCy maintains a few internal caches that improve speed, but cause memory to increase
  slightly over time.
- displaCy supports rendering both Doc and Span objects, as well as lists of Doc or
  Span.
- Rendering multiple documents on one page can confuse, use titles to clarify.
- The 'train' command expects a config file containing all settings and hyperparameters.
- Mixed-precision support is currently an experimental feature.
- The training CLI exposes a `train` helper function.
- Requires installation of spacy-huggingface-hub package and login via huggingface-cli
- 'By default, displaCy links to # for entities without a kb_id set on their span.'
- File readers expect a local path and should only be used in config blocks that are
  not executed at runtime.
- Uses Thincs configuration system for data validation
- Utility functions are mostly intended for internal use within spaCy, their behavior
  may change with future releases.
- Uses spaCy models and pipelines for various NLP tasks.
- The `download` command is not recommended for automated processes.
- The `init` command includes helpful commands for initializing training config files.
- The converter can be specified on the command line, or chosen based on the file
  extension of the input file.
- Some config validation errors are blocking and will prevent the rest of the config
  from being resolved.
- The `train` command expects a single `config.cfg` file containing all settings for
  the pipeline, training process and hyperparameters.
- Supports various augmentation techniques
- Includes functionality for batching and training data management
- Commands include train, pretrain, evaluate, benchmark, apply, find-threshold, assemble,
  package, and project.
- Requires spacy-huggingface-hub package installed.
- Uses spaCy for NLP tasks
- displaCy provides several data structures for visualizing NLP outputs.
- Default values are used for options not specified.
- The serialization format is gzipped msgpack, which is faster and produces smaller
  data sizes than pickle.
- Uses local paths for file readers
- The file readers should only be used in config blocks that are not executed at runtime
- Utility functions are mostly intended for internal use within spaCy
- Behavior may change with future releases
- Utility functions are mostly intended for internal use within spaCy.
- Pipeline component for rule-based token attribute assignment
- This component is not yet integrated into spaCy core, and is available via the extension
  package spacy-experimental starting in version 0.6.0.
- The serialization format is gzipped msgpack.
- You can control which information is serialized by passing a list of attribute IDs.
- This component is available via the extension package spacy-curated-transformers.
- The current implementation of the alignment algorithm assumes that both tokenizations
  add up to the same string.
- A trainable component for assigning base forms to tokens.
- The lemmatization model predicts which edit tree is applicable to a token.
- EntityLinker requires a KnowledgeBase for disambiguation.
- The transition-based algorithm used encodes certain assumptions that are effective
  for 'traditional' named entity recognition tasks.
- Pipeline component for rule-based named entity recognition
- The attribute ruler lets you set token attributes for tokens identified by Matcher
  patterns.
- Typically used to handle exceptions for token attributes and to map values between
  attributes.
- The Lemmatizer is a standalone pipeline component that can be added to your pipeline.
- If the lemmatization mode is set to 'rule', make sure a Tagger or Morphologizer
  component is available in the pipeline.
- This component is not yet integrated into spaCy core, and is available via the extension
  package spacy-experimental.
- Trainable pipeline component for sentence segmentation
- Defaults to Tagger model
- Defaults to using the InMemoryLookupKB implementation.
- Pipeline component for rule-based sentence boundary detection
- The spancat component uses a Logistic layer where the output class probabilities
  are independent for each class.
- The spancat_singlelabel component uses a Softmax layer and treats the task as a
  multi-class problem.
- The entity recognizer identifies non-overlapping labelled spans of tokens.
- The entity ruler lets you add spans to the Doc.ents using token-based rules or exact
  phrase matches.
- The span finder identifies potentially overlapping, unlabeled spans.
- Patterns can either be a token pattern (list) or a phrase pattern (string).
- This component not yet integrated into spaCy core, and is available via the extension
  package spacy-experimental.
- The SpanRuler lets you add spans to the Doc.spans using token-based rules or exact
  phrase matches.
- The lemmatizer is a standalone pipeline component that can be added to your pipeline.
- If the lemmatization mode is set to 'rule', make sure a Tagger or Morphologizer
  is available in the pipeline.
- Requires setup of connected app in api
- Pipeline component for part-of-speech tagging
- A trainable pipeline component to predict part-of-speech tags for any part-of-speech
  tag set
- Pipeline component for predicting morphological features and coarse-grained POS
  tags following the Universal Dependencies UPOS and FEATS annotation guidelines.
- This feature is experimental.
- Uses 'textcat' for mutually exclusive classes and 'textcat_multilabel' for multi-label
  classification.
- Sentence segmentation is performed by the DependencyParser by default
- Default config for tokenizer is [nlp.tokenizer] @tokenizers = "spacy.Tokenizer.v1".
- Predicted spans will be saved in a SpanGroup on the doc under doc.spans[spans_key].
- This component is available via the extension package spacy-transformers.
- It supports all models that are available via the HuggingFace transformers library.
- This component not yet integrated into spaCy core, and is available via the extension
  package spacy-experimental starting in version 0.6.0.
- Pipeline component for rule-based span and named entity recognition
- The default spans key is 'ruler'.
- New in spacy-transformers v1.1.0.
- Returns Tuple instead of List as of spacy-transformers v1.1.0.
- A pattern can either be a token pattern (list) or a phrase pattern (string).
- Defaults to 'ruler' for spans_key.
- The tagger is a trainable pipeline component
- Rehearsal updates are experimental.
- Expects data in spaCys binary .spacy format
- Expects newline-delimited JSON with a key 'text' for each record
- The `textcat` component is used for mutually exclusive labels.
- The `textcat_multilabel` component is used for multi-label classification.
- Uses default model HashEmbedCNN.
- TrainablePipe class is implemented in Cython.
- The morphological analysis may be provided in the Universal Dependencies FEATS format
  as a string or in the tag map dictionary format.
- The transformer component is available via the extension package spacy-transformers.
- 'Supports two types of vector tables: default and floret.'
- Archived implementations available through spacy-legacy
- Provides backwards compatibility for archived functions that may still be used in
  projects.
- This API provides various pipeline functions for processing text.
- Requires a pretrained DependencyParser or other component that sets the Token.dep
  and Token.head attributes.
- Some architectures may have been replaced or modified in newer versions.
- The PhraseMatcher lets you efficiently match large terminology lists.
- Expects newline-delimited documents in UTF8 format.
- The default implementation of the KnowledgeBase interface. Stores all information
  in-memory.
- Morphology stores possible morphological analyses for a language.
- Vectors data is kept in the Vectors.data attribute, which should be an instance
  of numpy.ndarray (for CPU vectors) or cupy.ndarray (for GPU vectors).
errors:
- '400 Bad Request: Check the input format.'
- '500 Internal Server Error: Try again later.'
- No compatible package found for [lang] (spaCy vX.X.X).
- 'Import Error: No module named spacy'
- 'ImportError: No module named ''en_core_web_sm'''
- 'command not found: spacy'
- 'AttributeError: ''module'' object has no attribute ''load'''
- 'TypeError: unhashable type: ''list'''
- '401 Unauthorized: Recheck API key or permissions'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Invalid token pattern: Check token definitions.'
- 'Match not found: Ensure patterns are correctly defined.'
- 'Invalid entity annotations: Check your training data for accuracy.'
- 'Cyclic dependencies: Ensure your model''s components are properly defined.'
- 'Low data labels: Make sure you have enough examples for training.'
- 'INVALID_INPUT: Check the format of the training data.'
- 'TIMEOUT: Increase the timeout setting or check server load.'
- 'Invalid entity annotations: Check your training data for errors.'
- 'Cyclic dependencies: Ensure your components don''t reference each other in a loop.'
- 'Low data labels: Ensure your dataset is representative.'
- Encountering a segmentation fault due to invalid memory access if accessing objects
  that are supposed to no longer be in use.
- 'Config validation error: field required'
- extra fields not permitted
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Corpus
  - Example
  - doc.ents
  - token.ent_iob
  - token.ent_type
client:
  base_url: https://api.spacy.io
source_metadata: null
