resources:
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
    data_selector: ''
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
    data_selector: ''
    params: {}
- name: sql_table
  endpoint:
    path: /api/pandas.read_sql_table
    method: GET
    data_selector: records
- name: sql_query
  endpoint:
    path: /api/pandas.read_sql_query
    method: GET
    data_selector: records
- name: sql
  endpoint:
    path: /api/pandas.read_sql
    method: GET
    data_selector: records
- name: stata
  endpoint:
    path: /api/pandas.read_stata
    method: GET
    data_selector: records
- name: read_pickle
  endpoint:
    path: api/pandas.read_pickle.html
    method: GET
    data_selector: Load pickled pandas object (or any object) from file.
- name: DataFrame.to_pickle
  endpoint:
    path: api/pandas.DataFrame.to_pickle.html
    method: GET
    data_selector: Pickle (serialize) object to file.
- name: read_table
  endpoint:
    path: api/pandas.read_table.html
    method: GET
    data_selector: Read general delimited file into DataFrame.
- name: read_csv
  endpoint:
    path: api/pandas.read_csv.html
    method: GET
    data_selector: Read a comma-separated values (csv) file into DataFrame.
- name: DataFrame.to_csv
  endpoint:
    path: api/pandas.DataFrame.to_csv.html
    method: GET
    data_selector: Write object to a comma-separated values (csv) file.
- name: read_fwf
  endpoint:
    path: api/pandas.read_fwf.html
    method: GET
    data_selector: Read a table of fixed-width formatted lines into DataFrame.
- name: read_excel
  endpoint:
    path: api/pandas.read_excel.html
    method: GET
    data_selector: Read an Excel file into a pandas DataFrame.
- name: DataFrame.to_excel
  endpoint:
    path: api/pandas.DataFrame.to_excel.html
    method: GET
    data_selector: Write object to an Excel sheet.
- name: read_json
  endpoint:
    path: api/pandas.read_json.html
    method: GET
    data_selector: Convert a JSON string to pandas object.
- name: json_normalize
  endpoint:
    path: api/pandas.json_normalize.html
    method: GET
    data_selector: Normalize semi-structured JSON data into a flat table.
- name: DataFrame.to_json
  endpoint:
    path: api/pandas.DataFrame.to_json.html
    method: GET
    data_selector: Convert the object to a JSON string.
- name: read_html
  endpoint:
    path: api/pandas.read_html.html
    method: GET
    data_selector: Read HTML tables into a list of DataFrame objects.
- name: DataFrame.to_html
  endpoint:
    path: api/pandas.DataFrame.to_html.html
    method: GET
    data_selector: Render a DataFrame as an HTML table.
- name: read_xml
  endpoint:
    path: api/pandas.read_xml.html
    method: GET
    data_selector: Read XML document into a DataFrame object.
- name: DataFrame.to_xml
  endpoint:
    path: api/pandas.DataFrame.to_xml.html
    method: GET
    data_selector: Render a DataFrame to an XML document.
- name: DataFrame.to_latex
  endpoint:
    path: api/pandas.DataFrame.to_latex.html
    method: GET
    data_selector: Render object to a LaTeX tabular, longtable, or nested table.
- name: read_hdf
  endpoint:
    path: api/pandas.read_hdf.html
    method: GET
    data_selector: Read from the store, close it if we opened it.
- name: read_feather
  endpoint:
    path: api/pandas.read_feather.html
    method: GET
    data_selector: Load a feather-format object from the file path.
- name: read_parquet
  endpoint:
    path: api/pandas.read_parquet.html
    method: GET
    data_selector: Load a parquet object from the file path, returning a DataFrame.
- name: read_orc
  endpoint:
    path: api/pandas.read_orc.html
    method: GET
    data_selector: Load an ORC object from the file path, returning a DataFrame.
- name: read_sas
  endpoint:
    path: api/pandas.read_sas.html
    method: GET
    data_selector: Read SAS files stored as either XPORT or SAS7BDAT format files.
- name: read_spss
  endpoint:
    path: api/pandas.read_spss.html
    method: GET
    data_selector: Load an SPSS file from the file path, returning a DataFrame.
- name: melt
  endpoint:
    path: /api/pandas/melt
    method: GET
    data_selector: records
- name: pivot
  endpoint:
    path: /api/pandas/pivot
    method: GET
    data_selector: records
- name: pivot_table
  endpoint:
    path: /api/pandas/pivot_table
    method: GET
    data_selector: records
- name: crosstab
  endpoint:
    path: /api/pandas/crosstab
    method: GET
    data_selector: records
- name: cut
  endpoint:
    path: /api/pandas/cut
    method: GET
    data_selector: records
- name: qcut
  endpoint:
    path: /api/pandas/qcut
    method: GET
    data_selector: records
- name: merge
  endpoint:
    path: /api/pandas/merge
    method: GET
    data_selector: records
- name: merge_ordered
  endpoint:
    path: /api/pandas/merge_ordered
    method: GET
    data_selector: records
- name: merge_asof
  endpoint:
    path: /api/pandas/merge_asof
    method: GET
    data_selector: records
- name: concat
  endpoint:
    path: /api/pandas/concat
    method: GET
    data_selector: records
- name: get_dummies
  endpoint:
    path: /api/pandas/get_dummies
    method: GET
    data_selector: records
- name: from_dummies
  endpoint:
    path: /api/pandas/from_dummies
    method: GET
    data_selector: records
- name: factorize
  endpoint:
    path: /api/pandas/factorize
    method: GET
    data_selector: records
- name: unique
  endpoint:
    path: /api/pandas/unique
    method: GET
    data_selector: records
- name: lreshape
  endpoint:
    path: /api/pandas/lreshape
    method: GET
    data_selector: records
- name: wide_to_long
  endpoint:
    path: /api/pandas/wide_to_long
    method: GET
    data_selector: records
- name: isna
  endpoint:
    path: /api/pandas/isna
    method: GET
    data_selector: records
- name: isnull
  endpoint:
    path: /api/pandas/isnull
    method: GET
    data_selector: records
- name: notna
  endpoint:
    path: /api/pandas/notna
    method: GET
    data_selector: records
- name: notnull
  endpoint:
    path: /api/pandas/notnull
    method: GET
    data_selector: records
- name: to_numeric
  endpoint:
    path: /api/pandas/to_numeric
    method: GET
    data_selector: records
- name: to_datetime
  endpoint:
    path: /api/pandas/to_datetime
    method: GET
    data_selector: records
- name: to_timedelta
  endpoint:
    path: /api/pandas/to_timedelta
    method: GET
    data_selector: records
- name: date_range
  endpoint:
    path: /api/pandas/date_range
    method: GET
    data_selector: records
- name: bdate_range
  endpoint:
    path: /api/pandas/bdate_range
    method: GET
    data_selector: records
- name: period_range
  endpoint:
    path: /api/pandas/period_range
    method: GET
    data_selector: records
- name: timedelta_range
  endpoint:
    path: /api/pandas/timedelta_range
    method: GET
    data_selector: records
- name: infer_freq
  endpoint:
    path: /api/pandas/infer_freq
    method: GET
    data_selector: records
- name: interval_range
  endpoint:
    path: /api/pandas/interval_range
    method: GET
    data_selector: records
- name: eval
  endpoint:
    path: /api/pandas/eval
    method: GET
    data_selector: records
- name: guess_datetime_format
  endpoint:
    path: /api/pandas/guess_datetime_format
    method: GET
    data_selector: records
- name: hash_array
  endpoint:
    path: /api/pandas/hash_array
    method: GET
    data_selector: records
- name: hash_pandas_object
  endpoint:
    path: /api/pandas/hash_pandas_object
    method: GET
    data_selector: records
- name: from_dataframe
  endpoint:
    path: /api/pandas/from_dataframe
    method: GET
    data_selector: records
- name: to_pickle
  endpoint:
    path: /api/pandas.Series.to_pickle
    method: GET
- name: to_csv
  endpoint:
    path: /api/pandas.Series.to_csv
    method: GET
- name: to_dict
  endpoint:
    path: /api/pandas.Series.to_dict
    method: GET
- name: to_excel
  endpoint:
    path: /api/pandas.Series.to_excel
    method: GET
- name: to_frame
  endpoint:
    path: /api/pandas.Series.to_frame
    method: GET
- name: to_xarray
  endpoint:
    path: /api/pandas.Series.to_xarray
    method: GET
- name: to_hdf
  endpoint:
    path: /api/pandas.Series.to_hdf
    method: GET
- name: to_sql
  endpoint:
    path: /api/pandas.Series.to_sql
    method: GET
- name: to_json
  endpoint:
    path: /api/pandas.Series.to_json
    method: GET
- name: to_string
  endpoint:
    path: /api/pandas.Series.to_string
    method: GET
- name: to_clipboard
  endpoint:
    path: /api/pandas.Series.to_clipboard
    method: GET
- name: to_latex
  endpoint:
    path: /api/pandas.Series.to_latex
    method: GET
- name: to_markdown
  endpoint:
    path: /api/pandas.Series.to_markdown
    method: GET
- name: IntervalIndex
  endpoint:
    path: /api/pandas/IntervalIndex
    method: GET
- name: MultiIndex
  endpoint:
    path: /api/pandas/MultiIndex
    method: GET
- name: DatetimeIndex
  endpoint:
    path: /api/pandas/DatetimeIndex
    method: GET
- name: DatetimeIndex
  endpoint:
    path: /api/pandas/DatetimeIndex
    method: GET
    data_selector: methods
- name: TimedeltaIndex
  endpoint:
    path: /api/pandas/TimedeltaIndex
    method: GET
    data_selector: methods
- name: PeriodIndex
  endpoint:
    path: /api/pandas/PeriodIndex
    method: GET
    data_selector: methods
- name: CustomBusinessMonthEnd
  endpoint:
    path: /api/pandas.tseries.offsets.CustomBusinessMonthEnd
    method: GET
    data_selector: records
- name: CustomBusinessMonthBegin
  endpoint:
    path: /api/pandas.tseries.offsets.CustomBusinessMonthBegin
    method: GET
    data_selector: records
- name: SemiMonthEnd
  endpoint:
    path: /api/pandas.tseries.offsets.SemiMonthEnd
    method: GET
    data_selector: records
- name: SemiMonthBegin
  endpoint:
    path: /api/pandas.tseries.offsets.SemiMonthBegin
    method: GET
    data_selector: records
- name: DateOffset
  endpoint:
    path: /api/pandas.tseries.offsets.DateOffset
    method: GET
    data_selector: properties
- name: BusinessDay
  endpoint:
    path: /api/pandas.tseries.offsets.BusinessDay
    method: GET
    data_selector: properties
- name: BusinessHour
  endpoint:
    path: /api/pandas.tseries.offsets.BusinessHour
    method: GET
    data_selector: properties
- name: CustomBusinessDay
  endpoint:
    path: /api/pandas.tseries.offsets.CustomBusinessDay
    method: GET
    data_selector: properties
- name: CustomBusinessHour
  endpoint:
    path: /api/pandas.tseries.offsets.CustomBusinessHour
    method: GET
    data_selector: properties
- name: MonthEnd
  endpoint:
    path: /pandas/tseries/offsets/MonthEnd
    method: GET
    data_selector: records
- name: MonthBegin
  endpoint:
    path: /pandas/tseries/offsets/MonthBegin
    method: GET
    data_selector: records
- name: BusinessMonthEnd
  endpoint:
    path: /pandas/tseries/offsets/BusinessMonthEnd
    method: GET
    data_selector: records
- name: BusinessMonthBegin
  endpoint:
    path: /pandas/tseries/offsets/BusinessMonthBegin
    method: GET
    data_selector: records
- name: BYearEnd
  endpoint:
    path: /api/pandas.tseries.offsets.BYearEnd
    method: GET
    data_selector: frequency
- name: Week
  endpoint:
    path: /api/pandas/tseries/offsets/Week
    method: GET
- name: WeekOfMonth
  endpoint:
    path: /api/pandas/tseries/offsets/WeekOfMonth
    method: GET
- name: LastWeekOfMonth
  endpoint:
    path: /api/pandas/tseries/offsets/LastWeekOfMonth
    method: GET
- name: BQuarterEnd
  endpoint:
    path: /api/pandas/tseries/offsets/BQuarterEnd
    method: GET
- name: BYearEnd
  endpoint:
    path: /api/pandas.tseries.offsets.BYearEnd
    method: GET
- name: BYearBegin
  endpoint:
    path: /api/pandas.tseries.offsets.BYearBegin
    method: GET
- name: YearEnd
  endpoint:
    path: /api/pandas.tseries.offsets.YearEnd
    method: GET
- name: YearBegin
  endpoint:
    path: /api/pandas.tseries.offsets.YearBegin
    method: GET
- name: FY5253
  endpoint:
    path: /api/pandas.tseries.offsets.FY5253
    method: GET
- name: FY5253Quarter
  endpoint:
    path: /api/pandas/tseries/offsets/FY5253Quarter
    method: GET
- name: Easter
  endpoint:
    path: /api/pandas/tseries/offsets/Easter
    method: GET
- name: Tick
  endpoint:
    path: /api/pandas/tseries/offsets/Tick
    method: GET
- name: Day
  endpoint:
    path: /api/pandas/tseries/offsets/Day
    method: GET
- name: Day
  endpoint:
    path: /api/pandas.tseries.offsets.Day
    method: GET
- name: Hour
  endpoint:
    path: /api/pandas.tseries.offsets.Hour
    method: GET
- name: Minute
  endpoint:
    path: /api/pandas.tseries.offsets.Minute
    method: GET
- name: Second
  endpoint:
    path: /api/pandas.tseries.offsets.Second
    method: GET
- name: Milli
  endpoint:
    path: /api/pandas.tseries.offsets.Milli
    method: GET
- name: Micro
  endpoint:
    path: /api/pandas.tseries.offsets.Micro
    method: GET
- name: Rolling
  endpoint:
    path: /api/pandas.core.window.rolling
    method: GET
- name: Expanding
  endpoint:
    path: /api/pandas.core.window.expanding
    method: GET
- name: ExponentialMovingWindow
  endpoint:
    path: /api/pandas.core.window.ewm
    method: GET
- name: ExponentialMovingWindow.mean
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.mean
    method: GET
- name: ExponentialMovingWindow.sum
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.sum
    method: GET
- name: ExponentialMovingWindow.std
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.std
    method: GET
- name: ExponentialMovingWindow.var
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.var
    method: GET
- name: ExponentialMovingWindow.corr
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.corr
    method: GET
- name: ExponentialMovingWindow.cov
  endpoint:
    path: /api/pandas.core.window.ewm.ExponentialMovingWindow.cov
    method: GET
- name: DataFrameGroupBy
  endpoint:
    path: /api/pandas.core.groupby.DataFrameGroupBy
    method: GET
    data_selector: instances
- name: SeriesGroupBy
  endpoint:
    path: /api/pandas.core.groupby.SeriesGroupBy
    method: GET
    data_selector: instances
- name: DataFrameGroupBy
  endpoint:
    path: /api/pandas/core/groupby/DataFrameGroupBy
    method: GET
    data_selector: groupby_methods
    params: {}
- name: SeriesGroupBy
  endpoint:
    path: /api/pandas/core/groupby/SeriesGroupBy
    method: GET
    data_selector: groupby_methods
    params: {}
- name: Resampler
  endpoint:
    path: /api/pandas.core.resample.Resampler
    method: GET
    data_selector: records
- name: andrews_curves
  endpoint:
    path: /api/pandas.plotting.andrews_curves
    method: GET
    data_selector: records
- name: autocorrelation_plot
  endpoint:
    path: /api/pandas.plotting.autocorrelation_plot
    method: GET
    data_selector: records
- name: bootstrap_plot
  endpoint:
    path: /api/pandas.plotting.bootstrap_plot
    method: GET
    data_selector: records
- name: boxplot
  endpoint:
    path: /api/pandas.plotting.boxplot
    method: GET
    data_selector: records
- name: deregister_matplotlib_converters
  endpoint:
    path: /api/pandas.plotting.deregister_matplotlib_converters
    method: GET
    data_selector: records
- name: lag_plot
  endpoint:
    path: /api/pandas.plotting.lag_plot
    method: GET
    data_selector: records
- name: parallel_coordinates
  endpoint:
    path: /api/pandas.plotting.parallel_coordinates
    method: GET
    data_selector: records
- name: plot_params
  endpoint:
    path: /api/pandas.plotting.plot_params
    method: GET
    data_selector: records
- name: radviz
  endpoint:
    path: /api/pandas.plotting.radviz
    method: GET
    data_selector: records
- name: register_matplotlib_converters
  endpoint:
    path: /api/pandas.plotting.register_matplotlib_converters
    method: GET
    data_selector: records
- name: scatter_matrix
  endpoint:
    path: /api/pandas.plotting.scatter_matrix
    method: GET
    data_selector: records
- name: table
  endpoint:
    path: /api/pandas.plotting.table
    method: GET
    data_selector: records
- name: assert_frame_equal
  endpoint:
    path: /api/pandas.testing.assert_frame_equal
    method: GET
    data_selector: records
- name: assert_series_equal
  endpoint:
    path: /api/pandas.testing.assert_series_equal
    method: GET
    data_selector: records
- name: assert_index_equal
  endpoint:
    path: /api/pandas.testing.assert_index_equal
    method: GET
    data_selector: records
- name: assert_extension_array_equal
  endpoint:
    path: /api/pandas.testing.assert_extension_array_equal
    method: GET
    data_selector: records
- name: NA
  endpoint:
    path: /api/pandas.NA
    method: GET
    data_selector: missing_value_indicator
- name: NaT
  endpoint:
    path: /api/pandas.NaT
    method: GET
    data_selector: time_equivalent_of_NaN
- name: dataframe
  endpoint:
    path: /dataframe
    method: GET
    data_selector: records
    params: {}
- name: Cookbook
  endpoint:
    path: /docs/user_guide/cookbook.html
    method: GET
    data_selector: examples
    params: {}
- name: idioms
  endpoint:
    path: /user_guide/cookbook.html#idioms
    method: GET
    data_selector: examples
    params: {}
- name: selection
  endpoint:
    path: /user_guide/cookbook.html#selection
    method: GET
    data_selector: examples
    params: {}
- name: sorting
  endpoint:
    path: /sorting
    method: GET
    data_selector: records
- name: levels
  endpoint:
    path: /levels
    method: GET
    data_selector: records
- name: correlation
  endpoint:
    path: /correlation
    method: GET
    data_selector: records
- name: replace
  endpoint:
    path: /replace
    method: GET
    data_selector: records
- name: grouping
  endpoint:
    path: /grouping
    method: GET
    data_selector: records
- name: animal_weight
  endpoint:
    path: /animals/weight
    method: GET
    data_selector: weights
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
- name: iris_data
  endpoint:
    path: /data/iris.data
    method: GET
    data_selector: data
    params: {}
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
    data_selector: records
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
    data_selector: none
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
    data_selector: none
    params: {}
- name: baseball
  endpoint:
    path: data/baseball.csv
    method: GET
    data_selector: rows
    params: {}
- name: value_counts
  endpoint:
    path: /reference/api/pandas.Series.value_counts
    method: GET
    data_selector: records
    params: {}
- name: discretization_and_quantiling
  endpoint:
    path: /reference/api/pandas.cut
    method: GET
    data_selector: records
    params: {}
- name: function_application
  endpoint:
    path: /reference/api/pandas.DataFrame.pipe
    method: GET
    data_selector: records
    params: {}
- name: apply
  endpoint:
    path: /reference/api/pandas.DataFrame.apply.html
    method: GET
    data_selector: records
    params: {}
- name: aggregate
  endpoint:
    path: /reference/api/pandas.DataFrame.aggregate.html
    method: GET
    data_selector: records
    params: {}
- name: describe
  endpoint:
    path: /api/pandas/DataFrame/describe
    method: GET
    data_selector: summary_statistics
- name: value_counts
  endpoint:
    path: /api/pandas/Series/value_counts
    method: GET
    data_selector: histogram
- name: transform
  endpoint:
    path: /transform
    method: GET
    data_selector: records
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame
    method: GET
    data_selector: records
- name: align
  endpoint:
    path: /reference/api/pandas.Series.align
    method: GET
    data_selector: records
    params: {}
- name: reindex
  endpoint:
    path: /reference/api/pandas.Series.reindex
    method: GET
    data_selector: records
    params: {}
- name: drop
  endpoint:
    path: /reference/api/pandas.DataFrame.drop
    method: GET
    data_selector: records
    params: {}
- name: rename
  endpoint:
    path: /reference/api/pandas.DataFrame.rename
    method: GET
    data_selector: records
    params: {}
- name: iteration
  endpoint:
    path: /reference/api/pandas.iteration
    method: GET
    data_selector: records
    params: {}
- name: drop_labels
  endpoint:
    path: /api/pandas/drop
    method: POST
    data_selector: output
    params: {}
- name: rename_labels
  endpoint:
    path: /api/pandas/rename
    method: POST
    data_selector: output
    params: {}
- name: iteration
  endpoint:
    path: /api/pandas/iteration
    method: GET
    data_selector: output
    params: {}
- name: datetime_access
  endpoint:
    path: /api/pandas/datetime
    method: GET
    data_selector: output
    params: {}
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: data
    params: {}
- name: align
  endpoint:
    path: /reference/api/pandas.Series.align.html
    method: GET
    data_selector: records
    params: {}
- name: reindex
  endpoint:
    path: /reference/api/pandas.Series.reindex.html
    method: GET
    data_selector: records
    params: {}
- name: drop
  endpoint:
    path: /reference/api/pandas.DataFrame.drop.html
    method: GET
    data_selector: records
    params: {}
- name: rename
  endpoint:
    path: /reference/api/pandas.DataFrame.rename.html
    method: GET
    data_selector: records
    params: {}
- name: iteration
  endpoint:
    path: /reference/api/pandas.DataFrame.iterrows.html
    method: GET
    data_selector: records
    params: {}
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: result
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame
    method: GET
    data_selector: records
    params: {}
- name: Series
  endpoint:
    path: /reference/api/pandas.Series
    method: GET
    data_selector: records
    params: {}
- name: describe
  endpoint:
    path: /api/pandas.Series.describe
    method: GET
    data_selector: summary_statistics
- name: value_counts
  endpoint:
    path: /api/pandas.Series.value_counts
    method: GET
    data_selector: histogram
- name: cumsum
  endpoint:
    path: /api/pandas.Series.cumsum
    method: GET
    data_selector: cumulative_sum
- name: cumprod
  endpoint:
    path: /api/pandas.Series.cumprod
    method: GET
    data_selector: cumulative_product
- name: nunique
  endpoint:
    path: /api/pandas.Series.nunique
    method: GET
    data_selector: unique_count
- name: idxmin
  endpoint:
    path: /api/pandas.Series.idxmin
    method: GET
    data_selector: index_of_min
- name: idxmax
  endpoint:
    path: /api/pandas.Series.idxmax
    method: GET
    data_selector: index_of_max
- name: describe
  endpoint:
    path: /api/series/describe
    method: GET
    data_selector: description
    params: {}
- name: df_sub
  endpoint:
    path: /sub
    method: GET
    data_selector: records
- name: dfmi_sub
  endpoint:
    path: /mi/sub
    method: GET
    data_selector: records
- name: transform
  endpoint:
    path: /transform
    method: GET
    data_selector: data
    params: {}
- name: searchsorted
  endpoint:
    path: /reference/api/pandas.Series.searchsorted
    method: GET
    data_selector: array
- name: nsmallest
  endpoint:
    path: /reference/api/pandas.Series.nsmallest
    method: GET
    data_selector: nsmallest_values
- name: nlargest
  endpoint:
    path: /reference/api/pandas.Series.nlargest
    method: GET
    data_selector: nlargest_values
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: results
- name: dataframe_operations
  endpoint:
    path: /dataframe/operations
    method: GET
    data_selector: operations
    params: {}
- name: DataFrame
  endpoint:
    path: /dataframe
    method: GET
    data_selector: records
    params: {}
- name: Series
  endpoint:
    path: /series
    method: GET
    data_selector: records
    params: {}
- name: transform
  endpoint:
    path: /api/transform
    method: POST
    data_selector: data
    params: {}
- name: align
  endpoint:
    path: /align
    method: GET
    data_selector: records
- name: reindex
  endpoint:
    path: /reindex
    method: GET
    data_selector: records
- name: drop
  endpoint:
    path: /drop
    method: GET
    data_selector: records
- name: rename
  endpoint:
    path: /rename
    method: GET
    data_selector: records
- name: transform
  endpoint:
    path: /transform
    method: GET
    data_selector: data
    params: {}
- name: align
  endpoint:
    path: /reference/api/pandas.Series.align
    method: GET
    data_selector: data
    params: {}
- name: reindex
  endpoint:
    path: /reference/api/pandas.Series.reindex
    method: GET
    data_selector: data
    params: {}
- name: drop
  endpoint:
    path: /reference/api/pandas.DataFrame.drop
    method: GET
    data_selector: data
    params: {}
- name: rename
  endpoint:
    path: /reference/api/pandas.DataFrame.rename
    method: GET
    data_selector: data
    params: {}
- name: iteration
  endpoint:
    path: /reference/api/pandas.iteration
    method: GET
    data_selector: data
    params: {}
- name: transform
  endpoint:
    path: /transform
    method: POST
    data_selector: result
- name: astype
  endpoint:
    path: /reference/api/pandas.DataFrame.astype.html
    method: GET
    data_selector: records
    params: {}
- name: to_numpy
  endpoint:
    path: /reference/api/pandas.DataFrame.to_numpy.html
    method: GET
    data_selector: records
    params: {}
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: records
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: records
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: records
- name: XML
  endpoint:
    path: /read_xml
    method: GET
    data_selector: records
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: records
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: records
- name: Feather
  endpoint:
    path: /read_feather
    method: GET
    data_selector: records
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: records
- name: ORC
  endpoint:
    path: /read_orc
    method: GET
    data_selector: records
- name: Stata
  endpoint:
    path: /read_stata
    method: GET
    data_selector: records
- name: SAS
  endpoint:
    path: /read_sas
    method: GET
    data_selector: records
- name: SPSS
  endpoint:
    path: /read_spss
    method: GET
    data_selector: records
- name: Pickle
  endpoint:
    path: /read_pickle
    method: GET
    data_selector: records
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
    data_selector: records
- name: Google BigQuery
  endpoint:
    path: /read_gbq
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: records
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: records
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: records
- name: XML
  endpoint:
    path: /read_xml
    method: GET
    data_selector: records
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: records
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: records
- name: Feather
  endpoint:
    path: /read_feather
    method: GET
    data_selector: records
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: records
- name: ORC
  endpoint:
    path: /read_orc
    method: GET
    data_selector: records
- name: Stata
  endpoint:
    path: /read_stata
    method: GET
    data_selector: records
- name: SAS
  endpoint:
    path: /read_sas
    method: GET
    data_selector: records
- name: SPSS
  endpoint:
    path: /read_spss
    method: GET
    data_selector: records
- name: Pickle
  endpoint:
    path: /read_pickle
    method: GET
    data_selector: records
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
    data_selector: records
- name: Google BigQuery
  endpoint:
    path: /read_gbq
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
- name: JSON
  endpoint:
    path: /read_json
    method: GET
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: records
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: records
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: records
- name: data
  endpoint:
    path: /data/csv
    method: GET
    data_selector: records
    params: {}
- name: dialect
  endpoint:
    path: /read_csv/dialect
    method: GET
    data_selector: dialect_options
    params: {}
- name: quoting
  endpoint:
    path: /read_csv/quoting
    method: GET
    data_selector: quoting_options
    params: {}
- name: fixed_width_columns
  endpoint:
    path: /read_fwf
    method: GET
    data_selector: fixed_width_columns_options
    params: {}
- name: implicit_index_column
  endpoint:
    path: /read_csv/implicit_index
    method: GET
    data_selector: data
    params: {}
- name: multi_index
  endpoint:
    path: /read_csv/multi_index
    method: GET
    data_selector: data
    params: {}
- name: multi_index_columns
  endpoint:
    path: /read_csv/multi_index_columns
    method: GET
    data_selector: data
    params: {}
- name: sniff_delimiter
  endpoint:
    path: /read_csv/sniff_delimiter
    method: GET
    data_selector: data
    params: {}
- name: CSV
  endpoint:
    path: /read_csv
    method: POST
- name: JSON
  endpoint:
    path: /read_json
    method: POST
- name: HTML
  endpoint:
    path: /read_html
    method: POST
- name: XML
  endpoint:
    path: /read_xml
    method: POST
- name: Excel
  endpoint:
    path: /read_excel
    method: POST
- name: HDF5
  endpoint:
    path: /read_hdf
    method: POST
- name: Feather
  endpoint:
    path: /read_feather
    method: POST
- name: Parquet
  endpoint:
    path: /read_parquet
    method: POST
- name: ORC
  endpoint:
    path: /read_orc
    method: POST
- name: Stata
  endpoint:
    path: /read_stata
    method: POST
- name: SAS
  endpoint:
    path: /read_sas
    method: POST
- name: SPSS
  endpoint:
    path: /read_spss
    method: POST
- name: Pickle
  endpoint:
    path: /read_pickle
    method: POST
- name: SQL
  endpoint:
    path: /read_sql
    method: POST
- name: data
  endpoint:
    path: /data
    method: GET
    data_selector: records
    params: {}
- name: read_csv
  endpoint:
    path: /read_csv
    method: POST
    data_selector: df
    params: {}
- name: csv_file
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: CSV
  endpoint:
    path: /reference/api/pandas.read_csv.html
    method: GET
    data_selector: reader
- name: JSON
  endpoint:
    path: /reference/api/pandas.read_json.html
    method: GET
    data_selector: reader
- name: HTML
  endpoint:
    path: /reference/api/pandas.read_html.html
    method: GET
    data_selector: reader
- name: XML
  endpoint:
    path: /reference/api/pandas.read_xml.html
    method: GET
    data_selector: reader
- name: Excel
  endpoint:
    path: /reference/api/pandas.read_excel.html
    method: GET
    data_selector: reader
- name: HDF5
  endpoint:
    path: /reference/api/pandas.read_hdf.html
    method: GET
    data_selector: reader
- name: Parquet
  endpoint:
    path: /reference/api/pandas.read_parquet.html
    method: GET
    data_selector: reader
- name: SQL
  endpoint:
    path: /reference/api/pandas.read_sql.html
    method: GET
    data_selector: reader
- name: csv_reading
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: series
  endpoint:
    path: /series
    method: GET
    data_selector: fields
- name: dataframe
  endpoint:
    path: /dataframe
    method: GET
    data_selector: fields
- name: books
  endpoint:
    path: /books.xml
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: reader
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: reader
    params: {}
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: reader
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: reader
    params: {}
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: reader
    params: {}
- name: Feather
  endpoint:
    path: /read_feather
    method: GET
    data_selector: reader
    params: {}
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: reader
    params: {}
- name: ORC
  endpoint:
    path: /read_orc
    method: GET
    data_selector: reader
    params: {}
- name: Stata
  endpoint:
    path: /read_stata
    method: GET
    data_selector: reader
    params: {}
- name: SAS
  endpoint:
    path: /read_sas
    method: GET
    data_selector: reader
    params: {}
- name: SPSS
  endpoint:
    path: /read_spss
    method: GET
    data_selector: reader
    params: {}
- name: Pickle
  endpoint:
    path: /read_pickle
    method: GET
    data_selector: reader
    params: {}
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
    data_selector: reader
    params: {}
- name: Google BigQuery
  endpoint:
    path: /read_gbq
    method: GET
    data_selector: reader
    params: {}
- name: Sheet1
  endpoint:
    path: Sheet1
    method: GET
    data_selector: data
    params:
      index_col: null
      na_values:
      - NA
- name: Sheet2
  endpoint:
    path: Sheet2
    method: GET
    data_selector: data
    params:
      index_col: 1
- name: test_table
  endpoint:
    path: /test.sql
    method: GET
    data_selector: records
    params: {}
- name: test_fixed
  endpoint:
    path: /test_fixed.hdf
    method: GET
    data_selector: records
    params: {}
- name: test_feather
  endpoint:
    path: /test.feather
    method: GET
    data_selector: records
    params: {}
- name: test_parquet
  endpoint:
    path: /test.parquet
    method: GET
    data_selector: records
    params: {}
- name: test_pickle
  endpoint:
    path: /test.pkl
    method: GET
    data_selector: records
    params: {}
- name: test_csv
  endpoint:
    path: /test.csv
    method: GET
    data_selector: records
    params: {}
- name: html_table
  endpoint:
    path: /read_html
    method: GET
    data_selector: dataframes
    params: {}
- name: books
  endpoint:
    path: /books.xml
    method: GET
    data_selector: records
    params: {}
- name: csv
  endpoint:
    path: /read_csv
    method: GET
    data_selector: null
    params: {}
- name: json
  endpoint:
    path: /read_json
    method: GET
    data_selector: null
    params: {}
- name: html
  endpoint:
    path: /read_html
    method: GET
    data_selector: null
    params: {}
- name: excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: null
    params: {}
- name: hdf
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: null
    params: {}
- name: feather
  endpoint:
    path: /read_feather
    method: GET
    data_selector: null
    params: {}
- name: parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: null
    params: {}
- name: orc
  endpoint:
    path: /read_orc
    method: GET
    data_selector: null
    params: {}
- name: stata
  endpoint:
    path: /read_stata
    method: GET
    data_selector: null
    params: {}
- name: sas
  endpoint:
    path: /read_sas
    method: GET
    data_selector: null
    params: {}
- name: spss
  endpoint:
    path: /read_spss
    method: GET
    data_selector: null
    params: {}
- name: pickle
  endpoint:
    path: /read_pickle
    method: GET
    data_selector: null
    params: {}
- name: sql
  endpoint:
    path: /read_sql
    method: GET
    data_selector: null
    params: {}
- name: gbq
  endpoint:
    path: /read_gbq
    method: GET
    data_selector: null
    params: {}
- name: excel_files
  endpoint:
    path: /read_excel
    method: GET
    data_selector: DataFrame
    params: {}
- name: open_document_spreadsheets
  endpoint:
    path: /read_open_document
    method: GET
    data_selector: DataFrame
    params: {}
- name: binary_excel_files
  endpoint:
    path: /read_binary_excel
    method: GET
    data_selector: DataFrame
    params: {}
- name: read_excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: DataFrame
    params: {}
- name: to_excel
  endpoint:
    path: /to_excel
    method: POST
    data_selector: None
    params: {}
- name: HDF5
  endpoint:
    path: /hdf5
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: data
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: dataframe
  endpoint:
    path: /read_csv
    method: GET
    data_selector: dataframe
    params: {}
- name: csv_reading
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: fixed_width_reading
  endpoint:
    path: /read_fwf
    method: GET
    data_selector: data
    params: {}
- name: ExcelFile
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: df_with_missing
  endpoint:
    path: file.h5
    method: read_hdf
    data_selector: df_with_missing
    params: {}
- name: fixed_format
  endpoint:
    path: test_fixed.h5
    method: read_hdf
    data_selector: df
    params: {}
- name: table_format
  endpoint:
    path: store.h5
    method: append
    data_selector: df
    params: {}
- name: CSV
  endpoint:
    path: /io/read_csv
    method: GET
- name: JSON
  endpoint:
    path: /io/read_json
    method: GET
- name: HTML
  endpoint:
    path: /io/read_html
    method: GET
- name: XML
  endpoint:
    path: /io/read_xml
    method: GET
- name: Excel
  endpoint:
    path: /io/read_excel
    method: GET
- name: HDF5
  endpoint:
    path: /io/read_hdf
    method: GET
- name: Feather
  endpoint:
    path: /io/read_feather
    method: GET
- name: Parquet
  endpoint:
    path: /io/read_parquet
    method: GET
- name: ORC
  endpoint:
    path: /io/read_orc
    method: GET
- name: Stata
  endpoint:
    path: /io/read_stata
    method: GET
- name: SAS
  endpoint:
    path: /io/read_sas
    method: GET
- name: SPSS
  endpoint:
    path: /io/read_spss
    method: GET
- name: Pickle
  endpoint:
    path: /io/read_pickle
    method: GET
- name: SQL
  endpoint:
    path: /io/read_sql
    method: GET
- name: BigQuery
  endpoint:
    path: /io/read_gbq
    method: GET
- name: series
  endpoint:
    path: /build_table_schema
    method: GET
    data_selector: fields
    params: {}
- name: books
  endpoint:
    path: https://www.w3schools.com/xml/books.xml
    method: GET
    data_selector: bookstore/book
    params: {}
- name: ExcelFile
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: data
    params: {}
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: data
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: data
    params: {}
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: data
    params: {}
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
    data_selector: data
    params: {}
- name: html_table
  endpoint:
    path: /read_html
    method: GET
    data_selector: tables
    params: {}
- name: ExcelFile
  endpoint:
    path: /read_excel
    method: GET
    data_selector: DataFrame
    params: {}
- name: test_table
  endpoint:
    path: /test_table
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: data
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: read_csv
  endpoint:
    path: /read_csv
    method: GET
    data_selector: results
    params: {}
- name: csv_data
  endpoint:
    path: ''
    method: GET
    data_selector: ''
    params: {}
- name: series
  endpoint:
    path: /series
    method: GET
    data_selector: records
- name: dataframe
  endpoint:
    path: /dataframe
    method: GET
    data_selector: records
- name: books
  endpoint:
    path: /xml/books.xml
    method: GET
    data_selector: records
- name: read_excel
  endpoint:
    path: /read_excel
    method: POST
    data_selector: data
    params: {}
- name: to_excel
  endpoint:
    path: /to_excel
    method: POST
    data_selector: data
    params: {}
- name: df
  endpoint:
    path: /df
    method: GET
    data_selector: records
    params: {}
- name: df_mixed
  endpoint:
    path: /df_mixed
    method: GET
    data_selector: records
    params: {}
- name: df_mi
  endpoint:
    path: /df_mi
    method: GET
    data_selector: records
    params: {}
- name: dfq
  endpoint:
    path: /dfq
    method: GET
    data_selector: records
    params: {}
- name: df_dc
  endpoint:
    path: /select_column/df_dc
    method: GET
    data_selector: index
- name: df_coord
  endpoint:
    path: /select_as_coordinates/df_coord
    method: GET
    data_selector: index
- name: df_mask
  endpoint:
    path: /select/df_mask
    method: GET
    data_selector: where
- name: df1_mt
  endpoint:
    path: /select/df1_mt
    method: GET
    data_selector: records
- name: df2_mt
  endpoint:
    path: /select/df2_mt
    method: GET
    data_selector: records
- name: test_sql_write
  endpoint:
    path: /test_sql_write
    method: POST
    data_selector: write_test
    params: {}
- name: test_sql_read
  endpoint:
    path: /test_sql_read
    method: GET
    data_selector: read_test
    params: {}
- name: test_hdf_fixed_write
  endpoint:
    path: /test_hdf_fixed_write
    method: POST
    data_selector: write_hdf_fixed
    params: {}
- name: test_hdf_fixed_read
  endpoint:
    path: /test_hdf_fixed_read
    method: GET
    data_selector: read_hdf_fixed
    params: {}
- name: test_csv_write
  endpoint:
    path: /test_csv_write
    method: POST
    data_selector: write_csv
    params: {}
- name: test_csv_read
  endpoint:
    path: /test_csv_read
    method: GET
    data_selector: read_csv
    params: {}
- name: test_feather_write
  endpoint:
    path: /test_feather_write
    method: POST
    data_selector: write_feather
    params: {}
- name: test_feather_read
  endpoint:
    path: /test_feather_read
    method: GET
    data_selector: read_feather
    params: {}
- name: test_pickle_write
  endpoint:
    path: /test_pickle_write
    method: POST
    data_selector: write_pickle
    params: {}
- name: test_pickle_read
  endpoint:
    path: /test_pickle_read
    method: GET
    data_selector: read_pickle
    params: {}
- name: test_parquet_write
  endpoint:
    path: /test_parquet_write
    method: POST
    data_selector: write_parquet
    params: {}
- name: test_parquet_read
  endpoint:
    path: /test_parquet_read
    method: GET
    data_selector: read_parquet
    params: {}
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
- name: JSON
  endpoint:
    path: /read_json
    method: GET
- name: HTML
  endpoint:
    path: /read_html
    method: GET
- name: XML
  endpoint:
    path: /read_xml
    method: GET
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
- name: Feather
  endpoint:
    path: /read_feather
    method: GET
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
- name: ORC
  endpoint:
    path: /read_orc
    method: GET
- name: Stata
  endpoint:
    path: /read_stata
    method: GET
- name: SAS
  endpoint:
    path: /read_sas
    method: GET
- name: SPSS
  endpoint:
    path: /read_spss
    method: GET
- name: Pickle
  endpoint:
    path: /read_pickle
    method: GET
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
- name: Google BigQuery
  endpoint:
    path: /read_gbq
    method: GET
- name: files
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: data_frame
  endpoint:
    path: /data/frame
    method: GET
    data_selector: data
    params: {}
- name: excel_file
  endpoint:
    path: /path/to/excel
    method: GET
    data_selector: records
    params: {}
- name: df
  endpoint:
    path: /df
    method: GET
    data_selector: records
    params: {}
- name: df_mixed
  endpoint:
    path: /df_mixed
    method: GET
    data_selector: records
    params: {}
- name: df_mi
  endpoint:
    path: /df_mi
    method: GET
    data_selector: records
    params: {}
- name: dfq
  endpoint:
    path: /dfq
    method: GET
    data_selector: records
    params: {}
- name: df_dc
  endpoint:
    path: /select_column
    method: GET
    data_selector: records
    params: {}
- name: df_coord
  endpoint:
    path: /append
    method: POST
    data_selector: records
    params: {}
- name: df_mask
  endpoint:
    path: /append
    method: POST
    data_selector: records
    params: {}
- name: df_mt
  endpoint:
    path: /append_to_multiple
    method: POST
    data_selector: records
    params: {}
- name: sql_table
  endpoint:
    path: /pandas/read_sql_table
    method: GET
    data_selector: data
    params: {}
- name: sql_query
  endpoint:
    path: /pandas/read_sql_query
    method: GET
    data_selector: data
    params: {}
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: data
    params: {}
- name: HTML
  endpoint:
    path: /read_html
    method: GET
    data_selector: data
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: HDF5
  endpoint:
    path: /read_hdf
    method: GET
    data_selector: data
    params: {}
- name: Feather
  endpoint:
    path: /read_feather
    method: GET
    data_selector: data
    params: {}
- name: Parquet
  endpoint:
    path: /read_parquet
    method: GET
    data_selector: data
    params: {}
- name: ORC
  endpoint:
    path: /read_orc
    method: GET
    data_selector: data
    params: {}
- name: Stata
  endpoint:
    path: /read_stata
    method: GET
    data_selector: data
    params: {}
- name: SAS
  endpoint:
    path: /read_sas
    method: GET
    data_selector: data
    params: {}
- name: SPSS
  endpoint:
    path: /read_spss
    method: GET
    data_selector: data
    params: {}
- name: Pickle
  endpoint:
    path: /read_pickle
    method: GET
    data_selector: data
    params: {}
- name: SQL
  endpoint:
    path: /read_sql
    method: GET
    data_selector: data
    params: {}
- name: Google BigQuery
  endpoint:
    path: /read_gbq
    method: GET
    data_selector: data
    params: {}
- name: excel_file
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: df
  endpoint:
    path: /df
    method: GET
    data_selector: records
    params: {}
- name: df_mixed
  endpoint:
    path: /df_mixed
    method: GET
    data_selector: records
    params: {}
- name: df_mi
  endpoint:
    path: /df_mi
    method: GET
    data_selector: records
    params: {}
- name: feather
  endpoint:
    path: /to_feather
    method: POST
    data_selector: data
    params: {}
- name: parquet
  endpoint:
    path: /to_parquet
    method: POST
    data_selector: data
    params: {}
- name: orc
  endpoint:
    path: /to_orc
    method: POST
    data_selector: data
    params: {}
- name: read_sql_table
  endpoint:
    path: /reference/api/pandas.read_sql_table.html
    method: GET
    data_selector: table_name
- name: read_sql_query
  endpoint:
    path: /reference/api/pandas.read_sql_query.html
    method: GET
    data_selector: sql
- name: read_sql
  endpoint:
    path: /reference/api/pandas.read_sql.html
    method: GET
    data_selector: sql
- name: DataFrame.to_sql
  endpoint:
    path: /reference/api/pandas.DataFrame.to_sql.html
    method: GET
    data_selector: name
- name: CSV
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: JSON
  endpoint:
    path: /read_json
    method: GET
    data_selector: data
    params: {}
- name: Excel
  endpoint:
    path: /read_excel
    method: GET
    data_selector: data
    params: {}
- name: datetime_parsing
  endpoint:
    path: /datetime/parsing
    method: GET
    data_selector: records
- name: schema
  endpoint:
    path: /schema
    method: GET
    data_selector: fields
    params: {}
- name: rides
  endpoint:
    path: /response
    method: GET
    data_selector: row
- name: read_excel
  endpoint:
    path: /pandas/read_excel
    method: GET
    data_selector: DataFrame
    params: {}
- name: to_excel
  endpoint:
    path: /pandas/to_excel
    method: POST
    data_selector: None
    params: {}
- name: df
  endpoint:
    path: /df
    method: GET
    data_selector: records
    params: {}
- name: df_mixed
  endpoint:
    path: /df_mixed
    method: GET
    data_selector: records
    params: {}
- name: df_mi
  endpoint:
    path: /df_mi
    method: GET
    data_selector: records
    params: {}
- name: dfq
  endpoint:
    path: /dfq
    method: GET
    data_selector: records
    params: {}
- name: df
  endpoint:
    path: /select
    method: GET
    data_selector: records
    params:
      columns: '[''A'', ''B'']'
- name: dftd
  endpoint:
    path: /append
    method: POST
    data_selector: records
    params:
      data_columns: true
- name: df_mi
  endpoint:
    path: /select
    method: GET
    data_selector: records
    params:
      foo: baz
      bar: two
- name: df_dc
  endpoint:
    path: /append
    method: POST
    data_selector: records
    params:
      data_columns: '[''B'', ''C'', ''string'', ''string2'']'
- name: dfeq
  endpoint:
    path: /append
    method: POST
    data_selector: records
    params:
      data_columns: '[''number'']'
- name: feather
  endpoint:
    path: /feather
    method: POST
    data_selector: data
    params: {}
- name: parquet
  endpoint:
    path: /parquet
    method: POST
    data_selector: data
    params: {}
- name: orc
  endpoint:
    path: /orc
    method: POST
    data_selector: data
    params: {}
- name: data
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: data
  endpoint:
    path: /read_csv
    method: GET
    data_selector: data
    params: {}
- name: dataframe_schema
  endpoint:
    path: /dataframe/schema
    method: GET
    data_selector: fields
- name: ExcelFile
  endpoint:
    path: /excel_file
    method: GET
    data_selector: records
    params: {}
- name: df
  endpoint:
    path: /df
    method: GET
    data_selector: records
- name: df_mixed
  endpoint:
    path: /df_mixed
    method: GET
    data_selector: records
- name: df_mi
  endpoint:
    path: /df_mi
    method: GET
    data_selector: records
- name: data
  endpoint:
    path: /data
    method: GET
    data_selector: records
    params: {}
- name: read_spss
  endpoint:
    path: /read_spss
    method: GET
    data_selector: DataFrame
    params: {}
- name: data
  endpoint:
    path: /data
    method: GET
    data_selector: records
    params: {}
- name: data_selection
  endpoint:
    path: /indexing-and-selecting-data
    method: GET
    data_selector: data
    params: {}
- name: dataframe_where
  endpoint:
    path: /api/dataframe/where
    method: GET
    data_selector: data
    params: {}
- name: series_where
  endpoint:
    path: /api/series/where
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
    params: {}
- name: dataframe_operations
  endpoint:
    path: /dataframe/operations
    method: GET
    data_selector: operations
    params: {}
- name: Indexing
  endpoint:
    path: /indexing
    method: GET
    data_selector: data
    params: {}
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: data
    params: {}
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
- name: dataframe_mask
  endpoint:
    path: /dataframe/mask
    method: GET
    data_selector: data
    params: {}
- name: dataframe_query
  endpoint:
    path: /dataframe/query
    method: GET
    data_selector: data
    params: {}
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: data
- name: Series
  endpoint:
    path: /indexing/series
    method: GET
- name: DataFrame
  endpoint:
    path: /indexing/dataframe
    method: GET
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: data
  endpoint:
    path: /path/to/data
    method: GET
    data_selector: data
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame
    method: GET
    data_selector: records
- name: query
  endpoint:
    path: /reference/api/pandas.DataFrame.query
    method: GET
    data_selector: records
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
- name: series
  endpoint:
    path: /series
    method: GET
    data_selector: records
- name: data_frame
  endpoint:
    path: /data/frame
    method: GET
    data_selector: records
    params: {}
- name: dataframe_operations
  endpoint:
    path: /operations/dataframe
    method: GET
    data_selector: operations
    params: {}
- name: Series
  endpoint:
    path: /indexing/series
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /indexing/dataframe
    method: GET
    data_selector: records
- name: data_frame
  endpoint:
    path: /services/data/vXX.X/sobjects/dfa
    method: GET
    data_selector: records
- name: data_frame
  endpoint:
    path: /data_frame
    method: GET
    data_selector: records
- name: series
  endpoint:
    path: /series
    method: GET
    data_selector: records
- name: slicing_with_labels
  endpoint:
    path: /slicing/with_labels
    method: GET
    data_selector: records
- name: selection_by_position
  endpoint:
    path: /selection/by_position
    method: GET
    data_selector: records
- name: selection_by_callable
  endpoint:
    path: /selection/by_callable
    method: GET
    data_selector: records
- name: data_frame
  endpoint:
    path: /data/frame
    method: GET
    data_selector: records
- name: series_selection
  endpoint:
    path: /series/selection
    method: GET
    data_selector: records
- name: dataframe_selection
  endpoint:
    path: /dataframe/selection
    method: GET
    data_selector: records
- name: index_objects
  endpoint:
    path: /reference/api/pandas.Index.html
    method: GET
    data_selector: Index
    params: {}
- name: duplicate_data
  endpoint:
    path: /reference/api/pandas.DataFrame.drop_duplicates.html
    method: GET
    data_selector: DataFrame
    params: {}
- name: dictionary_like_get
  endpoint:
    path: /reference/api/pandas.DataFrame.get.html
    method: GET
    data_selector: get
    params: {}
- name: set_operations
  endpoint:
    path: /reference/api/pandas.Index.html#set-operations
    method: GET
    data_selector: set_operations
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame
    method: GET
    data_selector: records
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: dataframe
  endpoint:
    path: /dataframe
    method: GET
    data_selector: data
    params: {}
- name: series
  endpoint:
    path: /indexing/series
    method: GET
    data_selector: data
    params: {}
- name: dataframe
  endpoint:
    path: /indexing/dataframe
    method: GET
    data_selector: data
    params: {}
- name: slicing_with_labels
  endpoint:
    path: /slicing/labels
    method: GET
    data_selector: records
- name: selection_by_position
  endpoint:
    path: /selection/position
    method: GET
    data_selector: records
- name: selection_by_callable
  endpoint:
    path: /selection/callable
    method: GET
    data_selector: records
- name: dataframe_operations
  endpoint:
    path: /dataframe/operations
    method: GET
    data_selector: records
- name: boolean_selection
  endpoint:
    path: /boolean/selection
    method: GET
    data_selector: records
- name: data_frame_operations
  endpoint:
    path: /data/frame/operations
    method: GET
    data_selector: records
- name: DataFrame
  endpoint:
    path: /query
    method: GET
    data_selector: records
    params: {}
- name: MultiIndex
  endpoint:
    path: /user_guide/advanced.html#multiindex-advanced-indexing
    method: GET
    data_selector: records
- name: rename
  endpoint:
    path: /reference/api/pandas.DataFrame.rename
    method: GET
    data_selector: records
    params: {}
- name: rename_axis
  endpoint:
    path: /reference/api/pandas.DataFrame.rename_axis
    method: GET
    data_selector: records
    params: {}
- name: set_names
  endpoint:
    path: /reference/api/pandas.Index.set_names
    method: GET
    data_selector: records
    params: {}
- name: swaplevel
  endpoint:
    path: /reference/api/pandas.MultiIndex.swaplevel
    method: GET
    data_selector: records
    params: {}
- name: reorder_levels
  endpoint:
    path: /reference/api/pandas.MultiIndex.reorder_levels
    method: GET
    data_selector: records
    params: {}
- name: interval_range
  endpoint:
    path: /reference/api/pandas.interval_range
    method: GET
    data_selector: IntervalIndex
    params: {}
- name: merge
  endpoint:
    path: /reference/api/pandas.merge.html
    method: GET
    data_selector: ''
    params: {}
- name: result
  endpoint:
    path: /resulting_keys
    method: GET
    data_selector: result
    params: {}
- name: append
  endpoint:
    path: /appending_rows_to_dataframe
    method: GET
    data_selector: result
    params: {}
- name: merge
  endpoint:
    path: /merge
    method: GET
    data_selector: result
    params: {}
- name: merge_ordered
  endpoint:
    path: /reference/api/pandas.merge_ordered.html
    method: GET
    data_selector: records
- name: merge_asof
  endpoint:
    path: /reference/api/pandas.merge_asof.html
    method: GET
    data_selector: records
- name: compare
  endpoint:
    path: /reference/api/pandas.Series.compare.html
    method: GET
    data_selector: records
- name: concat
  endpoint:
    path: /reference/api/pandas.concat.html
    method: GET
    data_selector: records
- name: DataFrame.join
  endpoint:
    path: /reference/api/pandas.DataFrame.join.html
    method: GET
    data_selector: records
- name: DataFrame.combine_first
  endpoint:
    path: /reference/api/pandas.DataFrame.combine_first.html
    method: GET
    data_selector: records
- name: merge
  endpoint:
    path: /reference/api/pandas.merge.html
    method: GET
    data_selector: records
- name: merge_ordered
  endpoint:
    path: /reference/api/pandas.merge_ordered.html
    method: GET
    data_selector: records
- name: merge_asof
  endpoint:
    path: /reference/api/pandas.merge_asof.html
    method: GET
    data_selector: records
- name: Series.compare
  endpoint:
    path: /reference/api/pandas.Series.compare.html
    method: GET
    data_selector: records
- name: DataFrame.compare
  endpoint:
    path: /reference/api/pandas.DataFrame.compare.html
    method: GET
    data_selector: records
- name: pivot_table
  endpoint:
    path: /api/pandas/pivot_table
    method: GET
    data_selector: records
    params: {}
- name: stack
  endpoint:
    path: /api/pandas/stack
    method: GET
    data_selector: records
    params: {}
- name: unstack
  endpoint:
    path: /api/pandas/unstack
    method: GET
    data_selector: records
    params: {}
- name: stack
  endpoint:
    path: /reference/api/pandas.DataFrame.stack
    method: GET
    data_selector: stack
    params: {}
- name: unstack
  endpoint:
    path: /reference/api/pandas.DataFrame.unstack
    method: GET
    data_selector: unstack
    params: {}
- name: pivot
  endpoint:
    path: /reference/api/pandas.pivot.html
    method: GET
    data_selector: pivot methods
- name: pivot_table
  endpoint:
    path: /reference/api/pandas.pivot_table.html
    method: GET
    data_selector: pivot_table methods
- name: stack
  endpoint:
    path: /reference/api/pandas.DataFrame.stack.html
    method: GET
    data_selector: stack methods
- name: unstack
  endpoint:
    path: /reference/api/pandas.DataFrame.unstack.html
    method: GET
    data_selector: unstack methods
- name: melt
  endpoint:
    path: /reference/api/pandas.melt.html
    method: GET
    data_selector: melt methods
- name: wide_to_long
  endpoint:
    path: /reference/api/pandas.wide_to_long.html
    method: GET
    data_selector: wide_to_long methods
- name: get_dummies
  endpoint:
    path: /reference/api/pandas.get_dummies.html
    method: GET
    data_selector: get_dummies methods
- name: from_dummies
  endpoint:
    path: /reference/api/pandas.from_dummies.html
    method: GET
    data_selector: from_dummies methods
- name: explode
  endpoint:
    path: /reference/api/pandas.Series.explode.html
    method: GET
    data_selector: explode methods
- name: crosstab
  endpoint:
    path: /reference/api/pandas.crosstab.html
    method: GET
    data_selector: crosstab methods
- name: cut
  endpoint:
    path: /reference/api/pandas.cut.html
    method: GET
    data_selector: cut methods
- name: factorize
  endpoint:
    path: /reference/api/pandas.factorize.html
    method: GET
    data_selector: factorize methods
- name: missing_data
  endpoint:
    path: /working_with_missing_data
    method: GET
    data_selector: missing_values
- name: CategoricalData
  endpoint:
    path: /categorical_data
    method: GET
    data_selector: records
    params: {}
- name: categorical_data
  endpoint:
    path: /categorical/data
    method: GET
    data_selector: records
    params: {}
- name: categories
  endpoint:
    path: /categories
    method: GET
    data_selector: records
    params: {}
- name: unique_values
  endpoint:
    path: /unique
    method: GET
    data_selector: records
    params: {}
- name: categorical_data
  endpoint:
    path: /categorical
    method: GET
    data_selector: data
    params: {}
- name: plot
  endpoint:
    path: /plot
    method: GET
    data_selector: data
- name: basic_plotting
  endpoint:
    path: /plot
    method: GET
    data_selector: records
- name: bar_plots
  endpoint:
    path: /bar_plot
    method: GET
    data_selector: records
- name: histograms
  endpoint:
    path: /histogram
    method: GET
    data_selector: records
- name: box_plots
  endpoint:
    path: /box_plot
    method: GET
    data_selector: records
- name: Styler
  endpoint:
    path: /reference/api/pandas.io.formats.style.Styler.html
    method: GET
    data_selector: Styler
    params: {}
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
    data_selector: DataFrame
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: speeds
  endpoint:
    path: /groupby/speeds
    method: GET
    data_selector: max_speed
    params: {}
- name: data_df
  endpoint:
    path: /groupby/data_df
    method: GET
    data_selector: records
    params: {}
- name: groupby
  endpoint:
    path: /groupby
    method: GET
    data_selector: records
    params: {}
- name: groupby
  endpoint:
    path: /user_guide/groupby.html
    method: GET
    data_selector: content
    params: {}
- name: transform
  endpoint:
    path: /reference/api/pandas.core.groupby.DataFrameGroupBy.transform
    method: GET
    data_selector: transform
    params: {}
- name: groupby_examples
  endpoint:
    path: /groupby/examples
    method: GET
    data_selector: examples
    params: {}
- name: windowing_operations
  endpoint:
    path: /windowing_operations
    method: GET
    data_selector: operations
    params: {}
- name: ExponentiallyWeightedMean
  endpoint:
    path: /api/v1/ewm/mean
    method: POST
    data_selector: result
    params:
      halflife: 4 days
      ignore_na: false
- name: datetime_conversion
  endpoint:
    path: /datetime/conversion
    method: POST
    data_selector: results
    params: {}
- name: datetime_conversion
  endpoint:
    path: /datetime/conversion
    method: POST
    data_selector: results
    params: {}
- name: datetime_conversion
  endpoint:
    path: /datetime_conversion
    method: GET
    data_selector: records
    params: {}
- name: date_conversion
  endpoint:
    path: /datetime/conversion
    method: POST
    data_selector: results
    params: {}
- name: timestamp_generation
  endpoint:
    path: /datetime/generate
    method: POST
    data_selector: results
    params: {}
- name: PeriodIndex
  endpoint:
    path: /periodindex
    method: GET
    data_selector: records
    params: {}
- name: datetime_conversion
  endpoint:
    path: /datetime/conversion
    method: GET
    data_selector: records
- name: epoch_timestamps
  endpoint:
    path: /epoch/timestamps
    method: GET
    data_selector: records
- name: asfreq
  endpoint:
    path: /asfreq
    method: GET
    data_selector: records
    params: {}
- name: resample
  endpoint:
    path: /resample
    method: GET
    data_selector: records
    params: {}
- name: period_dtype
  endpoint:
    path: /period_dtype
    method: GET
    data_selector: periods
- name: partial_string_indexing
  endpoint:
    path: /partial_string_indexing
    method: GET
    data_selector: partial_index
- name: frequency_conversion
  endpoint:
    path: /frequency_conversion
    method: GET
    data_selector: frequency
- name: time_zone_handling
  endpoint:
    path: /time_zone_handling
    method: GET
    data_selector: time_zones
- name: timedelta
  endpoint:
    path: /timedeltas
    method: GET
    data_selector: timedelta_records
- name: DataFrame
  endpoint:
    path: /reference/api/pandas.DataFrame.html
    method: GET
    data_selector: DataFrame methods
    params: {}
- name: Series
  endpoint:
    path: /reference/api/pandas.Series.html
    method: GET
    data_selector: Series methods
    params: {}
- name: SparseArray
  endpoint:
    path: /reference/api/pandas.arrays.SparseArray
    method: GET
    data_selector: Sparse[float64, nan]
- name: SparseDtype
  endpoint:
    path: /reference/api/pandas.SparseDtype
    method: GET
    data_selector: Sparse[float64, nan]
- name: DataFrame_memory_usage
  endpoint:
    path: /reference/api/pandas.DataFrame.memory_usage.html
    method: GET
    data_selector: memory_usage
- name: DataFrame_info
  endpoint:
    path: /reference/api/pandas.DataFrame.info.html
    method: GET
    data_selector: info
notes:
- You are highly encouraged to install these libraries, as they provide speed improvements,
  especially when working with large data sets.
- The easiest way to install pandas is to install it as part of the Anaconda distribution.
- It is recommended to install and run pandas from a virtual environment.
- pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked
  in Cython code.
- This feature is experimental, and the API can change in a future release without
  warning.
- Uses MultiIndex for hierarchical data representation
- 'Titles: Note : level and axis are optional, and default to zero'
- Changes are only written to disk when the HDFStore is closed.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Using a raw binary file format like this for general data storage is not recommended,
  as it is not cross platform.
- Adding a column to a DataFrame is relatively fast. However, adding a row requires
  a copy, and may be expensive.
- If you are attempting to perform a boolean operation on a Series or DataFrame you
  might see an exception.
- pandas supports non-unique index values. If an operation that does not support duplicate
  index values is attempted, an exception will be raised at that time.
- NaN (not a number) is the standard missing data marker used in pandas.
- In general, we chose to make the default result of operations between differently
  indexed objects yield the union of the indexes in order to avoid loss of information.
- assign() always returns a copy of the data, leaving the original DataFrame untouched.
- All methods have a `skipna` option signaling whether to exclude missing data (`True`
  by default).
- Methods like `cumsum()` and `cumprod()` preserve the location of `NaN` values.
- Upcasting occurs during dtype conversion when using loc()
- Series.dt will raise a TypeError if you access with a non-datetime-like values.
- Includes methods for dropping labels, renaming labels, and iterating over DataFrames.
- Allows multiple operations at the same time rather than one-by-one.
- Not all functions can be vectorized.
- You should never modify something you are iterating over.
- This is not guaranteed to work in all cases. Depending on the data types, the iterator
  returns a copy and not a view, and writing to it will have no effect!
- Because iterrows() returns a Series for each row, it does not preserve dtypes across
  the rows (dtypes are preserved across columns for DataFrames).
- To preserve dtypes while iterating over the rows, it is better to use itertuples()
  which returns namedtuples of the values and which is generally much faster than
  iterrows().
- The column names will be renamed to positional names if they are invalid Python
  identifiers, repeated, or start with an underscore. With a large number of columns
  (>255), regular tuples are returned.
- Iterating through pandas objects is generally slow.
- This is not guaranteed to work in all cases.
- When trying to convert a subset of columns to a specified type using astype() and
  loc(), upcasting occurs.
- loc() tries to fit in what we are assigning to the current dtypes, while [] will
  overwrite them taking the dtype from the right hand side.
- Flexible comparisons supported
- Arithmetic functions support fill_value
- If a string matches both a column name and an index level name then a warning is
  issued and the column takes precedence.
- This is not guaranteed to work in all cases. Depending on the data types, the iterator
  returns a copy and not a view, and writing to it will have no effect.
- Upcasting occurs when converting dtypes.
- The transform() method allows multiple operations at the same time.
- By default integer types are int64 and float types are float64, regardless of platform
  (32-bit or 64-bit).
- Types can potentially be upcasted when combined with other types.
- Basic iteration produces values for Series and column labels for DataFrames.
- Basic iteration over Series produces values; DataFrame produces column labels.
- Iterating over dataframes with iterrows() does not preserve dtypes across rows.
- Upcasting occurs when converting dtypes in pandas.
- The dtype_backends are still experimental.
- Added in version 2.0.
- 'Default behavior is to infer the column names: if no names are passed the behavior
  is identical to header=0 and column names are inferred from the first non-blank
  line of the file, if column names are passed explicitly then the behavior is identical
  to header=None.'
- If `na_filter` is passed in as `False`, the `keep_default_na` and `na_values` parameters
  will be ignored.
- Detect missing value markers (empty strings and the value of na_values). In data
  without any NAs, passing `na_filter=False` can improve the performance of reading
  a large file.
- The `usecols` argument allows you to select any subset of the columns in a file,
  either using the column names, position numbers or a callable.
- The `usecols` argument can also be used to specify which columns not to use in the
  final result.
- By default, completely blank lines will be ignored as well.
- If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines.
- The presence of ignored lines might create ambiguities involving line numbers.
- Sometimes comments or meta data may be included in a file.
- The workhorse function for reading text files is read_csv()
- For examples that use the StringIO class, make sure you import it with from io import
  StringIO for Python 3.
- Read CSV files into DataFrame
- Handles comments and blank lines by default
- To control which values are parsed as missing values (which are signified by NaN),
  specify a string in na_values.
- If you specify a list of strings, then all values in it are considered to be missing
  values.
- If you specify a number (a float, like 5.0 or an integer like 5), the corresponding
  equivalent values will also imply a missing value.
- To completely override the default values that are recognized as missing, specify
  keep_default_na=False.
- The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
  '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN',
  'nan', '-nan', 'None', ''].
- The pandas I/O API is a set of top level reader functions accessed.
- A fast-path exists for iso8601-formatted dates.
- Setting dtype_backend="numpy_nullable" will result in nullable dtypes for every
  column.
- 'Default behavior is to infer the column names: if no names are passed the behavior
  is identical to header=0.'
- pandas cannot natively represent a column or index with mixed timezones.
- If your CSV file contains columns with a mixture of timezones, the default result
  will be an object-dtype column with strings.
- Can handle comments and empty lines when parsing CSV.
- Date parsing can be customized using parse_dates and date_format.
- Timings are machine dependent and small differences should be ignored.
- Added in version 1.3.0.
- If an index_col is not specified, then any names on the columns index will be lost.
- If `na_filter` is passed in as `False`, the `keep_default_na` and na_values parameters
  will be ignored.
- Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for every
  column.
- If both `header` and `skiprows` are specified, `header` will be relative to the
  end of `skiprows`.
- Uses Excel dialect by default
- Handles quotes and escape characters using options like escapechar
- To parse the mixed-timezone values as a datetime column, read in as object dtype
  and then call to_datetime() with utc=True.
- Format inference is sensitive to dayfirst.
- If pandas fails to guess the format, a warning will be raised.
- The safest way to parse dates is to explicitly set format=.
- To control which values are parsed as missing values, specify a string in na_values.
- inf like values will be parsed as np.inf and -inf as -np.inf.
- The common values True, False, TRUE, and FALSE are all recognized as boolean.
- You can elect to skip bad lines.
- Widths are a list of integers
- The parser will raise one of ValueError/TypeError/AssertionError if the JSON is
  not parseable.
- Handles CSV reading with multiple options including ignoring comments and parsing
  dates.
- Uses `pd.read_csv` for reading CSV files
- The `thousands` keyword allows integers to be parsed correctly
- Handles mixed datetime formats with `format='mixed'`
- Handles bad lines with `on_bad_lines` parameter
- By default, read_csv uses the Excel dialect.
- read_csv can infer delimited files by specifying sep=None.
- Or specify that all timestamps are in nanoseconds
- All dates are converted to UTC when serializing.
- A primaryKey field, containing an array of labels, is included if the index is unique.
- There are some versioning issues surrounding the libraries that are used to parse
  HTML tables in the top-level pandas io function `read_html`.
- DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
  using the Styler.to_latex() method over DataFrame.to_latex() due to the former’s
  greater flexibility with conditional styling, and the latter’s possible future deprecation.
- Data from the URL changes every Monday
- The lxml backend will raise an error on a failed parse if that is the only parser
  you provide
- Uses lxml for parsing and transforming XML documents
- Supports memory-efficient parsing for large XML files with iterparse
- Uses openpyxl for reading Excel 2007+ files
- Excel 2003 files can be read using xlrd
- Binary Excel files can be read using pyxlsb
- Added in version 1.2.0.
- The `comment` parameter allows completely commented lines to be ignored. By default,
  completely blank lines will be ignored as well.
- The `encoding` argument should be used for encoded unicode data, which will result
  in byte strings being decoded to unicode in the result.
- Ordinarily, you can achieve the index column behavior using the `index_col` option.
- It is important to remember that if multiple text columns are to be parsed into
  a single date column, then a new column is prepended to the data.
- By default, read_csv uses the Excel dialect and treats the double quote as the quote
  character.
- The parser will take care of extra white spaces around the columns.
- Added in version 2.0.0.
- 'Options unsupported by the C and pyarrow engines include: sep other than a single
  character, skipfooter, sep=None with delim_whitespace=False.'
- Specifying unsupported options will produce a ParserWarning unless the python engine
  is selected explicitly.
- For very large XML files that can range in hundreds of megabytes to gigabytes, pandas.read_xml
  supports parsing such sizeable files using lxml’s iterparse and etree’s iterparse
  which are memory-efficient methods to iterate through an XML tree and extract specific
  elements and attributes without holding entire tree in memory.
- The top-level read_xml() function can accept an XML string/file/URL and will parse
  nodes and attributes into a pandas DataFrame.
- The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv()
  that generally return a pandas object.
- Files with a .xlsx extension will be written using xlsxwriter (if available) or
  openpyxl.
- 'pandas chooses an Excel writer via two methods: the engine keyword argument and
  the filename extension.'
- If you try to parse a column of date strings, pandas will attempt to guess the format
  from the first non-NaN element.
- Not all of the possible options for DataFrame.to_html are shown here for brevity.
- Currently there are no methods to read from LaTeX, only output methods.
- C and pyarrow engines are faster, while the python engine is more feature-complete.
- The optional dependency ‘python-calamine’ needs to be installed.
- Currently pandas only supports reading binary Excel files. Writing is not implemented.
- Loading pickled data received from untrusted sources can be unsafe.
- read_pickle() is only guaranteed backwards compatible back to a few minor release.
- Uses openpyxl for .xlsx files and xlrd for .xls files
- When engine=None, pandas will determine the engine based on file extension
- 'Warning: Loading pickled data received from untrusted sources can be unsafe.'
- If `True` and parse_dates is enabled for a column, attempt to infer the datetime
  format to speed up the processing.
- Reader and writer functions are accessed through pandas.
- Supports reading CSV files into DataFrame objects
- Handles various parameters for data parsing and manipulation
- To parse mixed-timezone values as a datetime column, read in as object dtype and
  then call to_datetime() with utc=True.
- Uses JSON schema for tabular datasets
- Line-delimited JSON supported
- Data from the URL changes every Monday.
- Default behavior of read_csv is to infer delimiter.
- read_fwf infers column specifications from the first 100 rows.
- There are some versioning issues surrounding the libraries that are used to parse
  HTML tables in the top-level pandas io function read_html.
- Some browsers may not show a difference in the rendering of the previous two HTML
  tables.
- You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use
  clipboard methods.
- Supports reading and writing Excel files.
- Can handle multiple sheets in a single file.
- HDFStore will by default not drop rows that are all missing.
- A fixed format will raise a TypeError if you try to retrieve using a where.
- Uses `usecols` to filter selected columns
- The `comment` parameter can ignore commented lines
- The `encoding` argument should be used for encoded unicode data
- Index columns can be specified using `index_col`
- Date columns can be specified using `parse_dates`
- Handles quotes and escape characters in embedded fields
- Automatically infers column specifications for fixed-width files
- Uses Table Schema for describing tabular datasets.
- For very large XML files, supports parsing using lxml’s iterparse or etree’s iterparse.
- Since there is no standard XML structure, read_xml works best with flatter, shallow
  versions.
- Uses pandas for reading and writing Excel files
- Supports multiple engines for different Excel file formats
- Hierarchical keys cannot be retrieved as dotted (attribute) access.
- Currently pandas only supports *reading* binary Excel files. Writing is not implemented.
- You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use
  these methods.
- This format is specified by format='table' or format='t' to append or put or to_hdf.
- You can also create a table by passing format='table' or format='t' to a put operation.
- we have provided a minimum string column size
- 'Default behavior is to infer the column names: if no names are passed the behavior
  is identical to `header=0` and column names are inferred from the first non-blank
  line of the file.'
- Uses mixed timezone handling when parsing dates
- Parsing with format='mixed' is available for mixed datetime formats
- The parser will take care of extra white spaces around the columns so it’s ok to
  have extra separation between the columns in the file.
- Added in version 1.5.0.
- Not all of the possible options for DataFrame.to_html are shown here for brevity’s
  sake.
- xpath identifies the parent of content to be parsed, only immediate descendants
  which include child nodes or current attributes are parsed.
- lxml can transform original nested document into a flatter output for easier parse
  into DataFrame.
- pandas support for msgpack has been removed in version 1.0.0.
- The `sheet_names` property will generate a list of the sheet names in the file.
- If the same parsing parameters are used for all sheets, a list of sheet names can
  simply be passed to `read_excel` with no loss in performance.
- The files test.pkl.compress, test.parquet and test.feather took the least space
  on disk.
- select and delete operations have an optional criterion that can be specified to
  select/delete only a subset of the data.
- Valid comparison operators are =, ==, !=, >, >=, <, <=.
- 'Valid boolean expressions are combined with | : or, & : and, ( and ) : for grouping.'
- Passing a string to a query by interpolating it into the query expression is not
  recommended.
- Creating a table index is highly encouraged.
- HDFStore supports another PyTables format on disk, the table format.
- A table may be appended to in the same or other sessions. In addition, delete and
  query type operations are supported.
- Keys to a store can be specified as a string. These can be in a hierarchical path-name
  like format.
- Removal operations can remove everything in the sub-store and below, so be careful.
- The index keyword is reserved and cannot be used as a level name.
- Performance comparisons of various IO methods using pandas
- Functions for testing write and read speeds for different formats
- Uses `to_datetime()` with utc=True to parse mixed-timezone values.
- The `usecols` argument allows you to select any subset of the columns in a file.
- The `comment` parameter can be specified to ignore completely commented lines.
- Since xpath identifies the parent of content to be parsed, only immediate descendants
  which include child nodes or current attributes are parsed.
- For very large XML files that can range in hundreds of megabytes to gigabytes, pandas.read_xml()
  supports parsing such sizeable files using lxml’s iterparse and etree’s iterparse.
- XML documents can have namespaces with prefixes and default namespaces without prefixes.
- select will raise a ValueError if the query expression has an unknown variable reference.
- select will raise a SyntaxError if the query expression is not valid.
- Supports reading and writing Excel files with various engines.
- Can read OpenDocument spreadsheets using odfpy.
- This store must be selected in its entirety
- The index keyword is reserved and cannot be use as a level name.
- HDFStore is not-threadsafe for writing. The underlying PyTables only supports concurrent
  reads.
- If you use locks to manage write access between multiple processes, consider using
  fsync() before releasing write locks.
- Once a table is created, columns are fixed; only exactly the same columns can be
  appended.
- Be aware that timezones may not be equal across timezone versions.
- Combining date columns inside read_csv is deprecated. Use pd.to_datetime on the
  relevant result columns instead.
- Uses `to_datetime()` with `utc=True` to parse mixed-timezone values.
- Format inference is sensitive to `dayfirst`.
- The top three functions in terms of speed for writing are test_feather_write, test_hdf_fixed_write,
  and test_hdf_fixed_write_compress.
- The top three functions in terms of speed for reading are test_feather_read, test_pickle_read,
  and test_hdf_fixed_read.
- Uses read_csv function to read delimited files
- Requires the `pyarrow` package for pyarrow engine.
- Complex URLs for accessing data in compressed archives are supported.
- XPath does not require namespaces if it does not reference node names such as default
  or /*.
- Immediate descendants including child nodes or current attributes are parsed, not
  grandchildren.
- For very large XML files, pandas.read_xml supports parsing using lxml’s iterparse
  and etree’s iterparse, which are memory-efficient methods.
- The top-level read_xml() function can accept an XML string/file/URL.
- Read in the content of XML as instance of StringIO or BytesIO.
- Uses pandas to read and write Excel files
- Supports multiple Excel formats including .xlsx, .xls, and .ods
- select will raise a ValueError if the query expression has an unknown variable reference.
  Usually this means that you are trying to select on a column that is **not** a data_column.
- String columns will serialize a np.nan (a missing value) with the nan_rep string
  representation. This defaults to the string value nan.
- You could inadvertently turn an actual nan value into a missing value.
- tables format come with a writing performance penalty as compared to fixed stores.
- You can pass chunksize=<int> to append, specifying the write chunksize (default
  is 50000).
- You can pass expectedrows=<int> to the first append, to set the TOTAL number of
  rows that PyTables will expect.
- Duplicate rows can be written to tables, but are filtered out in selection.
- A PerformanceWarning will be raised if you are attempting to store types that will
  be pickled by PyTables.
- HDF5 does not reclaim space automatically on deletes.
- Use ptrepack to reorganize and clean the file.
- Uses SQLAlchemy for database connections
- ADBC drivers provide improved performance
- If `parse_dates` is True, attempt to infer the datetime format to speed up processing.
- Periods are converted to timestamps before serialization.
- Categoricals use the any type and an enum constraint.
- For very large XML files that can range in hundreds of megabytes to gigabytes, pandas.read_xml()
  supports parsing such sizeable files using lxml’s iterparse and etree’s iterparse
  which are memory-efficient methods to iterate through an XML tree and extract specific
  elements and attributes without holding entire tree in memory.
- The read_excel() method can read Excel 2007+ (.xlsx) files using the openpyxl Python
  module.
- Excel 2003 (.xls) files can be read using xlrd.
- Binary Excel (.xlsb) files can be read using pyxlsb.
- select will raise a ValueError if the query expression has an unknown variable reference.
  Usually this means that you are trying to select on a column that is not a data_column.
- HDFStore is not-threadsafe for writing.
- PyTables will show a NaturalNameWarning if a column name cannot be used as an attribute
  selector.
- HDFStore supports a table format for appending and querying data.
- With some databases, writing large DataFrames can result in errors due to packet
  size limitations being exceeded.
- For the best odds at preserving database types users are advised to use ADBC drivers
  when available.
- SQL queries can utilize ADBC drivers for optimal performance
- Feather format does not write index or MultiIndex
- Parquet supports partitioning of data based on column values
- The function read_sql() is a convenience wrapper around read_sql_table() and read_sql_query()
  and will delegate to specific function depending on the provided input.
- The workhorse function for reading text files is read_csv.
- read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g “2000-01-01T00:01:02+00:00”
  and similar variations.
- Uses Excel dialect by default for CSV files.
- Quotes in embedded fields can be handled using escape characters.
- To parse mixed-timezone values as a datetime column, read in as object dtype and
  then call `to_datetime()` with `utc=True`.
- If pandas fails to guess the format, a warning will be raised and each row will
  be parsed individually by `dateutil.parser.parse`.
- Uses lxml for parsing large XML files efficiently
- For large XML files, use iterparse for memory efficiency
- The 'engine' parameter is optional but recommended when writing Excel files.
- pandas supports reading and writing OpenDocument spreadsheets using the odfpy module.
- Once a table is created, columns are fixed.
- HDFStore supports table format for appending and querying.
- Hierarchical keys are absolute and cannot be accessed via dotted notation.
- Table names do not need to be quoted if they have special characters.
- When you open a connection to a database you are also responsible for closing it.
- The pandas.io.sql module provides a collection of query wrappers to facilitate data
  retrieval.
- When writing timezone aware data to databases that do not support timezones, the
  data will be written as timezone naive timestamps that are in local time with respect
  to the timezone.
- When reading TIMESTAMP WITH TIME ZONE types, pandas will convert the data to UTC.
- ADBC drivers will map database types directly back to arrow types.
- For other drivers note that pandas infers column dtypes from query outputs.
- Uses the Excel dialect by default
- Can specify the `dialect` keyword for different file formats
- Automatically infers delimiter when `sep=None` is specified
- Large integer values may be converted to dates if convert_dates=True and the data
  and / or column labels appear ‘date-like’.
- When reading JSON data, automatic coercing into dtypes has some quirks.
- Uses read_xml() function to parse XML strings, files, or URLs.
- For deeply nested XML documents, use the stylesheet feature for transformation.
- All XML documents adhere to W3C specifications.
- With very large XML files, XPath and XSLT can become memory-intensive operations.
- If you need reading and writing at the same time, you need to serialize these operations
  in a single thread.
- Hierarchical keys cannot be retrieved as dotted access.
- For best performance, use Apache Arrow ADBC drivers where available.
- Due to the limited support for timedelta’s in the different database flavors, columns
  with type timedelta64 will be written as integer values as nanoseconds to the database
  and a warning will be raised.
- Stata data files have limited data type support; only strings with 244 or fewer
  characters, int8, int16, int32, float32 and float64 can be stored in .dta files.
- Exporting a non-missing value that is outside of the permitted range in Stata for
  a particular data type will retype the variable to the next larger size.
- It is not possible to export missing data values for integer data types.
- Conversion from int64 to float64 may result in a loss of precision if int64 values
  are larger than 2**53.
- 'Setting preserve_dtypes=False will upcast to the standard pandas data types: int64
  for all integer types and float64 for floating point data.'
- Ensure you have installed the minimum supported PyArrow version.
- Accessing via label slices is supported.
- The axis labeling information in pandas objects serves many purposes.
- Allows intuitive getting and setting of subsets of the data set.
- Out of range slice indexes are handled gracefully just as in Python/NumPy.
- The 'where' method preserves the shape of the original data.
- Supports basic DataFrame operations.
- The Python and NumPy indexing operators `[]` and attribute operator `.` provide
  quick and easy access to pandas data structures across a wide range of use cases.
- None of the indexing functionality is time series specific unless specifically stated.
- in and not in are evaluated in Python, since numexpr has no equivalent of this operation.
- only the in/not in expression itself is evaluated in vanilla Python.
- Comparing a list of values to a column using ==/!= works similarly to in/not in.
- You can negate boolean expressions with the word not or the ~ operator.
- Pandas operates with DataFrames and Series for data manipulation.
- '`in` and `not in` are evaluated in Python, since `numexpr` has no equivalent of
  this operation.'
- Only the `in`/`not in` expression itself is evaluated in vanilla Python.
- Uses numpy for conditional settings in DataFrames
- This documentation covers indexing and selection methods in pandas.
- Uses pandas for data manipulation
- Supports boolean indexing and scalar value access
- Uses integer-based indexing with 0-based index.
- Note that `in` and `not in` are evaluated in Python, since `numexpr` has no equivalent
  of this operation.
- However, **only the** `in`/`not in` **expression itself** is evaluated in vanilla
  Python.
- Be cautious of chained assignment.
- in and not in are evaluated in Python, since numexpr has no equivalent of this operation
- only the in/not in expression itself is evaluated in vanilla Python
- Uses a DataFrame structure for data representation
- Index should be a valid Python identifier
- Supports selection by label and position.
- Includes methods for handling out of range indexes.
- In general, any operations that can be evaluated using `numexpr` will be.
- Duplicates are allowed in Index objects.
- Index objects can infer dtype from data.
- Index.fillna can fill missing values.
- Index elements must be valid Python identifiers.
- The primary focus will be on Series and DataFrame as they have received more development
  attention in this area.
- Even though Index can hold missing values (NaN), it should be avoided if you do
  not want any unexpected results.
- 'Warning: Copy-on-Write will become the new default in pandas 3.0.'
- Reshaping and Comparison operations on a CategoricalIndex must have the same categories
  or a TypeError will be raised.
- The take method on pandas objects is not intended to work on boolean indices and
  may return unexpected results.
- Index.is_monotonic_increasing and Index.is_monotonic_decreasing only check that
  an index is weakly monotonic. To check for strict monotonicity, you can combine
  one of those with the is_unique() attribute.
- Copy-on-Write can be enabled through the configuration option `copy_on_write`.
- 'The option can be turned on globally through either of the following: pd.set_option("mode.copy_on_write",
  True) or pd.options.mode.copy_on_write = True.'
- Copy-on-Write will become the default in pandas 3.0.
- Copy-on-Write was first introduced in version 1.5.0.
- Starting from version 2.0 most of the optimizations that become possible through
  CoW are implemented and supported.
- All possible optimizations are supported starting from pandas 2.1.
- concat() makes a full copy of the data, and iteratively reusing concat() can create
  unnecessary copies.
- When concatenating DataFrame with named axes, pandas will attempt to preserve these
  index/column names whenever possible.
- Unstacking can result in missing values if subgroups do not have the same set of
  labels.
- By default, missing values will be replaced with the default fill value for that
  data type.
- StringArray is currently considered experimental. The implementation and parts of
  the API may change without warning.
- The behaviour of `NA` can still change without warning.
- Currently, ufuncs involving an ndarray and NA will return an object-dtype filled
  with NA values.
- This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels
  value. In future versions it is expected that every method taking or returning one
  or more DataFrame or Series objects will propagate allows_duplicate_labels.
- Index objects are not required to be unique; you can have duplicate row or column
  labels.
- Some pandas methods (`Series.reindex()` for example) just don’t work with duplicates
  present.
- Checking whether an index is unique is somewhat expensive for large datasets.
- New categorical data are not automatically ordered. You must explicitly pass ordered=True
  to indicate an ordered Categorical.
- The result of unique() is not always the same as Series.cat.categories, because
  Series.unique() has a couple of guarantees, namely that it returns categories in
  the order of appearance, and it only includes values that are actually present.
- In contrast to R’s factor, categorical data can have categories of other types than
  string.
- Categories must be unique or a ValueError is raised.
- Categories must also not be NaN or a ValueError is raised.
- Constructing a Series from a Categorical will not copy the input Categorical.
- Using copy=True prevents changes in the original Categorical.
- The is in contrast to R’s `factor` function, where `factor(c(1,2,3))[1]` returns
  a single value `factor`.
- Categorical data has a specific category dtype.
- New categorical data are not automatically ordered.
- The accessors .dt and .str will work if the s.cat.categories are of an appropriate
  type
- Merges that result in non-categorical dtypes will likely have higher memory usage
- Categorical data must have unique categories
- Categories cannot be null
- If the number of categories approaches the length of the data, the `Categorical`
  will use nearly the same or more memory than an equivalent `object` dtype representation.
- The memory usage of a Categorical is proportional to the number of categories plus
  the length of the data.
- Categorical data can have categories of other types than string.
- Uses categorical data types to optimize memory usage.
- Categorical data and the underlying Categorical is implemented as a Python object
  and not as a low-level NumPy array dtype.
- Categorical variables take on a limited, and usually fixed, number of possible values.
- Categorical data might have an order but numerical operations are not possible.
- Constructing a Series from a Categorical will not copy the input Categorical. This
  means that changes to the Series will in most cases change the original Categorical.
- Use copy=True to prevent such a behaviour or simply don’t reuse Categoricals.
- Currently, categorical data and the underlying Categorical is implemented as a Python
  object and not as a low-level NumPy array dtype.
- Using NumPy functions on a Series of type category should not work as Categorials
  are not numeric data (even in the case that .categories is numeric).
- pandas currently does not preserve the dtype in apply functions.
- IntegerArray is currently experimental. Its API or implementation may change without
  warning.
- Uses pandas.NA as the missing value.
- BooleanArray is currently experimental. Its API or implementation may change without
  warning.
- Uses standard matplotlib API for visualization
- The 'Iris' dataset is available at https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv.
- Custom formatters for timeseries plots change the formatting of the axis labels
  for dates and times.
- Pandas includes automatic tick resolution adjustment for regular frequency time-series
  data.
- Handles NaN values in various ways depending on plot type
- Handles NaN automatically depending on plot type.
- pandas can be extended with third-party plotting backends.
- Table styles cannot be exported to Excel.
- Some styles have been overwritten by others.
- Adding tooltips since version 1.3.0 can be done using the .set_tooltips() method.
- Only label-based slicing is supported right now, not positional, and not callables.
- The Styler is primarily designed for small data.
- Default `dropna` is set to True, which will exclude NaNs in keys
- Default dropna is set to True, which will exclude NaNs in keys
- Users are encouraged to use the shorthand, agg. It will operate as if the corresponding
  method was called.
- Aggregating with a UDF is often less performant than using the pandas built-in methods
  on GroupBy.
- In general, the output column names should be unique, but pandas will allow you
  apply to the same function (or two functions with the same name) to the same column.
- Aggregation functions will not return the groups that you are aggregating over as
  named columns when as_index=True, the default.
- Passing as_index=False will return the groups that you are aggregating over as named
  columns, regardless if they are named indices or columns in the inputs.
- Supports grouping by multiple levels.
- Aggregation methods include sum, mean, max, etc.
- Passing as_index=False will return the groups that you are aggregating over as named
  columns.
- Split-apply-combine process for grouping data
- Aggregation, transformation, and filtration operations available
- Transforming by supplying transform with a UDF is often less performant than using
  the built-in methods on GroupBy.
- The split step is the most straightforward.
- An operation that is split into multiple steps using built-in GroupBy operations
  will be more efficient than using the apply method with a user-defined Python function.
- Examples demonstrate grouping and aggregation functionalities in pandas.
- The inclusion of the interval endpoints in rolling window calculations can be specified
  with the closed parameter.
- For some problems knowledge of the future is available for analysis.
- Numba will be applied in potentially two routines.
- Windowing operations currently only support numeric data (integer and float) and
  will always return float64 values.
- Some windowing aggregation, mean, sum, var and std methods may suffer from numerical
  imprecision due to the underlying windowing algorithms accumulating sums.
- Some windowing aggregation methods may suffer from numerical imprecision due to
  the underlying windowing algorithms.
- pandas contains extensive capabilities and features for working with time series
  data for all domains.
- While pandas does not force you to have a sorted date index, some of these methods
  may have unexpected or incorrect behavior if the dates are unsorted.
- The default behavior, errors='raise', is to raise when unparsable data is encountered.
- Passing errors='coerce' converts unparsable data to NaT.
- The default behavior, errors='raise', is to raise when unparsable.
- Conversion of float epoch times can lead to inaccurate and unexpected results.
- Supports custom frequency date ranges using weekmask and holidays parameters.
- DatetimeIndex resolution cannot be less precise than day.
- The `normalize` option will be effective for addition and subtraction.
- They also observe International Workers' Day so let's add that for a couple of years
- Uses rollforward() and rollback() methods for moving a date forward or backward
  to a valid offset date.
- Operations preserve time (hour, minute, etc.) information by default.
- DatetimeIndex can be used like a regular index and offers all of its intelligent
  functionality like selection, slicing, etc.
- The default behavior, errors='raise', raises an error when unparsable data is encountered.
- Using errors='coerce' converts unparsable data to NaT.
- BusinessHour's valid offset dates are Monday through Friday
- Uses CustomBusinessHour which skips specified custom holidays.
- Functions like date_range() will only return valid timestamps between start_date
  and end_date.
- Monday is skipped because it's a holiday
- business hour starts from 10:00
- Uses custom frequency ranges for date generation
- DatetimeIndex class contains optimizations for time series data
- Partial string indexing works on DataFrame with MultiIndex
- Monday is skipped because it's a holiday, business hour starts from 10:00
- Uses datetime indexing for efficient time series data handling
- DatetimeIndex can be used like a regular index and offers advanced time series specific
  methods
- Default behavior for pd.to_datetime is errors='raise'.
- Monday is skipped because it's a holiday, business hour starts from 10:00.
- No authentication required.
- Date ranges can be generated with various frequencies.
- Supports partial string slicing with non-monotonic indexes.
- Timestamp limitations apply for data outside of `Timestamp` bounds.
- pandas supports extensive capabilities and features for working with time series
  data for all domains.
- pandas allows you to capture both representations and convert between them.
- The default behavior, `errors='raise'`, is to raise when unparsable.
- Epoch times will be rounded to the nearest nanosecond.
- Indexing a DatetimeIndex with a partial string depends on the accuracy of the period.
- When using pytz time zones, DatetimeIndex will construct a different time zone object
  than a Timestamp for the same time zone input.
- Be wary of conversions between libraries. For some time zones, pytz and dateutil
  have different definitions of the zone.
- Be aware that a time zone definition across versions of time zone libraries may
  not be considered equal.
- For pytz time zones, it is incorrect to pass a time zone object directly into the
  datetime.datetime constructor.
- Be aware that for times in the future, correct conversion between time zones (and
  UTC) cannot be guaranteed by any time zone library because a timezone’s offset from
  UTC may be changed by the respective government.
- If you are using dates beyond 2038-01-18, due to current deficiencies in the underlying
  libraries caused by the year 2038 problem, daylight saving time adjustments to timezone
  aware dates will not be applied.
- Aliases A, H, T, S, L, U, and N are deprecated in favour of the aliases Y, h, min,
  s, ms, us, and ns.
- The `resample` function allows for frequency conversion and aggregation on time
  series data.
- Backward resample added in version 1.3.0.
- A DST transition may also shift the local time ahead by 1 hour creating nonexistent
  local times ('clocks spring forward').
- The behavior of localizing a timeseries with nonexistent times can be controlled
  by the `nonexistent` argument.
- Supports partial string indexing with non-monotonic indexes.
- Be wary of conversions between libraries. For some time zones, `pytz` and `dateutil`
  have different definitions of the zone.
- For `pytz` time zones, it is incorrect to pass a time zone object directly into
  the `datetime.datetime` constructor.
- Be aware that for times in the future, correct conversion between time zones (and
  UTC) cannot be guaranteed by any time zone library.
- If you are using dates beyond 2038-01-18, due to current deficiencies in the underlying
  libraries caused by the year 2038 problem, daylight saving time (DST) adjustments
  to timezone aware dates will not be applied.
- The default behavior for pd.to_datetime is to raise an error on unparsable data.
- The behavior of localizing a timeseries with nonexistent times can be controlled
  by the 'nonexistent' argument.
- The default frequency for timedelta_range is calendar day.
- Timedeltas are differences in times, expressed in difference units, e.g. days, hours,
  minutes, seconds.
- Timedeltas can be both positive and negative.
- Enabling display.unicode.east_asian_width allows pandas to check each character’s
  “East Asian Width” property.
- Users interested in enhancing performance are highly encouraged to install the recommended
  dependencies for pandas.
- The `@jit` compilation will add overhead to the runtime of the function, so performance
  benefits may not be realized especially when using small data sets.
- The @jit compilation will add overhead to the runtime of the function, so performance
  benefits may not be realized especially when using small data sets.
- In terms of performance, the first time a function is run using the Numba engine
  will be slow as Numba will have some function compilation overhead.
- Numba allows you to write a pure Python function which can be JIT compiled to native
  machine instructions.
- Numba is best at accelerating functions that apply numerical functions to NumPy
  arrays.
- You should not use eval() for simple expressions or for expressions involving small
  DataFrames.
- Pandas provides data structures for in-memory analytics, which makes using pandas
  to analyze datasets that are larger than memory datasets somewhat tricky.
- Sparse objects exist for memory efficiency reasons.
- Functionally, their behavior should be nearly identical to their dense counterparts.
- The new default string dtype is an instance of the pandas.StringDtype class.
- The missing value sentinel is now always NaN
- The new default string dtype uses NaN for missing values.
- The dtype is no longer a numpy 'object' dtype
- '''setitem'' operations will now raise an error for non-string data'
- Invalid unicode input
- pandas is not 100% thread safe. The known issues relate to the copy method.
- By default the display option is set to True but can be explicitly overridden by
  passing the memory_usage argument when invoking info()
errors:
- 'ImportError: Python couldn’t find pandas in the list of available libraries.'
- 'AbstractMethodError: Raise this error instead of NotImplementedError for abstract
  methods.'
- 'AttributeConflictWarning: Warning raised when index attributes conflict when using
  HDFStore.'
- 'CategoricalConversionWarning: Warning is raised when reading a partial labeled
  Stata file using a iterator.'
- 'ChainedAssignmentError: Warning raised when trying to set using chained assignment.'
- 'ClosedFileError: Exception is raised when trying to perform an operation on a closed
  HDFStore file.'
- 'CSSWarning: Warning is raised when converting css styling fails.'
- 'DatabaseError: Error is raised when executing sql with bad syntax or sql that throws
  an error.'
- 'DataError: Exception raised when performing an operation on non-numerical data.'
- 'DtypeWarning: Warning raised when reading different dtypes in a column from a file.'
- 'DuplicateLabelError: Error raised when an operation would introduce duplicate labels.'
- 'EmptyDataError: Exception raised in pd.read_csv when empty data or header is encountered.'
- 'IncompatibilityWarning: Warning raised when trying to use where criteria on an
  incompatible HDF5 file.'
- 'IndexingError: Exception is raised when trying to index and there is a mismatch
  in dimensions.'
- 'InvalidColumnName: Warning raised by to_stata the column contains a non-valid stata
  name.'
- 'InvalidComparison: Exception is raised by _validate_comparison_value to indicate
  an invalid comparison.'
- 'InvalidIndexError: Exception raised when attempting to use an invalid index key.'
- 'InvalidVersion: An invalid version was found, users should refer to PEP 440.'
- 'IntCastingNaNError: Exception raised when converting (astype) an array with NaN
  to an integer type.'
- 'LossySetitemError: Raised when trying to do a __setitem__ on an np.ndarray that
  is not lossless.'
- 'MergeError: Exception raised when merging data.'
- 'NoBufferPresent: Exception is raised in _get_data_buffer to signal that there is
  no requested buffer.'
- 'NullFrequencyError: Exception raised when a freq cannot be null.'
- 'NumbaUtilError: Error raised for unsupported Numba engine routines.'
- 'NumExprClobberingError: Exception raised when trying to use a built-in numexpr
  name as a variable name.'
- 'OptionError: Exception raised for pandas.options.'
- 'OutOfBoundsDatetime: Raised when the datetime is outside the range that can be
  represented.'
- 'OutOfBoundsTimedelta: Raised when encountering a timedelta value that cannot be
  represented.'
- 'ParserError: Exception that is raised by an error encountered in parsing file contents.'
- 'ParserWarning: Warning raised when reading a file that doesn''t use the default
  ''c'' parser.'
- 'PerformanceWarning: Warning raised when there is a possible performance impact.'
- 'PossibleDataLossError: Exception raised when trying to open a HDFStore file when
  already opened.'
- 'PossiblePrecisionLoss: Warning raised by to_stata on a column with a value outside
  or equal to int64.'
- 'PyperclipException: Exception raised when clipboard functionality is unsupported.'
- 'PyperclipWindowsException: Exception raised when clipboard functionality is unsupported
  by Windows.'
- 'SettingWithCopyError: Exception raised when trying to set on a copied slice from
  a DataFrame.'
- 'SettingWithCopyWarning: Warning raised when trying to set on a copied slice from
  a DataFrame.'
- 'SpecificationError: Exception raised by agg when the functions are ill-specified.'
- 'UndefinedVariableError: Exception raised by query or eval when using an undefined
  variable name.'
- 'UnsortedIndexError: Error raised when slicing a MultiIndex which has not been lexsorted.'
- 'UnsupportedFunctionCall: Exception raised when attempting to call a unsupported
  numpy function.'
- 'ValueLabelTypeMismatch: Warning raised by to_stata on a category column that contains
  non-string values.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'KeyError: ''f'''
- 'ValueError: Can only compare identically-labeled Series objects'
- 'ValueError: If the date parsing fails'
- 'ParserError: raise an ParserError when a bad line is encountered.'
- 'Warning: print a warning when a bad line is encountered and skip that line.'
- 'ParserError: Check the structure of the CSV file for inconsistencies.'
- 'UnicodeDecodeError: Ensure the correct encoding is specified.'
- 'RuntimeError: Unhandled numpy dtype 15'
- 'FileNotFoundError: Ensure the file path is correct'
- 'ValueError: Check the sheet name or index'
- Specifies what to do upon encountering a bad line (a line with too many fields).
- 'ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4'
- 'FILE_NOT_FOUND: Check the file path.'
- 'INVALID_FORMAT: Ensure the file is a valid Excel format.'
- 'ParserError: raised when a bad line is encountered.'
- 'TypeError: cannot pass a where specification when reading from a Fixed format store.'
- Error in parsing due to unmatched quotes
- 'IndexError: If the number of header entries does not match the number of data columns'
- 'FILE_NOT_FOUND: Ensure the file path is correct.'
- 'INVALID_FORMAT: Check that the file is a supported Excel format.'
- 'TypeError: cannot pass a column specification when reading a Fixed format store.'
- 'NaturalNameWarning: A column name cannot be used as an attribute selector.'
- 'TypeError: Unsupported data type for serialization'
- 'ValueError: Cannot serialize non-string column names'
- 'FileNotFoundError: Check the file path provided.'
- 'ValueError: Ensure that the sheet name exists in the workbook.'
- 'InvalidDate: If a column or index contains an unparsable date, the entire column
  or index will be returned unaltered as an object data type.'
- 'ValueError: Unknown variable reference in query expression.'
- 'SyntaxError: Invalid query expression.'
- 'TypeError: cannot pass a column specification when reading from a Fixed format
  store.'
- 'ValueError: Subsequent attempts at appending longer strings will raise a ValueError.'
- 'KeyError: Raised when the items are not found.'
- 'IndexError: Raised if a requested indexer is out-of-bounds.'
- 'IndexError: positional indexers are out-of-bounds'
- 'ValueError: cannot reindex on an axis with duplicate labels'
- 'IndexError: indices are out-of-bounds'
- 'KeyError: Raised when index is not found.'
- 'IndexError: Raised when indexers are out-of-bounds.'
- 'KeyError: Raised when index labels are not found.'
- 'IndexError: Raised when positional indexers are out-of-bounds.'
- 'TypeError: cannot do slice indexing on DatetimeIndex with these indexers.'
- 'KeyError: ''Cannot get right slice bound for non-unique label: 3'''
- 'ValueError: cannot handle a non-unique multi-index!'
- 'DuplicateLabelError: Index has duplicates.'
- 'ValueError: Categorical categories must be unique'
- 'ValueError: Categorical categories cannot be null'
- 'TypeError: Cannot compare a Categorical for op __gt__ with type <class ''numpy.ndarray''>.'
- 'ValueError: time data doesn''t match format'
- 'TypeError: Invalid argument type'
- 'ValueError: time data "asd" doesn''t match format "%Y/%m/%d"'
- 'ValueError: time data doesn''t match format.'
- 'AmbiguousTimeError: Cannot infer dst time from timestamp, try using the ''ambiguous''
  argument.'
- 'NonExistentTimeError: Raises an error when attempting to localize nonexistent times.'
- 'SyntaxError: The ''@'' prefix is not allowed in top-level eval calls.'
- 'TypeError: Cannot perform reduction ''prod'' with string dtype'
- 'ValueError: Big-endian buffer not supported on little-endian compiler'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  base_url: https://pandas.pydata.org
  headers:
    Accept: application/json
source_metadata: null
