resources:
- name: MultiLevelCascadeAttentionWrapper
  endpoint:
    path: /cascade/multi-level-attention
    method: POST
    data_selector: results
- name: BatchDecodeWithSharedPrefixPagedKVCacheWrapper
  endpoint:
    path: /cascade/batch-decode
    method: POST
    data_selector: results
- name: single_decode_with_kv_cache
  endpoint:
    path: /generated/flashinfer.decode.single_decode_with_kv_cache
    method: GET
    data_selector: attention output
- name: BatchDecodeWithPagedKVCacheWrapper
  endpoint:
    path: /generated/flashinfer.decode.BatchDecodeWithPagedKVCacheWrapper
    method: GET
- name: CUDAGraphBatchDecodeWithPagedKVCacheWrapper
  endpoint:
    path: /generated/flashinfer.decode.CUDAGraphBatchDecodeWithPagedKVCacheWrapper
    method: GET
- name: single_prefill_with_kv_cache
  endpoint:
    path: /generated/flashinfer/prefill/single_prefill_with_kv_cache
    method: GET
    data_selector: attention output
- name: single_prefill_with_kv_cache_return_lse
  endpoint:
    path: /generated/flashinfer/prefill/single_prefill_with_kv_cache_return_lse
    method: GET
    data_selector: attention output
- name: BatchPrefillWithPagedKVCacheWrapper
  endpoint:
    path: /generated/flashinfer/prefill/cudnn_batch_prefill_with_kv_cache
    method: GET
    data_selector: records
    params: {}
- name: BatchPrefillWithRaggedKVCacheWrapper
  endpoint:
    path: /flashinfer/prefill/BatchPrefillWithRaggedKVCacheWrapper
    method: GET
- name: plan
  endpoint:
    path: /flashinfer/plan
    method: POST
    data_selector: response
    params: {}
- name: reset_workspace_buffer
  endpoint:
    path: /flashinfer/reset_workspace_buffer
    method: POST
    data_selector: response
    params: {}
- name: run
  endpoint:
    path: /flashinfer/run
    method: POST
    data_selector: response
    params: {}
- name: BatchMLAPagedAttentionWrapper
  endpoint:
    path: /flashinfer/mla/BatchMLAPagedAttentionWrapper
    method: GET
    data_selector: records
- name: single_prefill_with_kv_cache
  endpoint:
    path: /flashinfer/prefill/single_prefill_with_kv_cache
    method: POST
    data_selector: output
- name: single_prefill_with_kv_cache_return_lse
  endpoint:
    path: /flashinfer/prefill/single_prefill_with_kv_cache_return_lse
    method: POST
    data_selector: return
    params: {}
- name: mm_fp4
  endpoint:
    path: /generated/flashinfer.gemm.mm_fp4.html
    method: GET
    data_selector: ''
    params: {}
- name: bmm_fp8
  endpoint:
    path: /generated/flashinfer.gemm.bmm_fp8.html
    method: GET
    data_selector: ''
    params: {}
- name: gemm_fp8_nt_groupwise
  endpoint:
    path: /generated/flashinfer.gemm.gemm_fp8_nt_groupwise.html
    method: GET
    data_selector: ''
    params: {}
- name: group_gemm_fp8_nt_groupwise
  endpoint:
    path: /generated/flashinfer.gemm.group_gemm_fp8_nt_groupwise.html
    method: GET
    data_selector: ''
    params: {}
- name: group_deepgemm_fp8_nt_groupwise
  endpoint:
    path: /generated/flashinfer.gemm.group_deepgemm_fp8_nt_groupwise.html
    method: GET
    data_selector: ''
    params: {}
- name: batch_deepgemm_fp8_nt_groupwise
  endpoint:
    path: /generated/flashinfer.gemm.batch_deepgemm_fp8_nt_groupwise.html
    method: GET
    data_selector: ''
    params: {}
- name: group_gemm_mxfp4_nt_groupwise
  endpoint:
    path: /generated/flashinfer.gemm.group_gemm_mxfp4_nt_groupwise.html
    method: GET
    data_selector: ''
    params: {}
- name: SegmentGEMMWrapper
  endpoint:
    path: /flashinfer.gemm.SegmentGEMMWrapper
    method: GET
    data_selector: ''
    params: {}
- name: bmm_fp8
  endpoint:
    path: /flashinfer/gemm/bmm_fp8
    method: POST
    data_selector: out
    params: {}
- name: group_gemm_fp8_nt_groupwise
  endpoint:
    path: /flashinfer/gemm/group_gemm_fp8_nt_groupwise
    method: GET
    data_selector: out
- name: batch_deepgemm_fp8_nt_groupwise
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: group_gemm_mxfp4_nt_groupwise
  endpoint:
    path: /flashinfer/gemm/group_gemm_mxfp4_nt_groupwise
    method: POST
    data_selector: out
    params: {}
- name: fused_moe_trtllm_fp4_block_scale_moe
  endpoint:
    path: /flashinfer/fused_moe/trtllm_fp4_block_scale_moe
    method: GET
    data_selector: List[Tensor]
    params: {}
- name: fused_moe_trtllm_fp8_block_scale_moe
  endpoint:
    path: /flashinfer/fused_moe/trtllm_fp8_block_scale_moe
    method: GET
    data_selector: output_tensor
- name: fused_moe_trtllm_fp8_per_tensor_scale_moe
  endpoint:
    path: /flashinfer/fused_moe/trtllm_fp8_per_tensor_scale_moe
    method: GET
    data_selector: output_tensor
- name: merge_state_in_place
  endpoint:
    path: /flashinfer/cascade/merge_state_in_place
    method: POST
    data_selector: parameters
    params: {}
- name: merge_states
  endpoint:
    path: /flashinfer/cascade/merge_states
    method: GET
    data_selector: records
    params: {}
- name: create_shared_buffer
  endpoint:
    path: /flashinfer/comm/create_shared_buffer
    method: POST
    data_selector: pointers
- name: AllReduceStrategyType
  endpoint:
    path: /flashinfer/comm/AllReduceStrategyType
    method: GET
    data_selector: attributes
- name: trtllm_moe_allreduce_fusion
  endpoint:
    path: /flashinfer/comm/trtllm_moe_allreduce_fusion
    method: GET
    data_selector: parameters
    params: {}
- name: lamport_initialize_all
  endpoint:
    path: /flashinfer/comm/trtllm_lamport_initialize_all
    method: POST
    data_selector: None
    params: {}
- name: McastGPUBuffer
  endpoint:
    path: /flashinfer/comm/mnnvl/McastGPUBuffer
    method: GET
- name: create_tensor_from_cuda_memory
  endpoint:
    path: /flashinfer/comm/mnnvl/create_tensor_from_cuda_memory
    method: GET
    data_selector: Tensor
- name: trtllm_mnnvl_all_reduce
  endpoint:
    path: /trtllm_mnnvl_all_reduce
    method: POST
    data_selector: result
    params: {}
- name: trtllm_mnnvl_fused_allreduce_rmsnorm
  endpoint:
    path: /flashinfer/comm/trtllm_mnnvl_ar/trtllm_mnnvl_fused_allreduce_rmsnorm
    method: POST
    data_selector: results
- name: BlockSparseAttentionWrapper
  endpoint:
    path: /flashinfer/sparse/BlockSparseAttentionWrapper
    method: GET
    data_selector: records
    params: {}
- name: VariableBlockSparseAttentionWrapper
  endpoint:
    path: /flashinfer/sparse/VariableBlockSparseAttentionWrapper
    method: GET
    data_selector: records
    params: {}
- name: BlockSparseAttentionWrapper
  endpoint:
    path: /flashinfer/sparse/BlockSparseAttentionWrapper
    method: GET
    data_selector: records
- name: VariableBlockSparseAttentionWrapper
  endpoint:
    path: /flashinfer/sparse/VariableBlockSparseAttentionWrapper
    method: GET
    data_selector: records
- name: append_paged_kv_cache
  endpoint:
    path: /generated/flashinfer.page.append_paged_kv_cache
    method: GET
    data_selector: records
- name: append_paged_mla_kv_cache
  endpoint:
    path: /generated/flashinfer.page.append_paged_mla_kv_cache
    method: GET
    data_selector: records
- name: get_batch_indices_positions
  endpoint:
    path: /generated/flashinfer.page.get_batch_indices_positions
    method: GET
    data_selector: records
- name: get_batch_indices_positions
  endpoint:
    path: /flashinfer/page/get_batch_indices_positions
    method: GET
    data_selector: batch_indices, positions
    params: {}
- name: sampling_from_probs
  endpoint:
    path: /sampling_from_probs
    method: POST
    data_selector: samples
- name: top_k_renorm_probs
  endpoint:
    path: flashinfer/sampling/top_k_renorm_probs
    method: POST
    data_selector: renorm_probs
    params: {}
- name: top_k_mask_logits
  endpoint:
    path: /flashinfer/sampling/top_k_mask_logits
    method: POST
    data_selector: masked_logits
    params: {}
- name: LogitsProcessor
  endpoint:
    path: /generated/flashinfer/logits_processor/LogitsProcessor
    method: GET
    data_selector: records
- name: Temperature
  endpoint:
    path: /generated/flashinfer/logits_processor/Temperature
    method: GET
    data_selector: records
- name: Softmax
  endpoint:
    path: /generated/flashinfer/logits_processor/Softmax
    method: GET
    data_selector: records
- name: TopK
  endpoint:
    path: /generated/flashinfer/logits_processor/TopK
    method: GET
    data_selector: records
- name: TopP
  endpoint:
    path: /generated/flashinfer/logits_processor/TopP
    method: GET
    data_selector: records
- name: MinP
  endpoint:
    path: /generated/flashinfer/logits_processor/MinP
    method: GET
    data_selector: records
- name: Sample
  endpoint:
    path: /generated/flashinfer/logits_processor/Sample
    method: GET
    data_selector: records
- name: LogitsProcessor
  endpoint:
    path: /generated/flashinfer.logits_processor.LogitsProcessor
    method: GET
    data_selector: processors
    params: {}
- name: Temperature
  endpoint:
    path: /generated/flashinfer.logits_processor.Temperature
    method: GET
    data_selector: processors
    params: {}
- name: Softmax
  endpoint:
    path: /generated/flashinfer.logits_processor.Softmax
    method: GET
    data_selector: processors
    params: {}
- name: TopP
  endpoint:
    path: /generated/flashinfer.logits_processor.TopP
    method: GET
    data_selector: processors
    params: {}
- name: Sample
  endpoint:
    path: /generated/flashinfer.logits_processor.Sample
    method: GET
    data_selector: processors
    params: {}
- name: TopP
  endpoint:
    path: /flashinfer/logits_processor/TopP
    method: GET
    data_selector: params
    params: {}
- name: Sample
  endpoint:
    path: /flashinfer/logits_processor/Sample
    method: GET
- name: rmsnorm
  endpoint:
    path: /flashinfer/norm/rmsnorm
    method: POST
    data_selector: output
    params: {}
- name: fused_add_rmsnorm
  endpoint:
    path: /flashinfer/norm/fused_add_rmsnorm
    method: POST
    data_selector: result
    params: {}
- name: gemma_rmsnorm
  endpoint:
    path: /flashinfer/norm/gemma_rmsnorm
    method: POST
    data_selector: output
    params:
      input: torch.Tensor
      weight: torch.Tensor
      eps: float
      out: torch.Tensor | None
      enable_pdl: bool | None
- name: layernorm
  endpoint:
    path: /flashinfer/norm/layernorm
    method: GET
    data_selector: output
- name: apply_llama31_rope_inplace
  endpoint:
    path: /flashinfer/rope/apply_llama31_rope_inplace
    method: POST
    data_selector: output
    params:
      q: torch.Tensor
      k: torch.Tensor
      indptr: torch.Tensor
      offsets: torch.Tensor
      rotary_dim: None
      interleave: 'False'
      rope_scale: 8
      rope_theta: 500000.0
      low_freq_factor: 1
      high_freq_factor: 4
      old_context_len: 8192
- name: apply_rope
  endpoint:
    path: /flashinfer/rope/apply_rope
    method: GET
    data_selector: records
- name: apply_llama31_rope
  endpoint:
    path: /flashinfer/rope/apply_llama31_rope
    method: POST
- name: apply_rope_pos_ids
  endpoint:
    path: /flashinfer/rope/apply_rope_pos_ids
    method: GET
    data_selector: records
    params: {}
- name: apply_llama31_rope_pos_ids_inplace
  endpoint:
    path: /flashinfer/rope/apply_llama31_rope_pos_ids_inplace
    method: GET
- name: apply_rope_with_cos_sin_cache_inplace
  endpoint:
    path: /flashinfer/rope/apply_rope_with_cos_sin_cache_inplace
    method: POST
    data_selector: result
    params: {}
- name: silu_and_mul
  endpoint:
    path: /flashinfer/activation/silu_and_mul
    method: POST
    data_selector: output
- name: gelu_and_mul
  endpoint:
    path: /flashinfer/activation/gelu_and_mul
    method: POST
- name: packbits
  endpoint:
    path: /flashinfer/quantization/packbits
    method: GET
    data_selector: y
    params: {}
- name: fp4_quantize
  endpoint:
    path: /generated/flashinfer.fp4_quantization.fp4_quantize
    method: GET
    data_selector: input
    params: {}
- name: nvfp4_quantize
  endpoint:
    path: /generated/flashinfer.fp4_quantization.nvfp4_quantize
    method: GET
    data_selector: a
    params: {}
- name: nvfp4_batched_quantize
  endpoint:
    path: /generated/flashinfer.fp4_quantization.nvfp4_batched_quantize
    method: GET
    data_selector: a
    params: {}
- name: nvfp4_block_scale_interleave
  endpoint:
    path: /generated/flashinfer.fp4_quantization.nvfp4_block_scale_interleave
    method: GET
    data_selector: unswizzled_sf
    params: {}
- name: e2m1_and_ufp8sf_scale_to_float
  endpoint:
    path: /generated/flashinfer.fp4_quantization.e2m1_and_ufp8sf_scale_to_float
    method: GET
    data_selector: e2m1_tensor
    params: {}
- name: shuffle_matrix_a
  endpoint:
    path: /generated/flashinfer.fp4_quantization.shuffle_matrix_a
    method: GET
    data_selector: input_tensor
    params: {}
- name: shuffle_matrix_sf_a
  endpoint:
    path: /generated/flashinfer.fp4_quantization.shuffle_matrix_sf_a
    method: GET
    data_selector: input_tensor
    params: {}
- name: fp4_quantize
  endpoint:
    path: /flashinfer/fp4_quantization/fp4_quantize
    method: POST
    data_selector: return
    params: {}
- name: nvfp4_quantize
  endpoint:
    path: /flashinfer/fp4_quantization/nvfp4_quantize
    method: POST
    data_selector: quantized_tensor
    params: {}
- name: nvfp4_batched_quantize
  endpoint:
    path: /flashinfer/fp4_quantization/nvfp4_batched_quantize
    method: GET
    data_selector: tuple
    params: {}
- name: e2m1_and_ufp8sf_scale_to_float
  endpoint:
    path: /flashinfer/fp4_quantization/e2m1_and_ufp8sf_scale_to_float
    method: POST
    data_selector: tensor
    params: {}
- name: layout_128x4
  endpoint: {}
- name: layout_8x4
  endpoint: {}
- name: layout_linear
  endpoint: {}
- name: set_seed
  endpoint:
    path: /flashinfer/testing/set_seed
    method: GET
    data_selector: null
    params:
      random_seed: int
- name: sleep_after_kernel_run
  endpoint:
    path: /flashinfer/testing/sleep_after_kernel_run
    method: GET
    data_selector: None
    params:
      execution_time: float
- name: attention_flops
  endpoint:
    path: /flashinfer/testing/attention_flops
    method: POST
    data_selector: total_flops
    params:
      batch_size: int
      qo_seqlen: int
      kv_seqlen: int
      head_dim_qk: int
      head_dim_vo: int
      num_qo_heads: int
      causal: bool
- name: attention_flops_with_actual_seq_lens
  endpoint:
    path: /flashinfer/testing/attention_flops_with_actual_seq_lens
    method: GET
    data_selector: total_flops
    params:
      actual_seq_lens_q: torch.Tensor
      actual_seq_lens_kv: torch.Tensor
      head_dim_qk: int
      head_dim_vo: int
      num_qo_heads: int
      causal: bool
- name: attention_tflops_per_sec
  endpoint:
    path: /flashinfer/testing/attention_tflops_per_sec
    method: GET
    data_selector: tflops_per_sec
    params: {}
- name: attention_tflops_per_sec_with_actual_seq_lens
  endpoint:
    path: /flashinfer/testing/attention_tflops_per_sec_with_actual_seq_lens
    method: GET
    data_selector: tflops_per_sec
    params: {}
- name: attention_tb_per_sec
  endpoint:
    path: /flashinfer/testing/attention_tb_per_sec
    method: GET
    data_selector: tb_per_sec
    params: {}
- name: attention_tb_per_sec_with_actual_seq_lens
  endpoint:
    path: /flashinfer/testing/attention_tb_per_sec_with_actual_seq_lens
    method: GET
    data_selector: tb_per_sec
    params: {}
- name: bench_gpu_time_with_cuda_event
  endpoint:
    path: /flashinfer/testing/bench_gpu_time_with_cuda_event
    method: GET
    data_selector: measured_times
    params: {}
- name: bench_gpu_time_with_cudagraph
  endpoint:
    path: /flashinfer/testing/bench_gpu_time_with_cudagraph
    method: GET
    data_selector: measured_times
    params: {}
- name: benchmark_gpu_time
  endpoint:
    path: /flashinfer/testing/bench_gpu_time_with_cupti
    method: POST
    data_selector: times
    params:
      dry_run_time_ms: 25
      repeat_time_ms: 100
      l2_flush: true
      l2_flush_size_mb: 256
      l2_flush_device: cuda
      sleep_after_run: false
      use_cuda_graph: false
notes:
- The API assumes all levels KV-Cache are stored in a unified paged table.
- This API will be deprecated in the future, please use MultiLevelCascadeAttentionWrapper
  instead.
- For faster initialization and offline usage, install the optional packages to have
  most kernels pre-compiled.
- The generalized score s is also known as log-sum-exp (`lse` for short).
- The ⊕ operator is commutative and associative, which means we can safely offload
  the self-attention computation on a subset of KV to different devices and merge
  the results in any order.
- To accelerate computation, FlashInfer’s batch decode attention creates some auxiliary
  data structures, these data structures can be reused across multiple batch decode
  attention calls (e.g. different Transformer layers).
- The `plan()` method could not be captured by CUDAGraph.
- To accelerate computation, FlashInfer’s batch prefill/append attention operators
  create some auxiliary data structures, these data structures can be reused across
  multiple prefill/append attention calls.
- The num_qo_heads must be a multiple of num_kv_heads.
- Currently only supports causal attention (causal must be True)
- All tensors must be contiguous and on the same CUDA device
- Query and KV heads can have different sizes (num_heads_qo >= num_heads_kv)
- If num_qo_heads is not equal to num_kv_heads, the function will use grouped query
  attention.
- When using cuda graph, actual_seq_lens_q and actual_seq_lens_kv must be on the same
  device as q
- Head dimension of query and key must be 128 or 192
- Head dimension of value and output must be 128
- When cudnn/cutlass backend is used, both a and b should quantized with nvfp4_quantize
  using the 128x4 scale factor layout and do_shuffle=False.
- When trtllm backend is used, b must be quantized with 128x4 layout and do_shuffle=True.
  a can be quantized with either 128x4 or 8x4 layout (controlled by use_8x4_sf_layout)
  and do_shuffle=False.
- The m should be padded to a multiple of 4 before calling this function, to accommodate
  the kernel’s requirement.
- Each value in m_indptr should be padded to a multiple of 4 before calling this function,
  to accommodate the kernel’s requirement.
- This function requires NVIDIA Blackwell (SM100) architecture
- The scaling factors should be generated using appropriate quantization functions
  like `per_token_cast_to_fp8` for a and `per_block_cast_to_fp8` for b
- The function internally uses the DeepGEMM backend for optimized FP8 computation
- All input tensors must be on the same CUDA device
- The block size for scaling is determined by the `scale_granularity_mnk` parameter
- The scaling factors should be generated using appropriate quantization functions
  like per_token_cast_to_fp8 for a and per_block_cast_to_fp8 for b
- The block size for scaling is determined by the scale_granularity_mnk parameter
- The function supports various data types including FP32, FP16, BF16, FP8, and NVFP4.
- It implements both tensor parallelism and expert parallelism.
- Currently, some advanced features like FP8 block scaling and minimum latency mode
  are not implemented for Blackwell architecture.
- FP4 block scale MoE operation
- This function creates a new DLPack capsule each time it’s called, even with the
  same pointer. Each capsule is consumed only once.
- Regarding the use_oneshot parameter, you could force to use the one-shot strategy
  based on your use case.
- Otherwise, it would be enabled if token_num is less than the one-shot max token
  number (currently 128) for min-latency mode.
- This function is used to create a workspace for all reduce.
- The workspace should be initialized before calling trtllm_custom_all_reduce.
- The workspace should be destroyed after calling trtllm_custom_all_reduce.
- The workspace can be reused for multiple all reduce calls under the same configuration.
- This function is used to destroy a workspace for all reduce fusion.
- The workspace should be destroyed after calling trtllm_custom_all_reduce_fusion.
- The workspace can be reused for multiple all reduce fusion calls under the same
  configuration.
- The function assumes that the space for appended k/v has already been allocated,
  which means kv_indices, kv_indptr, kv_last_page_len has incorporated appended k/v.
- current only support ckv=512 and kpe=64
- This function expects float32 inputs, and the output is int32.
- This combination of top_p_renorm_probs and sampling_from_probs should be equivalent
  to top_p_sampling_from_probs.
- This combination of top_k_renorm_probs and sampling_from_probs should be equivalent
  to top_k_sampling_from_probs.
- The combination of `top_k_mask_logits` and `softmax` should be equivalent to `top_k_renorm_probs`.
- The pipeline automatically validates type compatibility between operations.
- Operations are fused when possible
- Runtime parameters (like temperature, top_k) are passed with pipe.call().
- The output is always a plain torch.Tensor, not a TaggedTensor.
- Subclasses must implement the `legalize()` method to convert the high-level processor
  into one or more low-level operators with specific input/output types
- Can only appear once in a pipeline.
- When applied to TensorType.LOGITS, sets non-top-k values to -inf.
- When applied to TensorType.PROBS, zeros out non-top-k values and renormalizes.
- Outputs `TensorType.INDICES` - no operators can follow
- TensorType represents the semantic type of tensors in the pipeline.
- TaggedTensor is primarily for internal use by LogitsPipe
- Users typically work with plain tensors; tagging happens automatically
- Epsilon for numerical stability is set to 1e-06.
- Input tensor must be bfloat16.
- Gemma and beta tensors must be float32.
- torch.compile is not supported for this function because it’s data dependent.
- The length of the returned streams and resources is `num_groups + 1`, where the
  last one is the remaining SMs.
- The following examples show how the SM count is rounded up to meet the alignment
  and granularity requirements.
- Quantize input tensor to FP4 format.
- Supports various input data types and scale factor layouts.
- Causal must be false for decode as this function assumes qo_seqlen == kv_seqlen.
errors:
- 'NotImplementedError: If any of the following features are requested but not implemented:
  Minimum Latency Mode'
- 'RuntimeError: when requested SM allocation exceeds device capacity'
- 'RuntimeError: If the requested SM allocation exceeds device capacity'
- 'ValueError: If sm_counts is empty or contains invalid values (e.g., negative values)'
- 'NotImplementedError: If BFloat16 input when BFloat16 is not enabled'
- 'NotImplementedError: If FP8 input when FP8 is not enabled'
- 'NotImplementedError: If sf_vec_size other than 16 or 32'
auth_info:
  mentioned_objects: []
client:
  base_url: https://flashinfer.ai
  headers:
    Accept: application/json
source_metadata: null
