resources:
- name: network_volume
  endpoint:
    path: /
    method: GET
    data_selector: records
- name: network_volume
  endpoint:
    path: /network_volumes
    method: GET
    data_selector: volumes
- name: flux_dev
  endpoint:
    path: /v2/black-forest-labs-flux-1-dev/run
    method: POST
    data_selector: output
    params: {}
- name: flux_schnell
  endpoint:
    path: /v2/black-forest-labs-flux-1-schnell/run
    method: POST
    data_selector: output
    params: {}
- name: deep_cognito
  endpoint:
    path: /v2/deep-cogito-v2-llama-70b/run
    method: POST
    data_selector: output
    params: {}
- name: qwen3
  endpoint:
    path: /v2/qwen3-32b-awq/run
    method: POST
    data_selector: output
    params: {}
- name: hub
  endpoint:
    path: /hub
    method: GET
    data_selector: repositories
    params: {}
- name: flux_dev
  endpoint:
    path: /v2/black-forest-labs-flux-1-dev/run
    method: POST
    data_selector: output
    params: {}
- name: flux_schnell
  endpoint:
    path: /v2/black-forest-labs-flux-1-schnell/run
    method: POST
    data_selector: output
    params: {}
- name: deep_cognito
  endpoint:
    path: /v2/deep-cogito-v2-llama-70b/
    method: POST
    data_selector: output
    params: {}
- name: qwen3
  endpoint:
    path: /v2/qwen3-32b-awq/
    method: POST
    data_selector: output
    params: {}
- name: Pods
  endpoint:
    path: /api-reference/pods
    method: GET
    data_selector: pods
    params: {}
- name: Pods
  endpoint:
    path: /pods
    method: GET
- name: Savings plans
  endpoint:
    path: /savings-plans
    method: GET
- name: Billing
  endpoint:
    path: /billing
    method: GET
- name: hub
  endpoint:
    path: /hub
    method: GET
    data_selector: repos
    params: {}
- name: Pods
  endpoint:
    path: /api/pods
    method: GET
    data_selector: pods
    params: {}
- name: Pricing
  endpoint:
    path: /api/pricing
    method: GET
    data_selector: pricingOptions
    params: {}
- name: Slurm Cluster
  endpoint:
    path: /instant-clusters/slurm-clusters
    method: POST
    data_selector: cluster
    params: {}
- name: fine_tuning
  endpoint:
    path: /fine-tune
    method: POST
    data_selector: results
    params: {}
- name: Slurm Cluster
  endpoint:
    path: /instant-clusters/slurm-clusters
    method: POST
    data_selector: cluster
    params: {}
- name: fine_tuning
  endpoint:
    path: /fine-tune
    method: POST
    data_selector: results
- name: referral_program
  endpoint:
    path: /api/referral
    method: GET
- name: affiliate_program
  endpoint:
    path: /api/affiliate
    method: GET
- name: creator_program
  endpoint:
    path: /api/creator
    method: GET
- name: referral_program
  endpoint:
    path: /api/referral
    method: GET
    data_selector: data
    params: {}
- name: affiliate_program
  endpoint:
    path: /api/affiliate
    method: GET
    data_selector: data
    params: {}
- name: creator_program
  endpoint:
    path: /api/creator
    method: GET
    data_selector: data
    params: {}
- name: gpu_types
  endpoint:
    path: /references/gpu-types
    method: GET
    data_selector: table
- name: gpu_pools
  endpoint:
    path: /references/gpu-pools
    method: GET
    data_selector: table
- name: processors
  endpoint:
    path: /api/v1/processors
    method: GET
    data_selector: processors
    params: {}
- name: vLLM_worker
  endpoint:
    path: /serverless/vllm/get-started
    method: POST
    data_selector: output
    params: {}
- name: cpu_info
  endpoint:
    path: /api/cpu_info
    method: GET
    data_selector: records
    params: {}
- name: worker
  endpoint:
    path: /serverless/workers/overview
    method: GET
- name: ComfyUI Pod
  endpoint:
    path: /pods
    method: GET
- name: SSH password setup
  endpoint:
    path: /community-solutions/ssh-password-migration/overview
    method: GET
    data_selector: tools
    params: {}
- name: SCP migration
  endpoint:
    path: /community-solutions/ssh-password-migration/scp_migration
    method: GET
    data_selector: tools
    params: {}
- name: CopyParty
  endpoint:
    path: /install.sh
    method: GET
- name: vLLM_worker
  endpoint:
    path: /serverless/vllm
    method: POST
    data_selector: output
- name: container_volume
  endpoint:
    path: /serverless/storage/container-volume
    method: GET
    data_selector: records
- name: network_volume
  endpoint:
    path: /serverless/storage/network-volume
    method: GET
    data_selector: records
- name: s3_compatible_storage
  endpoint:
    path: /serverless/storage/s3-compatible-storage
    method: GET
    data_selector: records
- name: serverless_worker
  endpoint:
    path: /serverless
    method: POST
    data_selector: output
    params: {}
- name: network_volume
  endpoint:
    path: /serverless/storage/network-volumes
    method: POST
    data_selector: volume
    params: {}
- name: s3_compatible_api
  endpoint:
    path: /serverless/storage/s3-api
    method: GET
    data_selector: files
    params: {}
- name: ComfyUI Pod
  endpoint:
    path: /pods/comfyui
    method: GET
    data_selector: pods
    params: {}
- name: handler
  endpoint:
    path: /serverless/workers/handler-functions
    method: GET
    data_selector: handler
    params: {}
- name: file_transfer
  endpoint:
    path: /file-transfer
    method: POST
    data_selector: transferDetails
- name: job_states
  endpoint:
    path: /serverless/endpoints/job-states
    method: GET
- name: endpoint_metrics
  endpoint:
    path: /serverless/endpoints/metrics
    method: GET
- name: SSH password setup
  endpoint:
    path: /community-solutions/ssh-password-migration/overview
    method: GET
- name: SCPMigration
  endpoint:
    path: /raw.githubusercontent.com/justinwlin/Runpod-SSH-Password/refs/heads/main/SCPMigration
    method: GET
- name: CopyParty
  endpoint:
    path: /community-solutions/copyparty-file-manager
    method: GET
- name: workers
  endpoint:
    path: /serverless/workers
    method: GET
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
- name: status
  endpoint:
    path: /status
    method: GET
    data_selector: results
- name: stream
  endpoint:
    path: /stream
    method: GET
    data_selector: incremental_results
- name: cancel
  endpoint:
    path: /cancel
    method: POST
- name: retry
  endpoint:
    path: /retry
    method: POST
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
- name: health
  endpoint:
    path: /health
    method: GET
- name: handler_function
  endpoint:
    path: /serverless/workers/handler-functions
    method: POST
    data_selector: handler
    params: {}
- name: job_states
  endpoint:
    path: /serverless/endpoints/job-states
    method: GET
    data_selector: job_states
- name: endpoint_metrics
  endpoint:
    path: /serverless/endpoints/metrics
    method: GET
    data_selector: metrics
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
- name: status
  endpoint:
    path: /status
    method: GET
- name: stream
  endpoint:
    path: /stream
    method: GET
- name: cancel
  endpoint:
    path: /cancel
    method: POST
- name: retry
  endpoint:
    path: /retry
    method: POST
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
- name: health
  endpoint:
    path: /health
    method: GET
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: id
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: id
- name: status
  endpoint:
    path: /status
    method: GET
    data_selector: status
- name: stream
  endpoint:
    path: /stream
    method: GET
    data_selector: results
- name: cancel
  endpoint:
    path: /cancel
    method: POST
- name: retry
  endpoint:
    path: /retry
    method: POST
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
- name: health
  endpoint:
    path: /health
    method: GET
- name: vLLM worker
  endpoint:
    path: /v2/<endpoint_id>/run
    method: POST
    data_selector: input
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: completions
  endpoint:
    path: /completions
    method: POST
    data_selector: choices
    params: {}
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: data
    params: {}
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
    params: {}
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
    params: {}
- name: status
  endpoint:
    path: /status
    method: GET
    data_selector: status
    params: {}
- name: stream
  endpoint:
    path: /stream
    method: GET
    data_selector: stream
    params: {}
- name: cancel
  endpoint:
    path: /cancel
    method: POST
    data_selector: cancel
    params: {}
- name: retry
  endpoint:
    path: /retry
    method: POST
    data_selector: retry
    params: {}
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
    data_selector: purge
    params: {}
- name: health
  endpoint:
    path: /health
    method: GET
    data_selector: health
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
- name: text_completions
  endpoint:
    path: /completions
    method: POST
- name: list_models
  endpoint:
    path: /models
    method: GET
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
- name: status
  endpoint:
    path: /status
    method: GET
    data_selector: status
- name: stream
  endpoint:
    path: /stream
    method: GET
    data_selector: results
- name: cancel
  endpoint:
    path: /cancel
    method: POST
    data_selector: jobId
- name: retry
  endpoint:
    path: /retry
    method: POST
    data_selector: jobId
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
    data_selector: none
- name: health
  endpoint:
    path: /health
    method: GET
    data_selector: status
- name: vllm_worker
  endpoint:
    path: /v2/<endpoint_id>/run
    method: POST
    data_selector: input
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: choices
    params: {}
- name: text_completions
  endpoint:
    path: /completions
    method: POST
    data_selector: choices
    params: {}
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: data
    params: {}
- name: runsync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
    params: {}
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
    params: {}
- name: status
  endpoint:
    path: /status/{job_id}
    method: GET
    data_selector: ''
    params: {}
- name: stream
  endpoint:
    path: /stream/{job_id}
    method: GET
    data_selector: ''
    params: {}
- name: cancel
  endpoint:
    path: /cancel/{job_id}
    method: POST
    data_selector: ''
    params: {}
- name: retry
  endpoint:
    path: /retry/{job_id}
    method: POST
    data_selector: ''
    params: {}
- name: purge-queue
  endpoint:
    path: /purge-queue
    method: POST
    data_selector: ''
    params: {}
- name: health
  endpoint:
    path: /health
    method: GET
    data_selector: ''
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
- name: text_completions
  endpoint:
    path: /completions
    method: POST
- name: list_models
  endpoint:
    path: /models
    method: GET
- name: gemma_model
  endpoint:
    path: /serverless/endpoints/send-requests
    method: POST
    data_selector: response
    params: {}
- name: run_sync
  endpoint:
    path: /runsync
    method: POST
    data_selector: input
- name: run
  endpoint:
    path: /run
    method: POST
    data_selector: input
- name: status
  endpoint:
    path: /status
    method: GET
    data_selector: status
- name: stream
  endpoint:
    path: /stream
    method: GET
    data_selector: incremental_results
- name: cancel
  endpoint:
    path: /cancel
    method: POST
    data_selector: cancel_job
- name: retry
  endpoint:
    path: /retry
    method: POST
    data_selector: retry_job
- name: purge_queue
  endpoint:
    path: /purge-queue
    method: POST
    data_selector: purge_jobs
- name: health
  endpoint:
    path: /health
    method: GET
    data_selector: health_status
- name: submit_job
  endpoint:
    path: /{ENDPOINT_ID}/run
    method: POST
    data_selector: input
    params: {}
- name: check_job_status
  endpoint:
    path: /{ENDPOINT_ID}/status/{JOB_ID}
    method: GET
    data_selector: status
    params: {}
- name: run
  endpoint:
    path: /ENDPOINT_ID/run
    method: POST
    data_selector: output
    params: {}
- name: status
  endpoint:
    path: /ENDPOINT_ID/status/JOB_ID
    method: GET
    data_selector: status
    params: {}
- name: generate_image
  endpoint:
    path: /v2/${YOUR_ENDPOINT}/runsync
    method: POST
    data_selector: input
    params: {}
- name: runsync
  endpoint:
    path: /$ENDPOINT_ID/runsync
    method: POST
    data_selector: input
- name: run
  endpoint:
    path: /$ENDPOINT_ID/run
    method: POST
    data_selector: input
- name: status
  endpoint:
    path: /$ENDPOINT_ID/status/YOUR_JOB_ID
    method: GET
- name: stream
  endpoint:
    path: /$ENDPOINT_ID/stream/YOUR_JOB_ID
    method: GET
- name: cancel
  endpoint:
    path: /$ENDPOINT_ID/cancel/YOUR_JOB_ID
    method: POST
- name: retry
  endpoint:
    path: /$ENDPOINT_ID/retry/YOUR_JOB_ID
    method: POST
- name: purge-queue
  endpoint:
    path: /$ENDPOINT_ID/purge-queue
    method: POST
- name: health
  endpoint:
    path: /$ENDPOINT_ID/health
    method: GET
- name: ollama_server
  endpoint:
    path: /tutorials/serverless/run-ollama-inference
    method: POST
    data_selector: output
    params: {}
- name: run
  endpoint:
    path: /v2/ENDPOINT_ID/run
    method: POST
    data_selector: id
    params: {}
- name: status
  endpoint:
    path: /v2/ENDPOINT_ID/status/JOB_ID
    method: GET
    data_selector: status
    params: {}
- name: run_job
  endpoint:
    path: /ENDPOINT_ID/run
    method: POST
    data_selector: output
    params: {}
- name: check_status
  endpoint:
    path: /ENDPOINT_ID/status/JOB_ID
    method: GET
    data_selector: output
    params: {}
- name: tags
  endpoint:
    path: /api/tags
    method: GET
- name: generate
  endpoint:
    path: /api/generate
    method: POST
- name: generate_image
  endpoint:
    path: /v2/${YOUR_ENDPOINT}/runsync
    method: POST
    data_selector: input
    params: {}
- name: Ollama Server
  endpoint:
    path: /tutorials/serverless/run-ollama-inference
    method: POST
    data_selector: output
    params:
      input:
        method_name: generate
        input:
          prompt: why the sky is blue?
- name: dataset_transfer
  endpoint:
    path: /runpodctl/send
    method: POST
    data_selector: output
- name: dataset_receive
  endpoint:
    path: /runpodctl/receive
    method: POST
    data_selector: output
- name: fine_tune_llm
  endpoint:
    path: /axolotl.cli.train
    method: POST
    data_selector: output
- name: inference
  endpoint:
    path: /axolotl.cli.inference
    method: POST
    data_selector: output
- name: merge_model
  endpoint:
    path: /axolotl.cli.merge_lora
    method: POST
    data_selector: output
- name: upload_hugging_face
  endpoint:
    path: /huggingface-cli/upload
    method: POST
    data_selector: output
- name: SmolLM3
  endpoint:
    path: /tutorials/pods/run-your-first
    method: GET
    data_selector: records
- name: Jupyter Notebook
  endpoint:
    path: /api/jupyter/notebook
    method: POST
    data_selector: notebook
    params: {}
- name: models
  endpoint:
    path: /api/tags
    method: GET
    data_selector: output
- name: generate
  endpoint:
    path: /api/generate
    method: POST
    data_selector: output
- name: fine_tune_llm
  endpoint:
    path: /fine-tune
    method: POST
    data_selector: results
notes:
- Network volumes are billed hourly at $0.07/GB/month for the first 1TB, and $0.05/GB/month
  for additional storage.
- ListObjects runs slowly or fails with 'same next token' error when running aws s3
  ls or ListObjects on a directory with many files or large amounts of data (typically
  >10,000 files or >10 GB of data).
- 'Storage capacity: Network volumes have a fixed storage capacity, unlike the virtually
  unlimited storage of standard S3 buckets.'
- 'Maximum file size: 4TB (the maximum size of a network volume).'
- 'Object names containing special characters (e.g., #) may need to be URL encoded
  to ensure proper processing.'
- Requests that are out of time sync by 1 hour will be rejected.
- When using aws s3 commands, you must pass in the endpoint URL for your network volume.
- ListObjects runs slowly or fails with 'same next token' error when running aws s3
  ls or ListObjects on a directory with many files or large amounts of data (typically
  >10,000 files or >10 GB of data) for the first time.
- Object names in the Runpod S3-compatible API correspond to actual file paths on
  your network volume.
- To avoid incurring unnecessary charges, follow these steps to clean up your Pod
  resources.
- Team accounts enable multiple users to collaborate on projects and share resources.
- Legacy API keys generated before November 11, 2024 have either Read/Write or Read
  Only access to GraphQL based on what was set for that key.
- Runpod does not store your API key, so you may wish to save it elsewhere.
- Planning to share compute resources with your team? You can convert your personal
  account to a team account later.
- When managing team accounts, establish clear role assignments based on each member’s
  responsibilities.
- Pay-as-you-go compute for AI models and compute-intensive workloads.
- No idle costs when your application isn’t processing requests.
- Serverless offers flexible, pay-per-second pricing with no upfront costs.
- Billing starts when the system signals a worker to wake up and ends when the worker
  is fully stopped.
- To improve security, generate a new key with Restricted permission and select the
  minimum permission needed for your use case.
- All requests require authentication using your Runpod API key, passed in the Authorization
  header.
- Requires setup of connected app in api
- When working with public endpoints, following best practices will help you achieve
  better results and optimize performance.
- All Pods are billed by the second for compute and storage, with no additional fees
  for data ingress or egress.
- Container and disk volume storage will be included in your Pod’s displayed hourly
  cost during deployment.
- Every Hub repo requires a `hub.json` and `tests.json` file.
- If you want to manage Pods using the Runpod CLI, you’ll need to install Runpod CLI
  and set your API key in the configuration.
- Storage is provided to support compute tasks.
- Slurm Clusters are currently in beta.
- Slurm Clusters provide a managed high-performance computing and scheduling solution.
- When you terminate the Pod, data in the `/workspace` directory will be preserved
  in the network volume.
- If your Pod has a network volume attached, it cannot be stopped, only terminated.
- A Runpod account with access to the Fine Tuning feature is required.
- Clusters are pre-configured with static IP address management.
- All necessary environment variables for distributed training are pre-configured.
- Pods are scheduled for removal if adequate credit balance is not maintained.
- Your volume data will be preserved, and your container will run the same command
  as it ran the first time you started renting it.
- We implement a spending limit for newer accounts that will grow over time.
- Runpod is fully compliant with the General Data Protection Regulation (GDPR) requirements
  for any data processed within our European data center regions.
- All billing, including per-hour compute and storage billing, is charged per minute.
- Requires a Runpod account with access to the Fine Tuning feature
- Runpod does not provide customer support for community templates.
- Runpod is not designed to be a cloud storage system; storage is provided in the
  pursuit of running tasks using its GPUs, and not meant to be a long-term backup
  solution.
- Referral commissions are based on actual usage, not purchases.
- New accounts only. Referral links only work for brand new Runpod users.
- Card declines are more common than you might think, and the reasons for them might
  not always be clear.
- If you’re using a prepaid card, it’s recommended to deposit in transactions of at
  least $100 to avoid unexpected blocks due to Stripe’s minimums for prepaid cards.
- It’s recommended to use environment variables to set your API key.
- You can create and manage API keys on the Runpod account settings page
- All API requests require authentication using your API key in the request headers.
- 'Worker fails to initialize: Check that your model is compatible with vLLM and your
  GPU has enough VRAM.'
- 'Slow response times: Consider using a more powerful GPU or optimizing your request
  parameters.'
- 'Out of memory errors: Try increasing GPU size or reducing MAX_MODEL_LEN.'
- 'API errors: Verify your endpoint ID and API key are correct.'
- Requires a Runpod account with at least $10 in credits.
- Changing TCP ports will restart your pod and erase all data outside of `/workspace`.
- You can create and manage API keys on the Runpod account settings page.
- Simple tools for migrating data between Runpod instances
- Requires terminal access on your Runpod instances
- Always store important files in /workspace to survive pod restarts
- Choose a port that doesn’t conflict with other services (avoid 8888 if using Jupyter)
- CopyParty handles large file transfers well, making it ideal for model weights and
  datasets
- Uses API key for authentication — requires setup of Runpod account
- Container volumes provide fast read and write speeds since they are locally attached
  to workers.
- Network volumes are ideal for sharing datasets between workers.
- Runpod’s S3-compatible storage integration allows you to connect your Serverless
  endpoints to external object storage services.
- Uses Docker to deploy the worker.
- Requires setup of a Runpod account.
- Network volumes are billed hourly at a rate of $0.07 per GB per month for the first
  1TB, and $0.05 per GB per month for additional storage beyond that.
- Data from the network volume will be accessible to all workers for that endpoint
  from the /runpod-volume directory.
- Deploy ComfyUI on Runpod to create AI-generated images.
- Initial deployment may take up to 30 minutes to fully initialize.
- Workers process request inputs using a handler function and store results for retrieval.
- Runpod automatically manages the worker lifecycle, starting them when needed and
  stopping them when idle to optimize resource usage.
- Handler functions form the core of your Runpod Serverless applications.
- Input validation is recommended to avoid errors during execution.
- 'Payload limits: /run endpoint: 10 MB, /runsync endpoint: 20 MB.'
- Ensure Port 22 is exposed in your Pod for SFTP transfers.
- Endpoints are RESTful APIs that accept HTTP requests, execute your code, and return
  the result via HTTP response.
- You can optimize cost and availability by specifying GPU preferences in order of
  priority.
- After deployment, your endpoint takes time to initialize before it is ready to process
  requests.
- Network volumes are ideal for sharing datasets between workers, storing large models
  that need to be accessed by multiple workers, and preserving data that needs to
  outlive any individual worker.
- Runpod’s S3-compatible storage integration allows you to connect your Serverless
  endpoints to external object storage services, giving you the flexibility to use
  your own storage provider with standardized access protocols.
- Network volumes offer persistent storage that exists independently of the lifecycle
  of a Serverless worker.
- Network volumes can be attached to multiple Serverless endpoints, making them ideal
  for sharing data.
- Uses API key for authentication — requires setup of API key in console.
- Payload limit for /runsync is 20 MB
- Payload limit for /run is 10 MB
- Job results are available for 60 seconds after completion for /runsync
- Job results are available for 30 minutes after completion for /run
- Uses GPU preferences in order of priority.
- Endpoint takes time to initialize before ready to process requests.
- Use asynchronous requests for jobs that take more than a few seconds to complete.
- Implement polling with backoff when checking status of asynchronous jobs.
- Set appropriate timeouts in your client applications and monitor endpoint health
  regularly to detect issues early.
- Implement comprehensive error handling for all API calls.
- Use webhooks for notification-based workflows instead of polling to reduce API calls.
- Cancel unneeded jobs to free up resources and reduce costs.
- During development, use the console testing interface before implementing programmatic
  integration.
- Default active workers is 0
- Default max workers is 3
- Default GPUs per worker is 1
- Default idle timeout is 5 seconds
- Default execution timeout is 600 seconds (10 minutes)
- Default job TTL is 86400000 milliseconds (24 hours)
- vLLM workers come with the vLLM inference engine pre-configured.
- OpenAI API compatibility allows using existing OpenAI client code by changing the
  endpoint URL and API key.
- Implement retry logic with exponential backoff for failed requests.
- Use appropriate timeouts based on model size and complexity.
- The MODEL_NAME environment variable is essential for all OpenAI-compatible API requests.
- Payload limit for /runsync is 20 MB and for /run is 10 MB.
- Results from /runsync are available for 60 seconds, while /run results are available
  for 30 minutes.
- The vLLM worker aims for high compatibility with the OpenAI API.
- Start conservative with max workers and scale up as needed.
- Monitor throttling and adjust max workers accordingly.
- OpenAI API compatibility allows use of existing OpenAI client code by changing the
  endpoint URL and API key.
- Quick-deployed workers download models during initialization, which can take time.
- Use appropriate timeouts based on your model size and complexity.
- Payload limit for /runsync is 20 MB, results available for 60 seconds after completion.
- Payload limit for /run is 10 MB, results available for 30 minutes after completion.
- Queue-based endpoints provide synchronous and asynchronous job processing with automatic
  worker scaling based on demand.
- Job ID is crucial for tracking your request's progress.
- You have up to 30 minutes to retrieve your results via the status endpoint, after
  which results will be automatically deleted for security.
- Job results are available for 60 seconds after completion for /runsync and 30 minutes
  for /run.
- Use a Network volume to attach to your Worker so that it can cache the LLM and decrease
  cold start times.
- Allocate sufficient container disk space for your model. Typically, 20 GB should
  suffice for most models.
- The first time you run this code, it will download the SmolLM3 model (approximately
  6 GB), which may take a minute or two depending on your Pod’s internet connection.
- Requires setup of Hugging Face access token and acceptance of Google’s terms of
  service
- Requires at least $5 in Runpod credits.
- Ensure you’re using a GPU with at least 24 GB VRAM, or try reducing the batch size.
- Check your internet connection and try running the cell again if model download
  fails.
- Wait a few minutes after Pod deployment for services to fully start.
- You have up to 30 minutes to retrieve your results via the status endpoint. After
  this time, the results are automatically deleted for security.
- The minimal requirement to run Fooocus is 4GB Nvidia GPU memory and 8GB system memory.
- Runpod’s GPU Pods use custom Docker images to run your code.
- You can’t directly spin up your own Docker instance or build Docker containers on
  a GPU Pod.
- Use a Network volume to attach to your Worker to cache the LLM and decrease cold
  start times.
- For optimal compatibility, use A100, H100, V100, or RTX 3090 Pods for Axolotl fine-tuning.
- Use the SSH connection to your Pod as it is a persistent connection.
- Requires at least $5 in Runpod credits
- The first time you run the model, it will download approximately 6 GB.
- If the JupyterLab tab is blank when you open it, try stopping and then restarting
  the Pod.
- Many use cases can be addressed by creating a custom template with the desired Docker
  image.
errors:
- 'fatal error: Error during pagination: The same next token was received twice: ...'
- Timeout errors during the CompleteMultipartUpload operation when uploading large
  files (10GB+).
- '502 Bad Gateway: Try increasing AWS_MAX_ATTEMPTS to 10 or more.'
- Requests that are out of time sync by 1 hour will be rejected.
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Recheck API key or token expiration'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- '400 Bad Request: Invalid request format or parameters.'
- '404 Not Found: The requested resource could not be found.'
- '500 Internal Server Error: An error occurred on the server.'
- 'Jobs stuck in pending state: Check resource availability with sinfo.'
- 'Authentication errors: Verify the munge service is running on all nodes.'
- All your Pods are stopped automatically when you don’t have enough funds to keep
  your Pods running for at least ten more minutes.
- Pods are scheduled for removal if adequate credit balance is not maintained.
- If you fail to do so, your Pods will be terminated, and Pod volumes will be removed.
- 'Connection refused: Cannot connect via SFTP, getting ''Connection refused'' error'
- 'Permission denied: Getting ''Permission denied'' when trying to connect via SFTP'
- 'SFTP client can''t find host: SFTP client cannot resolve the host address'
- 'Worker fails to initialize: Check that your model is compatible with vLLM and your
  GPU has enough VRAM'
- 'Slow response times: Consider using a more powerful GPU or optimizing your request
  parameters'
- 'Out of memory errors: Try increasing GPU size or reducing MAX_MODEL_LEN'
- Input is missing the 'seed' key. Please include a seed.
- 'Connection refused: Ensure port 22 is exposed.'
- 'Permission denied: Check password and username.'
- 'SFTP client can''t find host: Verify Pod status and IP address.'
- Ensure your account remains funded to prevent data loss.
- '401 Unauthorized: Check API key validity and permissions.'
- '429 (Too Many Requests): Implement appropriate retry logic with exponential backoff
  to handle rate limiting'
- '400: Bad Request: Check your request format and parameters'
- '401: Unauthorized: Verify your API key is correct and has permission'
- '404: Not Found: Check your endpoint ID'
- '429: Too Many Requests: Implement backoff and retry logic'
- '500: Internal Server Error: Check endpoint logs; worker may have crashed'
- '429: Rate limit exceeded. Try again later.'
- '500: Server error. The model may be having trouble loading.'
- Connection error. Check your network and endpoint ID.
- 'Invalid API key: Ensure that the provided API key is correct.'
- '401 Unauthorized: Check your API key or authentication method.'
- '429: Too Many Requests - Implement retry logic with exponential backoff.'
- '429 (Too Many Requests): Implement appropriate retry logic with exponential backoff
  to handle rate limiting gracefully.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency.'
- 'QUERY_TIMEOUT: Break down filters or add selectivity.'
- '401 Unauthorized: Recheck API key or permissions'
- '429 (Too Many Requests): Implement retry logic with exponential backoff.'
auth_info:
  mentioned_objects: []
client:
  base_url: https://console.runpod.io
  auth:
    type: apikey
source_metadata: null
