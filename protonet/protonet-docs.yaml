resources:
- name: MobileNet
  endpoint:
    path: /keras/applications/MobileNet
    method: GET
    data_selector: model
- name: MobileNetV2
  endpoint:
    path: /keras/applications/MobileNetV2
    method: GET
    data_selector: model
- name: MobileNetV3Small
  endpoint:
    path: /keras/applications/MobileNetV3Small
    method: GET
    data_selector: model
- name: MobileNetV3Large
  endpoint:
    path: /keras/applications/MobileNetV3Large
    method: GET
    data_selector: model
- name: predictions
  endpoint:
    path: /predict
    method: POST
    data_selector: results
    params: {}
- name: inference_set
  endpoint:
    path: inference_set.zip
    method: GET
    data_selector: files
    params: {}
- name: cifar-10
  endpoint:
    path: /data
    method: GET
    data_selector: datasets
    params: {}
- name: model
  endpoint:
    path: /services/data/vXX.X/sobjects/Model
    method: GET
    data_selector: records
- name: training_dataset
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: validation_dataset
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: train_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/TrainDataset
    method: GET
    data_selector: records
    params: {}
- name: test_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/TestDataset
    method: GET
    data_selector: records
    params: {}
- name: train_dataset
  endpoint:
    params:
      batch_size: 32
- name: test_dataset
  endpoint:
    params:
      batch_size: 32
- name: train_videos
  endpoint:
    path: /train_images
    method: GET
    data_selector: train_videos
- name: valid_videos
  endpoint:
    path: /val_images
    method: GET
    data_selector: valid_videos
- name: test_videos
  endpoint:
    path: /test_images
    method: GET
    data_selector: test_videos
- name: model_training
  endpoint:
    path: /train
    method: POST
    data_selector: results
    params:
      epochs: 2
- name: mobilevit_block
  endpoint:
    path: /mobilevit
    method: POST
    data_selector: blocks
    params: {}
- name: model_summary
  endpoint:
    path: /model/summary
    method: GET
    data_selector: summary
- name: train_dataset
  endpoint:
    path: /train
    method: GET
    data_selector: records
    params: {}
- name: val_dataset
  endpoint:
    path: /validation
    method: GET
    data_selector: records
    params: {}
- name: images
  endpoint:
    path: /images
    method: GET
    data_selector: images
- name: camera_poses
  endpoint:
    path: /camera_poses
    method: GET
    data_selector: poses
- name: focal_length
  endpoint:
    path: /focal_length
    method: GET
    data_selector: focal
- name: model
  endpoint:
    path: /models
    method: POST
    data_selector: model
    params: {}
- name: model_training
  endpoint:
    path: /train/model
    method: POST
    data_selector: results
    params: {}
- name: train_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/train_dataset
    method: GET
    data_selector: records
- name: val_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/val_dataset
    method: GET
    data_selector: records
- name: train_images
  endpoint:
    path: ./lol_dataset/our485/low/*
    method: GET
    data_selector: images
    params: {}
- name: val_images
  endpoint:
    path: ./lol_dataset/our485/low/*
    method: GET
    data_selector: images
    params: {}
- name: test_images
  endpoint:
    path: ./lol_dataset/eval15/low/*
    method: GET
    data_selector: images
    params: {}
- name: cifar10
  endpoint:
    path: /~kriz/cifar-10-python.tar.gz
    method: GET
- name: cats_vs_dogs_dataset
  endpoint:
    path: /cats_vs_dogs
    method: GET
    data_selector: records
- name: scheduled_learning_rates
  endpoint:
    path: /learning_rates
    method: GET
    data_selector: lrs
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: train
  endpoint:
    path: /load_data
    method: GET
    data_selector: train
    params: {}
- name: test
  endpoint:
    path: /load_data
    method: GET
    data_selector: test
    params: {}
- name: model
  endpoint:
    path: /gcvitxxtiny.keras
    method: GET
    data_selector: ''
    params: {}
- name: cifar10_data
  endpoint:
    path: /services/data/vXX.X/sobjects/CIFAR10
    method: GET
    data_selector: records
    params: {}
- name: models
  endpoint:
    path: /releases/download/v1.0.0/probing_vits.zip
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: train_dataset
  endpoint:
    path: /coco/2017/train
    method: GET
    data_selector: records
    params: {}
- name: val_dataset
  endpoint:
    path: /coco/2017/validation
    method: GET
    data_selector: records
    params: {}
- name: data
  endpoint:
    path: /data.zip
    method: GET
- name: point_cloud
  endpoint:
    path: /services/data/vXX.X/sobjects/PointCloud
    method: GET
    data_selector: records
- name: Airplane
  endpoint:
    path: /tmp/.keras/datasets/PartAnnotation/metadata.json
    method: GET
    data_selector: records
    params: {}
- name: encoder
  endpoint:
    path: /models/encoder
    method: GET
    data_selector: output_shape
    params: {}
- name: autoencoder
  endpoint:
    path: /models/autoencoder
    method: GET
    data_selector: output_shape
    params: {}
- name: fit_model
  endpoint:
    path: /fit
    method: POST
    data_selector: records
- name: words
  endpoint:
    path: /data/words.txt
    method: GET
    data_selector: lines
    params: {}
- name: images
  endpoint:
    path: /images.tar
    method: GET
    data_selector: images
- name: annotations
  endpoint:
    path: /StanfordExtra_V12/StanfordExtra_v12.json
    method: GET
    data_selector: annotations
- name: train_dataset
  endpoint:
    path: training_data/
    method: GET
    data_selector: image_dataset
    params:
      labels: inferred
      label_mode: categorical
      batch_size: 32
      image_size: (256, 256)
- name: validation_dataset
  endpoint:
    path: validation_data/
    method: GET
    data_selector: image_dataset
    params:
      labels: inferred
      label_mode: categorical
      batch_size: 32
      image_size: (256, 256)
- name: train_videos
  endpoint:
    path: train.csv
    method: GET
    data_selector: ''
    params: {}
- name: test_videos
  endpoint:
    path: test.csv
    method: GET
    data_selector: ''
    params: {}
- name: model_training
  endpoint:
    path: /train
    method: POST
    data_selector: results
- name: depth_data
  endpoint:
    path: /val.tar.gz
    method: GET
    data_selector: data
    params: {}
- name: campaign_member
  endpoint:
    path: /services/data/vXX.X/sobjects/CampaignMember
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: contact
  endpoint:
    path: /services/data/vXX.X/sobjects/Contact
    method: GET
    data_selector: records
    params: {}
- name: training_images
  endpoint:
    path: ./lol_dataset/our485/low/*
    method: GET
    data_selector: images
    params: {}
- name: validation_images
  endpoint:
    path: ./lol_dataset/our485/low/*
    method: GET
    data_selector: images
    params: {}
- name: test_images
  endpoint:
    path: ./lol_dataset/eval15/low/*
    method: GET
    data_selector: images
    params: {}
- name: normal_CT_scans
  endpoint:
    path: /CT-0.zip
    method: GET
    data_selector: files
    params: {}
- name: abnormal_CT_scans
  endpoint:
    path: /CT-23.zip
    method: GET
    data_selector: files
    params: {}
- name: train_dataset
  endpoint:
    path: /tf_flowers
    method: GET
    data_selector: records
    params: {}
- name: inception_resnet_v2
  endpoint:
    path: /keras/applications/inception_resnet_v2
    method: GET
    data_selector: model_instance
    params: {}
- name: weights
  endpoint:
    path: /uc
    method: GET
    data_selector: data
    params:
      id: 1OWKouuAQ7XpXZbWA3mmxDPrFGW71Axrg
- name: train_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/TrainDataset
    method: GET
    data_selector: records
- name: val_dataset
  endpoint:
    path: /services/data/vXX.X/sobjects/ValDataset
    method: GET
    data_selector: records
- name: bit_teacher_flowers
  endpoint:
    path: /bit_teacher_flowers.zip
    method: GET
    data_selector: records
- name: image_classification
  endpoint:
    path: /class_activation_heatmap
    method: POST
    data_selector: heatmap
    params: {}
- name: xception_model
  endpoint:
    path: /xception
    method: POST
    data_selector: model
    params:
      include_top: 'True'
      weights: imagenet
      input_shape: (299, 299, 3)
      classifier_activation: softmax
- name: instance_level_human_parsing
  endpoint:
    path: /keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
    method: GET
    data_selector: data
    params: {}
- name: DeviceMesh
  endpoint:
    path: /services/data/vXX.X/sobjects/DeviceMesh
    method: GET
    data_selector: records
    params: {}
- name: TensorLayout
  endpoint:
    path: /services/data/vXX.X/sobjects/TensorLayout
    method: GET
    data_selector: records
    params: {}
- name: DataParallel
  endpoint:
    path: /services/data/vXX.X/sobjects/DataParallel
    method: GET
    data_selector: records
    params: {}
- name: ModelParallel
  endpoint:
    path: /services/data/vXX.X/sobjects/ModelParallel
    method: GET
    data_selector: records
    params: {}
- name: images
  endpoint:
    path: /records/mzrjq-6wc02/files/caltech-101.zip
    method: GET
    data_selector: files
    params: {}
- name: optimizer
  endpoint:
    path: /optimizers
    method: GET
    data_selector: optimizers
    params: {}
- name: self_driving_car_dataset
  endpoint:
    path: /kaggle/input/dataset/data/images/
    method: GET
- name: train_ds
  endpoint:
    path: /train
    method: GET
    data_selector: records
    params: {}
- name: val_ds
  endpoint:
    path: /val
    method: GET
    data_selector: records
    params: {}
- name: test_ds
  endpoint:
    path: /test
    method: GET
    data_selector: records
    params: {}
- name: train_images
  endpoint:
    path: /train/images.tfrec
    method: GET
    data_selector: records
    params: {}
- name: train_paths
  endpoint:
    path: /train/paths.tfrec
    method: GET
    data_selector: records
    params: {}
- name: test_images
  endpoint:
    path: /test/images.tfrec
    method: GET
    data_selector: records
    params: {}
- name: test_paths
  endpoint:
    path: /test/paths.tfrec
    method: GET
    data_selector: records
    params: {}
- name: Xception
  endpoint:
    path: N/A
    method: N/A
    data_selector: N/A
    params: {}
- name: VGG16
  endpoint:
    path: N/A
    method: N/A
    data_selector: N/A
    params: {}
- name: VGG19
  endpoint:
    path: N/A
    method: N/A
    data_selector: N/A
    params: {}
- name: ResNet50
  endpoint:
    path: N/A
    method: N/A
    data_selector: N/A
    params: {}
- name: InceptionV3
  endpoint:
    path: N/A
    method: N/A
    data_selector: N/A
    params: {}
- name: model_config
  endpoint:
    path: /services/data/vXX.X/sobjects/ModelConfig
    method: GET
    data_selector: records
notes:
- Model can be saved in TF SavedModel format only.
- Recommended format for saving models.
- Requires TensorFlow 2.9 or higher
- Uses a warmup cosine decay learning rate schedule.
- Uses Keras for model training and evaluation
- PSNR values indicate the quality of image reconstruction.
- This example requires TensorFlow 2.4 or higher
- Trained models are evaluated on corrupted datasets.
- Data augmentation is important when working with point cloud data.
- Epochs range from 1 to 20.
- Sparse categorical accuracy is used as a metric for model evaluation.
- Uses trimesh package to read and visualize the .off mesh files
- The videos are lightweight and easy to train on.
- Uses tf.distribute.MirroredStrategy for synchronous training across multiple GPUs.
- Ensure to use tf.data.Dataset objects for efficient data loading.
- This example should be run with Tensorflow 2.13 and higher.
- 'Number of training examples: 3303'
- 'Number of validation examples: 367'
- Learning rate is set to 0.002
- Label smoothing factor is set to 0.1
- 'Total epochs for training: 30'
- The performance of our model is far from optimal, because it was trained on a small
  dataset.
- The model has successfully learned the entire volumetric space through the sparse
  set of images in only 20 epochs.
- You can view the rendered video saved locally, named rgb_video.mp4.
- Uses TensorFlow and Keras for model training.
- Functional API allows for more flexible model creation than Sequential API.
- Models can handle non-linear topology and shared layers.
- FixRes leads to a better performance.
- Another advantage of FixRes is the improved total training time and reduction in
  GPU memory usage.
- FixRes is model-agnostic, you can use it on any image classification model to potentially
  boost performance.
- Uses a smaller resolution dataset for initial training and a larger one for fine-tuning.
- Uses Adam optimizer with learning rate adjustments.
- Uses 300 image pairs from the LoL Dataset's training set for training.
- Uses the remaining 185 image pairs for validation.
- Generates random crops of size 128 x 128 from the image pairs.
- Keras 3 is significantly stricter than Keras 2 about when state can be created.
- Avoid creating state in call() method; create it in constructor or build() method.
- If you ignore state creation recommendations, Keras may attempt to build the layer
  automatically.
- Saving to the TF SavedModel format via model.save() is no longer supported in Keras
  3.
- Setting a tf.Variable as an attribute of a Keras 3 layer or model will not automatically
  track the variable.
- We try to align our implementation with the official implementation.
- Model parameters summary includes 20,867,629 total params, with 20,809,001 trainable
  params.
- 'Corrupt JPEG data: extraneous bytes before marker may affect data quality.'
- Uses a cosine annealing schedule with warmup for learning rates.
- We believe that with a more sophisticated hyperparameter tuning process and a longer
  pretraining it is possible to improve this performance further.
- For comparison, we took the encoder architecture and trained it from scratch in
  a fully supervised manner. This gave us ~76% test top-1 accuracy.
- The authors of MAE demonstrates strong performance on the ImageNet-1k dataset as
  well as other downstream tasks like object detection and semantic segmentation.
- This idea of using BERT flavored pretraining in computer vision was also explored
  in Selfie, but it could not demonstrate strong results.
- Another concurrent work that explores the idea of masked image modeling is SimMIM.
- Finally, as a fun fact, we, the authors of this example also explored the idea of
  'reconstruction as a pretext task' in 2020 but we could not prevent the network
  from representation collapse, and hence we did not get strong downstream performance.
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- Some objects like Contact may return nulls in deeply nested fields
- Uses AdamW optimizer with weight decay.
- Loss function is SparseCategoricalCrossentropy.
- Accuracy calculated as a sanity-check.
- Dataset contains a training set of 50,000 images for 10 classes.
- Standard image size is (32, 32, 3).
- Uses multi-backend Keras 3.0 for model implementation.
- Compatible with TensorFlow, PyTorch, and JAX.
- Model has both Transformer and CNN modules
- Layer 'head' expected 2 variables, but received 0 variables during loading
- The model used in this experiment is termed as ConvMixer-256/8 where 256 denotes
  the number of channels and 8 denotes the depth.
- The resulting model only has 0.8 million parameters.
- Pretrained models are not implemented in Keras but are available in the repository.
- 'Accuracy on CIFAR-10: 64% for this notebook. This is much better than the 10% we
  get from random guessing.'
- The Barlow twins algorithm is heavily reliant on Augmentation. One unique feature
  of the method is that sometimes, augmentations probabilistically occur.
- Change this to `model_dir` when not using the downloaded weights
- Uses a smaller subset of ~500 images for training in this example
- The PointNet model is designed for unordered sets of coordinates.
- Operations must be invariant to different representations of point cloud data.
- For the training the authors recommend using a learning rate schedule that decays
  the initial learning rate by half every 20 epochs.
- Uses random sampling for point clouds
- Normalization applied to point clouds for scale-invariance
- Each Keras Application expects a specific kind of input preprocessing.
- Call keras.applications.inception_v3.preprocess_input on your inputs before passing
  them to the model.
- We follow Semantic Versioning, and plan to provide backwards compatibility guarantees
  both for code and saved models built with our components.
- The functional API can handle models with non-linear topology, shared layers, and
  even multiple inputs or outputs.
- Uses Keras functional API for building models
- Models can have shared layers
- 'The authors explicitly instruct users not to share the JSON file, and this example
  respects this wish: you should obtain the JSON file yourself.'
- Mixed precision training is the use of lower-precision operations (`float16` and
  `bfloat16`) in a model during training to make it run faster and use less memory.
- Using mixed precision can improve performance by more than 3 times on modern GPUs
  and 60% on TPUs.
- Variables storage (as well as certain sensitive computations) should still be in
  `float32` to preserve numerical stability.
- To start using mixed precision on GPU, you would simply call `keras.mixed_precision.set_global_policy("mixed_float16")`
  at the start of your program.
- On TPU, you would call `keras.mixed_precision.set_global_policy("mixed_bfloat16")`.
- With the help of Shifted Patch Tokenization and Locality Self Attention, we were
  able to get ~3-4% top-1 accuracy gains on CIFAR100.
- The ideas on Shifted Patch Tokenization and Locality Self Attention are very intuitive
  and easy to implement.
- This example requires TensorFlow 2.6 or higher.
- 'The default directory where all Keras data is stored is: $HOME/.keras/'
- If Keras cannot create the above directory, /tmp/.keras/ is used as a backup.
- This example requires TensorFlow 2.5 or higher.
- To keep the runtime of this example relatively short, we just used a few training
  examples.
- Uses data parallelism for training on multiple GPUs.
- We only use the indoor images to train our depth estimation model.
- The validation set is used for training and evaluation subsets for our model.
- Uses OAuth2 with refresh token — requires setup of connected app in api
- Uses a dataset of low-light images consisting of training, validation, and test
  sets.
- The number of samples is very small (only 200) and we don't specify a random seed.
  As such, you can expect significant variance in the results.
- A variability of 6-7% in the classification performance is observed in both cases.
- The validation set is class-balanced, accuracy provides an unbiased representation
  of the model's performance.
- Files are provided in Nifti format with the extension .nii.
- CT scans store raw voxel intensity in Hounsfield units (HU).
- BiT performs well across a surprisingly wide range of data regimes – from 1 example
  per class to 1M total examples.
- Each Keras Application expects specific input preprocessing.
- Call keras.applications.inception_resnet_v2.preprocess_input on inputs before passing
  them to the model.
- BASNet was trained on DUTS-TR dataset, which has 10553 images.
- Model was trained for 400k iterations with a batch size of eight and without a validation
  dataset.
- Model was trained for 120k iterations to demonstrate capabilities due to computer
  power limitation.
- Uses hybrid loss function combining binary cross entropy, structural similarity
  and intersection-over-union losses.
- Uses TensorFlow and TensorFlow Addons for implementation
- Adjust the model parameters according to your dataset size
- Model is based on the BiT family of ResNets fine-tuned on the tf_flowers dataset.
- The teacher model has about 212 Million parameters.
- Default input image size for this model is 299x299
- Call keras.applications.xception.preprocess_input on inputs before passing them
  to the model
- The EANet model achieved ~73% test top-5 accuracy and ~43% top-1 accuracy after
  training 50 epochs with 0.3M parameters.
- The traditional Vit achieved a ~73% test top-5 accuracy and ~41% top-1 accuracy
  with 0.6M parameters.
- Uses a temperature of 0.1 for training
- Queue size of 10,000 used in the model
- Uses trained model hosted on Hugging Face Hub
- Training on the entire CIHP dataset with 38,280 images takes a lot of time, hence
  we will be using a smaller subset of 200 images for training our model in this example.
- KerasHub provides access to pre-trained models via the keras_hub.models API.
- These pre-trained models are provided on an "as is" basis, without warranties or
  conditions of any kind.
- The distribution API is only implemented for the JAX backend for now.
- The dataset focuses on images of airplanes from the Caltech 101 Dataset.
- You can either instantiate an optimizer or pass it by its string identifier.
- Learning rate schedules can be used to modulate the optimizer's learning rate over
  time.
- Uses TensorFlow's tf.data pipeline for custom dataset loading
- The model is trained for a maximum of 100 epochs.
- Class weights are defined to balance the training data.
- This example must be on Colab with the TPU runtime selected.
- Large image datasets should not be cached in memory.
- Weights are downloaded automatically when instantiating a model.
- Models can be used for prediction, feature extraction, and fine-tuning.
- Default configuration for CaiT models (cait_xxs24_224)
- Pretrained model available for inference
- Uses a Siamese Network with triplet loss function for image similarity comparison.
errors:
- 'ValueError: Total number of steps must be larger or equal to warmup steps.'
- 'ValueError: lr_start must be smaller or equal to lr_max.'
- 'Invalid input image: Ensure the image paths are correct.'
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- 'Validation accuracy: 86.1%'
- 'ValueError: Resolution is not supported.'
- ReduceLROnPlateau reducing learning rate when performance plateaus.
- 'ValueError: Invalid filepath extension for saving. Please add either a .keras extension
  for the native Keras format or a .h5 extension.'
- 'WARNING:tensorflow: From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83:
  Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness)
  is deprecated and will be removed after 2023-09-23.'
- Corrupt JPEG data may lead to data quality issues.
- 'Layer ''head'' expected 2 variables, but received 0 variables during loading. Expected:
  [''kernel'', ''bias'']'
- 'RuntimeError: Interruption'
- 'PermissionError: Keras cannot create the directory'
- Mean absolute error during evaluation on DUTS-TE dataset was 0.042.
- 'OUT_OF_MEMORY: Reduce batch size or model complexity'
- 'INVALID_ARGUMENT: Check input shapes and types'
- Model may overfit if not monitored carefully.
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
client:
  headers:
    Accept: application/json
  base_url: ~/.keras
  auth:
    type: oauth2
    flow: refresh_token
    token_url: https://login.api.com/services/oauth2/token
    client_id: '{{ dlt.secrets[''api_client_id''] }}'
    client_secret: '{{ dlt.secrets[''api_client_secret''] }}'
    refresh_token: '{{ dlt.secrets[''api_refresh_token''] }}'
    location: header
    header_name: Authorization
  paginator:
    default_page_size: 200
    type: cursor
    cursor_path: nextRecordsUrl
    cursor_param: nextUrl
    page_size_param: pageSize
source_metadata: null
