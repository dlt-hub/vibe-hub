resources:
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
    params:
      formats:
      - markdown
      - html
      - json
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
    params: {}
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
    params: {}
- name: get_crawl_status
  endpoint:
    path: /crawl/{crawl-id}
    method: GET
    data_selector: data
    params: {}
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
- name: get_crawl_status
  endpoint:
    path: /crawl/status
    method: GET
    data_selector: data
- name: cancel_crawl
  endpoint:
    path: /crawl/cancel
    method: POST
    data_selector: data
- name: map
  endpoint:
    path: /map
    method: POST
    data_selector: data
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: scrapeResponse
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: crawlResponse
- name: firecrawl_scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: content
    params: {}
- name: firecrawl_batch_scrape
  endpoint:
    path: /batch_scrape
    method: POST
    data_selector: content
    params: {}
- name: firecrawl_check_batch_status
  endpoint:
    path: /check_batch_status
    method: GET
    data_selector: status
    params: {}
- name: firecrawl_search
  endpoint:
    path: /search
    method: GET
    data_selector: results
    params: {}
- name: firecrawl_extract
  endpoint:
    path: /extract
    method: POST
    data_selector: content
    params:
      urls:
      - https://example.com/page1
      - https://example.com/page2
      prompt: Extract product information including name, price, and description
      systemPrompt: You are a helpful assistant that extracts product information
      schema:
        type: object
        properties:
          name:
            type: string
          price:
            type: number
          description:
            type: string
        required:
        - name
        - price
      allowExternalLinks: false
      enableWebSearch: false
      includeSubdomains: false
- name: firecrawl_deep_research
  endpoint:
    path: /deep_research
    method: POST
    data_selector: finalAnalysis
    params:
      query: how does carbon capture technology work?
      maxDepth: 3
      timeLimit: 120
      maxUrls: 50
- name: firecrawl_generate_llmstxt
  endpoint:
    path: /generate_llmstxt
    method: POST
    data_selector: llmstxt
    params:
      url: https://example.com
      maxUrls: 20
      showFullText: true
- name: firecrawl_scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: content
- name: firecrawl_batch_scrape
  endpoint:
    path: /batch_scrape
    method: POST
    data_selector: content
- name: firecrawl_check_batch_status
  endpoint:
    path: /check_batch_status
    method: GET
    data_selector: content
- name: firecrawl_search
  endpoint:
    path: /search
    method: GET
    data_selector: content
- name: firecrawl_crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: content
- name: firecrawl_extract
  endpoint:
    path: /firecrawl_extract
    method: POST
    data_selector: content
    params: {}
- name: firecrawl_deep_research
  endpoint:
    path: /firecrawl_deep_research
    method: POST
    data_selector: content
    params: {}
- name: firecrawl_generate_llmstxt
  endpoint:
    path: /firecrawl_generate_llmstxt
    method: POST
    data_selector: content
    params: {}
- name: scrape
  endpoint:
    path: /v2/scrape
    method: POST
    data_selector: doc
    params: {}
- name: crawl
  endpoint:
    path: /v2/crawl
    method: POST
    data_selector: id
    params: {}
- name: map
  endpoint:
    path: /v2/map
    method: POST
    data_selector: links
    params: {}
- name: search
  endpoint:
    path: /v2/search
    method: POST
    data_selector: data
    params:
      limit: 3
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: data
    params: {}
- name: map
  endpoint:
    path: /v2/map
    method: POST
    data_selector: links
- name: extract
  endpoint:
    path: /v2/extract
    method: POST
    data_selector: data
    params: {}
- name: extract
  endpoint:
    path: /extract
    method: POST
    data_selector: data
    params: {}
- name: crawl
  endpoint:
    path: /v0/crawl
    method: POST
    data_selector: ''
    params: {}
- name: test
  endpoint:
    path: /test
    method: GET
    data_selector: ''
    params: {}
- name: crawl
  endpoint:
    path: /v0/crawl
    method: POST
    data_selector: null
    params: {}
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
    params: {}
- name: batch_scrape
  endpoint:
    path: /batch/scrape
    method: POST
- name: get_batch_scrape_status
  endpoint:
    path: /batch/scrape/get
    method: GET
- name: get_batch_scrape_errors
  endpoint:
    path: /batch/scrape/get-errors
    method: GET
- name: batch_scrape_status
  endpoint:
    path: /v2/batch/scrape/{id}
    method: GET
    data_selector: data
    params: {}
- name: Cancel Batch Scrape
  endpoint:
    path: /batch/scrape/{id}
    method: DELETE
- name: get_batch_scrape_errors
  endpoint:
    path: /v2/batch/scrape/{id}/errors
    method: GET
    data_selector: errors
    params: {}
- name: search
  endpoint:
    path: /search
    method: POST
    data_selector: data.web
    params: {}
- name: map
  endpoint:
    path: /map
    method: POST
    data_selector: links
- name: crawl
  endpoint:
    path: /crawl
    method: POST
- name: get_crawl_status
  endpoint:
    path: /crawl-get
    method: GET
- name: cancel_crawl
  endpoint:
    path: /crawl-delete
    method: DEL
- name: get_crawl_errors
  endpoint:
    path: /crawl-get-errors
    method: GET
- name: get_active_crawls
  endpoint:
    path: /crawl-active
    method: GET
- name: crawl_status
  endpoint:
    path: /v2/crawl/{id}
    method: GET
    data_selector: data
- name: crawl_params_preview
  endpoint:
    path: /v2/crawl/params-preview
    method: POST
    data_selector: data
    params: {}
- name: cancel_crawl
  endpoint:
    path: /crawl/{id}
    method: DELETE
- name: get_crawl_errors
  endpoint:
    path: /v2/crawl/{id}/errors
    method: GET
    data_selector: errors
    params: {}
- name: active_crawls
  endpoint:
    path: /v2/crawl/active
    method: GET
    data_selector: crawls
- name: extract
  endpoint:
    path: /extract
    method: POST
- name: extract_status
  endpoint:
    path: /extract/{id}
    method: GET
    data_selector: data
    params: {}
- name: credit_usage
  endpoint:
    path: /team/credit-usage
    method: GET
    data_selector: data
    params: {}
- name: token_usage
  endpoint:
    path: /v2/team/token-usage
    method: GET
    data_selector: data.remainingTokens
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
- name: get_crawl_status
  endpoint:
    path: /crawl/{crawl-id}
    method: GET
    data_selector: data
- name: scrape
  endpoint:
    path: /v1/scrape
    method: POST
    data_selector: data
- name: crawl
  endpoint:
    path: /v1/crawl
    method: POST
    data_selector: data
- name: get_crawl_status
  endpoint:
    path: /v1/crawl/{crawl-id}
    method: GET
    data_selector: data
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
    params: {}
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
    params: {}
- name: get_crawl_status
  endpoint:
    path: /crawl/{crawl-id}
    method: GET
    data_selector: data
    params: {}
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
- name: get_crawl_status
  endpoint:
    path: /crawl/{crawl-id}
    method: GET
    data_selector: data
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data
- name: crawl
  endpoint:
    path: /crawl
    method: POST
    data_selector: data
- name: get_crawl_status
  endpoint:
    path: /crawl/{crawl-id}
    method: GET
    data_selector: data
- name: batch_scrape
  endpoint:
    path: /v1/batch/scrape
    method: POST
    data_selector: data
    params: {}
- name: scrape
  endpoint:
    path: /scrape
    method: POST
    data_selector: data.json
- name: change_tracking
  endpoint:
    path: /api/changeTracking
    method: POST
    data_selector: changeTracking
    params: {}
- name: proxies
  endpoint:
    path: /features/proxies
    method: GET
- name: webhook
  endpoint:
    path: /v2/crawl
    method: POST
    data_selector: null
    params: {}
- name: batch_scrape
  endpoint:
    path: /v2/batch/scrape
    method: POST
    data_selector: null
    params: {}
notes:
- 'Default freshness window: maxAge = 172800000 ms (2 days).'
- Batch scrape jobs expire after 24 hours.
- Firecrawl converts web pages into markdown, ideal for LLM applications.
- 'Handles dynamic content: dynamic websites, js-rendered sites, PDFs, images.'
- API Key is required to use the API.
- Set the API key as an environment variable named FIRECRAWL_API_KEY or pass it as
  a parameter to the Firecrawl class.
- API key must be set as an environment variable named FIRECRAWL_API_KEY.
- This SDK currently uses the v1 version of the Firecrawl API, which is not the most
  recent (v2 is available). Some features and improvements may only be available in
  v2.
- Requires API key to access the cloud API.
- Supports both cloud and self-hosted modes.
- Requires API key for authentication.
- Use SSE for real-time updates.
- Supports both cloud AI and self-hosted LLM extraction.
- Requests are cached with maxAge defaulting to 2 days, and sensible defaults like
  blockAds, skipTlsVerification, and removeBase64Images are enabled.
- You can now specify 'summary' as a format to directly receive a concise summary
  of the page content.
- 'JSON extraction and change tracking now use an object format: { type: ''json'',
  prompt, schema }.'
- Rate limits are enforced to ensure fair usage and availability of the API for all
  users.
- 'Use `parsers: ["pdf"]` to ensure PDF parsing.'
- Default timeout is 30000 milliseconds.
- When using the search endpoint with scraping enabled, be aware of cost factors.
- 'Set parsers: [] if you don’t need PDF content.'
- This endpoint prioritizes speed, so it may not capture all website links.
- Extract is billed differently than other endpoints.
- The /extract endpoint is still in Beta.
- The /extract endpoint supports extracting structured data using a prompt without
  needing specific URLs.
- To turn on DB authentication, you need to set up supabase.
- You’re going to need to open 3 terminals for running the services.
- You can run the test with `npm run test:local-no-auth` if you’d like to run the
  tests without authentication.
- Self-hosting Firecrawl is ideal for those who need full control over their scraping
  and data processing environments but comes with the trade-off of additional maintenance
  and configuration efforts.
- Requests are cached with maxAge defaulting to 2 days
- blockAds, skipTlsVerification, and removeBase64Images are enabled by default
- Search endpoint combines web search with Firecrawl’s scraping capabilities.
- Authorization is required as a Bearer token in the header.
- Authorization is a Bearer token included in the header.
- Authorization header of the form `Bearer <token>`
- Uses API key for authentication
- To use the API, you need to sign up on Firecrawl and get an API key.
- Firecrawl is open source available under the AGPL-3.0 license
- 'Default freshness: maxAge = 172800000 ms (2 days).'
- If our data is older than maxAge, we scrape fresh automatically.
- You’ll never get data older than your specified maxAge.
- Batch jobs expire after 24 hours
- Extract structured data from web pages using LLMs.
- The change tracking feature is currently in beta.
- Stealth proxy requests cost 5 credits per request when used.
- Firecrawl routes all requests through proxies to help ensure reliability and access.
- If you do not specify a proxy or location, Firecrawl will automatically select the
  best option based on the target site and your request.
- Webhooks allow real-time notifications for Firecrawl operations.
- Default events include started, page, completed, and failed.
- Webhooks allow you to receive real-time notifications about your Firecrawl operations
  as they progress.
errors:
- '400: Verify the correctness of the parameters.'
- '401: The API key was not provided.'
- '429: The rate limit has been surpassed.'
- '400 Bad Request: Check request parameters.'
- '401 Unauthorized: Ensure API key is valid.'
- '401 Unauthorized: Check API key.'
- '429 Too Many Requests: Rate limit exceeded.'
- '500 Internal Server Error: Try again later.'
- 'REQUEST_LIMIT_EXCEEDED: Increase your limits or reduce the number of requests.'
- 'QUERY_TIMEOUT: Break down the query or try again.'
- '401 Unauthorized: Check API key validity.'
- '401 Unauthorized: Recheck API key validity'
- '429 Too Many Requests: Throttle your requests'
- ERROR - Attempted to access Supabase client when it's not configured.
- WARN - You're bypassing authentication
- '200: Successful response'
- '402: Payment required'
- '429: Too many requests'
- '500: Internal server error'
- '200'
- '402'
- '429'
- '500'
- '408: Request Timeout'
- '500: Internal Server Error'
- '402: Payment Required'
- '429: Too Many Requests'
- '400: Bad Request'
- '401: Unauthorized'
- '404'
- '401 Unauthorized: Recheck API key'
- 'REQUEST_TIMEOUT: Handle potentially missing changeTracking object in the response.'
auth_info:
  mentioned_objects: []
client:
  base_url: https://api.firecrawl.dev
  auth:
    type: apikey
source_metadata: null
