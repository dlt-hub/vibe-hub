client:
  auth: To access the server, you will need to provide an API key, which can be passed
    as the `--api-key` argument or set as the environment variable `VLLM_API_KEY`
    in the header. Additionally, for remote files, a Hugging Face token is required,
    which can be obtained by running `huggingface-cli login`.
  most_recurring_base_url: http://localhost:8000
  paginator: Pagination information not found.
endpoints:
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/contributing/index.html':
  - http://127.0.0.1:8000
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/cli/index.html':
  - http://{vllm-serve-host
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/getting_started/quickstart.html':
  - /v1/models
  - /v1
  - /v1/completions
  - /`.
  - /v1/chat/completions
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/design/v1/metrics.html':
  - http://0.0.0.0:8000/metrics
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/multimodal_inputs.html':
  - /v1
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/usage/metrics.html':
  - http://0.0.0.0:8000/metrics
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/serving/openai_compatible_server.html':
  - /v1/score
  - http://127.0.0.1:8000/classify
  - http://127.0.0.1:8000/score
  - /v1
  - /v1/embeddings
  - http://127.0.0.1:8000/v1/rerank
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/deployment/k8s.html':
  - http://mistral-7b.default.svc.cluster.local/v1/completions
  - http://0.0.0.0:8000
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/deployment/nginx.html':
  - http://backend;
  - http://0.0.0.0:8000
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/models/pooling_models.html':
  - http://127.0.0.1:8000/v1/embeddings
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/lora.html':
  - /v1/load_lora_adapter
  - /v1/unload_lora_adapter
  - /v1/models
  - /v1/completions
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/reasoning_outputs.html':
  - /v1
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/spec_decode.html':
  - /v1
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/structured_outputs.html':
  - http://0.0.0.0:8000/v1
  - /v1
- 'endpoints source: https://docs.vllm.ai/en/v0.10.0/features/tool_calling.html':
  - /v1
