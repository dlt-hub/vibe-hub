resources:
- name: DebeziumServer
  endpoint:
    path: /debezium-server
    method: POST
- name: MongoDB
  endpoint:
    path: /documentation/reference/3.4/mongodb
    method: GET
    data_selector: records
- name: PostgreSQL
  endpoint:
    path: /documentation/reference/3.4/postgresql
    method: GET
    data_selector: records
- name: MySQL
  endpoint:
    path: /documentation/reference/3.4/mysql
    method: GET
    data_selector: records
- name: MariaDB
  endpoint:
    path: /documentation/reference/3.4/mariadb
    method: GET
    data_selector: records
- name: SQL Server
  endpoint:
    path: /documentation/reference/3.4/sqlserver
    method: GET
    data_selector: records
- name: CockroachDB
  endpoint:
    path: /documentation/reference/3.4/cockroachdb
    method: GET
    data_selector: records
- name: cassandra_3_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-3/2.7.4.Final/debezium-connector-cassandra-3-2.7.4.Final-plugin.tar.gz
    method: GET
- name: cassandra_4_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-4/2.7.4.Final/debezium-connector-cassandra-4-2.7.4.Final-plugin.tar.gz
    method: GET
- name: db2_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-db2/2.7.4.Final/debezium-connector-db2-2.7.4.Final-plugin.tar.gz
    method: GET
- name: informix_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-informix/2.7.4.Final/debezium-connector-informix-2.7.4.Final-plugin.tar.gz
    method: GET
- name: jdbc_sink_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-jdbc/2.7.4.Final/debezium-connector-jdbc-2.7.4.Final-plugin.tar.gz
    method: GET
- name: mariadb_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mariadb/2.7.4.Final/debezium-connector-mariadb-2.7.4.Final-plugin.tar.gz
    method: GET
- name: mongodb_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mongodb/2.7.4.Final/debezium-connector-mongodb-2.7.4.Final-plugin.tar.gz
    method: GET
- name: mysql_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mysql/2.7.4.Final/debezium-connector-mysql-2.7.4.Final-plugin.tar.gz
    method: GET
- name: oracle_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-oracle/2.7.4.Final/debezium-connector-oracle-2.7.4.Final-plugin.tar.gz
    method: GET
- name: postgresql_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-postgres/2.7.4.Final/debezium-connector-postgres-2.7.4.Final-plugin.tar.gz
    method: GET
- name: spanner_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-spanner/2.7.4.Final/debezium-connector-spanner-2.7.4.Final-plugin.tar.gz
    method: GET
- name: sql_server_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-sqlserver/2.7.4.Final/debezium-connector-sqlserver-2.7.4.Final-plugin.tar.gz
    method: GET
- name: vitess_plugin
  endpoint:
    path: /maven2/io/debezium/debezium-connector-vitess/2.7.4.Final/debezium-connector-vitess-2.7.4.Final-plugin.tar.gz
    method: GET
- name: Cassandra 3 Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-cassandra-3/2.7.1.Final/debezium-connector-cassandra-3-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 4 Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-cassandra-4/2.7.1.Final/debezium-connector-cassandra-4-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Db2 Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-db2/2.7.1.Final/debezium-connector-db2-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Db2 for IBM i Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-ibmi/2.7.1.Final/debezium-connector-ibmi-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Informix Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-informix/2.7.1.Final/debezium-connector-informix-2.7.1.Final-plugin.tar.gz
    method: GET
- name: JDBC Sink Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-jdbc/2.7.1.Final/debezium-connector-jdbc-2.7.1.Final-plugin.tar.gz
    method: GET
- name: MariaDB Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mariadb/2.7.1.Final/debezium-connector-mariadb-2.7.1.Final-plugin.tar.gz
    method: GET
- name: MongoDB Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mongodb/2.7.1.Final/debezium-connector-mongodb-2.7.1.Final-plugin.tar.gz
    method: GET
- name: MySQL Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/2.7.1.Final/debezium-connector-mysql-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Oracle Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-oracle/2.7.1.Final/debezium-connector-oracle-2.7.1.Final-plugin.tar.gz
    method: GET
- name: PostgreSQL Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.7.1.Final/debezium-connector-postgres-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Google Spanner Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-spanner/2.7.1.Final/debezium-connector-spanner-2.7.1.Final-plugin.tar.gz
    method: GET
- name: SQL Server Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/2.7.1.Final/debezium-connector-sqlserver-2.7.1.Final-plugin.tar.gz
    method: GET
- name: Vitess Connector Plug-in
  endpoint:
    path: https://repo1.maven.org/maven2/io/debezium/debezium-connector-vitess/2.7.1.Final/debezium-connector-vitess-2.7.1.Final-plugin.tar.gz
    method: GET
- name: cdc_updates
  endpoint:
    path: /services/data/vXX.X/sobjects/CDCUpdates
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: schema_change
  endpoint:
    path: /schema/change
    method: POST
    data_selector: payload
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot_info
    params: {}
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot
    params:
      type: incremental
- name: incremental_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-ad-hoc-incremental-snapshots
    method: INSERT
    data_selector: data-collections
    params: {}
- name: stop_snapshot
  endpoint:
    path: N/A
    method: N/A
    data_selector: data
    params:
      type: incremental
      data-collections:
      - db1.table1
      - db1.table2
- name: snapshot_events
  endpoint:
    path: /path/to/snapshot/events
    method: GET
    data_selector: events
    params: {}
- name: blocking_snapshot
  endpoint:
    path: /path/to/blocking/snapshot
    method: GET
    data_selector: snapshots
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: truncate_event
  endpoint:
    path: /events/truncate
    method: POST
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params:
      database.hostname: 192.168.99.100
      database.port: '3306'
      database.user: debezium-user
      database.password: debezium-user-pw
      database.server.id: '184054'
      topic.prefix: fullfillment
      database.include.list: inventory
      schema.history.internal.kafka.bootstrap.servers: kafka:9092
      schema.history.internal.kafka.topic: schemahistory.fullfillment
      include.schema.changes: 'true'
- name: schema_history
  endpoint:
    path: /schema/history
    method: GET
    data_selector: records
- name: signal
  endpoint:
    path: /signal
    method: GET
    data_selector: records
- name: notification_sink
  endpoint:
    path: /notification/sink
    method: GET
    data_selector: records
- name: snapshot_metrics
  endpoint:
    path: /mysql/snapshot/metrics
    method: GET
    data_selector: attributes
- name: streaming_metrics
  endpoint:
    path: /mysql/streaming/metrics
    method: GET
    data_selector: attributes
- name: schema_history_metrics
  endpoint:
    path: /mysql/schema-history/metrics
    method: GET
    data_selector: attributes
- name: mongo_change_events
  endpoint:
    path: /mongodb/change-streams
    method: GET
    data_selector: events
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot
    params:
      type: incremental
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector/config
    method: POST
    data_selector: config
- name: mongodb
  endpoint:
    path: /mongodb
    method: GET
    data_selector: records
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: <topic.prefix>-signal
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: kafka-signal
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: No default
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: '100'
    method: ''
    data_selector: ''
    params: {}
- name: notification.sink.topic.name
  endpoint:
    path: No default
    method: ''
    data_selector: ''
    params: {}
- name: publication
  endpoint:
    path: /path/to/publication/endpoint
    method: POST
    data_selector: data
    params:
      publication.autocreate.mode: filtered
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - schema.table
- name: customers
  endpoint:
    path: /path/to/customers
    method: GET
    data_selector: payload.after
    params: {}
- name: default_values
  endpoint:
    path: /default/values
    method: GET
    data_selector: records
- name: custom_converters
  endpoint:
    path: /custom/converters
    method: GET
    data_selector: records
- name: setting_up_postgresql
  endpoint:
    path: /setting/up/postgresql
    method: GET
    data_selector: records
- name: postgresql_in_the_cloud
  endpoint:
    path: /postgresql/in/the/cloud
    method: GET
    data_selector: records
- name: installing_logical_decoding_plugin
  endpoint:
    path: /installing/logical/decoding/plugin
    method: GET
    data_selector: records
- name: plugin_differences
  endpoint:
    path: /plugin/differences
    method: GET
    data_selector: records
- name: configuring_postgresql_server
  endpoint:
    path: /configuring/postgresql/server
    method: GET
    data_selector: records
- name: debezium_signal
  endpoint:
    path: /myschema/debezium_signal
    method: INSERT
    data_selector: data
    params: {}
- name: customers
  endpoint:
    path: /services/data/vXX.X/sobjects/customers
    method: GET
    data_selector: records
    params: {}
- name: fulfillment-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: inventory.customers
  endpoint:
    path: /services/data/vXX.X/sobjects/Inventory.Customers
    method: GET
    data_selector: records
    params:
      message_key: pk1,pk2
- name: purchase.orders
  endpoint:
    path: /services/data/vXX.X/sobjects/Purchase.Orders
    method: GET
    data_selector: records
    params:
      message_key: pk3,pk4
- name: create_publication
  endpoint:
    path: /services/data/vXX.X/sobjects/CreatePublication
    method: POST
    data_selector: records
    params:
      publish_via_partition_root: 'true'
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: data
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: temporal_types
  endpoint:
    path: /temporal_types
    method: GET
    data_selector: records
    params: {}
- name: rowid_types
  endpoint:
    path: /rowid_types
    method: GET
    data_selector: records
    params: {}
- name: xml_types
  endpoint:
    path: /xml_types
    method: GET
    data_selector: records
    params: {}
- name: user_defined_types
  endpoint:
    path: /user_defined_types
    method: GET
    data_selector: records
    params: {}
- name: oracle_supplied_types
  endpoint:
    path: /oracle_supplied_types
    method: GET
    data_selector: records
    params: {}
- name: default_values
  endpoint:
    path: /default_values
    method: GET
    data_selector: records
    params: {}
- name: custom_converters
  endpoint:
    path: /custom_converters
    method: GET
    data_selector: records
    params: {}
- name: setup_oracle
  endpoint:
    path: /setup_oracle
    method: GET
    data_selector: records
    params: {}
- name: creating_connector_users
  endpoint:
    path: /creating_connector_users
    method: GET
    data_selector: records
    params: {}
- name: archive_log_destinations
  endpoint:
    path: /archive_log_destinations
    method: GET
    data_selector: records
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: Debezium Oracle Connector
  endpoint:
    path: /connector
    method: POST
    data_selector: config
    params: {}
- name: configuration
  endpoint:
    path: /configuration
    method: GET
    data_selector: records
    params: {}
- name: schema.history.internal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /metrics/snapshot
    method: GET
    data_selector: metrics
- name: streaming_metrics
  endpoint:
    path: /metrics/streaming
    method: GET
    data_selector: metrics
- name: schema_history_metrics
  endpoint:
    path: /metrics/schema-history
    method: GET
    data_selector: metrics
- name: streaming_metrics
  endpoint:
    path: /connector-metrics/streaming
    method: GET
    data_selector: attributes
    params: {}
- name: schema_history_metrics
  endpoint:
    path: /connector-metrics/schema-history
    method: GET
    data_selector: attributes
    params: {}
- name: schema_changes
  endpoint:
    path: /schema/changes
    method: POST
    data_selector: data
    params: {}
- name: change_events
  endpoint:
    path: /connector/oracle/change_events
    method: GET
    data_selector: events
- name: incremental_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-ad-hoc-incremental-snapshots
    method: INSERT
    data_selector: data-collections
    params: {}
- name: stop_incremental_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-source-signaling-channel
    method: INSERT
    data_selector: data-collections
    params: {}
- name: customers
  endpoint:
    path: /server1/DEBEZIUM/CUSTOMERS
    method: GET
    data_selector: payload
    params: {}
- name: XMLTYPE
  endpoint:
    path: /path/to/xmltype/endpoint
    method: GET
    data_selector: data
    params: {}
- name: NUMBER
  endpoint:
    path: /path/to/number/endpoint
    method: GET
    data_selector: data
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: OpenLogReplicator
  endpoint:
    path: /scripts/OpenLogReplicator.json
    method: GET
    data_selector: source
    params: {}
- name: offsets
  endpoint:
    path: /offsets
    method: GET
    data_selector: offsets
    params: {}
- name: snapshot
  endpoint:
    path: /db2/snapshot
    method: GET
    data_selector: records
- name: execute-snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: signal
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-kafka-signaling-channel
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - schema1.table1
      - schema1.table2
- name: transaction_metadata
  endpoint:
    path: /transaction/metadata
    method: GET
    data_selector: records
- name: data_change_events
  endpoint:
    path: /data/change/events
    method: GET
    data_selector: records
- name: db2-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: Debezium Database Connector
  endpoint:
    path: /services/data/vXX.X/sobjects/DebeziumDatabaseConnector
    method: POST
    data_selector: records
- name: schema.history.internal.kafka.topic
  endpoint:
    path: /schema.history.internal.kafka.topic
    method: GET
    data_selector: topic
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: /schema.history.internal.kafka.bootstrap.servers
    method: GET
    data_selector: bootstrap.servers
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: /schema.history.internal.kafka.recovery.poll.interval.ms
    method: GET
    data_selector: poll.interval
    params:
      default: 100
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: /schema.history.internal.kafka.query.timeout.ms
    method: GET
    data_selector: query.timeout
    params:
      default: 3000
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: /schema.history.internal.kafka.create.timeout.ms
    method: GET
    data_selector: create.timeout
    params:
      default: 30000
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: /schema.history.internal.kafka.recovery.attempts
    method: GET
    data_selector: recovery.attempts
    params:
      default: 100
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: /schema.history.internal.skip.unparseable.ddl
    method: GET
    data_selector: skip.unparseable
    params:
      default: false
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: /schema.history.internal.store.only.captured.tables.ddl
    method: GET
    data_selector: store.only.captured.tables
    params:
      default: false
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: /schema.history.internal.store.only.captured.databases.ddl
    method: GET
    data_selector: store.only.captured.databases
    params:
      default: false
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot_data
    params: {}
- name: stop_snapshot
  endpoint:
    path: /debezium/signal
    method: POST
    data_selector: value
    params:
      type: incremental
      data-collections:
      - database.schema.debezium_signal
- name: customers
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: transaction_metadata
  endpoint:
    path: /path/to/transaction_metadata
    method: GET
    data_selector: records
- name: cdc_configuration
  endpoint:
    path: /path/to/cdc_configuration
    method: GET
    data_selector: records
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: data
    params: {}
- name: insert_insert
- name: insert_delete
- name: max.iteration.transactions
  endpoint:
    path: max.iteration.transactions
    method: GET
    data_selector: '500'
- name: incremental.snapshot.option.recompile
  endpoint:
    path: incremental.snapshot.option.recompile
    method: GET
    data_selector: 'false'
- name: topic.naming.strategy
  endpoint:
    path: topic.naming.strategy
    method: GET
    data_selector: io.debezium.schema.SchemaTopicNamingStrategy
- name: topic.delimiter
  endpoint:
    path: topic.delimiter
    method: GET
    data_selector: .
- name: topic.cache.size
  endpoint:
    path: topic.cache.size
    method: GET
    data_selector: '10000'
- name: topic.heartbeat.prefix
  endpoint:
    path: topic.heartbeat.prefix
    method: GET
    data_selector: __debezium-heartbeat
- name: topic.transaction
  endpoint:
    path: topic.transaction
    method: GET
    data_selector: transaction
- name: snapshot.max.threads
  endpoint:
    path: snapshot.max.threads
    method: GET
    data_selector: '1'
- name: custom.metric.tags
  endpoint:
    path: custom.metric.tags
    method: GET
    data_selector: No default
- name: errors.max.retries
  endpoint:
    path: errors.max.retries
    method: GET
    data_selector: '-1'
- name: data.query.mode
  endpoint:
    path: data.query.mode
    method: GET
    data_selector: function
- name: database.query.timeout.ms
  endpoint:
    path: database.query.timeout.ms
    method: GET
    data_selector: '600000'
- name: streaming.fetch.size
  endpoint:
    path: streaming.fetch.size
    method: GET
    data_selector: '0'
- name: guardrail.collections.max
  endpoint:
    path: guardrail.collections.max
    method: GET
    data_selector: '0'
- name: guardrail.collections.limit.action
  endpoint:
    path: guardrail.collections.limit.action
    method: GET
    data_selector: warn
- name: extended.headers.enabled
  endpoint:
    path: extended.headers.enabled
    method: GET
    data_selector: 'true'
- name: schema.history.internal.kafka.topic
  endpoint:
    path: /sqlserver/property/database/history/kafka/topic
    method: GET
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/database/history/kafka/bootstrap/servers
    method: GET
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/recovery/poll/interval/ms
    method: GET
    data_selector: '100'
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/query/timeout/ms
    method: GET
    data_selector: '3000'
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/create/timeout/ms
    method: GET
    data_selector: '30000'
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: /sqlserver/property/database/history/kafka/recovery/attempts
    method: GET
    data_selector: '100'
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: /sqlserver/property/database/history/skip/unparseable/ddl
    method: GET
    data_selector: 'false'
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: /sqlserver/property/database/history/store/only/captured/tables/ddl
    method: GET
    data_selector: 'false'
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: /sqlserver/property/database/history/store/only/captured/databases/ddl
    method: GET
    data_selector: 'false'
- name: signal.kafka.topic
  endpoint:
    path: /sqlserver/property/signal/kafka/topic
    method: GET
    data_selector: <topic.prefix>-signal
- name: signal.kafka.groupId
  endpoint:
    path: /sqlserver/property/signal/kafka/groupId
    method: GET
    data_selector: kafka-signal
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/signal/kafka/bootstrap/servers
    method: GET
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: /sqlserver/property/signal/kafka/poll/timeout/ms
    method: GET
    data_selector: '100'
- name: notification.sink.topic.name
  endpoint:
    path: /sqlserver/property/notification/sink/topic/name
    method: GET
- name: streaming_metrics
  endpoint:
    path: debezium.sql_server:type=connector-metrics,server=<topic.prefix>,task=<task.id>,context=streaming
    method: GET
    data_selector: attributes
    params: {}
- name: schema_history_metrics
  endpoint:
    path: debezium.sql_server:type=connector-metrics,context=schema-history,server=<topic.prefix>,task=<task.id>
    method: GET
    data_selector: attributes
    params: {}
- name: change_events
  endpoint:
    path: /path/to/change/events
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /customers
    method: GET
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: Vitess Connector
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: Users
  endpoint:
    path: /users
    method: GET
    data_selector: payload
    params: {}
- name: change_stream
  endpoint:
    path: /services/data/vXX.X/sobjects/ChangeStream
    method: GET
    data_selector: records
    params:
      gcp.spanner.change.stream: changeStreamAll
      gcp.spanner.project.id: Project
      gcp.spanner.instance.id: Instance
      gcp.spanner.database.id: Database
- name: spanner_connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: POST
    data_selector: messages
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: GET
    data_selector: records
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: GET
    data_selector: records
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: POST
    data_selector: records
    params: {}
- name: Apicurio API and Schema Registry
  endpoint:
    path: http://apicurio:8080/apis/registry/v2
    method: GET
    data_selector: schemas
    params: {}
- name: Confluent Schema Registry
  endpoint:
    path: https://github.com/confluentinc/schema-registry
    method: GET
    data_selector: schemas
    params: {}
- name: schema_registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: records
    params: {}
- name: event_flattening
  endpoint:
    path: /transformations/event-flattening
    method: GET
    data_selector: messages
    params: {}
- name: delete.tombstone.handling.mode
  endpoint:
    params:
      default: tombstone
- name: route.by.field
  endpoint:
    params: {}
- name: add.fields.prefix
  endpoint:
    params:
      default: __
- name: add.headers.prefix
  endpoint:
    params:
      default: __
- name: drop.fields.from.key
  endpoint:
    params:
      default: 'false'
- name: drop.fields.keep.schema.compatible
  endpoint:
    params:
      default: 'true'
- name: replace.null.with.default
  endpoint:
    params:
      default: 'true'
- name: MariaDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mariadb/3.3.1.Final/debezium-connector-mariadb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: MySQL Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mysql/3.3.1.Final/debezium-connector-mysql-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Postgres Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-postgres/3.3.1.Final/debezium-connector-postgres-3.3.1.Final-plugin.tar.gz
    method: GET
- name: MongoDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mongodb/3.3.1.Final/debezium-connector-mongodb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: SQL Server Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-sqlserver/3.3.1.Final/debezium-connector-sqlserver-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Oracle Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-oracle/3.3.1.Final/debezium-connector-oracle-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Db2 Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-db2/3.3.1.Final/debezium-connector-db2-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 3.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-3/3.3.1.Final/debezium-connector-cassandra-3-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 4.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-4/3.3.1.Final/debezium-connector-cassandra-4-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 5.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-5/3.3.1.Final/debezium-connector-cassandra-5-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Vitess Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-vitess/3.3.1.Final/debezium-connector-vitess-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Spanner Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-spanner/3.3.1.Final/debezium-connector-spanner-3.3.1.Final-plugin.tar.gz
    method: GET
- name: JDBC Sink Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-jdbc/3.3.1.Final/debezium-connector-jdbc-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Informix Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-informix/3.3.1.Final/debezium-connector-informix-3.3.1.Final-plugin.tar.gz
    method: GET
- name: CockroachDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cockroachdb/3.3.1.Final/debezium-connector-cockroachdb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: inventory
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory.addresses
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `customer_id`, `street`, `city`, `state`, `zip`, `type`
        FROM `inventory`.`addresses`
- name: customers
  endpoint:
    path: /inventory.customers
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `first_name`, `last_name`, `email` FROM `inventory`.`customers`
- name: geom
  endpoint:
    path: /inventory.geom
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `g`, `h` FROM `inventory`.`geom`
- name: orders
  endpoint:
    path: /inventory.orders
    method: SELECT
    data_selector: records
    params:
      query: SELECT `order_number`, `order_date`, `purchaser`, `quantity`, `product_id`
        FROM `inventory`.`orders`
- name: products
  endpoint:
    path: /inventory.products
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `name`, `description`, `weight` FROM `inventory`.`products`
- name: products_on_hand
  endpoint:
    path: /inventory.products_on_hand
    method: SELECT
    data_selector: records
    params:
      query: SELECT `product_id`, `quantity` FROM `inventory`.`products_on_hand`
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: MongoDB
  endpoint:
    path: /releases/3.4/
    method: GET
- name: PostgreSQL
  endpoint:
    path: /releases/3.3/
    method: GET
- name: MariaDB
  endpoint:
    path: /releases/2.7/
    method: GET
- name: MariaDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mariadb/3.3.1.Final/debezium-connector-mariadb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: MySQL Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mysql/3.3.1.Final/debezium-connector-mysql-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Postgres Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-postgres/3.3.1.Final/debezium-connector-postgres-3.3.1.Final-plugin.tar.gz
    method: GET
- name: MongoDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-mongodb/3.3.1.Final/debezium-connector-mongodb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: SQL Server Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-sqlserver/3.3.1.Final/debezium-connector-sqlserver-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Oracle Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-oracle/3.3.1.Final/debezium-connector-oracle-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Db2 Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-db2/3.3.1.Final/debezium-connector-db2-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 3.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-3/3.3.1.Final/debezium-connector-cassandra-3-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 4.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-4/3.3.1.Final/debezium-connector-cassandra-4-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Cassandra 5.x Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cassandra-5/3.3.1.Final/debezium-connector-cassandra-5-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Vitess Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-vitess/3.3.1.Final/debezium-connector-vitess-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Spanner Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-spanner/3.3.1.Final/debezium-connector-spanner-3.3.1.Final-plugin.tar.gz
    method: GET
- name: JDBC sink Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-jdbc/3.3.1.Final/debezium-connector-jdbc-3.3.1.Final-plugin.tar.gz
    method: GET
- name: Informix Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-informix/3.3.1.Final/debezium-connector-informix-3.3.1.Final-plugin.tar.gz
    method: GET
- name: CockroachDB Connector
  endpoint:
    path: /maven2/io/debezium/debezium-connector-cockroachdb/3.3.1.Final/debezium-connector-cockroachdb-3.3.1.Final-plugin.tar.gz
    method: GET
- name: decoderbufs
  endpoint:
    path: /postgresql/decoderbufs
    method: GET
    data_selector: records
    params: {}
- name: Apicurio API and Schema Registry
  endpoint:
    path: /apis/registry/v2
    method: GET
- name: Confluent Schema Registry
  endpoint:
    path: /schemas
    method: GET
- name: schema_registry
  endpoint:
    path: /services/data/v1/schemas
    method: GET
    data_selector: schemas
    params: {}
- name: kafka_connect
  endpoint:
    path: /services/data/v1/connect
    method: GET
    data_selector: connects
    params: {}
- name: default
  endpoint:
    path: /
    method: GET
    data_selector: default
    params:
      topic.creation.default.replication.factor: 3
      topic.creation.default.partitions: 10
      topic.creation.default.cleanup.policy: compact
      topic.creation.default.compression.type: lz4
- name: inventory
  endpoint:
    path: /
    method: GET
    data_selector: inventory
    params:
      topic.creation.inventory.include: dbserver1\.inventory\.*
      topic.creation.inventory.partitions: 20
      topic.creation.inventory.cleanup.policy: compact
      topic.creation.inventory.delete.retention.ms: 7776000000
- name: applicationlogs
  endpoint:
    path: /
    method: GET
    data_selector: applicationlogs
    params:
      topic.creation.applicationlogs.include: dbserver1\.logs\.applog-.*
      topic.creation.applicationlogs.exclude: dbserver1\.logs\.applog-old-.*
      topic.creation.applicationlogs.replication.factor: 1
      topic.creation.applicationlogs.partitions: 20
      topic.creation.applicationlogs.cleanup.policy: delete
      topic.creation.applicationlogs.retention.ms: 7776000000
      topic.creation.applicationlogs.compression.type: lz4
- name: custom_action
  endpoint:
    path: /debezium/custom/action
    method: POST
    data_selector: actions
    params: {}
- name: sink_notification_channel
  endpoint:
    path: /connect-api/sink
    method: POST
    data_selector: notifications
    params: {}
- name: log_notification_channel
  endpoint:
    path: /log
    method: POST
    data_selector: notifications
    params: {}
- name: jmx_notification_channel
  endpoint:
    path: /jmx
    method: POST
    data_selector: notifications
    params: {}
- name: custom_notification_channel
  endpoint:
    path: /kafka/connect
    method: POST
    data_selector: notification.enabled.channels
    params: {}
- name: offset_storage
  endpoint:
    params:
      offset.storage: org.apache.kafka.connect.storage.KafkaOffsetBackingStore
      offset.storage.topic: connect-offsets
      offset.storage.partitions: 25
      offset.storage.replication.factor: 3
- name: internal_schema_history_store
  endpoint:
    params:
      schema.history.internal: io.debezium.storage.redis.history.RedisSchemaHistory
      schema.history.internal.redis.key: metadata:debezium:schema_history
      schema.history.internal.redis.db.index: 0
      schema.history.internal.storage.redis.ssl.enabled: false
      schema.history.internal.storage.redis.ssl.hostname.verification.enabled: false
      schema.history.internal.storage.redis.ssl.truststore.type: JKS
      schema.history.internal.storage.redis.ssl.keystore.type: JKS
      schema.history.internal.storage.redis.connection.timeout.ms: 2000
      schema.history.internal.storage.redis.socket.timeout.ms: 2000
      schema.history.internal.storage.redis.retry.initial.delay.ms: 300
      schema.history.internal.storage.redis.retry.max.delay.ms: 10000
      schema.history.internal.storage.redis.retry.max.attempts: 10
      schema.history.internal.storage.redis.wait.enabled: false
      schema.history.internal.storage.redis.wait.timeout.ms: 1000
      schema.history.internal.storage.redis.wait.retry.enabled: false
      schema.history.internal.storage.redis.wait.retry.delay.ms: 1000
- name: schema_history
  endpoint:
    path: /schema/history
    method: GET
- name: offset_storage
  endpoint:
    path: /offset/storage
    method: GET
- name: offset_store
  endpoint:
    path: /offset/storage
    method: POST
    data_selector: offsets
    params:
      offset.storage: io.debezium.storage.redis.offset.RedisOffsetBackingStore
      offset.storage.redis.key: metadata:debezium:offsets
      offset.storage.redis.db.index: 0
      offset.storage.redis.connection.timeout.ms: 2000
      offset.storage.redis.socket.timeout.ms: 2000
      offset.storage.redis.retry.max.attempts: 10
      offset.storage.redis.wait.timeout.ms: 1000
- name: schema_history_store
  endpoint:
    path: /schema/history
    method: POST
    data_selector: schema_history
    params:
      schema.history.internal: io.debezium.storage.redis.history.RedisSchemaHistory
      schema.history.internal.redis.key: metadata:debezium:schema_history
      schema.history.internal.redis.db.index: 0
      schema.history.internal.storage.redis.connection.timeout.ms: 2000
      schema.history.internal.storage.redis.socket.timeout.ms: 2000
      schema.history.internal.storage.redis.retry.max.attempts: 10
      schema.history.internal.storage.redis.wait.timeout.ms: 1000
- name: rocketmq_schema_history_store
  endpoint:
    path: /rocketmq/schema/history
    method: POST
    data_selector: rocketmq_schema_history
    params:
      schema.history.internal: io.debezium.storage.rocketmq.history.RocketMqSchemaHistory
      schema.history.internal.rocketmq.topic: schema_history_topic
      schema.history.internal.rocketmq.acl.enabled: false
      schema.history.internal.rocketmq.retry.max.attempts: 10
- name: schema_change_topic
  endpoint:
    path: /path/to/schema/change/topic
    method: POST
    data_selector: payload
    params: {}
- name: execute-snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: data
    params: {}
- name: stop-snapshot
  endpoint:
    path: /stop-snapshot
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - db1.table1
      - db1.table2
- name: blocking_snapshot
  endpoint:
    path: /blocking_snapshot
    method: POST
    data_selector: data
    params:
      data-collections:
      - schema1.table1
      - schema1.table2
      additional-conditions:
      - data-collection: schema1.table1
        filter: SELECT * FROM [schema1].[table1] WHERE column1 = 0 ORDER BY column2
          DESC
      - data-collection: schema1.table2
        filter: SELECT * FROM [schema1].[table2] WHERE column2 > 0
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: skipped.operations
  endpoint:
    path: /skipped/operations
    method: GET
    data_selector: default
    params: {}
- name: snapshot.delay.ms
  endpoint:
    path: /snapshot/delay/ms
    method: GET
    data_selector: default
    params: {}
- name: snapshot.fetch.size
  endpoint:
    path: /snapshot/fetch/size
    method: GET
    data_selector: unset
    params: {}
- name: snapshot.include.collection.list
  endpoint:
    path: /snapshot/include/collection/list
    method: GET
    data_selector: default
    params: {}
- name: snapshot.lock.timeout.ms
  endpoint:
    path: /snapshot/lock/timeout/ms
    method: GET
    data_selector: '10000'
    params: {}
- name: snapshot.locking.mode
  endpoint:
    path: /snapshot/locking/mode
    method: GET
    data_selector: minimal
    params: {}
- name: snapshot.max.threads
  endpoint:
    path: /snapshot/max/threads
    method: GET
    data_selector: '1'
    params: {}
- name: snapshot.mode
  endpoint:
    path: /snapshot/mode
    method: GET
    data_selector: initial
    params: {}
- name: snapshot.query.mode
  endpoint:
    path: /snapshot/query/mode
    method: GET
    data_selector: select_all
    params: {}
- name: source.struct.version
  endpoint:
    path: /source/struct/version
    method: GET
    data_selector: v2
    params: {}
- name: streaming.delay.ms
  endpoint:
    path: /streaming/delay/ms
    method: GET
    data_selector: '0'
    params: {}
- name: table.ignore.builtin
  endpoint:
    path: /table/ignore/builtin
    method: GET
    data_selector: 'true'
    params: {}
- name: topic.cache.size
  endpoint:
    path: /topic/cache/size
    method: GET
    data_selector: '10000'
    params: {}
- name: topic.delimiter
  endpoint:
    path: /topic/delimiter
    method: GET
    data_selector: .
    params: {}
- name: topic.heartbeat.prefix
  endpoint:
    path: /topic/heartbeat/prefix
    method: GET
    data_selector: __debezium-heartbeat
    params: {}
- name: topic.naming.strategy
  endpoint:
    path: /topic/naming/strategy
    method: GET
    data_selector: io.debezium.schema.DefaultTopicNamingStrategy
    params: {}
- name: topic.transaction
  endpoint:
    path: /topic/transaction
    method: GET
    data_selector: transaction
    params: {}
- name: use.nongraceful.disconnect
  endpoint:
    path: /use/nongraceful/disconnect
    method: GET
    data_selector: 'false'
    params: {}
- name: guardrail.collections.max
  endpoint:
    path: /guardrail/collections/max
    method: GET
    data_selector: '0'
    params: {}
- name: guardrail.collections.limit.action
  endpoint:
    path: /guardrail/collections/limit/action
    method: GET
    data_selector: warn
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/SnapshotMetrics
    method: GET
    data_selector: records
- name: streaming_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/StreamingMetrics
    method: GET
    data_selector: records
- name: schema_history_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/SchemaHistoryMetrics
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /customers
    method: POST
    data_selector: payload
    params: {}
- name: truncate_event
  endpoint:
    path: /path/to/truncate_event
    method: POST
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector/config
    method: POST
    data_selector: config
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=snapshot,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: streaming_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=streaming,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: schema_history_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=schema-history,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: mongodb
  endpoint:
    path: /services/data/vXX.X/sobjects/MongoDB
    method: GET
    data_selector: records
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: <topic.prefix>-signal
    method: GET
    data_selector: records
- name: signal.kafka.groupId
  endpoint:
    path: kafka-signal
    method: GET
    data_selector: records
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: GET
    data_selector: records
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: '100'
    method: GET
    data_selector: records
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: GET
    data_selector: records
- name: publications
  endpoint:
    path: /postgresql/publications
    method: POST
    data_selector: publications
- name: stop_snapshot
  endpoint:
    path: /stop_snapshot
    method: POST
    data_selector: data
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
    params: {}
- name: signaling_table
  endpoint:
    path: /myschema/debezium_signal
    method: INSERT
    data_selector: data
    params:
      type: execute-snapshot
      data-collections: []
- endpoint:
    params:
      data-collections:
      - schema1.products
- name: customers
  endpoint:
    path: /services/data/vXX.X/sobjects/customers
    method: GET
    data_selector: records
    params: {}
- name: fulfillment-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: flushed_lsn_source
  endpoint:
    params:
      flush.lsn.source: 'true'
- name: retriable_restart_connector_wait
  endpoint:
    params:
      retriable.restart.connector.wait.ms: '10000'
- name: skipped_operations
  endpoint:
    params:
      skipped.operations: t
- name: incremental_snapshot_chunk_size
  endpoint:
    params:
      incremental.snapshot.chunk.size: '1024'
- name: incremental_snapshot_watermarking_strategy
  endpoint:
    params:
      incremental.snapshot.watermarking.strategy: insert_insert
- name: read_only
  endpoint:
    params:
      read.only: 'false'
- name: xmin_fetch_interval
  endpoint:
    params:
      xmin.fetch.interval.ms: '0'
- name: topic_naming_strategy
  endpoint:
    params:
      topic.naming.strategy: io.debezium.schema.SchemaTopicNamingStrategy
- name: topic_delimiter
  endpoint:
    params:
      topic.delimiter: .
- name: topic_cache_size
  endpoint:
    params:
      topic.cache.size: '10000'
- name: topic_heartbeat_prefix
  endpoint:
    params:
      topic.heartbeat.prefix: __debezium-heartbeat
- name: topic_transaction
  endpoint:
    params:
      topic.transaction: transaction
- name: snapshot_max_threads
  endpoint:
    params:
      snapshot.max.threads: '1'
- name: errors_max_retries
  endpoint:
    params:
      errors.max.retries: '-1'
- name: database_query_timeout
  endpoint:
    params:
      database.query.timeout.ms: '600000'
- name: guardrail_collections_max
  endpoint:
    params:
      guardrail.collections.max: '0'
- name: guardrail_collections_limit_action
  endpoint:
    params:
      guardrail.collections.limit.action: warn
- name: extended_headers_enabled
  endpoint:
    params:
      extended.headers.enabled: true
- name: signal.kafka.topic
  endpoint:
    path: <topic.prefix>-signal
    method: GET
- name: signal.kafka.groupId
  endpoint:
    path: kafka-signal
    method: GET
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: GET
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: '100'
    method: GET
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: GET
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: data
    params:
      type: stop-snapshot
      data-collections:
      - db1.schema1.table1
      - db1.schema1.table2
- name: customers
  endpoint:
    path: /inventory/customers
    method: POST
    data_selector: records
    params: {}
- name: truncate_event
  endpoint:
    path: /path/to/truncate/event/endpoint
    method: POST
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: connector
  endpoint:
    path: /config
    method: POST
    data_selector: config
    params: {}
- name: schema.history
  endpoint:
    path: /schema/history
    method: POST
- name: signal.kafka
  endpoint:
    path: /signal/kafka
    method: POST
- name: notification.sink
  endpoint:
    path: /notification/sink
    method: POST
- name: snapshot_mode
  endpoint:
    path: /snapshot/mode
    method: GET
    data_selector: settings
    params: {}
- name: debezium_signal
  endpoint:
    path: /debezium_signal
    method: INSERT
    data_selector: data
    params: {}
- name: blocking_snapshots
  endpoint:
    path: /configuration/blocking-snapshots
    method: GET
    data_selector: snapshots
    params: {}
- name: topic_names
  endpoint:
    path: /configuration/topic-names
    method: GET
    data_selector: topics
    params: {}
- name: schema_history_topic
  endpoint:
    path: /configuration/schema-history-topic
    method: GET
    data_selector: schemaHistory
    params: {}
- name: schema_change_topic
  endpoint:
    path: /configuration/schema-change-topic
    method: GET
    data_selector: schemaChange
    params: {}
- name: transaction_metadata
  endpoint:
    path: /configuration/transaction-metadata
    method: GET
    data_selector: transactionMetadata
    params: {}
- name: logminer_mining_strategies
  endpoint:
    path: /configuration/logminer-mining-strategies
    method: GET
    data_selector: miningStrategies
    params: {}
- name: change_events
  endpoint:
    path: /v1/change-events
    method: GET
    data_selector: events
- name: customers
  endpoint:
    path: /server1/DEBEZIUM/CUSTOMERS
    method: POST
    data_selector: payload
    params: {}
- name: CLOB_NCLOB_BLOB
  endpoint:
    path: /services/data/vXX.X/sobjects/CLOB_NCLOB_BLOB
    method: GET
    data_selector: records
- name: NUMBER_TYPE
  endpoint:
    path: /services/data/vXX.X/sobjects/NUMBER_TYPE
    method: GET
    data_selector: records
- name: BOOLEAN_TYPE
  endpoint:
    path: /services/data/vXX.X/sobjects/BOOLEAN_TYPE
    method: GET
    data_selector: records
- name: TIMESTAMP_TYPE
  endpoint:
    path: /services/data/vXX.X/sobjects/TIMESTAMP_TYPE
    method: GET
    data_selector: records
- name: ROWID_TYPE
  endpoint:
    path: /services/data/vXX.X/sobjects/ROWID_TYPE
    method: GET
    data_selector: records
- name: XMLTYPE
  endpoint:
    path: /services/data/vXX.X/sobjects/XMLTYPE
    method: GET
    data_selector: records
- name: archive_log_destination
  endpoint:
    path: /path/to/archive/destination
    method: GET
    data_selector: archiveLogDestinations
    params: {}
- name: oracle_connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: inventory-connector
  endpoint:
    path: /services/data/vXX.X/sobjects/InventoryConnector
    method: GET
    data_selector: records
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot
- name: debezium_signal
  endpoint:
    path: /debezium_signal
    method: POST
    data_selector: data
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: value
    params: {}
- name: blocking_snapshot
  endpoint:
    path: /blocking_snapshot
    method: POST
    data_selector: data
    params:
      data-collections:
      - schema1.table1
      - schema1.table2
      additional-conditions:
      - data-collection: schema1.table1
        filter: SELECT * FROM [schema1].[table1] WHERE column1 = 0 ORDER BY column2
          DESC
      - data-collection: schema1.table2
        filter: SELECT * FROM [schema1].[table2] WHERE column2 > 0
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /path/to/customers
    method: DELETE
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector/config
    method: POST
    data_selector: config
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot_data
    params:
      snapshot.mode: initial
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot_response
    params: {}
- name: debezium_signal
  endpoint:
    path: /debezium_signal
    method: INSERT
    data_selector: data
    params: {}
- name: transaction_metadata
  endpoint:
    path: /api/v1/transaction_metadata
    method: GET
    data_selector: events
    params: {}
- name: data_change_events
  endpoint:
    path: /api/v1/data_change_events
    method: GET
    data_selector: events
    params: {}
- name: db2-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: snapshot_max_threads
  endpoint:
    path: /db2/property/snapshot.max.threads
    method: GET
    data_selector: '1'
- name: custom_metric_tags
  endpoint:
    path: /db2/property/custom.metric.tags
    method: GET
    data_selector: No default
- name: errors_max_retries
  endpoint:
    path: /db2/property/errors.max.retries
    method: GET
    data_selector: '-1'
- name: database_query_timeout_ms
  endpoint:
    path: /db2/property/database.query.timeout.ms
    method: GET
    data_selector: '600000'
- name: cdc_control_schema
  endpoint:
    path: /db2/property/cdc.control.schema
    method: GET
    data_selector: ASNCDC
- name: cdc_change_tables_schema
  endpoint:
    path: /db2/property/cdc.change.tables.schema
    method: GET
    data_selector: ASNCDC
- name: schema.history
  endpoint:
    path: /schema/history
    method: POST
    data_selector: schema
    params: {}
- name: signal
  endpoint:
    path: /signal
    method: GET
    data_selector: signal
    params: {}
- name: notification
  endpoint:
    path: /notification
    method: POST
    data_selector: notification
    params: {}
- name: cdc_raw
  endpoint:
    path: /cdc_raw
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /cassandra/customers
    method: GET
    data_selector: records
    params: {}
- name: cassandra
  endpoint:
    path: /path/to/application/configuration.conf
    method: GET
- name: transaction_metadata
  endpoint:
    path: /services/data/vXX.X/sobjects/TransactionMetadata
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: streaming_metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: metrics
    params: {}
- name: Users
  endpoint:
    path: /path/to/users
    method: GET
    data_selector: payload
    params: {}
- name: change_stream
  endpoint:
    path: /spanner/change_streams/changeStreamAll
    method: GET
    data_selector: records
    params:
      project_id: Project
      instance_id: Instance
      database_id: Database
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot_data
    params: {}
- name: incremental_snapshot
  endpoint:
    path: /configuration/signalling.html#sending-signals-to-a-debezium-connector
    method: INSERT
    data_selector: data-collections
    params:
      type: incremental
- name: transaction_metadata
  endpoint:
    path: /transaction_metadata
    method: GET
    data_selector: transaction_events
    params: {}
- name: data_change_events
  endpoint:
    path: /data_change_events
    method: GET
    data_selector: change_events
    params: {}
- name: customers
  endpoint:
    path: /mydatabase.myschema/customers
    method: POST
    data_selector: records
    params: {}
- name: connector
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: JDBC
  endpoint:
    path: /reference/stable/connectors/index-sink.html
    method: GET
- name: MongoDB Sink
  endpoint:
    path: /reference/stable/connectors/index-sink.html
    method: GET
- name: events
  endpoint:
    path: /connectors/jdbc
    method: POST
    data_selector: records
    params:
      delete.enabled: 'true'
- name: orders
  endpoint:
    path: /topics/orders
    method: POST
    data_selector: records
    params: {}
- name: mongodb-sink-connector
  endpoint:
    path: /connectors/mongodb-sink-connector
    method: POST
    data_selector: config
    params: {}
- name: topic_routing
  endpoint:
    path: /transformations/topic-routing
    method: GET
    data_selector: configuration_options
- name: event_flattening
  endpoint:
    path: /transformations/event-flattening
    method: GET
    data_selector: records
- name: delete.tombstone.handling.mode
  endpoint:
    path: /extract-new-record-state-delete-tombstone-handling-mode
    method: GET
    data_selector: default
    params: {}
- name: route.by.field
  endpoint:
    path: /extract-new-record-state-route-by-field
    method: GET
    data_selector: default
    params: {}
- name: add.fields.prefix
  endpoint:
    path: /extract-new-record-state-add-fields-prefix
    method: GET
    data_selector: default
    params: {}
- name: add.fields
  endpoint:
    path: /extract-new-record-state-add-fields
    method: GET
    data_selector: default
    params: {}
- name: add.headers.prefix
  endpoint:
    path: /extract-new-record-state-add-headers-prefix
    method: GET
    data_selector: default
    params: {}
- name: add.headers
  endpoint:
    path: /extract-new-record-state-add-headers
    method: GET
    data_selector: default
    params: {}
- name: drop.fields.header.name
  endpoint:
    path: /extract-new-record-state-drop-fields-header-name
    method: GET
    data_selector: default
    params: {}
- name: drop.fields.from.key
  endpoint:
    path: /extract-new-record-state-drop-fields-from-key
    method: GET
    data_selector: default
    params: {}
- name: drop.fields.keep.schema.compatible
  endpoint:
    path: /extract-new-record-state-drop-fields-keep-schema-compatible
    method: GET
    data_selector: default
    params: {}
- name: replace.null.with.default
  endpoint:
    path: /replace-null-with-default
    method: GET
    data_selector: default
    params: {}
- name: event_changes
  endpoint:
    path: /transformations/event-changes
    method: GET
    data_selector: records
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: POST
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: payload
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: emitted_outbox_messages
  endpoint:
    path: /outbox/messages
    method: POST
    data_selector: records
- name: outbox_event
  endpoint:
    path: /events/outbox
    method: POST
    data_selector: events
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: outbox_event
  endpoint:
    path: /services/data/vXX.X/sobjects/OutboxEvent
    method: GET
    data_selector: records
    params: {}
- name: filter
  endpoint:
    path: /transformations/filtering
    method: GET
    data_selector: records
    params: {}
- name: schema_change_event_filter
  endpoint:
    path: /transformations/schema-change-event-filter
    method: GET
    data_selector: records
    params: {}
- name: content_based_routing
  endpoint:
    path: /transformations/content-based-routing
    method: GET
    data_selector: records
- name: transformation_predicates
  endpoint:
    path: /reference/transformations/applying-transformations-selectively.html
    method: GET
    data_selector: predicates
- name: PartitionRouting
  endpoint:
    path: /transformations/partition-routing.html
    method: GET
    data_selector: records
    params: {}
- name: HeaderToValue
  endpoint:
    path: /transformations/header-to-value
    method: GET
- name: convertTimezone
  endpoint:
    path: /timezone/converter
    method: POST
    data_selector: transforms.convertTimezone.converted.timezone
    params: {}
- name: timescaledb-connector
  endpoint:
    path: /connectors/timescaledb-connector
    method: POST
    data_selector: config
    params: {}
- name: decodeLogicalDecodingMessageContent
  endpoint:
    path: /transformations/decode-logical-decoding-message-content
    method: GET
    data_selector: configuration_options
- name: VectorToJson
  endpoint:
    path: /transformations/vector-to-json
    method: GET
    data_selector: examples
- name: engine
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: asynchronous_engine_properties
  endpoint:
    path: /engine/properties
    method: GET
    data_selector: properties
    params: {}
- name: database_schema_history_properties
  endpoint:
    path: /schema/history/properties
    method: GET
    data_selector: properties
    params: {}
- name: custom_converter
  endpoint:
    path: /documentation/development/converters
    method: GET
    data_selector: converters
    params: {}
- name: JSON SerDe
  endpoint:
    path: /documentation/serdes
    method: GET
    data_selector: events
    params: {}
- name: OrderCreatedEvent
  endpoint:
    path: /debezium/outbox/event
    method: POST
    data_selector: event
    params: {}
- name: cloudevent
  endpoint:
    path: /cloudevents
    method: POST
    data_selector: data
    params: {}
- name: metadata_source
  endpoint:
    path: /cloud-events/metadata
    method: GET
    data_selector: records
    params:
      metadata.source: value,id:generate,type:generate,traceparent:header,dataSchemaName:generate
- name: debezium
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
    params: {}
- name: DebeziumContainer
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
- name: embeddings
  endpoint:
    path: /ai/embeddings
    method: POST
    data_selector: embeddings
- name: postgres
  endpoint:
    path: /debezium/source/postgres
    method: POST
    data_selector: records
    params:
      incremental: updated_at
- name: kinesis
  endpoint:
    path: /kinesis
    method: POST
    data_selector: records
    params:
      debezium.sink.type: kinesis
      debezium.sink.kinesis.region: ''
      debezium.sink.kinesis.endpoint: ''
      debezium.sink.kinesis.credentials.profile: ''
      debezium.sink.kinesis.null.key: default
- name: pubsub
  endpoint:
    path: /pubsub
    method: POST
    data_selector: records
    params:
      debezium.sink.type: pubsub
      debezium.sink.pubsub.project.id: ''
      debezium.sink.pubsub.ordering.enabled: 'true'
      debezium.sink.pubsub.null.key: default
      debezium.sink.pubsub.batch.delay.threshold.ms: '100'
      debezium.sink.pubsub.batch.element.count.threshold: 100L
      debezium.sink.pubsub.batch.request.byte.threshold: 10000000L
      debezium.sink.pubsub.flowcontrol.enabled: 'false'
      debezium.sink.pubsub.retry.total.timeout.ms: '60000'
      debezium.sink.pubsub.retry.initial.delay.ms: '5'
      debezium.sink.pubsub.retry.delay.multiplier: '2.0'
      debezium.sink.pubsub.wait.message.delivery.timeout.ms: '30000'
      debezium.sink.pubsub.concurrency.threads: '0'
      debezium.sink.pubsub.compression.threshold.bytes: '-1'
- name: pubsublite
  endpoint:
    path: /pubsublite
    method: POST
    data_selector: records
    params:
      debezium.sink.type: pubsublite
      debezium.sink.pubsublite.project.id: ''
      debezium.sink.pubsublite.region: ''
      debezium.sink.pubsublite.ordering.enabled: 'true'
      debezium.sink.pubsublite.null.key: default
      debezium.sink.pubsublite.wait.message.delivery.timeout.ms: '30000'
- name: httpclient
  endpoint:
    path: /httpclient
    method: POST
    data_selector: records
    params:
      debezium.sink.type: http
      debezium.sink.http.url: ''
      debezium.sink.http.timeout.ms: '60000'
      debezium.sink.http.retries: '5'
      debezium.sink.http.retry.interval.ms: '1000'
      debezium.sink.http.headers.prefix: X-DEBEZIUM-
      debezium.sink.http.headers.encode.base64: 'true'
      debezium.sink.http.authentication.type: ''
      debezium.sink.http.authentication.jwt.username: ''
      debezium.sink.http.authentication.jwt.password: ''
      debezium.sink.http.authentication.jwt.url: ''
      debezium.sink.http.authentication.jwt.token_expiration: ''
      debezium.sink.http.authentication.jwt.refresh_token_expiration: ''
      debezium.sink.http.authentication.webhook.secret: ''
- name: pulsar
  endpoint:
    path: /pulsar
    method: POST
    data_selector: records
    params:
      debezium.sink.type: pulsar
      debezium.sink.pulsar.timeout: '0'
      debezium.sink.pulsar.tenant: public
      debezium.sink.pulsar.namespace: default
      debezium.sink.pulsar.null.key: default
      debezium.sink.pulsar.producer.batcherBuilder: DEFAULT
- name: eventhubs
  endpoint:
    path: /eventhubs
    method: POST
    data_selector: records
    params:
      debezium.sink.type: eventhubs
      debezium.sink.eventhubs.connectionstring: ''
      debezium.sink.eventhubs.hubname: ''
      debezium.sink.eventhubs.partitionid: ''
      debezium.sink.eventhubs.partitionkey: ''
      debezium.sink.eventhubs.maxbatchsize: ''
      debezium.sink.eventhubs.hashmessagekeyfunction: ''
- name: qdrant
  endpoint:
    path: /qdrant
    method: POST
    data_selector: records
- name: instructlab
  endpoint:
    path: /instructlab
    method: POST
    data_selector: records
- name: cassandra_connector
  endpoint:
    path: /debezium/source/connector/class
    method: POST
    data_selector: records
    params:
      debezium.source.connector.class: io.debezium.connector.cassandra.Cassandra4Connector
      debezium.source.cassandra.node.id: sample_node_01
      debezium.source.cassandra.hosts: 127.0.0.1
      debezium.source.cassandra.port: '9042'
      debezium.source.cassandra.config: /opt/cassandra/conf/cassandra.yaml
      debezium.source.commit.log.relocation.dir: cassandra/relocdir
      debezium.source.offset.storage: io.debezium.server.redis.RedisOffsetBackingStore
      debezium.source.topic.prefix: sample_prefix
      debezium.source.http.port: '8040'
- name: DebeziumServer
  endpoint:
    path: /debezium-server
    method: POST
    data_selector: spec
    params: {}
- name: offset
  endpoint:
    path: /offset
    method: POST
    data_selector: offsets
    params: {}
- name: schemaHistory
  endpoint:
    path: /schemaHistory
    method: POST
    data_selector: schemaHistories
    params: {}
- name: customers
  endpoint:
    path: /mysql.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /mysql.inventory.customers
    method: GET
    data_selector: payload.after
    params: {}
- name: mysql
  endpoint:
    path: /example-mysql
    method: GET
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: records
    params:
      CLUSTER_ID: <YOUR_UNIQUE_CLUSTER_IDENTIFIER>
      NODE_ID: '1'
      NODE_ROLE: combined
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: events
- name: products
  endpoint:
    path: /dbserver1.inventory.products
    method: GET
    data_selector: events
- name: products_on_hand
  endpoint:
    path: /dbserver1.inventory.products_on_hand
    method: GET
    data_selector: events
- name: orders
  endpoint:
    path: /dbserver1.inventory.orders
    method: GET
    data_selector: events
- name: customers
  endpoint:
    path: /connectors/inventory-connector/tasks
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/
    method: POST
    data_selector: ''
    params: {}
- name: status
  endpoint:
    path: /connectors/inventory-connector
    method: GET
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /services/data/vXX.X/sobjects/inventory.products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
- name: kafka
  endpoint:
    path: /kafka
    method: POST
    data_selector: records
    params:
      CLUSTER_ID: <YOUR_UNIQUE_CLUSTER_IDENTIFIER>
      NODE_ID: '1'
      NODE_ROLE: combined
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://kafka:9092,CONTROLLER://kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
- name: mysql
  endpoint:
    path: /mysql
    method: POST
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: products
  endpoint:
    path: dbserver1.inventory.products
    method: GET
    data_selector: payload
    params: {}
- name: products_on_hand
  endpoint:
    path: dbserver1.inventory.products_on_hand
    method: GET
    data_selector: payload
    params: {}
- name: orders
  endpoint:
    path: dbserver1.inventory.orders
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: products
  endpoint:
    path: dbserver1.inventory.products
    method: GET
    data_selector: payload
    params: {}
- name: products_on_hand
  endpoint:
    path: dbserver1.inventory.products_on_hand
    method: GET
    data_selector: payload
    params: {}
- name: orders
  endpoint:
    path: dbserver1.inventory.orders
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /connectors
    method: POST
    data_selector: payload
    params: {}
- name: inventory
  endpoint:
    path: /example-mysql
    method: GET
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: events
    params: {}
- name: products
  endpoint:
    path: /dbserver1.inventory.products
    method: GET
    data_selector: events
    params: {}
- name: products_on_hand
  endpoint:
    path: /dbserver1.inventory.products_on_hand
    method: GET
    data_selector: events
    params: {}
- name: orders
  endpoint:
    path: /dbserver1.inventory.orders
    method: GET
    data_selector: events
    params: {}
- name: customers
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
- name: mysql
  endpoint:
    path: /example-mysql
    method: GET
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: rows
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: rows
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: rows
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: rows
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: rows
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: rows
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
- name: mysql
  endpoint:
    path: /examples/mysql
    method: GET
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: records
- name: products
  endpoint:
    path: /dbserver1.inventory.products
    method: GET
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /dbserver1.inventory.products_on_hand
    method: GET
    data_selector: records
- name: orders
  endpoint:
    path: /dbserver1.inventory.orders
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: payload
    params: {}
- name: zookeeper
  endpoint:
    path: /zookeeper
    method: GET
    data_selector: status
    params: {}
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: status
    params: {}
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: status
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: inventory.addresses
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `customer_id`, `street`, `city`, `state`, `zip`, `type`
        FROM `inventory`.`addresses`
- name: customers
  endpoint:
    path: inventory.customers
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `first_name`, `last_name`, `email` FROM `inventory`.`customers`
- name: geom
  endpoint:
    path: inventory.geom
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `g`, `h` FROM `inventory`.`geom`
- name: orders
  endpoint:
    path: inventory.orders
    method: SELECT
    data_selector: records
    params:
      query: SELECT `order_number`, `order_date`, `purchaser`, `quantity`, `product_id`
        FROM `inventory`.`orders`
- name: products
  endpoint:
    path: inventory.products
    method: SELECT
    data_selector: records
    params:
      query: SELECT `id`, `name`, `description`, `weight` FROM `inventory`.`products`
- name: products_on_hand
  endpoint:
    path: inventory.products_on_hand
    method: SELECT
    data_selector: records
    params:
      query: SELECT `product_id`, `quantity` FROM `inventory`.`products_on_hand`
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: payload
    params: {}
- name: zookeeper
  endpoint:
    path: /zookeeper
    method: GET
- name: kafka
  endpoint:
    path: /kafka
    method: GET
- name: mysql
  endpoint:
    path: /mysql
    method: GET
- name: inventory-connector
  endpoint:
    path: /connectors/
    method: POST
    data_selector: name
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
- name: products
  endpoint:
    path: /inventory/products
    method: GET
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /binlog
    method: GET
    data_selector: currentRowNumber
    params: {}
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: events
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: zookeeper
  endpoint:
    path: /zookeeper
    method: GET
- name: kafka
  endpoint:
    path: /kafka
    method: GET
- name: mysql
  endpoint:
    path: /mysql
    method: GET
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /connectors/inventory-connector/tasks
    method: GET
    data_selector: tasks
- name: zookeeper
  endpoint:
    path: /zookeeper
    method: GET
    data_selector: status
    params: {}
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: status
    params: {}
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: status
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/
    method: POST
    data_selector: config
    params: {}
- name: inventory.addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: payload
    params: {}
- name: mysql
  endpoint:
    path: /example-mysql
    method: GET
    data_selector: records
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: DELETE
    data_selector: payload
    params: {}
- name: zookeeper
  endpoint:
    path: /zookeeper
    method: GET
    data_selector: ''
    params: {}
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: ''
    params: {}
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: ''
    params:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: inventory.addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: inventory.products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: inventory.products_on_hand
    method: GET
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: payload
    params: {}
- name: kafka
  endpoint:
    path: /kafka
    method: GET
    data_selector: records
- name: mysql
  endpoint:
    path: /mysql
    method: GET
    data_selector: records
- name: inventory-connector
  endpoint:
    path: /connectors/
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: products
  endpoint:
    path: /dbserver1.inventory.products
    method: GET
    data_selector: payload
    params: {}
- name: products_on_hand
  endpoint:
    path: /dbserver1.inventory.products_on_hand
    method: GET
    data_selector: payload
    params: {}
- name: orders
  endpoint:
    path: /dbserver1.inventory.orders
    method: GET
    data_selector: payload
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: POST
    data_selector: payload
    params: {}
- name: decoderbufs
  endpoint:
    path: /postgres-decoderbufs
    method: GET
    data_selector: records
    params: {}
- name: apicurio_registry
  endpoint:
    path: /apis/registry/v2
    method: GET
    data_selector: records
- name: schema-registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: ''
    params: {}
- name: connect
  endpoint:
    path: /connect
    method: GET
    data_selector: ''
    params: {}
- name: avro-consumer
  endpoint:
    path: /avro-consumer
    method: GET
    data_selector: ''
    params: {}
- name: default
  endpoint:
    path: /connectors/default
    method: POST
    data_selector: config
    params:
      topic.creation.default.replication.factor: 3
      topic.creation.default.partitions: 10
      topic.creation.default.cleanup.policy: compact
      topic.creation.default.compression.type: lz4
      topic.creation.groups: inventory,applicationlogs
      topic.creation.inventory.include: dbserver1\.inventory\.*
      topic.creation.inventory.partitions: 20
      topic.creation.inventory.cleanup.policy: compact
      topic.creation.inventory.delete.retention.ms: 7776000000
      topic.creation.applicationlogs.include: dbserver1\.logs\.applog-.*
      topic.creation.applicationlogs.exclude: dbserver1\.logs\.applog-old-.*
      topic.creation.applicationlogs.replication.factor: 1
      topic.creation.applicationlogs.partitions: 20
      topic.creation.applicationlogs.cleanup.policy: delete
      topic.creation.applicationlogs.retention.ms: 7776000000
      topic.creation.applicationlogs.compression.type: lz4
- name: SignalAction
  endpoint:
    path: /io/debezium/pipeline/signal/actions/SignalAction
    method: GET
    data_selector: records
    params: {}
- name: SignalActionProvider
  endpoint:
    path: /io/debezium/pipeline/signal/actions/SignalActionProvider
    method: GET
    data_selector: records
    params: {}
- name: custom_notification_channel
  endpoint:
    path: /configuring/connectors/custom_notification_channel
    method: POST
    data_selector: notification.enabled.channels
    params: {}
- name: offset_store
  endpoint:
    path: /offset/storage/kafka
    method: GET
    data_selector: properties
    params: {}
- name: internal_schema_history_store
  endpoint:
    path: /schema/history/internal
    method: GET
    data_selector: properties
    params: {}
- name: offset_storage
  endpoint:
    path: /offset/storage
    method: GET
- name: schema_history
  endpoint:
    path: /schema/history
    method: GET
- name: schema_history_internal
  endpoint:
    path: /schema.history.internal
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_topic
  endpoint:
    path: /schema.history.internal.rocketmq.topic
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_name_srv_addr
  endpoint:
    path: /schema.history.internal.rocketmq.name.srv.addr
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_access_key
  endpoint:
    path: /schema.history.internal.rocketmq.access.key
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_secret_key
  endpoint:
    path: /schema.history.internal.rocketmq.secret.key
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_recovery_attempts
  endpoint:
    path: /schema.history.internal.rocketmq.recovery.attempts
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_recovery_poll_interval_ms
  endpoint:
    path: /schema.history.internal.rocketmq.recovery.poll.interval.ms
    method: SET
    data_selector: records
- name: schema_history_internal_rocketmq_store_record_timeout_ms
  endpoint:
    path: /schema.history.internal.rocketmq.store.record.timeout.ms
    method: SET
    data_selector: records
- name: source_connector_configuration
  endpoint:
    path: /api/source-connector/configuration
    method: POST
    data_selector: configuration
    params:
      exactly.once.support: required
      transaction.boundary: poll
- name: schema_change_event
  endpoint:
    path: /schema_change_event
    method: POST
    data_selector: payload
    params: {}
- name: stop-snapshot
  endpoint:
    path: /stop-snapshot
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - db1.table1
      - db1.table2
- name: snapshot_events
  endpoint:
    path: /snapshot/events
    method: GET
    data_selector: events
- name: data_change_events
  endpoint:
    path: /data/change/events
    method: GET
    data_selector: events
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: config
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /services/data/vXX.X/snapshot_metrics
    method: GET
- name: streaming_metrics
  endpoint:
    path: /services/data/vXX.X/streaming_metrics
    method: GET
- name: schema_history_metrics
  endpoint:
    path: /services/data/vXX.X/schema_history_metrics
    method: GET
- name: schema_change_topic
  endpoint:
    path: /schema/change/topic
    method: POST
    data_selector: payload
    params: {}
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot
    params:
      type: incremental
- name: Blocking Snapshot
  endpoint:
    path: /blocking-snapshot
    method: POST
- name: Change Events
  endpoint:
    path: /change-events
    method: GET
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params:
      database.hostname: 192.168.99.100
      database.port: '3306'
      database.user: debezium-user
      database.password: debezium-user-pw
      database.server.id: '184054'
      topic.prefix: fullfillment
      database.include.list: inventory
      schema.history.internal.kafka.bootstrap.servers: kafka:9092
      schema.history.internal.kafka.topic: schemahistory.fullfillment
      include.schema.changes: 'true'
- name: snapshot_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=snapshot,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: streaming_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=streaming,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: schema_history_metrics
  endpoint:
    path: /mbean/debezium.mariadb:type=connector-metrics,context=schema-history,server=<topic.prefix>
    method: GET
    data_selector: metrics
    params: {}
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: execute-snapshot
    params:
      type: incremental
      data-collections: []
- name: customers
  endpoint:
    path: /path/to/customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: mongodb
  endpoint:
    path: /mongodb
    method: GET
    data_selector: records
    params: {}
- name: PostgreSQL publications
  endpoint:
    path: /postgresql/publications
    method: GET
    data_selector: publications
- name: stop_snapshot
  endpoint:
    path: /stop-snapshot
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - schema.table
- name: blocking_snapshot
  endpoint:
    path: /blocking_snapshot
    method: POST
    data_selector: data
    params:
      data-collections: schema1.table1,schema1.table2
      additional-conditions:
      - data-collection: schema1.table1
        filter: SELECT * FROM [schema1].[table1] WHERE column1 = 0 ORDER BY column2
          DESC
      - data-collection: schema1.table2
        filter: SELECT * FROM [schema1].[table2] WHERE column2 > 0
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
- name: PostgreSQL
  endpoint:
    path: /logical_decoding
    method: GET
    data_selector: records
- name: snapshot
  endpoint:
    path: /api/v1/snapshot
    method: POST
    data_selector: snapshot_records
- name: changes
  endpoint:
    path: /api/v1/changes
    method: GET
    data_selector: change_records
- name: products
  endpoint:
    data_selector: data-collections
- name: customers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: network_address_types
  endpoint:
    path: /mappings/network_address_types
    method: GET
    data_selector: records
- name: postgis_types
  endpoint:
    path: /mappings/postgis_types
    method: GET
    data_selector: records
- name: pgvector_types
  endpoint:
    path: /mappings/pgvector_types
    method: GET
    data_selector: records
- name: fulfillment-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: dbz_publication
  endpoint:
    path: /services/data/vXX.X/sobjects/dbz_publication
    method: GET
    data_selector: records
    params: {}
- name: change_events
  endpoint:
    path: /path/to/endpoint
    method: GET
    data_selector: events
    params:
      incremental: schema
- name: initial_snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot_data
    params:
      snapshot.mode: initial
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot_request
    params: {}
- name: incremental_snapshot
  endpoint:
    path: /configuration/signalling.html#sending-signals-to-a-debezium-connector
    method: INSERT
    data_selector: data-collections
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: value
    params: {}
- name: CLOB
  endpoint:
    path: /services/data/vXX.X/sobjects/CLOB
    method: GET
    data_selector: records
    params: {}
- name: NCLOB
  endpoint:
    path: /services/data/vXX.X/sobjects/NCLOB
    method: GET
    data_selector: records
    params: {}
- name: BLOB
  endpoint:
    path: /services/data/vXX.X/sobjects/BLOB
    method: GET
    data_selector: records
    params: {}
- name: archive_destination
  endpoint:
    path: /path/to/archive/destination
    method: GET
    data_selector: archive.destination.name
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: config
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /connectors/debezium.oracle/snapshot_metrics
    method: GET
- name: streaming_metrics
  endpoint:
    path: /connectors/debezium.oracle/streaming_metrics
    method: GET
- name: schema_history_metrics
  endpoint:
    path: /connectors/debezium.oracle/schema_history_metrics
    method: GET
- name: snapshot_mode
  endpoint:
    path: /snapshot/mode
    method: GET
    data_selector: settings
- name: incremental_snapshot
  endpoint:
    path: /debezium_signal
    method: INSERT
    data_selector: data
    params:
      type: incremental
- name: blocking_snapshot
  endpoint:
    path: /blocking/snapshot
    method: POST
    data_selector: data
    params: {}
- name: drop_transaction
  endpoint:
    path: /drop/transaction
    method: POST
    data_selector: data
    params: {}
- name: transaction_boundary_event
  endpoint:
    path: /transaction_boundary_event
    method: GET
    data_selector: records
    params: {}
- name: change_data_event
  endpoint:
    path: /change_data_event
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /services/data/vXX.X/sobjects/customers
    method: GET
    data_selector: records
    params: {}
- name: truncate_event
  endpoint:
    path: /path/to/truncate_event
    method: POST
    data_selector: source
    params:
      op: t
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: ''
    params: {}
- name: log_mining
  endpoint:
    path: /oracle/log/mining
    method: GET
- name: schema.history.internal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: openlogreplicator
  endpoint:
    path: /OpenLogReplicator
    method: GET
    data_selector: records
    params:
      openlogreplicator.source: ORACLE
      openlogreplicator.host: <ip address or hostname of OpenLogReplicator>
      openlogreplicator.port: <port OpenLogReplicator is listening on>
- name: change_events
  endpoint:
    path: /sqlserver/change/events
    method: POST
    data_selector: events
    params: {}
- name: snapshot
  endpoint:
    path: /path/to/snapshot/endpoint
    method: GET
    data_selector: records
    params: {}
- name: debezium_signal
  endpoint:
    path: /debezium_signal
    method: INSERT
    data_selector: data-collections
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configured/Kafka/signaling/topic
    method: POST
    data_selector: value
    params:
      type: incremental
      data-collections:
      - database.schema.table
- name: customers
  endpoint:
    path: /customers
    method: POST
    data_selector: payload
    params: {}
- name: transaction_boundary
  endpoint:
    path: /path/to/transaction_boundary
    method: GET
    data_selector: transaction
    params: {}
- name: change_data_capture
  endpoint:
    path: /path/to/change_data_capture
    method: GET
    data_selector: change
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: schema.history.internal.kafka.topic
  endpoint:
    path: /sqlserver/property/database/history/kafka/topic
    method: GET
    data_selector: records
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/database/history/kafka/bootstrap-servers
    method: GET
    data_selector: records
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/recovery/poll/interval/ms
    method: GET
    data_selector: records
    params:
      default: 100
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/query/timeout/ms
    method: GET
    data_selector: records
    params:
      default: 3000
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: /sqlserver/property/database/history/kafka/create/timeout/ms
    method: GET
    data_selector: records
    params:
      default: 30000
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: /sqlserver/property/database/history/kafka/recovery/attempts
    method: GET
    data_selector: records
    params:
      default: 100
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: /sqlserver/property/database/history/skip/unparseable/ddl
    method: GET
    data_selector: records
    params:
      default: false
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: /sqlserver/property/database/history/store/only/captured/tables/ddl
    method: GET
    data_selector: records
    params:
      default: false
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: /sqlserver/property/database/history/store/only/captured/databases/ddl
    method: GET
    data_selector: records
    params:
      default: false
- name: signal.kafka.topic
  endpoint:
    path: /sqlserver/property/signal/kafka/topic
    method: GET
    data_selector: records
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: /sqlserver/property/signal/kafka/groupId
    method: GET
    data_selector: records
    params:
      default: kafka-signal
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/signal/kafka/bootstrap-servers
    method: GET
    data_selector: records
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: /sqlserver/property/signal/kafka/poll/timeout/ms
    method: GET
    data_selector: records
    params:
      default: 100
- name: notification.sink.topic.name
  endpoint:
    path: /sqlserver/property/notification/sink/topic/name
    method: GET
    data_selector: records
    params: {}
- name: initial_snapshot
  endpoint:
    path: /api/snapshot
    method: POST
    data_selector: snapshot_data
    params:
      snapshot.mode: initial
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: execute-snapshot
- name: stop_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-kafka-signaling-channel
    method: POST
    data_selector: data
    params: {}
- name: transaction_events
  endpoint:
    path: /path/to/transaction/events
    method: GET
    data_selector: records
    params: {}
- name: change_events
  endpoint:
    path: /path/to/data/change/events
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /path/to/customers
    method: GET
    data_selector: payload
    params: {}
- name: db2-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params:
      name: unique_connector_name
      connector.class: io.debezium.connector.db2.Db2Connector
      tasks.max: 1
      database.hostname: db2_hostname
      database.port: 50000
      database.user: db2_user
      database.password: db2_password
      database.dbname: db2_dbname
      db2.platform: LUW
      topic.prefix: topic_prefix
      table.include.list: include_list
      table.exclude.list: exclude_list
      column.include.list: column_include_list
      column.exclude.list: column_exclude_list
      time.precision.mode: adaptive
      tombstones.on.delete: true
      include.schema.changes: true
- name: schema.history.internal.kafka.topic
  endpoint:
    path: schema.history.internal.kafka.topic
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: schema.history.internal.kafka.bootstrap.servers
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: schema.history.internal.kafka.recovery.poll.interval.ms
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: schema.history.internal.kafka.query.timeout.ms
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: schema.history.internal.kafka.create.timeout.ms
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: schema.history.internal.kafka.recovery.attempts
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: schema.history.internal.skip.unparseable.ddl
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: schema.history.internal.store.only.captured.tables.ddl
    method: GET
    data_selector: ''
    params: {}
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: schema.history.internal.store.only.captured.databases.ddl
    method: GET
    data_selector: ''
    params: {}
- name: signal.kafka.topic
  endpoint:
    path: signal.kafka.topic
    method: GET
    data_selector: ''
    params: {}
- name: signal.kafka.groupId
  endpoint:
    path: signal.kafka.groupId
    method: GET
    data_selector: ''
    params: {}
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: signal.kafka.bootstrap.servers
    method: GET
    data_selector: ''
    params: {}
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: signal.kafka.poll.timeout.ms
    method: GET
    data_selector: ''
    params: {}
- name: notification.sink.topic.name
  endpoint:
    path: notification.sink.topic.name
    method: GET
    data_selector: ''
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: INSERT, UPDATE, DELETE
    data_selector: records
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /debezium.cassandra:type=connector-metrics,context=snapshot,server=<topic.prefix>
    method: GET
    data_selector: metrics
- name: streaming_metrics
  endpoint:
    path: /debezium.cassandra:type=connector-metrics,context=streaming,server=<topic.prefix>
    method: GET
    data_selector: metrics
- name: cassandra_config
  endpoint:
    path: /cassandra/config
    method: GET
    data_selector: config
    params: {}
- name: change_events
  endpoint:
    path: /vstream
    method: gRPC
    data_selector: events
    params: {}
- name: data_change_events
  endpoint:
    path: /events
    method: POST
    data_selector: records
- name: customers
  endpoint:
    path: /path/to/customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: config
    params: {}
- name: streaming_metrics
  endpoint:
    path: /debezium/vitess/type=connector-metrics/context=streaming/server=<topic.prefix>
    method: GET
- name: vitess
  endpoint:
    path: /services/data/vXX.X/vitess
    method: GET
    data_selector: records
    params: {}
- name: Users
  endpoint:
    path: /path/to/users
    method: POST
    data_selector: payload
    params: {}
- name: change_stream
  endpoint:
    path: /gcp/spanner/change.stream
    method: GET
    data_selector: changeStreamAll
    params:
      project_id: Project
      instance_id: Instance
      database_id: Database
- name: connector
  endpoint:
    path: null
    method: null
    data_selector: null
    params: {}
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: data
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-kafka-signaling-channel
    method: POST
    data_selector: data
    params: {}
- name: blocking_snapshot
  endpoint:
    path: /blocking/snapshot
    method: POST
    data_selector: data
    params:
      data-collections:
      - schema1.table1
      - schema1.table2
      additional-conditions:
      - data-collection: schema1.table1
        filter: SELECT * FROM [schema1].[table1] WHERE column1 = 0 ORDER BY column2
          DESC
      - data-collection: schema1.table2
        filter: SELECT * FROM [schema1].[table2] WHERE column2 > 0
- name: transaction_boundary
  endpoint:
    path: /transaction_boundary
    method: GET
    data_selector: records
- name: data_change_event
  endpoint:
    path: /data_change_event
    method: GET
    data_selector: records
- name: informix-connector
  endpoint:
    path: /connectors/informix-connector/config
    method: POST
    data_selector: config
    params: {}
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: records
    params: {}
- name: topic_cache_size
  endpoint:
    path: topic.cache.size
    method: GET
    data_selector: '10000'
- name: topic_heartbeat_prefix
  endpoint:
    path: topic.heartbeat.prefix
    method: GET
    data_selector: __debezium-heartbeat
- name: topic_transaction
  endpoint:
    path: topic.transaction
    method: GET
    data_selector: transaction
- name: snapshot_max_threads
  endpoint:
    path: snapshot.max.threads
    method: GET
    data_selector: '1'
- name: custom_metric_tags
  endpoint:
    path: custom.metric.tags
    method: GET
    data_selector: No default
- name: errors_max_retries
  endpoint:
    path: errors.max.retries
    method: GET
    data_selector: '-1'
- name: database_query_timeout_ms
  endpoint:
    path: database.query.timeout.ms
    method: GET
    data_selector: '600000'
- name: extended_headers_enabled
  endpoint:
    path: extended.headers.enabled
    method: GET
    data_selector: true
- name: schema.history.internal.kafka.topic
  endpoint:
    path: schema.history.internal.kafka.topic
    method: GET
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: schema.history.internal.kafka.bootstrap.servers
    method: GET
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: schema.history.internal.kafka.recovery.poll.interval.ms
    method: GET
    data_selector: '100'
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: schema.history.internal.kafka.query.timeout.ms
    method: GET
    data_selector: '3000'
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: schema.history.internal.kafka.create.timeout.ms
    method: GET
    data_selector: '30000'
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: schema.history.internal.kafka.recovery.attempts
    method: GET
    data_selector: '100'
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: schema.history.internal.skip.unparseable.ddl
    method: GET
    data_selector: 'false'
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: schema.history.internal.store.only.captured.tables.ddl
    method: GET
    data_selector: 'false'
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: schema.history.internal.store.only.captured.databases.ddl
    method: GET
    data_selector: 'false'
- name: signal.kafka.topic
  endpoint:
    path: signal.kafka.topic
    method: GET
    data_selector: <topic.prefix>-signal
- name: signal.kafka.groupId
  endpoint:
    path: signal.kafka.groupId
    method: GET
    data_selector: kafka-signal
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: signal.kafka.bootstrap.servers
    method: GET
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: signal.kafka.poll.timeout.ms
    method: GET
    data_selector: '100'
- name: notification.sink.topic.name
  endpoint:
    path: notification.sink.topic.name
    method: GET
- name: jdbc_sink
  endpoint:
    path: /jdbc/sink
    method: POST
    data_selector: records
- name: jdbc-connector
  endpoint:
    path: ''
    method: POST
    data_selector: ''
    params: {}
- name: sink.database
  endpoint:
    path: /mongodb/sink/database
    method: POST
    data_selector: config
    params: {}
- name: topic_routing
  endpoint:
    path: /documentation/reference/nightly/transformations/topic-routing.html
    method: GET
    data_selector: records
- name: event_flattening
  endpoint:
    path: /transformations/event-flattening
    method: GET
    data_selector: records
- name: ExtractNewDocumentState
  endpoint:
    path: /mongodb/extractNewDocumentState
    method: GET
    data_selector: records
    params: {}
- name: event_changes
  endpoint:
    path: /transformations/event-changes
    method: GET
    data_selector: events
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: POST
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: GET
    data_selector: records
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: POST
    data_selector: records
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: event
    params: {}
- name: collection.op.invalid.behavior
  endpoint:
    params:
      default: warn
- name: collection.field.event.id
  endpoint:
    params:
      default: _id
- name: collection.field.event.key
  endpoint:
    params:
      default: aggregateid
- name: collection.field.event.payload
  endpoint:
    params:
      default: payload
- name: collection.expand.json.payload
  endpoint:
    params:
      default: 'false'
- name: route.by.field
  endpoint:
    params:
      default: aggregatetype
- name: route.topic.regex
  endpoint:
    params:
      default: (?<routedByValue>.*)
- name: route.topic.replacement
  endpoint:
    params:
      default: outbox.event.${routedByValue}
- name: route.tombstone.on.empty.payload
  endpoint:
    params:
      default: 'false'
- name: tracing.span.context.field
  endpoint:
    params:
      default: tracingspancontext
- name: tracing.operation.name
  endpoint:
    params:
      default: debezium-read
- name: tracing.with.context.field.only
  endpoint:
    params:
      default: 'false'
- name: outbox_event
  endpoint:
    path: collection.op.invalid.behavior
    method: GET
    data_selector: records
    params: {}
- name: filter
  endpoint:
    path: /transformations/filtering.html
    method: GET
    data_selector: records
    params: {}
- name: schema_change_event_filter
  endpoint:
    path: /documentation/reference/stable/transformations/schema-change-event-filter.html
    method: GET
    data_selector: records
    params: {}
- name: ContentBasedRouter
  endpoint:
    path: /transformations/content-based-routing
    method: GET
- name: single_message_transformation
  endpoint:
    path: /documentation/transformations/applying-transformations-selectively.html
    method: GET
    data_selector: records
    params: {}
- name: PartitionRouting
  endpoint:
    path: /reference/transformations/partition-routing.html
    method: GET
    data_selector: configuration options
    params: {}
- name: HeaderToValue
  endpoint:
    path: /transformations/header-to-value
    method: GET
- name: TimezoneConverter
  endpoint:
    path: /reference/transformations/timezone-converter.html
    method: GET
    data_selector: records
    params: {}
- name: GeometryFormatTransformer
  endpoint:
    path: /geometry-format-transformer
    method: POST
    data_selector: transforms
    params: {}
- name: DecodeLogicalDecodingMessageContent
  endpoint:
    path: /transformations/decode-logical-decoding-message-content
    method: GET
    data_selector: records
    params: {}
- name: vector_to_json
  endpoint:
    path: /transformations/vector-to-json
    method: GET
    data_selector: records
- name: swap_geometry_coordinates
  endpoint:
    path: /transformations/swap-geometry-coordinates
    method: GET
    data_selector: configuration_options
    params: {}
- name: engine
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: engine_properties
  endpoint:
    path: /engine/properties
    method: GET
    data_selector: properties
- name: JSON SerDe
  endpoint:
    path: /documentation/reference/nightly/serdes.html
    method: GET
    data_selector: schema
- name: outbox_event
  endpoint:
    path: /quarkus/debezium-outbox
    method: GET
    data_selector: records
    params: {}
- name: inventory
  endpoint:
    path: /inventory
    method: GET
    data_selector: records
    params: {}
- name: inventory.inventory.products
  endpoint:
    path: inventory.inventory.products
    method: GET
    data_selector: records
    params: {}
- name: inventory.inventory.customers
  endpoint:
    path: inventory.inventory.customers
    method: GET
    data_selector: records
    params: {}
- name: ActivateTracingSpan
  endpoint:
    path: /documentation/reference/integrations/tracing.html
    method: GET
    data_selector: records
    params: {}
- name: Kafka Producer tracing
  endpoint:
    path: /documentation/reference/integrations/tracing.html
    method: GET
    data_selector: records
    params: {}
- name: Outbox Extension
  endpoint:
    path: /documentation/reference/integrations/outbox.html
    method: GET
    data_selector: records
    params: {}
- name: Event Router SMT
  endpoint:
    path: /documentation/reference/configuration/outbox-event-router.html
    method: GET
    data_selector: records
    params: {}
- name: cdc_configuration
  endpoint:
    path: /quarkus/debezium/configuration
    method: GET
    data_selector: quarkus.debezium.*
    params: {}
- name: datasource_configuration
  endpoint:
    path: /quarkus/datasource/configuration
    method: GET
    data_selector: quarkus.datasource.*
    params: {}
- name: DebeziumPostgresConnector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params: {}
- name: PostgreSQL
  endpoint:
    path: /connectors/postgresql
    method: GET
- name: MySQL
  endpoint:
    path: /connectors/mysql
    method: GET
- name: MongoDB
  endpoint:
    path: /connectors/mongodb
    method: GET
- name: Oracle
  endpoint:
    path: /connectors/oracle
    method: GET
- name: SQL Server
  endpoint:
    path: /connectors/sqlserver
    method: GET
- name: Db2
  endpoint:
    path: /connectors/db2
    method: GET
- name: Cassandra
  endpoint:
    path: /connectors/cassandra
    method: GET
- name: kinesis
  endpoint:
    path: /debezium/sink/kinesis
    method: POST
    data_selector: records
    params:
      type: kinesis
      region: eu-central-1
- name: amazon_kinesis
  endpoint:
    path: /kinesis
    method: POST
    data_selector: ''
    params:
      debezium.sink.type: kinesis
- name: google_cloud_pubsub
  endpoint:
    path: /pubsub
    method: POST
    data_selector: ''
    params:
      debezium.sink.type: pubsub
- name: http_client
  endpoint:
    path: /http
    method: POST
    data_selector: ''
    params:
      debezium.sink.type: http
- name: apache_pulsar
  endpoint:
    path: /pulsar
    method: POST
    data_selector: ''
    params:
      debezium.sink.type: pulsar
- name: azure_event_hubs
  endpoint:
    path: /eventhubs
    method: POST
    data_selector: ''
    params:
      debezium.sink.type: eventhubs
- name: nats-streaming
  endpoint:
    path: /nats-streaming
    method: POST
- name: nats-jetstream
  endpoint:
    path: /nats-jetstream
    method: POST
- name: kafka
  endpoint:
    path: /kafka
    method: POST
- name: pravega
  endpoint:
    path: /pravega
    method: POST
- name: infinispan
  endpoint:
    path: /infinispan
    method: POST
- name: rocketmq
  endpoint:
    path: /rocketmq
    method: POST
- name: rabbitmq
  endpoint:
    path: /rabbitmq
    method: POST
- name: rabbitmqstream
  endpoint:
    path: /rabbitmqstream
    method: POST
- name: milvus
  endpoint:
    path: /milvus
    method: POST
- name: nats-streaming
  endpoint:
    path: /nats-streaming
    method: POST
- name: nats-jetstream
  endpoint:
    path: /nats-jetstream
    method: POST
- name: kafka
  endpoint:
    path: /kafka
    method: POST
- name: pravega
  endpoint:
    path: /pravega
    method: POST
- name: infinispan
  endpoint:
    path: /infinispan
    method: POST
- name: rocketmq
  endpoint:
    path: /rocketmq
    method: POST
- name: rabbitmq
  endpoint:
    path: /rabbitmq
    method: POST
- name: rabbitmqstream
  endpoint:
    path: /rabbitmqstream
    method: POST
- name: milvus
  endpoint:
    path: /milvus
    method: POST
- name: debezium_sink
  endpoint:
    path: /debezium/sink
    method: POST
- name: cassandra_connector
  endpoint:
    path: /debezium/source/configuration
    method: GET
    data_selector: ''
    params: {}
- name: source
  endpoint:
    path: /source
    method: POST
    data_selector: data
    params: {}
- name: destination
  endpoint:
    path: /destination
    method: POST
    data_selector: data
    params: {}
- name: source
  endpoint:
    path: /sources
    method: POST
- name: destination
  endpoint:
    path: /destinations
    method: POST
- name: pipeline
  endpoint:
    path: /pipelines
    method: POST
- name: Debezium marker
  config:
    type: io.debezium.transforms.ExtractNewRecordState
    add.fields: op
    add.headers: db,table
- name: customers
  endpoint:
    path: /mysql.inventory.customers
    method: GET
    data_selector: payload.after
    params: {}
- name: mysql.inventory.customers
  endpoint:
    path: /mysql.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: mysql
  endpoint:
    path: /example-mysql
    method: GET
- name: kafka
  endpoint:
    path: /kafka
    method: GET
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: SELECT
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: SELECT
    data_selector: records
    params: {}
- name: geom
  endpoint:
    path: /inventory/geom
    method: SELECT
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /inventory/orders
    method: SELECT
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /inventory/products
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: SELECT
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /dbserver1.inventory.customers
    method: GET
    data_selector: records
    params: {}
- name: products
  endpoint:
    path: /dbserver1.inventory.products
    method: GET
    data_selector: records
    params: {}
- name: products_on_hand
  endpoint:
    path: /dbserver1.inventory.products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: orders
  endpoint:
    path: /dbserver1.inventory.orders
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /connectors/inventory-connector
    method: POST
    data_selector: payload
    params: {}
- name: mysql
  endpoint:
    path: /connectors/mysql
    method: POST
    data_selector: records
    params:
      incremental: updated_at
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: addresses
  endpoint:
    path: /inventory/addresses
    method: GET
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
- name: geom
  endpoint:
    path: /inventory/geom
    method: GET
- name: orders
  endpoint:
    path: /inventory/orders
    method: GET
- name: products
  endpoint:
    path: /inventory/products
    method: GET
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
- name: products_on_hand
  endpoint:
    path: /inventory/products_on_hand
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: dbserver1.inventory.customers
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /customers
    method: DELETE
    data_selector: payload
    params: {}
- name: Apicurio API and Schema Registry
  endpoint:
    path: /apis/registry/v2
    method: GET
    data_selector: schemas
    params: {}
- name: Confluent Schema Registry
  endpoint:
    path: /
    method: GET
    data_selector: schemas
    params: {}
- name: schema_registry
  endpoint:
    path: /schema-registry
    method: GET
    data_selector: ''
    params: {}
- name: debezium_connector
  endpoint:
    path: /debezium/connector
    method: POST
    data_selector: ''
    params:
      key.converter: io.confluent.connect.avro.AvroConverter
      key.converter.schema.registry.url: http://localhost:8081
      value.converter: io.confluent.connect.avro.AvroConverter
      value.converter.schema.registry.url: http://localhost:8081
- name: signaling
  endpoint:
    path: /configuration/signalling
    method: GET
    data_selector: signals
    params: {}
- name: execute_snapshot
  endpoint:
    path: /debezium/execute-snapshot
    method: POST
    data_selector: data
    params: {}
- name: notifications
  endpoint:
    path: /notifications
    method: POST
    data_selector: notifications
    params: {}
- name: custom_notification_channel
  endpoint:
    path: /kafka/connect
    method: POST
    data_selector: notification.enabled.channels
    params: {}
- name: offset_store
  endpoint:
    path: /offset/storage
    method: GET
    data_selector: records
    params: {}
- name: schema_history_store
  endpoint:
    path: /schema/history
    method: GET
    data_selector: records
    params: {}
- name: schema_history
  endpoint:
    path: /schema/history
    method: GET
- name: offset_storage
  endpoint:
    path: /offset/storage
    method: GET
- name: offset_storage
  endpoint:
    path: /offset/storage
    method: GET
    data_selector: records
    params: {}
- name: schema_history_internal
  endpoint:
    path: /schema/history/internal
    method: GET
    data_selector: records
    params: {}
- name: rocketmq_schema_history
  endpoint:
    path: /rocketmq/schema/history
    method: GET
    data_selector: records
    params: {}
- name: schema_change
  endpoint:
    path: <topicPrefix>
    method: POST
    data_selector: payload
    params: {}
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: signal
    params:
      type: incremental
- name: incremental_snapshot
  endpoint:
    path: /path/to/incremental_snapshot
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - db1.table1
      - db1.table2
- name: blocking_snapshot
  endpoint:
    path: /blocking/snapshot
    method: POST
    data_selector: data
    params:
      data-collections: schema1.table1,schema1.table2
      additional-conditions:
      - data-collection: schema1.table1
        filter: SELECT * FROM [schema1].[table1] WHERE column1 = 0 ORDER BY column2
          DESC
      - data-collection: schema1.table2
        filter: SELECT * FROM [schema1].[table2] WHERE column2 > 0
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: change_events
  endpoint:
    path: /your/endpoint/path
    method: POST
    data_selector: records
    params:
      incremental: updated_at
- name: snapshot
  endpoint:
    path: /snapshot
    method: GET
    data_selector: records
- name: snapshot_metrics
  endpoint:
    path: /metrics/snapshot
    method: GET
    data_selector: null
    params: {}
- name: streaming_metrics
  endpoint:
    path: /metrics/streaming
    method: GET
    data_selector: null
    params: {}
- name: schema_history_metrics
  endpoint:
    path: /metrics/schema-history
    method: GET
    data_selector: null
    params: {}
- name: schema_change
  endpoint:
    path: /schema/change/topic
    method: POST
    data_selector: payload
    params: {}
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-kafka-signaling-channel
    method: POST
    data_selector: data
    params:
      type: incremental
      data-collections:
      - db1.table1
      - db1.table2
- name: customers
  endpoint:
    path: /customers
    method: POST
    data_selector: payload
    params: {}
- name: truncate_event
  endpoint:
    path: /events/truncate
    method: POST
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: signaling_channels
  endpoint:
    path: /signaling/channels
    method: GET
    data_selector: channels
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /path/to/snapshot_metrics
    method: GET
    data_selector: snapshot_metrics
- name: streaming_metrics
  endpoint:
    path: /path/to/streaming_metrics
    method: GET
    data_selector: streaming_metrics
- name: schema_history_metrics
  endpoint:
    path: /path/to/schema_history_metrics
    method: GET
    data_selector: schema_history_metrics
- name: snapshot
  endpoint:
    path: /snapshot
    method: POST
    data_selector: snapshot.records
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: snapshot.records
- name: incremental_snapshot
  endpoint:
    path: /incremental/snapshot
    method: POST
    data_selector: data
    params: {}
- name: customers
  endpoint:
    path: /inventory/customers
    method: GET
    data_selector: records
- name: delete_event
  endpoint:
    path: /delete
    method: POST
    data_selector: payload
    params: {}
- name: mongodb
  endpoint:
    path: /mongodb
    method: GET
- name: signal.kafka.topic
  endpoint:
    path: <topic.prefix>-signal
    method: GET
    data_selector: records
- name: signal.kafka.groupId
  endpoint:
    path: kafka-signal
    method: GET
    data_selector: records
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: No default
    method: GET
    data_selector: records
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: '100'
    method: GET
    data_selector: records
- name: notification.sink.topic.name
  endpoint:
    path: No default
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: signaling_table
  endpoint:
    path: /myschema/debezium_signal
    method: INSERT
    data_selector: data
    params: {}
- name: customers
  endpoint:
    path: /public/customers
    method: GET
- name: fulfillment-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: database
  endpoint:
    path: /database
    method: GET
    data_selector: records
- name: flush.lsn.source
  endpoint:
    params:
      default: 'true'
- name: retriable.restart.connector.wait.ms
  endpoint:
    params:
      default: 10000
- name: skipped.operations
  endpoint:
    params:
      default: t
- name: incremental.snapshot.chunk.size
  endpoint:
    params:
      default: 1024
- name: incremental.snapshot.watermarking.strategy
  endpoint:
    params:
      default: insert_insert
- name: read.only
  endpoint:
    params:
      default: 'false'
- name: xmin.fetch.interval.ms
  endpoint:
    params:
      default: 0
- name: topic.naming.strategy
  endpoint:
    params:
      default: io.debezium.schema.SchemaTopicNamingStrategy
- name: topic.delimiter
  endpoint:
    params:
      default: .
- name: topic.cache.size
  endpoint:
    params:
      default: 10000
- name: topic.heartbeat.prefix
  endpoint:
    params:
      default: __debezium-heartbeat
- name: topic.transaction
  endpoint:
    params:
      default: transaction
- name: snapshot.max.threads
  endpoint:
    params:
      default: 1
- name: errors.max.retries
  endpoint:
    params:
      default: -1
- name: database.query.timeout.ms
  endpoint:
    params:
      default: 600000
- name: guardrail.collections.max
  endpoint:
    params:
      default: 0
- name: guardrail.collections.limit.action
  endpoint:
    params:
      default: warn
- name: extended.headers.enabled
  endpoint:
    params:
      default: true
- name: change_events
  endpoint:
    path: /connector/change_events
    method: GET
    data_selector: events
- name: stop_snapshot
  endpoint:
    path: /kafka/signaling
    method: POST
    data_selector: value
    params: {}
- name: customers
  endpoint:
    path: /services/data/vXX.X/sobjects/customers
    method: GET
    data_selector: records
    params: {}
- name: temporal_types
  endpoint:
    path: /temporal_types
    method: GET
    data_selector: types
    params: {}
- name: rowid_types
  endpoint:
    path: /rowid_types
    method: GET
    data_selector: types
    params: {}
- name: xml_types
  endpoint:
    path: /xml_types
    method: GET
    data_selector: types
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: connector_configuration
  endpoint:
    path: /config
    method: POST
    data_selector: config
    params: {}
- name: connector
  endpoint:
    path: /services/data/vXX.X/sobjects/Connector
    method: GET
- name: schema_history
  endpoint:
    path: /schema/history
    method: GET
    data_selector: records
- name: signal
  endpoint:
    path: /signal
    method: GET
    data_selector: records
- name: notification_sink
  endpoint:
    path: /notification/sink
    method: GET
    data_selector: records
- name: snapshot_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/SnapshotMetrics
    method: GET
    data_selector: metrics
    params: {}
- name: streaming_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/StreamingMetrics
    method: GET
    data_selector: metrics
    params: {}
- name: schema_history_metrics
  endpoint:
    path: /services/data/vXX.X/sobjects/SchemaHistoryMetrics
    method: GET
    data_selector: metrics
    params: {}
- name: schema-changes
  endpoint:
    path: /
    method: POST
    data_selector: data
- name: database.connection.adapter
  endpoint:
    path: /path/to/endpoint
    method: GET
    data_selector: records
    params: {}
- name: snapshot_mode
  endpoint:
    path: /snapshot/mode
    method: GET
- name: ad_hoc_snapshots
  endpoint:
    path: /ad_hoc/snapshots
    method: POST
- name: incremental_snapshots
  endpoint:
    path: /incremental/snapshots
    method: POST
- name: incremental_snapshot
  endpoint:
    path: /<signalTable>
    method: INSERT
    data_selector: data
    params: {}
- name: transaction_boundary
  endpoint:
    path: /transaction/boundary
    method: GET
    data_selector: records
- name: schema_change
  endpoint:
    path: /schema/change
    method: GET
    data_selector: records
- name: V$LOGMNR_CONTENTS
  endpoint:
    path: V$LOGMNR_CONTENTS
    method: GET
    data_selector: records
    params: {}
- name: customers
  endpoint:
    path: /server1/DEBEZIUM/CUSTOMERS
    method: GET
    data_selector: payload
    params: {}
- name: archive_log_destination
  endpoint:
    path: /path/to/archive/log/destination
    method: GET
    data_selector: destination_name
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: OpenLogReplicator
  endpoint:
    path: /OpenLogReplicator
    method: GET
    data_selector: source
    params:
      openlogreplicator.source: ORACLE
      openlogreplicator.host: <ip address or hostname of OpenLogReplicator>
      openlogreplicator.port: <port OpenLogReplicator is listening on>
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: change events
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: DELETE
    data_selector: payload
    params: {}
- name: transaction_boundary
  endpoint:
    path: /transaction_boundary
    method: POST
    data_selector: transaction_events
    params: {}
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: ''
    params: {}
- name: sql_server_changes
  endpoint:
    path: /services/data/vXX.X/sobjects/SqlServerChanges
    method: GET
    data_selector: records
- name: incremental.snapshot.option
  endpoint:
    path: /sqlserver/incremental/snapshot
    method: GET
    data_selector: snapshot
    params: {}
- name: schema.history.internal.kafka.topic
  endpoint:
    path: /sqlserver/property/database-history/kafka-topic
    method: GET
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/database-history/kafka-bootstrap-servers
    method: GET
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: /sqlserver/property/database-history/kafka-recovery-poll-interval-ms
    method: GET
  params:
    default: 100
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: /sqlserver/property/database-history/kafka-query-timeout-ms
    method: GET
  params:
    default: 3000
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: /sqlserver/property/database-history/kafka-create-timeout-ms
    method: GET
  params:
    default: 30000
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: /sqlserver/property/database-history/kafka-recovery-attempts
    method: GET
  params:
    default: 100
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: /sqlserver/property/database-history/skip-unparseable-ddl
    method: GET
  params:
    default: false
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: /sqlserver/property/database-history/store-only-captured-tables-ddl
    method: GET
  params:
    default: false
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: /sqlserver/property/database-history/store-only-captured-databases-ddl
    method: GET
  params:
    default: false
- name: signal.kafka.topic
  endpoint:
    path: /sqlserver/property/signal/kafka-topic
    method: GET
- name: signal.kafka.groupId
  endpoint:
    path: /sqlserver/property/signal/kafka-groupId
    method: GET
- name: signal.kafka.bootstrap.servers
  endpoint:
    path: /sqlserver/property/signal/kafka-bootstrap-servers
    method: GET
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: /sqlserver/property/signal/kafka-poll-timeout-ms
    method: GET
  params:
    default: 100
- name: notification.sink.topic.name
  endpoint:
    path: /sqlserver/property/notification/sink-topic-name
    method: GET
- name: ad_hoc_snapshot
  endpoint:
    path: /execute-snapshot
    method: POST
    data_selector: signal
    params:
      type: incremental
- name: signaling_table
  endpoint:
    path: /schema.debezium_signal
    method: INSERT
    data_selector: data
    params: {}
- name: transaction_metadata
  endpoint:
    path: /transaction/metadata
    method: GET
    data_selector: transaction
    params: {}
- name: data_change_events
  endpoint:
    path: /data/change/events
    method: GET
    data_selector: events
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: db2-connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: config
    params: {}
- name: connector
  endpoint:
    path: /db2/connector
    method: POST
    data_selector: connector
    params: {}
- name: schema.history.internal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.bootstrap.servers
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: schema.history.internal.kafka.recovery.poll.interval.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: '100'
- name: schema.history.internal.kafka.query.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: '3000'
- name: schema.history.internal.kafka.create.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: '30000'
- name: schema.history.internal.kafka.recovery.attempts
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: '100'
- name: schema.history.internal.skip.unparseable.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: 'false'
- name: schema.history.internal.store.only.captured.tables.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: 'false'
- name: schema.history.internal.store.only.captured.databases.ddl
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: 'false'
- name: signal.kafka.topic
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: <topic.prefix>-signal
- name: signal.kafka.groupId
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: kafka-signal
- name: signal.kafka.poll.timeout.ms
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params:
      default: '100'
- name: notification.sink.topic.name
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /metrics/snapshot
    method: GET
    data_selector: metrics
- name: streaming_metrics
  endpoint:
    path: /metrics/streaming
    method: GET
    data_selector: metrics
- name: schema_history_metrics
  endpoint:
    path: /metrics/schema-history
    method: GET
    data_selector: metrics
- name: cdc_enabled
  endpoint:
    path: /cassandra.yaml
    method: GET
    data_selector: cdc_enabled
    params: {}
- name: cdc_raw_directory
  endpoint:
    path: /cassandra.yaml
    method: GET
    data_selector: cdc_raw_directory
    params: {}
- name: cdc_free_space_in_mb
  endpoint:
    path: /cassandra.yaml
    method: GET
    data_selector: cdc_free_space_in_mb
    params: {}
- name: cdc_free_space_check_interval_ms
  endpoint:
    path: /cassandra.yaml
    method: GET
    data_selector: cdc_free_space_check_interval_ms
    params: {}
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: records
    params: {}
- name: snapshot_metrics
  endpoint:
    path: /debezium.cassandra:type=connector-metrics,context=snapshot,server=<topic.prefix>
    method: GET
    data_selector: snapshot_metrics
    params: {}
- name: streaming_metrics
  endpoint:
    path: /debezium.cassandra:type=connector-metrics,context=streaming,server=<topic.prefix>
    method: GET
    data_selector: streaming_metrics
    params: {}
- name: connector_properties
  endpoint:
    path: /connector/properties
    method: GET
    data_selector: properties
    params: {}
- name: data_change_events
  endpoint:
    path: /path/to/data/change/events
    method: GET
    data_selector: events
- name: customers
  endpoint:
    path: /customers
    method: GET
    data_selector: payload
    params: {}
- name: inventory-connector
  endpoint:
    path: /connectors/inventory-connector/config
    method: POST
    data_selector: config
- name: streaming_metrics
  endpoint:
    path: /path/to/streaming/metrics
    method: GET
    data_selector: metrics
    params: {}
- name: vitess
  endpoint:
    path: /
    method: GET
    data_selector: records
    params: {}
- name: change_events
  endpoint:
    path: /change_streams
    method: GET
    data_selector: records
    params: {}
- name: Users
  endpoint:
    path: /path/to/users
    method: POST
    data_selector: payload
    params: {}
- name: change_stream
  endpoint:
    path: /gcp/spanner/change.stream
    method: POST
    data_selector: records
    params:
      gcp.spanner.change.stream: changeStreamAll
      gcp.spanner.project.id: Project
      gcp.spanner.instance.id: Instance
      gcp.spanner.database.id: Database
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params: {}
- name: stop_snapshot
  endpoint:
    path: /configuration/signalling.html#debezium-signaling-enabling-kafka-signaling-channel
    method: POST
    data_selector: data
    params: {}
- name: transaction_metadata
  endpoint:
    path: /transaction/metadata
    method: GET
    data_selector: records
- name: data_change_events
  endpoint:
    path: /data/change/events
    method: GET
    data_selector: records
- name: customers
  endpoint:
    path: /mydatabase/myschema/customers
    method: POST
    data_selector: records
    params: {}
- name: connector
  endpoint:
    path: /connectors
    method: POST
    data_selector: connector
    params: {}
- name: JDBC
  endpoint:
    path: /connectors/jdbc
    method: GET
- name: MongoDB Sink
  endpoint:
    path: /connectors/mongodb-sink
    method: GET
- name: orders
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: mongodb-sink-connector
  endpoint:
    path: /reference/connectors/mongodb-sink.html
    method: GET
    data_selector: ''
    params: {}
- name: topic_routing
  endpoint:
    path: /transformations/topic-routing
    method: GET
    data_selector: records
    params: {}
- name: event_flattening
  endpoint:
    path: /transformations/event-flattening
    method: GET
    data_selector: records
- name: ExtractNewDocumentState
  endpoint:
    path: /connectors/mongodb/event-flattening
    method: POST
    data_selector: records
    params: {}
- name: event_changes
  endpoint:
    path: /transformations/event-changes
    method: GET
    data_selector: records
    params: {}
- name: outbox_event_router
  endpoint:
    path: /outbox/event/router
    method: GET
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: GET
    data_selector: records
    params: {}
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: records
- name: outbox_event
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: message
    params: {}
- name: filter
  endpoint:
    path: /transformations/filtering
    method: GET
    data_selector: records
    params: {}
- name: schema_change_event_filter
  endpoint:
    path: /schema/change/event/filter
    method: POST
    data_selector: schema.change.event.exclude.list
    params: {}
- name: ContentBasedRouter
  endpoint:
    path: /transformations/content-based-routing
    method: GET
    data_selector: records
    params: {}
- name: single_message_transformation
  endpoint:
    path: /documentation/transformations/applying-transformations-selectively
    method: GET
    data_selector: records
- name: PartitionRouting
  endpoint:
    path: /transformations/partition-routing
    method: GET
    data_selector: configuration_options
    params: {}
- name: moveHeadersToValue
  endpoint:
    path: /header-to-value
    method: POST
    data_selector: transforms
    params: {}
- name: convertTimezone
  endpoint:
    path: /transformations/timezone-converter
    method: GET
    data_selector: records
    params: {}
- name: timescaledb
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: DecodeLogicalDecodingMessageContent
  endpoint:
    path: /documentation/reference/transformations/decode-logical-decoding-message-content.html
    method: GET
    data_selector: configuration options
    params: {}
- name: VectorToJson
  endpoint:
    path: /transformations/vector-to-json
    method: GET
    data_selector: examples
    params: {}
- name: engine
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: asynchronous_engine_properties
  endpoint:
    path: /async/engine/properties
    method: GET
    data_selector: properties
    params: {}
- name: database_schema_history_properties
  endpoint:
    path: /database/schema/history/properties
    method: GET
    data_selector: properties
    params: {}
- name: json_serde
  endpoint:
    path: /api/debezium/serde/json
    method: GET
    data_selector: configuration_options
    params: {}
- name: OrderCreatedEvent
  endpoint:
    path: /outbox/event
    method: POST
    data_selector: event
    params: {}
- name: outbox_event
  endpoint:
    path: /quarkus/debezium/outbox
    method: POST
- name: ActivateTracingSpan
  endpoint:
    path: /activateTracingSpan
    method: POST
    data_selector: records
- name: EventRouterSMT
  endpoint:
    path: /eventRouterSMT
    method: POST
    data_selector: records
- name: cdc_events
  endpoint:
    path: /cdc/events
    method: GET
    data_selector: events
    params: {}
- name: DebeziumContainer
  endpoint:
    path: /connectors
    method: POST
    data_selector: connectors
    params: {}
- name: embeddings
  endpoint:
    path: /ai/embeddings
    method: POST
    data_selector: results
    params: {}
- name: source_configuration
  endpoint:
    path: /debezium/source
    method: GET
    data_selector: configuration
    params: {}
- name: sink_configuration
  endpoint:
    path: /debezium/sink
    method: GET
    data_selector: configuration
    params: {}
- name: kinesis
  endpoint:
    path: /kinesis
    method: GET
- name: pubsub
  endpoint:
    path: /pubsub
    method: GET
- name: pubsublite
  endpoint:
    path: /pubsublite
    method: GET
- name: httpclient
  endpoint:
    path: /httpclient
    method: GET
- name: pulsar
  endpoint:
    path: /pulsar
    method: GET
- name: eventhubs
  endpoint:
    path: /eventhubs
    method: GET
- name: redis
  endpoint:
    path: /redis
    method: POST
    data_selector: records
    params:
      debezium.sink.type: redis
      debezium.sink.redis.address: host:port
      debezium.sink.redis.db.index: '0'
      debezium.sink.redis.null.key: default
      debezium.sink.redis.null.value: default
      debezium.sink.redis.batch.size: '500'
      debezium.sink.redis.retry.initial.delay.ms: '300'
      debezium.sink.redis.retry.max.delay.ms: '10000'
      debezium.sink.redis.connection.timeout.ms: '2000'
      debezium.sink.redis.socket.timeout.ms: '2000'
      debezium.sink.redis.wait.timeout.ms: '1000'
      debezium.sink.redis.message.format: compact
      debezium.sink.redis.memory.threshold.percentage: '85'
      debezium.sink.redis.skip.heartbeat.messages: 'true'
- name: nats-streaming
  endpoint:
    path: /nats-streaming
    method: POST
    data_selector: records
    params:
      debezium.sink.type: nats-streaming
      debezium.sink.nats-streaming.url: nats://host:port
      debezium.sink.nats-streaming.cluster.id: your_cluster_id
      debezium.sink.nats-streaming.client.id: your_client_id
- name: nats-jetstream
  endpoint:
    params:
      debezium.sink.type: nats-jetstream
- name: kafka
  endpoint:
    params:
      debezium.sink.type: kafka
- name: pravega
  endpoint:
    params:
      debezium.sink.type: pravega
- name: infinispan
  endpoint:
    params:
      debezium.sink.type: infinispan
- name: rocketmq
  endpoint:
    params:
      debezium.sink.type: rocketmq
- name: rabbitmq
  endpoint:
    params:
      debezium.sink.type: rabbitmq
- name: rabbitmqstream
  endpoint:
    params:
      debezium.sink.type: rabbitmqstream
- name: milvus
  endpoint:
    params:
      debezium.sink.type: milvus
- name: qdrant
  endpoint:
    params:
      debezium.sink.type: qdrant
- name: qdrant
  endpoint:
    path: /debezium/sink/qdrant
    method: POST
- name: instructlab
  endpoint:
    path: /debezium/sink/instructlab
    method: POST
- name: cassandra
  endpoint:
    path: /debezium/source
    method: GET
    data_selector: records
    params: {}
- name: offset
  endpoint:
    path: /offset
    method: GET
    data_selector: offsets
    params: {}
- name: schemaHistory
  endpoint:
    path: /schemaHistory
    method: GET
    data_selector: schemaHistory
    params: {}
- name: source
  endpoint:
    path: /source
    method: POST
    data_selector: config
- name: destination
  endpoint:
    path: /destination
    method: POST
    data_selector: config
- name: source
  endpoint:
    path: /documentation/faq
    method: GET
    data_selector: records
    params: {}
- name: destination
  endpoint:
    path: /documentation/reference/nightly/index.html
    method: GET
    data_selector: records
    params: {}
- name: unwrap
  config:
    name: Debezium marker
    description: Extract Debezium payload
    type: io.debezium.transforms.ExtractNewRecordState
    schema: string
    vaults: []
    config:
      add.fields: op
      add.headers: db,table
    predicate:
      type: org.apache.kafka.connect.transforms.predicates.TopicNameMatches
      config:
        pattern: inventory.inventory.products
      negate: false
- name: Kafka
  endpoint:
    path: /connectors/kafka
    method: GET
    data_selector: metrics
    params: {}
- name: Kafka Connect
  endpoint:
    path: /connectors/connect
    method: GET
    data_selector: metrics
    params: {}
- name: debezium-connector-mysql
  endpoint:
    path: /kafka/connectors
    method: POST
    data_selector: connector
    params: {}
- name: customers
  endpoint:
    path: /mysql.inventory.customers
    method: GET
    data_selector: payload
    params: {}
- name: offset
  endpoint:
    path: /offset
    method: GET
    data_selector: records
- name: schemaHistory
  endpoint:
    path: /schemaHistory
    method: GET
    data_selector: records
- name: offset
  endpoint:
    path: /offset
    method: POST
    data_selector: offsets
    params: {}
- name: schemaHistory
  endpoint:
    path: /schemaHistory
    method: POST
    data_selector: schemaHistory
    params: {}
- name: offset
  endpoint:
    path: offset
    method: POST
    data_selector: offset
    params: {}
- name: schemaHistory
  endpoint:
    path: schemaHistory
    method: POST
    data_selector: schemaHistory
    params: {}
notes:
- 'When you run into intermittent deserialization exceptions around 1 minute after
  starting connector, with a root cause of type EOFException or java.net.SocketException:
  Connection reset, updating MySQL server global properties like set global slave_net_timeout
  = 120; (default was 30sec) and set global thread_pool_idle_timeout = 120 will fix
  it.'
- Debezium is open, available under the Apache Software License 2.0
- Uses OAuth2 with refresh token  requires setup of connected app in api
- The snapshot mode can be customized using the snapshot.mode property.
- Initial snapshots can be performed with either global read locks or table-level
  locks.
- To enable Debezium to perform incremental snapshots, you must grant the connector
  permission to write to the signaling table.
- Data collection names are case-sensitive.
- The connector allows for running incremental snapshots with a read-only connection
  to the database.
- To switch to a read-only implementation, set the value of the read.only property
  to true.
- Blocking snapshots can be triggered at runtime.
- Change events contain schema information for their payload.
- MySQL connector events are designed to work with Kafka log compaction.
- A truncate event signals that a table has been truncated.
- If a single TRUNCATE statement applies to multiple tables, the connector emits one
  truncate change event record for each truncated table.
- The connector captures changes in all non-system tables in every database from which
  it is configured to capture changes.
- To match the name of a column, Debezium applies the regular expression that you
  specify as an anchored regular expression.
- A signaling topic is required to preserve signal ordering.
- Metrics are exposed via MBean names specific to each connector instance.
- Custom metrics tags can be configured to prevent monitoring disruptions.
- Debezium provides *exactly once* delivery of every change event record.
- In abnormal situations, Debezium provides *at least once* delivery of change events.
- The connector does not read the oplog directly.
- Try to avoid task reassignment and reconfiguration while the connector performs
  snapshots of any replica sets.
- The connector generates log messages to report on the progress of the snapshot.
- To provide for the greatest control, run a separate Kafka Connect cluster for each
  connector.
- Incremental snapshots require that the primary key for each table is stably ordered.
- Uses a supplementary ad hoc snapshot mechanism known as a blocking snapshot.
- The `after` value in the event should be handled as the at-point-of-time value of
  the document.
- MongoDB connector uses change streams to capture changes.
- Requires MongoDB replica sets or sharded clusters.
- The value of `mongodb.authsource` is `admin` by default.
- SSL is disabled by default.
- This property specifies whether Debezium adds context headers with the prefix `__debezium.context.`
  to the messages that it emits.
- Debezium is a distributed system that captures all changes in multiple upstream
  databases, and will never miss or lose an event.
- If the connector remains stopped for a long enough interval, it can occur that MongoDB
  purges the oplog during the time that the connector is inactive, resulting in the
  loss of information about the connectors last position.
- In certain failure situations, MongoDB can lose commits, which results in the MongoDB
  connector being unable to capture the lost changes.
- At this time, there is no way to prevent this side effect in MongoDB.
- The Debezium connector for PostgreSQL does not support schema changes while an incremental
  snapshot is running.
- Currently, the only valid option is 'incremental'.
- Although the column.exclude.list and column.include.list connector configuration
  properties allow you to capture only a subset of table columns, all columns in a
  primary or unique key are always included in the events key.
- If the table does not have a primary or unique key, then the change events key
  is null.
- Tables with REPLICA IDENTITY FULL - TOAST column values are part of the before and
  after fields in change events just like any other column.
- Tables with REPLICA IDENTITY DEFAULT - When receiving an UPDATE event from the database,
  any unchanged TOAST column value that is not part of the replica identity is not
  contained in the event.
- Logical decoding does not support DDL changes.
- Debezium currently supports databases with UTF-8 character encoding only.
- Debezium does not support schema changes while an incremental snapshot is running.
- Some objects like Contact may return nulls in deeply nested fields
- 'When a primary key changes, Debezium outputs three events: a DELETE event, a tombstone
  event with the old key, followed by an event with the new key.'
- For a consumer to process a delete event generated for a table without a primary
  key, set the tables REPLICA IDENTITY to FULL.
- If you do not want the connector to capture truncate events, use the skipped.operations
  option to filter them out.
- When the time.precision.mode configuration property is set to adaptive_time_microseconds,
  the connector determines the literal type and semantic type for temporal types based
  on the columns data type definition.
- PostgreSQL supports using +/-infinite values in TIMESTAMP columns.
- Capturing changes in columns that use PostgreSQL domain types requires special consideration.
- When the hstore.handling.mode property is set to json (the default), the connector
  represents HSTORE values as string representations of JSON values.
- While using the pgoutput plug-in, it is recommended that you configure filtered
  as the publication.autocreate.mode.
- Create a unique replication slot for each Debezium connector
- Ensure PostgreSQL is set up to run the Debezium connector
- If the expressions that you specify for this property match columns that are not
  part of the tables primary key, set the REPLICA IDENTITY of the table to FULL.
- The `replica.identity.autoset.values` property applies only to tables that the connector
  captures.
- Do not use this mode if schema changes might occur during the snapshot.
- If you set the value of this property to false, Debezium does not acknowledge the
  LSN. Failure to acknowledge the LSN can lead to uncontrolled growth of the WAL logs,
  which stresses storage capacity, and could result degraded performance, and even
  data loss. To maintain normal service, if you set this property to false, you must
  configure some other mechanism to commit the LSN.
- Debezium provides exactly once delivery of every change event record under normal
  operating conditions.
- In case of faults, some duplicate events might be emitted.
- Ensure to maintain replication slots to avoid data loss.
- A properly configured Kafka cluster is able to handle massive throughput.
- Kafka Connect is written according to Kafka best practices.
- How quickly this happens depends on the capabilities and performance of Kafka and
  the volume of changes being made to the data in PostgreSQL.
- The Debezium connector for Oracle does not support schema changes while an incremental
  snapshot is running.
- The signal type is stop-snapshot, and the data field must have the following fields.
- Blocking snapshots allow for more flexibility in managing snapshots.
- Never partition the database schema history topic.
- Debezium registers and receives metadata only for transactions that occur after
  you deploy the connector.
- If you configure the connector to use the redo_log_catalog mode, do not use multiple
  Debezium Oracle connectors to capture changes from the same logical database.
- SCN gap detection is available only if the large SCN increment occurs while the
  connector is running and processing near real-time events.
- The Oracle connectors events are designed to work with Kafka log compaction.
- A *truncate* event represents a change that is made to an entire table and it has
  no message key.
- If a single `TRUNCATE` operation affects multiple tables, the connector emits one
  *truncate* change event record for each truncated table.
- Ensure the Oracle user has the required permissions as outlined in the documentation.
- Use the internal.log.mining.read.only property for capturing events from logical
  standby databases.
- The Oracle connector always uses a single task and therefore does not use this value,
  so the default is always acceptable.
- In a container database environment, specify the name of the root container database
  (CDB), not the name of an included pluggable database (PDB).
- If you include this property in the configuration, do not also set the schema.exclude.list
  property.
- If you include this property in the configuration, do not set the schema.include.list
  property.
- If you include this property in the configuration, do not also set the table.include.list
  property.
- If you include this property in the configuration, do not also set the table.exclude.list
  property.
- If you include this property in the configuration, do not set the column.exclude.list
  property.
- If you include this property in the configuration, do not set the column.include.list
  property.
- Uses Oracle LogMiner for change data capture
- Connector requires appropriate database permissions
- After you set the connector to mine only the archive logs, the latency between an
  operation being committed and the connector emitting an associated change event
  might increase.
- Metrics are exposed via JMX.
- MBean names may change with configuration.
- The OpenLogReplicator ingestion adapter is currently in incubating state.
- The database must be placed in ARCHIVELOG mode before it can archive redo log files.
- For Debezium to generate change events that show the before and after states of
  a table row, supplemental logging must be active on the database.
- Uses LogMiner by default for change events
- Initial snapshot required to establish baseline state
- If you configure the connector to use the `redo_log_catalog` mode, do not use multiple
  Debezium Oracle connectors to capture changes from the same logical database.
- For connectors that use CDB mode to connect to Oracle, maintaining synchronization
  is more complicated.
- The connector cannot process columns that include unsupported data types.
- Support for XMLTYPE is currently in an incubating state.
- Oracle allows negative scales in NUMBER type.
- Oracle AWS RDS does not allow you to execute specific commands for LogMiner configuration.
- To enable Debezium to capture the before state of changed database rows, supplemental
  logging must be enabled.
- Oracle Database is required for operation.
- Ensure that the database is configured for use with Debezium.
- OpenLogReplicator ingestion adapter is currently in incubating state, i.e. exact
  semantics, configuration options etc. may change in future revisions, based on the
  feedback we receive.
- Whenever new tables are added to the Debezium Oracles connector configuration,
  you must configure supplemental logging for each table.
- If a table that is configured for capture is not correctly configured for supplemental
  logging, after the connector begins streaming, it returns a warning message.
- OpenLogReplicator does not distribute binaries, so you must build the third-party
  tool from code.
- Oracle LogMiner requires specific database configurations for optimal performance.
- Some operations may lead to exceptions if database parameters are not properly configured.
- Use of the Debezium connector for Db2 requires SQL replication to be enabled on
  the source database.
- You cannot use Debezium with Db2 for z/OS versions that use 6-bytes LSNs.
- Db2's replication feature is not designed to store the complete history of database
  changes.
- Ad hoc snapshots require the use of signaling tables
- Specify tables to capture by sending an execute-snapshot message to the signaling
  table
- The Debezium connector for Db2 does not support schema changes while an incremental
  snapshot is running.
- Currently Debezium supports only the incremental type.
- The connector emits transaction events to the `<topic.prefix>.transaction` topic
  unless overridden.
- Updating the columns for a rows primary/unique key changes the value of the rows
  key.
- 'When a key changes, Debezium outputs three events: a DELETE event and a tombstone
  event with the old key for the row, followed by an event with the new key for the
  row.'
- Unique name for the connector is required.
- Always use a value of io.debezium.connector.db2.Db2Connector for the connector.class.
- Do not use this mode to perform a snapshot if schema changes were committed to the
  database after the last connector shutdown.
- When you enable parallel initial snapshots, the threads that perform each table
  snapshot can require varying times to complete their work.
- If you experience this problem, revert the value of `snapshot.max.threads` to `1`,
  and retry the snapshot.
- Debezium connector captures schema changes but requires manual intervention for
  updates.
- An online schema update does not require application and data processing downtime.
- The new column name does not appear in Debezium change events until the connector
  is restarted.
- Cassandra is different from the other Debezium connectors since it is not implemented
  on top of the Kafka Connect framework.
- 'The following features are currently not supported by the Cassandra connector:
  TTL on collection-type columns, Range deletes, Static columns, Triggers, Materialized
  views, Secondary indices, Light-weight transactions.'
- Uses Cassandra connector to capture change events
- The connector does not include a 'before' field in change events
- Cassandra connector uses the Cassandra driver to configure connections to Cassandra,
  which must be supplied using a separate application.conf file.
- Ad hoc snapshots require the use of signaling tables.
- After the initial snapshot, the connector does not repeat the snapshot process unless
  explicitly triggered.
- The Debezium connector for SQL Server does not support schema changes while an incremental
  snapshot is running.
- SQL Server specifically requires the base object to be a table in order to create
  a change capture instance.
- Capturing changes from indexed views (aka. materialized views) is not supported
  by SQL Server.
- Each event contains the schema for its content or a schema ID if using a schema
  registry.
- The connector streams change event records to topics with names that are the same
  as the events originating table.
- SQL Server connector events are designed to work with Kafka log compaction.
- Debezium captures changes only for transactions after connector deployment.
- CDC must be enabled for the database and each table to capture changes.
- The Debezium container images that you obtain from quay.io do not undergo rigorous
  testing or security analysis.
- To mitigate risk in production deployments, deploy only containers that are actively
  maintained by trusted vendors.
- The connector captures changes from SQL Server databases.
- Heartbeats are only emitted during polling cycles. That is, in a Debezium environment,
  the actual interval between sending heartbeat messages is jointly controlled by
  the settings of the heartbeat.interval.ms and poll.interval.ms properties.
- You must refresh capture tables to enable the connector to resume processing change
  events after a schema change.
- Some schema changes are not supported on source tables that have CDC enabled.
- The connector generates a data change event for each row-level INSERT, UPDATE, and
  DELETE operation.
- Vitess connector events are designed to work with Kafka log compaction.
- Debezium does not require any specific configuration for use with Vitess.
- The Vitess connector only supports unauthenticated access to the VTGate gRPC server.
- To prevent monitoring disruptions that result from MBean name changes, you can configure
  custom metrics tags.
- A further benefit of using custom tags, is that you can use tags that reflect the
  architecture of your data pipeline.
- Debezium provides exactly once delivery of every change event record.
- During recovery, it might repeat some change events.
- If a snapshot fails to complete, the connector does not automatically reattempt
  the snapshot.
- The connector does not support the snapshot feature at the moment.
- Spanner connector events are designed to work with Kafka log compaction.
- The connector does not support streaming snapshot events.
- If watermarking is enabled in the connector, you cannot configure Debezium topic
  routing transformations.
- This connector currently does not support the PostgreSQL interface for Cloud Spanner.
- The outbox event router SMT is not compatible with the MongoDB connector.
- To apply the default outbox event router SMT configuration, your outbox table is
  assumed to have specific columns.
- Uses KafkaAvroSerializer to serialize GenericRecord instances.
- Third-party libraries are required for AvroConverter.
- Using Avro can be beneficial for message format governance and for ensuring that
  outbox event schemas evolve in a backwards-compatible way.
- By default, the payload column value (the Avro data) is the only message value.
- When data is stored in Avro format, the column format must be set to a binary data
  type, such as bytea in PostgreSQL.
- The Debezium connectors may be configured to emit heartbeat, transaction metadata,
  or schema change events (support varies by connector).
- These events cannot be serialized by the BinaryDataConverter so additional configuration
  must be provided.
- When data is stored in Avro format, the column format must be set to a binary data
  type.
- To use Apache Avro serialization, you must deploy a schema registry that manages
  Avro message schemas and their versions.
- The Apicurio Registry project also provides a JSON converter.
- Each Debezium connector provides a configuration property, field.name.adjustment.mode
  that you can set to avro if you have columns that do not adhere to Avro rules for
  names.
- The event flattening transformation simplifies the message format for downstream
  consumers.
- Java 17 or later is required to run the Debezium connectors or Debezium UI.
- This tutorial uses Docker and the Debezium container images to run the Kafka, Debezium
  and MySQL services.
- The snapshot process will take longer with your databases, but the connector outputs
  enough log messages that you can track what it is working on, even when the tables
  have a large number of rows.
- An exclusive write lock is used at the beginning of the snapshot process, but it
  should not last very long even for large databases.
- Connected to MySQL binlog at mysql:3306
- The Debezium MySQL connector captures change events for the 'customers' table in
  the 'inventory' database.
- Uses Debezium to capture change events for MySQL database.
- Debezium connectors are tested with specific versions of Java, Apache Kafka, and
  databases.
- The Debezium logical decoding plug-ins have only been installed and tested on Linux
  machines. For Windows and other platforms it may require different installation
  steps.
- Debezium uses PostgreSQLs logical decoding, which uses replication slots.
- Replication can only be performed by a database user that has appropriate permissions.
- Superusers have by default both of the above roles.
- To use the Apicurio Registry with Debezium, add Apicurio Registry converters and
  their dependencies to the Kafka Connect container image.
- Use key.converter=io.confluent.connect.avro.AvroConverter for key.
- Use value.converter=io.confluent.connect.avro.AvroConverter for value.
- Default group configuration must be defined before using Kafka Connect automatic
  topic creation.
- Custom groups can fall back to default group settings for mandatory properties.
- Custom signaling channels can be implemented as needed.
- Custom actions enable you to extend the Debezium signaling framework.
- To use a custom action with multiple connectors, you must place a copy of the custom
  signaling channel JAR file in the subdirectory for each connector.
- Default notification channels are sink, log, jmx.
- Must be set to `io.debezium.storage.redis.history.RedisSchemaHistory`
- The Redis key that Debezium uses to store the schema history data is metadata:debezium:schema_history
- The Debezium MySQL connector reads the binlog, produces change events for row-level
  INSERT, UPDATE, and DELETE operations, and emits the change events to Kafka topics.
- The connector must follow just one MySQL server instance.
- The format of the messages that a connector emits to its schema change topic is
  in an incubating state and is subject to change without notice.
- The connector captures the structure and data of the captured tables every time
  that it starts.
- Ad hoc snapshots require the use of signaling tables to initiate.
- The snapshot can capture the entire contents of the database or a subset of tables.
- Debezium captures each table in phases, in a series of configurable chunks.
- You can run incremental snapshots in parallel with streamed data capture.
- The Debezium MySQL connector allows for running incremental snapshots with a read-only
  connection to the database.
- 'Updating the columns for a rows primary/unique key changes the value of the rows
  key. When a key changes, Debezium outputs *three* events: a DELETE event and a tombstone
  event with the old key for the row, followed by an event with the new key for the
  row.'
- Metrics are exposed via MBean names that may change with configuration.
- Custom metric tags can be added to prevent disruptions in monitoring.
- Debezium provides exactly once delivery of every change event record when operating
  normally.
- In case of faults, it may provide at least once delivery.
- If MySQL becomes unavailable, the connector fails and must be restarted.
- You can specify the tables to capture by sending an execute-snapshot message to
  the signaling table.
- Incremental snapshots rely on capturing each table in phases, in a series of configurable
  chunks.
- The default chunk size for incremental snapshots is 1024 rows.
- The Debezium MariaDB connector allows for running incremental snapshots with a read-only
  connection to the database.
- To run an incremental snapshot with read-only access, the connector uses the executed
  global transaction IDs (GTID) set as high and low watermarks.
- MariaDB connector events are designed to work with Kafka log compaction.
- A truncate event represents a change that is made to an entire table and it has
  no message key.
- Connectors name when registered with the Kafka Connect service.
- Connectors class name.
- MariaDB server address.
- MariaDB server port number.
- MariaDB user with the appropriate privileges.
- MariaDB users password.
- Unique ID of the connector.
- Topic prefix for the MariaDB server or cluster.
- List of databases hosted by the specified server.
- List of Kafka brokers that the connector uses to write and recover DDL statements
  to the database schema history topic.
- Name of the database schema history topic. This topic is for internal use only and
  should not be used by consumers.
- Flag that specifies if the connector should generate events for DDL changes and
  emit them to the fulfillment schema change topic for use by consumers.
- The connector uses the Java class io.debezium.connector.mariadb.MariaDbConnector.
- Set the value to true to prevent the connector from capturing records when no changes
  are present in the included columns.
- If you set this property, do not also set the table.include.list property.
- If you set this property, do not also set the table.exclude.list property.
- If the Debezium MariaDB connector stops for too long, the MariaDB server purges
  older binlog files and the connectors last position may be lost.
- When the connector is restarted, the MariaDB server no longer has the starting point
  and the connector performs another initial snapshot.
- If the snapshot is disabled, the connector fails with an error.
- MongoDB does not recommend running a standalone server in production.
- Try to avoid task reassignment and reconfiguration while the connector performs
  snapshots of any replica sets. The connector generates log messages to report on
  the progress of the snapshot. To provide for the greatest control, run a separate
  Kafka Connect cluster for each connector.
- Blocking snapshots can be triggered at run time to manage snapshots more flexibly.
- When a blocking snapshot is initiated, streaming stops and a snapshot is taken.
- Change events for operations that create, update or delete data all have a value
  payload with an envelope structure.
- Connector works only with MongoDB replica sets or sharded clusters.
- The connector will use SSL to connect to MongoDB instances.
- A delete operation is represented by a delete event and a subsequent tombstone event.
- Debezium provides exactly once delivery of every change event.
- To help prevent failures related to missing resume tokens, optimize configuration
  of the oplog.
- Debezium uses include list and exclude list properties to specify how data is inserted
  in the publication.
- For Debezium to specify the capture configuration, the value of publication.autocreate.mode
  must be set to filtered.
- If a schema change is performed before the incremental snapshot start but after
  sending the signal then passthrough config option `database.autosave` is set to
  `conservative` to correctly process the schema change.
- Debezium supports only the incremental type for snapshots.
- To stop an incremental snapshot, send a signal message to the configured Kafka signaling
  topic.
- If the table does not have a primary or unique key, then the change events key
  is null. The rows in a table without a primary or unique key constraint cannot be
  uniquely identified.
- Support for the propagation of default values exists primarily to allow for safe
  schema evolution when using the PostgreSQL connector with a schema registry which
  enforces compatibility between schema versions.
- While using the `pgoutput` plug-in, it is recommended that you configure `filtered`
  as the `publication.autocreate.mode`.
- Incremental snapshots rely on Debezium signaling.
- The connector does not support schema changes while an incremental snapshot is running.
- The connector uses it for all events that it generates.
- For a consumer to be able to process a *delete* event generated for a table that
  does not have a primary key, set the tables `REPLICA IDENTITY` to `FULL`.
- This ensures that events exactly represent the values in the database, except all
  TIME fields are captured as microseconds.
- When the time.precision.mode configuration property is set to connect, the connector
  uses Kafka Connect logical types.
- This may be useful when consumers can handle only the built-in Kafka Connect logical
  types and are unable to handle variable-precision time values.
- However, since PostgreSQL supports microsecond precision, the events generated by
  a connector with the connect time precision mode results in a loss of precision
  when the database column has a fractional second precision value that is greater
  than 3.
- Set the time.precision.mode property to isostring to configure the connector to
  map temporal values as ISO-8601 formatted string at UTC time zone.
- When you apply this setting, the connector uses the semantic types io.debezium.time.IsoTimestamp,
  io.debezium.time.IsoTime, and io.debezium.time.IsoDate to map timestamp, datetime,
  date, and time values.
- Set the time.precision.mode property to microseconds to configure the connector
  to specify temporal values with microsecond precision.
- When you apply this setting, the connector uses the semantic types io.debezium.time.MicroTime
  and io.debezium.time.MicroTimestamp to map timestamp, datetime, and time values.
- Set the time.precision.mode property to nanoseconds to configure the connector to
  specify temporal values with nanosecond precision.
- When you apply this setting, the connector uses the semantic types io.debezium.time.NanoTime
  and io.debezium.time.NanoTimestamp.
- Debezium requires PostgreSQL to be set up for logical replication.
- The PostgreSQL connector always uses a single task and therefore does not use this
  value, so the default is always acceptable.
- Slot names must conform to PostgreSQL replication slot naming rules.
- The replica.identity.autoset.values property applies only to tables that the connector
  captures.
- During recovery from faults, some duplicate change events might be emitted.
- This feature is currently incubating and might change in future releases.
- The connector performs an initial consistent snapshot of the database.
- If the time needed to complete the initial snapshot exceeds the UNDO_RETENTION time,
  an ORA-01555 exception can occur.
- During a tables snapshot, an ORA-01466 exception may occur if the schema of the
  table is modified.
- By default, the connector writes change events for all operations to a single Kafka
  topic specific to each table.
- When supplying XML configuration as a JSON connector property value, line breaks
  must be omitted or replaced with a \n character.
- Cannot use the Debezium Oracle connector with user-defined types.
- The connector requires specific database user permissions to function correctly.
- The database must be configured properly for use with Debezium.
- The connector uses the native Oracle LogMiner API with connector-level buffering.
- Connector requires configuration for Kafka topics
- Automatic topic creation must be enabled or manually create topics
- Debezium generates a unique MBean name for each of the different connector metrics.
- Changes to the MBean name break the linkage between the connector instance and the
  MBean, disrupting monitoring activity.
- The OpenLogReplicator ingestion adapter is currently in incubating state, i.e. exact
  semantics, configuration options etc. may change in future revisions, based on the
  feedback we receive.
- Debezium for Oracle can work with a higher degree of supplemental logging at the
  database level, if that is already configured for other purposes. But if you do
  not require higher fidelity logging to support other applications, you can reduce
  database-level logging to the minimum level.
- An empty data-collections array means that no action is required, and no snapshot
  will be performed.
- Do not use multiple Debezium Oracle connectors to capture changes from the same
  logical database.
- The Infinispan buffer type is considered incubating; the cache formats may change
  between versions.
- The connector cannot process columns that include unsupported data types such as
  BFILE and BOOLEAN.
- Oracle only supplies column values for CLOB, NCLOB, and BLOB data types if theyre
  explicitly set or changed in a SQL statement.
- The Debezium connector for Oracle does not support BOOLEAN types in earlier versions.
- Oracle AWS RDS does not allow certain commands to be executed.
- The name that is assigned to the connector when you register it with a Kafka Connect
  service is 'inventory-connector'.
- The connector class is 'io.debezium.connector.oracle.OracleConnector'.
- The maximum number of tasks to create for this connector is '1'.
- The database hostname is '<ORACLE_IP_ADDRESS>'.
- The database port is '1521'.
- The database user is 'c##dbzuser'.
- The database password is 'dbz'.
- The database name is 'ORCLCDB'.
- The topic prefix is 'server1'.
- The database pluggable database name is 'ORCLPDB1'.
- The schema history internal kafka bootstrap servers is 'kafka:9092'.
- The schema history internal kafka topic is 'schema-changes.inventory'.
- The connector relies on an Apache Kafka producer to write schema changes to database
  schema history topics.
- The connector relies on a Kafka consumer to read from database schema history topics
  when a connector starts.
- For each captured table, you must explicitly configure a higher fidelity supplemental
  logging, called (ALL) COLUMNS.
- Whenever new tables are added to the Debezium Oracles connector configuration,
  you must configure supplemental logging for each table. If a table that is configured
  for capture is not correctly configured for supplemental logging, after the connector
  begins streaming, it returns a warning message.
- The Hybrid mode for log.mining.strategy is still a work-in-progress strategy, and
  therefore does not yet support all data types.
- SQL Server CDC is not designed to store a complete history of database changes.
- The type of the snapshot to be executed is currently only 'incremental'.
- Capturing changes from indexed views is not supported by SQL Server.
- The SQL Server connector ensures that all Kafka Connect schema names adhere to the
  Avro schema name format.
- Log compaction enables removal of some older messages as long as at least the most
  recent message for every key is kept.
- For Kafka to remove all messages that have the same key, the message value must
  be null.
- By default, JDBC connections to Microsoft SQL Server are protected by SSL encryption.
  If SSL is not enabled for a SQL Server database, or if you want to connect to the
  database without using SSL, you can disable SSL by setting the value of the database.encrypt
  property in connector configuration to false.
- After you change a column in a source table from 'NULL' to 'NOT NULL' or vice versa,
  the SQL Server connector cannot correctly capture the changed information until
  after you create a new capture instance.
- The Debezium Db2 connector cannot retrieve the entire history of the database from
  the logs.
- By default, the connector captures the data for all non-system tables.
- You can specify the tables to capture in the snapshot.
- Debezium includes a supplementary ad hoc snapshot mechanism, known as a blocking
  snapshot.
- A delay might exist between the time that you send the signal to trigger the snapshot,
  and the time when streaming stops and the snapshot starts.
- Debezium events are designed to work with Kafka log compaction.
- Specific guidance about how to configure Db2 capture agent parameters is beyond
  the scope of this documentation.
- It is vital to execute a schema update procedure completely before there is a new
  schema update on the same table. Consequently, the recommendation is to execute
  all DDLs in a single batch so the schema update procedure is done only once.
- When a table is in capture mode, after a change to a column name, the Db2 replication
  feature continues to use the old column name.
- You must restart the connector to see the new column name in change events.
- CDC must be enabled on the node level and table level.
- Cassandra connector handles change events for rows structured like the originating
  table.
- If the Cassandra agent use SSL to connect to Cassandra node, an SSL config file
  is required.
- Enriched data change event records include transaction metadata.
- Enabling transaction metadata increases data volume.
- Connector does not require specific configuration for use with Vitess.
- Ensure VTGate and VTCtld ports are accessible.
- To prevent monitoring disruptions that result from MBean name changes, configure
  custom metrics tags.
- The Informix replication feature is not designed to store the complete history of
  database changes.
- The Debezium connector for Informix does not support schema changes while an incremental
  snapshot is running.
- Blocking snapshots rely on the Debezium mechanism for sending signals to a Debezium
  connector.
- The database schema history topic is for internal connector use only.
- Specific guidance about optimizing Informix for change data capture is beyond the
  scope of this documentation.
- Informix does not support online schema updates while capturing changes.
- To minimize disruptions to downstream applications, its best to perform schema
  updates during a scheduled maintenance window.
- The connector does not support `BOOLEAN` data in Oracle 23. If you configure the
  JDBC connector to use an Oracle 23 database as the sink target, you cannot rely
  on schema evolution to automatically create a field that uses the Oracle 23 `BOOLEAN`
  data type. The connector maps `BOOLEAN` data to `BIT` data types, which are universal
  across Oracle versions. As a workaround, if you need a BOOLEAN data type for an
  Oracle 23 sink, add the field manually to the target table.
- If the database does not support time or timestamps with time zones, the mapping
  resolves to its equivalent without timezones.
- The connector connects to the database and consumes events from subscribed Kafka
  topics.
- The connector can process only change events that originate from the Debezium connectors
  for relational databases.
- Debezium does not validate the behavior that results from your configuration of
  the transformation.
- The event flattening transformation simplifies the format of event records that
  Debezium connectors produce.
- The SMT simplifies the structure of the original messages.
- Optionally, you can configure the event flattening SMT for MongoDB to modify messages
  in other ways during processing.
- Because MongoDB is a schemaless database, to ensure consistent column definitions
  when you use Debezium to stream changes to a schema-based data relational database,
  fields within a collection that have the same name must store the same type of data.
- Configure the SMT to produce messages in the format that is compatible with the
  sink connector.
- This single message transformation (SMT) is supported for only the SQL database
  connectors.
- Apache Avro is a common framework for serializing data.
- The value converter for the SMT must be set to BinaryDataConverter.
- Third-party libraries are required for the AvroConverter.
- This SMT is for use with the Debezium MongoDB connector only.
- To use this SMT, operations on the actual business collection(s) and the insert
  into the outbox collection must be done as part of a multi-document transaction.
- Default behavior assumes outbox collection functions as a queue; updates are not
  allowed.
- The SMT automatically filters out delete operations on an outbox collection.
- The filter SMT is not included with the Debezium connector archives and is provided
  in a separate artifact, `debezium-scripting-3.3.1.Final.tar.gz`.
- Use the SchemaChangeEventFilter single message transformation (SMT) to filter schema
  change events that the connector captures from a table and sends to Kafka.
- After you enable the SMT, the connector synchronizes only the unfiltered DDL events
  to Kafka.
- Debezium does not come with any implementations of the JSR 223 API.
- Kafka Connect applies SMT only to certain records based on predicates
- Default predicate types include HasHeaderKey, RecordIsTombstone, TopicNameMatches
- By default, when Debezium detects a change in a data collection, the change event
  that it emits is sent to a topic that uses a single Apache Kafka partition.
- The SMT extracts specified header fields from event records, and then copies or
  moves the header fields to values in the event record.
- To comply with daylight savings time, you must specify a geographic timezone in
  the converted.timezone configuration option.
- Uses logical decoding with the write-ahead log.
- It is recommended to create global publications for TimescaleDB.
- Specifies how the decoding process handles fields that have null values in the source
  message. By default, the transformation removes fields that have null values.
- Debezium maps vector data type columns using specific logical semantic representations.
- Post processors perform lightweight, per-message mutations, similar to the modifications
  that are performed by single message transformations (SMTs).
- Post processors are designed to modify change event records emitted by Debezium
  source connectors only. You cannot configure the Debezium JDBC sink connector to
  use a post processor.
- When using embedded connectors, applications will receive each source record exactly
  once during normal operation.
- This feature is currently in incubating state, i.e. exact semantics, configuration
  options etc. may change in future revisions, based on the feedback we receive.
- Custom converters are designed to modify messages emitted by Debezium relational
  database source connectors. You cannot configure either the Debezium MongoDB connector,
  or the Debezium JDBC sink connector to use a custom converter.
- This feature is currently in incubating state, i.e. exact semantics, configuration
  options etc. may change in future revisions.
- The Debezium Outbox SMT can be configured to read additional fields and emit those
  field values either as event headers, or as part of the event value.
- The build time configuration defaults will work with the Outbox Event Router SMT
  out of the box. When not using the default values, be sure that the SMT configuration
  matches.
- Support for CloudEvents is in an incubating state. This means that exact semantics,
  configuration options, and other details may change in future revisions based on
  feedback.
- The setting is ignored when `serializer.type` is `json`.
- Debezium Extensions for Quarkus integrates the Debezium Runtime into Quarkus applications.
- The support is experimental and in case of errors or issue you can disable it with
  the following properties
- This feature is currently in incubating state, i.e. exact semantics, configuration
  options, APIs etc. may change in future revisions, based on the feedback we receive.
- Debezium provides a ready-to-use application that streams change events from a source
  database to messaging infrastructure like Amazon Kinesis, Google Cloud Pub/Sub,
  Apache Pulsar.
- Debezium Server runs on top of the Quarkus framework.
- Debezium Server configuration includes properties for source, sink, format, runtime,
  transforms, predicates, and Quarkus
- Debezium uses Log4j for logging configuration.
- Default logging level is INFO, which can be changed to DEBUG or TRACE for troubleshooting.
- The Debezium container images use a LOG_LEVEL environment variable to set the log
  level for the root logger.
- You can override other log4j properties with the CONNECT_LOG4J_LOGGERS environment
  variable.
- These dashboards are not part of Debezium itself and are maintained on a best-effort
  basis.
- Uses MySQL as the data source
- Changes in the customers table can be observed on the Kafka topic
- Uses MySQL as a data source.
- Credentials are managed via Kubernetes Secrets.
- The containers for Kafka are ephemeral.
- This tutorial requires you to run each service in a different container.
- The `watch-topic` utility is intended for monitoring events and is not suitable
  for application consumption.
- Kafka Connect can capture change events even when it is not running.
- Debezium requires Docker to run the connectors.
- This example can also be run using Podman.
- In a production environment, you would run multiple instances of each service to
  provide performance, reliability, replication, and fault tolerance.
- Connected to MySQL binlog at mysql:3306, starting at mysql-bin.000003/156
- Watches all events since the topic was created.
- The watch-topic utility is not intended for application consumption.
- Docker is configured to automatically create the topics with just one replica.
- Although an exclusive write lock is used at the beginning of the snapshot process,
  it should not last very long even for large databases.
- Debezium captures create, update, and delete events from the MySQL database.
- Kafka Connect API is exposed on port 8083.
- Uses Debezium to capture change events from MySQL
- In a production environment, you would typically either use the Kafka tools to manually
  create the necessary topics, including specifying the number of replicas, or youd
  use the Kafka Connect mechanism for customizing the settings of auto-created topics.
- Kafka Connect automatically manages tasks for registered connectors.
- Debezium can report changes in a database even when it is not running.
- This tutorial uses Docker and the Debezium container images to run the ZooKeeper,
  Kafka, Debezium, and MySQL services.
- Kafka is configured to automatically create the topics with just one replica.
- In a production environment, you should use Kafka tools to create the necessary
  topics.
- Debezium captures change events even when not running, as long as it is restarted
  before MySQL purges from its binlog.
- For security reasons, you shouldnt put passwords or other secrets in plain text
  into connector configurations.
- Streaming from MySQL's binlog
- Kafka Connect must be running to capture change events.
- Debezium captures create, update, and delete events.
- Kafka is configured to automatically create topics with one replica.
- Uses Debezium MySQL connector to capture change events.
- Debezium captures create, update, and delete events from MySQL database.
- Running each service in a separate container simplifies the setup.
- The connector will start monitoring the MySQL database servers binlog.
- The connector has transitioned from its snapshot mode into continuously reading
  the MySQL servers binlog.
- Kafka Connect is configured to automatically create the topics with just one replica.
- The JSON representations of the events are much longer than the rows they describe.
- Use the Kafka Connect service's API to register the connector.
- Debezium captures change events from MySQL database.
- Snapshot process will take longer with your databases, but the connector outputs
  enough log messages that you can track what it is working on, even when the tables
  have a large number of rows.
- The Debezium MySQL connector captures change events for the customers table in the
  inventory database.
- Kafka Connect service automatically manages tasks for its registered connectors.
- The containers for ZooKeeper and Kafka are ephemeral.
- Kafka Connect exposes a REST API to manage the Debezium MySQL connector.
- Debezium captures create, update, and delete events from MySQL databases.
- In a production environment, you would run multiple instances of each service.
- The event structure contains a schema and a payload for both key and value.
- You can determine what changed in a row by comparing before and after states in
  the payload.
- Debezium captures create, update, and delete events from MySQL.
- Running each service in a different container simplifies the setup.
- In a production environment, run multiple instances of each service for performance.
- Although an exclusive write lock is used at the beginning of the snapshot process,
  it should not last very long even for large databases. This is because the lock
  is released before any data is copied.
- snapshotCompleted=true
- Kafka Connect automatically manages tasks for its registered connectors.
- The Debezium container images that you obtain from quay.io do not undergo rigorous
  testing or security analysis, and are provided for testing and evaluation purposes
  only. These images are not intended for use in production environments.
- The Debezium logical decoding plug-ins have only been installed and tested on Linux
  machines.
- Debezium logical decoding plug-ins have only been installed and tested on Linux
  machines.
- Replication can only be performed by a database user that has appropriate permissions
- Superusers have by default both REPLICATION and LOGIN roles
- Both Apicurio API and Schema Registry and Confluent Schema Registry are available
  options.
- 'In your Debezium connector configuration, specify the following properties: key.converter=io.confluent.connect.avro.AvroConverter,
  key.converter.schema.registry.url=http://localhost:8081, value.converter=io.confluent.connect.avro.AvroConverter,
  value.converter.schema.registry.url=http://localhost:8081'
- Debezium uses the columns name as the basis for the corresponding Avro field. This
  can lead to problems during serialization if the column name does not also adhere
  to the Avro naming rules.
- Signals in the signal file are expressed as JSON objects.
- Debezium notifications provide a mechanism to obtain status information about the
  connector.
- Debezium connectors require persistent storage to preserve their state between restarts.
- If you configure Debezium to use Redis Cluster mode, ensure that your Redis Cluster
  is properly configured and accessible. The Debezium Server instance must be able
  to communicate with the cluster nodes.
- Kafka must be running in distributed mode, and the Kafka Connect version must support
  exactly-once delivery (version 3.3.0 or higher).
- All Kafka Connect workers must have exactly-once delivery enabled by setting exactly.once.source.support=enabled.
- You can re-run a snapshot for a table by initiating an ad-hoc snapshot.
- Change events are emitted for INSERT, UPDATE, DELETE operations.
- If a single `TRUNCATE` statement applies to multiple tables, the connector emits
  one *truncate* change event record for each truncated table.
- If the MySQL server becomes unavailable, the connector stops and must be restarted.
- If the connector stops for too long, the MySQL server purges older binlog files.
- If the connector stops, it resumes streaming changes from where it previously left
  off.
- Incremental snapshots rely on the Debezium mechanism for sending signals to a Debezium
  connector.
- If the progress of an incremental snapshot is interrupted, you can resume it without
  losing any data.
- Transaction metadata is available only for transactions after deploying the connector.
- If you set the `table.exclude.list` property, do not also set the `table.include.list`
  property.
- If you set the `table.include.list` property, do not also set the `table.exclude.list`
  property.
- Uses snapshot locking mode 'minimal' by default.
- The connector can skip operations during streaming.
- In abnormal situations, Debezium provides at least once delivery of change events.
- Uses a signaling collection to trigger incremental snapshots.
- The size of a MongoDB change stream event is limited to 16 megabytes.
- The after value in the event should be handled as the at-point-of-time value of
  the document.
- Debezium MongoDB connector uses change streams to capture changes.
- The connector works only with MongoDB replica sets or sharded clusters.
- Connector uses MongoDB for change data capture
- If the connector remains stopped for a long enough interval, it can occur that MongoDB
  purges the oplog during the time that the connector is inactive.
- Currently, the only valid option for snapshot operation is 'incremental'.
- PostgreSQL version 13 or later is required.
- Blocking snapshots can be triggered at run time while the connector is running.
- For a consumer to be able to process a delete event generated for a table that does
  not have a primary key, set the tables REPLICA IDENTITY to FULL.
- PostgreSQL has a hard limit on the page size.
- The connector relies on PostgreSQL logical decoding which has limitations such as
  not supporting DDL changes.
- The connector does not support schema changes while an incremental snapshot is running
- PostgreSQL connector supports all PostGIS data types.
- PostgreSQL connector supports all pgvector extension data types.
- For the connector to start and to capture change events from a PostgreSQL database,
  a replication slot must be present.
- Debezium uses replication slots to stream changes from a database.
- Debezium applies its own include/exclude list filtering, if configured, to limit
  the publication to change events for the specific tables of interest.
- If the connector is gracefully stopped, the database can continue to be used. Any
  changes are recorded in the PostgreSQL WAL. When the connector restarts, it resumes
  streaming changes where it left off.
- Ad hoc snapshots can be triggered for various reasons such as configuration changes
  or data corruption.
- 'The signal type is stop-snapshot, and the data field must have the following fields:
  type and data.'
- This feature is in an incubating state. This means that exact semantics, configuration
  options, and other details may change in future revisions based on feedback.
- The Infinispan buffer implementation utilizes multiple cache configurations with
  different names.
- Oracle database must be configured with ARCHIVELOG mode.
- The connector cannot capture change events directly from a physical standby database.
- The adapter implementation that the connector uses when it streams database changes
  is logminer by default.
- Metrics are exposed via the MBean name for the connector.
- Custom metric tags can be configured to prevent monitoring disruptions.
- Debezium for Oracle can work with a higher degree of supplemental logging at the
  database level, if that is already configured for other purposes.
- Uses the log.mining.strategy configuration property to control how Oracle LogMiner
  handles the lookup of object identifiers.
- When using the hybrid mining strategy, avoid setting lob.enabled to true.
- The connector can capture both DDL and DML changes that result from user-initiated
  or application-level operations.
- Each event in Kafka Connect is self-contained.
- If a single TRUNCATE operation affects multiple tables, the connector emits one
  *truncate* change event record for each truncated table.
- Ordering is guaranteed only for topics that use a single partition.
- You cannot use the Debezium Oracle connector with user-defined types.
- You cannot use the Debezium Oracle connector with certain Oracle-supplied types.
- Oracle AWS RDS does not allow execution of certain commands.
- The ability to use the connector with a logical standby in an incubating state and
  can change without notice.
- There is an open Jira issue to investigate support for capturing changes from a
  physical standby.
- Controls whether or not the connector mines changes from just archive logs or a
  combination of the online redo logs and archive logs.
- Long-running transactions should be avoided in order to not overflow that buffer.
- The connector captures row-level changes for INSERT, UPDATE, or DELETE operations.
- The connector persists schema information in its internal database schema history
  topic.
- Debezium emits a change event record for each row-level INSERT, UPDATE, and DELETE
  operation.
- The connector ensures that all Kafka Connect schema names adhere to the Avro schema
  name format.
- Tombstone events are emitted after a delete event for log compaction.
- There is no way for Debezium to reliably identify when a transaction has ended.
- Uses Debezium connector for SQL Server
- When you enable parallel initial snapshots, the threads that perform each table
  snapshot can require varying times to complete their work. If a snapshot for one
  table requires significantly more time to complete than the snapshots for other
  tables, threads that have completed their work sit idle. In some environments, a
  network device such as a load balancer or firewall, terminates connections that
  remain idle for an extended interval. After the snapshot completes, the connector
  is unable to close the connection, resulting in an exception, and an incomplete
  snapshot, even in cases where the connector successfully transmitted all snapshot
  data. If you experience this problem, revert the value of `snapshot.max.threads`
  to `1`, and retry the snapshot.
- You must intervene to refresh the capture table to enable the connector to resume
  processing change events.
- Offline schema updates require you to stop the Debezium connector before you can
  update capture tables.
- Online schema updates can update capture tables while the Debezium connector is
  running.
- The MBean is debesium.sql_server:type=connector-metrics,server=<topic.prefix>,task=<task.id>,context=streaming.
- The MBean is debesium.sql_server:type=connector-metrics,context=schema-history,server=<topic.prefix>,task=<task.id>.
- Although the required replication tools are available in supported versions of Db2,
  use of SQL replication to support the Debezium connector requires a separate license
  to IBM Infosphere Data replication (IIDR).
- The snapshot can capture the entire contents of the database, or capture only a
  subset of the tables in the database.
- Debezium generates transaction boundary events for the BEGIN and END delimiters
  in every transaction.
- Metadata for transactions that occur before you deploy the connector is not available.
- Db2 connector events are designed to work with Kafka log compaction.
- The connector requires a unique name for registration.
- Db2 server platform can be either LUW or ZOS.
- Uses custom metrics tags to prevent monitoring disruptions from MBean name changes.
- Uses Debezium for change data capture from Cassandra.
- Cassandra connector has built-in support for JMX metrics.
- The connector supports pass-through configuration properties that are used when
  creating the Kafka producer.
- The connector requires a consistent snapshot of the database upon first connection.
- The connector streams change events from VStream.
- Enriched transaction metadata increases message size.
- Connector streams change events to topics named after the originating table.
- The connector can use more than 1 tasks if you enable offset.storage.per.task mode.
- If not configured, unauthenticated VTGate gRPC is used.
- Make sure to provide a project ID, Spanner instance ID, Spanner database ID, and
  change stream name.
- The connector requires the use of the Informix Change Streams API for Java, which
  is packaged as part of the Informix JDBC installation and is available on Maven
  Central alongside the latest JDBC drivers.
- The Informix connector has been tested with Informix for Linux.
- Due to licensing requirements, the Debezium Informix connector archive does not
  include the Informix JDBC driver and Change Stream client that Debezium requires
  to connect to a Informix database.
- The Debezium JDBC sink connector guarantees at-least-once delivery.
- The connector does not provide transactional guarantees across multiple CDC events
  or across multiple documents.
- Most of the above SMTs are available by default with the Debezium container image,
  but you need to opt-in for the scripting-based ones (Message Filtering or Content-based
  Routing).
- The event flattening transformation is a Kafka Connect SMT.
- To add metadata to the simplified Kafka records header, specify the add.headers
  option.
- To apply the transformation to a subset of events, you can define an SMT predicate
  statement.
- The event changes transformation is supported for only the SQL database connectors.
- Uses Apache Avro for message format governance.
- Additional configuration is required for third-party libraries.
- The filter SMT is not included with the Debezium connector archives and is provided
  in a separate artifact, `debezium-scripting-3.4.0.Alpha2.tar.gz`.
- Use the SchemaChangeEventFilter single message transformation (SMT) to filter schema
  change events.
- For security reasons, the content-based routing SMT is not included with the Debezium
  connector archives.
- 'The Groovy language needs the following libraries on the classpath: groovy, groovy-json
  (optional), groovy-jsr223.'
- 'The JavaScript language needs the following libraries on the classpath: graalvm.js,
  graalvm.js.scriptengine.'
- Kafka Connect applies transformations to every record by default.
- In environments that run Apache Kafka 2.6 or greater, you can append a predicate
  statement to a transformation.
- The 'change' prefix is a special keyword that enables the SMT to automatically refer
  to elements in the payload that describe the 'before' or 'after' states of the data.
- The `HeaderToValue` SMT extracts specified header fields from event records, and
  then copies or moves the header fields to values in the event record.
- You can configure the SMT to manipulate multiple headers in the original message.
- The include.list and exclude.list configuration options are mutually exclusive.
- The GeometryFormatTransformer converts between WKB and EWKB formats.
- The SMT converts the binary content of a PostgreSQL logical decoding message to
  a structured form.
- Debezium maps vector data type columns with specific logical semantic representations
  of the vector data.
- The transformation swaps the coordinate system only for the specified values. Other
  geometry values are passed as-is.
- Unique name for the connector instance.
- The name of the Java class for the connector, e.g io.debezium.connector.mysql.MySqlConnector
  for the MySQL connector.
- When the engine executes, its connector is actively recording the source offset
  inside each source record.
- Applications will receive each source record exactly once during normal operation.
- The Outbox extension requires the use of the `ExportedEvent` interface.
- Debezium Server currently only supports OpenLineage integration with the Kafka sink.
  MongoDB Sink and JDBC Sink connectors are only supported in Kafka Connect deployments.
- CDC connector for products database
- For sink connectors, the openlineage.integration.dataset.kafka.bootstrap.servers
  property is required to retrieve input dataset metadata from Kafka topics.
- 'Debezium was tested with a Debezium Kafka Connect image, and a Strimzi distribution
  that included the following configuration settings:  * `otel.traces.exporter=otlp`
  * `otel.propagators=tracecontext`'
- Debezium extensions integrate with Quarkus applications for CDC.
- A minimum configuration is mandatory for the Debezium connector.
- 'The support is experimental and in case of errors or issue you can disable it with
  the following properties: quarkus.datasource.devservices.enabled=false'
- 'You can override using an official Debezium image: quarkus.datasource.devservices.image-name=quay.io/debezium/postgres:15'
- Embeddings transformation requires Java 21 or higher.
- Debezium provides a ready-to-use application that streams change events from a source
  database to messaging infrastructure.
- The main configuration file is config/application.properties
- The source configuration uses the same configuration properties that are described
  on the specific connector documentation pages (just with debezium.source prefix),
  together with few more specific ones, necessary for running outside of Kafka Connect.
- To enable message filtering, set environment variable ENABLE_DEBEZIUM_SCRIPTING
  to true.
- Only the JVM mode of Quarkus is supported, but not native execution via GraalVM.
- node.id must be unique per each connector running on each Cassandra node
- Currently, the only supported environment is Kubernetes.
- The source and destination names are generated automatically.
- Debezium uses Log4j logging framework for Java.
- Default logging level is INFO, WARN, and ERROR.
- The containers use a LOG_LEVEL environment variable to set the log level for the
  root logger.
- You can use this environment variable to set the log level for the service running
  in the container.
- If you are running multiple services on the same machine, be sure to use distinct
  JMX ports for each service.
- Uses Debezium with MySQL connector
- Uses MySQL as data source with Debezium connector
- Credentials are stored in a Secret object
- The JSON converter includes the key and value schemas in every message.
- Debezium captures change events from the MySQL database.
- Connected to MySQL binlog at mysql:3306, starting at MySqlOffsetContext
- Uses Debezium to capture change events from MySQL database.
- If Kafka Connect goes offline, it will start any non-running tasks when it restarts.
- For Windows and other platforms it may require different installation steps.
- Add the following lines at the end of the pg_hba.conf PostgreSQL configuration file
- The PostgreSQL server should allow replication to take place between the server
  machine and the host on which the Debezium PostgreSQL connector is running
- To use converters provided by Apicurio Registry you need to provide apicurio.registry.url.
- The source channel is enabled by default, because it is required for incremental
  snapshot signals.
- Debezium supports incremental and blocking snapshot types.
- A custom actions Java project has compile dependencies on the Debezium core module.
- Must be set to `io.debezium.storage.jdbc.history.JdbcSchemaHistory`
- Debezium can use RocketMqSchemaHistory class to store and retrieve database schema
  changes in Apache RocketMQ.
- Incremental snapshots can run in parallel with streamed data capture.
- Progress of an incremental snapshot can be resumed without losing data.
- Read-only incremental snapshots require a read-only connection to the database.
- To enable read-only implementation, set `read.only` to true.
- Blocking snapshots can be triggered at runtime to provide flexibility in managing
  snapshots.
- The connector captures changes in all non-system tables in every database.
- Uses Debezium for capturing database changes
- Check the connector's schema history topic for schema changes
- Configuration changes can result in changes to the MBean names.
- Debezium is a distributed system that captures all changes in multiple upstream
  databases; it never misses or loses an event.
- If a fault does occur, the system does not lose any events.
- If the connector fails when trying to start, it reports an error or exception in
  the log, and stops running.
- If your MySQL server becomes unavailable, the Debezium MySQL connector fails with
  an error and the connector stops.
- If the Kafka brokers become unavailable, the Debezium MySQL connector pauses until
  the connection is reestablished.
- Ad hoc snapshots can capture the entire contents of the database or a subset of
  the tables.
- Incremental snapshots run in parallel with streamed data capture
- You can resume an incremental snapshot without losing data
- Data collection names are case-sensitive
- A truncate event represents a change that is made to an entire table.
- For topics with multiple partitions, there is no ordering guarantee for the change
  events.
- The maximum number of tasks to create for this connector is 1.
- By default, the connector captures changes in all non-system tables in every database.
- Default signaling channels are source, kafka, file, jmx
- If a fault occurs, it might repeat some change events.
- If the Debezium MariaDB connector stops for too long, the MariaDB server purges
  older binlog files.
- Uses MongoDB as a data source
- MongoDB Atlas only supports secure connections via SSL.
- The connector uses MongoDB authentication settings.
- This property specifies whether Debezium adds context headers with the prefix __debezium.context.
  to the messages that it emits.
- Debezium supports only the incremental type for snapshot operations
- The PostgreSQL connector ensures that all Kafka Connect schema names adhere to the
  Avro schema name format.
- For more detailed instructions about setting up and testing logical decoding plug-ins,
  see Logical Decoding Output Plug-in Installation for PostgreSQL.
- Replication can only be performed by a database user with REPLICATION and LOGIN
  roles.
- Use a value of io.debezium.connector.postgresql.PostgresConnector for the PostgreSQL
  connector.
- Always use a single task for this connector.
- Debezium can ingest change events from Oracle by using the native LogMiner database
  package, the XStream API, or OpenLogReplicator.
- If the time needed to complete the initial snapshot exceeds the UNDO_RETENTION time
  that is set for the database (fifteen minutes, by default), an ORA-01555 exception
  can occur.
- During a tables snapshot, its possible for Oracle to raise an ORA-01466 exception.
- The signal type is `stop-snapshot`, and the `data` field must have the following
  fields.
- Blocking snapshots allow for ad hoc snapshotting of tables while the connector is
  running.
- Debezium can generate events that represent transaction metadata boundaries and
  that enrich data change event messages.
- Database transactions are represented by a statement block that is enclosed between
  the `BEGIN` and `END` keywords.
- The Infinispan buffer type is considered incubating; the cache formats may change
  between versions and may require a re-snapshot.
- If a single TRUNCATE operation affects multiple tables, the connector emits one
  truncate change event record for each truncated table.
- User-defined types and Oracle-supplied types are not supported.
- Uses Oracle LogMiner user with specific permissions
- Configuration may vary based on security policies
- For CDB deployments, specify the property `database.pdb.name`.
- For non-CDB installations, do not specify the `database.pdb.name` property.
- Be sure to set the correct host, port, and service identifier (sid) in the `database.url`.
- In environments that use the LogMiner implementation, you must use POSIX regular
  expressions only.
- Initial snapshots capture the schema history for all tables.
- The envelope schemas are specific to each table.
- By default, the JSON representations of events are much larger than the rows they
  describe.
- To enable Debezium to capture events from an Oracle read-only logical standby database,
  set internal.log.mining.read.only=true.
- If the destination for CDC is the same for both the primary and standby instances,
  using a single value for archive.destination.name is sufficient.
- The name that is assigned to the connector when you register it with a Kafka Connect
  service.
- The address of the Oracle instance is specified using database.hostname.
- The port number of the Oracle instance is specified using database.port.
- The name of the Oracle user is specified using database.user.
- The password for the Oracle user is specified using database.password.
- The name of the database to capture changes from is specified using database.dbname.
- Topic prefix that identifies and provides a namespace for the Oracle database server
  is specified using topic.prefix.
- The maximum number of tasks to create for this connector is specified using tasks.max.
- The name of the Oracle pluggable database is specified using database.pdb.name.
- For each captured table, configure higher fidelity supplemental logging called (ALL)
  COLUMNS.
- If a table is not correctly configured for supplemental logging, a warning message
  is returned after streaming begins.
- Oracle LogMiner is not deprecated.
- Hybrid mining strategy does not support all data types.
- The Debezium SQL Server connector captures row-level changes that occur in the schemas
  of a SQL Server database.
- CDC must be enabled on both the database and on each table that you want to capture.
- By default, the connector streams change event records to topics with names that
  are the same as the events originating table.
- Tombstone events are emitted after a delete event with a null value.
- By default, JDBC connections to Microsoft SQL Server are protected by SSL encryption.
- The Debezium connector requires that CDC is enabled on SQL Server.
- The connector class must be io.debezium.connector.sqlserver.SqlServerConnector.
- You must be a SQL Server database operator with elevated privileges to refresh capture
  tables.
- Metrics available for streaming and schema history.
- Blocking snapshots can be triggered at runtime
- Never partition the database schema history topic
- The Db2 connector always uses a single task and therefore does not use the tasks.max
  value.
- If you experience this problem, revert the value of snapshot.max.threads to 1, and
  retry the snapshot.
- Debezium generates a unique MBean name for each connector metrics.
- The connector must be deployed locally on each node in the Cassandra cluster.
- The connector processes all local commit log segments as they are detected.
- If the connector stops, it continues reading from the last commit log position upon
  restart.
- Cassandra does not perform a read-before-write
- Uses commit log for change events
- The cipherSuites field is not mandatory, it simply allows you to add one (or more)
  ciphers that are not present.
- The connector requires a consistent snapshot of the database.
- Automatic retry of a failed snapshot is expected to be available in a future release.
- Messages sent to data change topics include a transaction metadata block.
- The Vitess connector can use more than 1 tasks if you enable offset.storage.per.task
  mode.
- During recovery from a fault, Debezium provides at least once delivery of change
  events.
- The connector is tolerant of failures.
- Provide a project ID, Spanner instance ID, Spanner database ID, and change stream
  name.
- The Debezium Informix connector cannot retrieve the entire history of the database
  from the logs.
- The initial snapshot captures the current state of each row in the captured tables.
- The Debezium container images that you obtain from quay.io do not undergo rigorous
  testing or security analysis, and are provided for testing and evaluation purposes
  only.
- Informix replication is enabled to expose change data for tables that are in capture
  mode.
- The Informix connector is installed.
- Use io.debezium.connector.informix.InformixConnector for the connector class.
- Maximum number of tasks is set to 1.
- The connector does not perform any DDL schema evolution.
- By default, the JDBC sink connector does not transform any of the fields in the
  source event into the primary key for the event.
- The topic routing transformation is a Kafka Connect SMT
- Debezium does not validate the behavior that results from your configuration of
  the transformation
- The event flattening transformation simplifies the format of event records.
- Use 'delete.tombstone.handling.mode=rewrite' to handle DELETE operations.
- The event flattening SMT for MongoDB extracts the after field from create or update
  change event messages emitted by the Debezium MongoDB connector.
- To ensure that Kafka retains the complete Debezium change event message in its original
  format, apply the SMT to a sink connector.
- Uses Apache Avro as the payload format
- The payload column value (the Avro data) must be set to a binary data type, such
  as bytea in PostgreSQL
- If the `collection.expand.json.payload` is set to true, it enables string conversion
  in the transformation.
- Uses JSR 223 for filtering logic
- Requires separate artifact for filter SMT
- Use the SchemaChangeEventFilter SMT to filter schema change events.
- Specify the types of events that you want to remove.
- Ensure that scripting expressions can be run only by authorized users.
- In environments that run Apache Kafka 2.6 or greater, you can append a predicate
  statement to a transformation to instruct Kafka Connect to apply the SMT only to
  certain records.
- By default, Debezium routes all change event records for a configured data collection
  to a single Apache Kafka topic.
- If a specified field is not present in the event message, the SMT ignores it.
- When the application and engine shutdown normally or crash, when they are restarted
  the engine and its connector will resume reading the source information from the
  last recorded offset.
- Applications will receive each source record exactly once during normal operation,
  but may receive duplicates immediately following a restart after a crash or improper
  shutdown.
- The `ExportedEvent` interface must use the same parameter types for all implementations.
- Additional field mappings can be configured in the `application.properties` file.
- Enable OpenLineage integration by setting openlineage.integration.enabled=true
- Path to OpenLineage configuration file specified by openlineage.integration.config.file.path
- The Postgres connector is the only supported connector at this time.
- Debezium configurations use the prefix `quarkus.debezium.*`.
- quarkus.datasource.devservices.enabled=false
- or override using an official Debezium image
- quarkus.datasource.devservices.image-name=quay.io/debezium/postgres:15
- This project is currently in an incubating state. The exact semantics, configuration
  options, and so forth are subject to change, based on the feedback that we receive.
- The sequence in which Debezium applies transformations to the source is significant
  and affects the final output.
- Transformations are shared among pipelines.
- The domain.url is the only required property; it is used as host in the Ingress
  definition.
- By default, all INFO, WARN, and ERROR messages are logged to the console.
- All log messages are sent to the Docker containers console (and thus the Docker
  logs)
- Log messages are also written to files under the /kafka/logs directory
- Uses Strimzi for deploying Kafka and Kafka Connect.
- Credentials for MySQL database are stored in Kubernetes Secrets.
- Requires setup of Kafka for offset and schema history.
- Uses Kafka for offset and schema history storage
- Requires configuration of Kafka brokers
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- An error indicating a new snapshot is required if the connector's position in the
  logs is unavailable.
- If the `data-collections` array is empty, Debezium interprets the empty array to
  mean that no action is required, and it does not perform a snapshot.
- 'CONNECTION_LOST: Check your database connection.'
- 'INVALID_QUERY: Review your snapshot query syntax.'
- Specifies how the connector responds after an operation that results in a retriable
  error, such as a connection error.
- 'Command failed with error 286 (ChangeStreamHistoryLost): ''PlanExecutor error during
  aggregation :: caused by :: Resume of change stream was not possible, as the resume
  point may no longer be in the oplog'
- If the connector cannot find the publication, the connector throws an exception
  and stops.
- 'Connector failed to start: Check the database connection settings.'
- Insufficient privileges for the Oracle user.
- 'LOG_MINING_SESSION_EXPIRED: Restart the connector'
- 'INVALID_QUERY: Check the SQL syntax or database connection'
- 'ORA-01555: Snapshot too old'
- 'ORA-01466: Unable to read data'
- 'PGA_AGGREGATE_LIMIT: Exceeds available memory'
- 'ORA-01555: snapshot too old: rollback segment number 12345 with name "_SYSSMU11_1234567890$"
  too small'
- 'ORA-04036: PGA memory used by the instance exceeds PGA_AGGREGATE_LIMIT'
- 'ORA-01882: timezone region not found'
- 'MISSING_SCHEMA: The connector fails to capture the table, and reports a missing
  schema error.'
- No limit. The connector always restarts automatically, and retries the operation,
  regardless of the number of previous failures.
- Disabled. The connector fails immediately, and never retries the operation. User
  intervention is required to restart the connector.
- The connector restarts automatically until it reaches the specified maximum number
  of retries.
- 'Configuration And Startup Errors: Invalid configurations may cause startup failure'
- 'Cassandra Becomes Unavailable: Connector fails if Cassandra node is unavailable'
- 'MISSING_SCHEMA: The connector is unable to capture the table, and an error results
  if the schema is not present in the schema history topic.'
- The connectors configuration is invalid.
- The connector cannot successfully connect to Vitess by using the specified connection
  parameters.
- The connector cannot successfully connect to Spanner by using the specified connection
  parameters.
- 'LEADER_NOT_AVAILABLE: Error while fetching metadata for topics due to new topics
  being created.'
- 'SCHEMA_REGISTRY_CONNECTION_ERROR: Check the schema registry URL or network connectivity.'
- The connector cannot successfully connect to the MariaDB server by using the specified
  connection parameters.
- The connector is attempting to restart at a position in the binlog for which MariaDB
  no longer has the history available.
- 'Invalid connection string: Ensure the mongodb.connection.string is correctly formatted'
- 'Authentication failed: Check mongodb.user and mongodb.password'
- Specifies how the connector responds after an operation that results in a retriable
  error.
- 'Command failed with error 286 (ChangeStreamHistoryLost): ''PlanExecutor error during
  aggregation :: caused by :: Resume of change stream was not possible, as the resume
  point may no longer be in the oplog.'
- 'No action required: empty data-collections array means no snapshot performed.'
- The connector cannot successfully connect to PostgreSQL by using the specified connection
  parameters.
- The connector is restarting from a previously-recorded position in the PostgreSQL
  WAL and PostgreSQL no longer has that history available.
- 'ORA-01466: Unable to read data - table structure modified'
- SCN gap detection is available only if the large SCN increment occurs while the
  connector is running and processing near real-time events.
- Unsupported data types include BFILE, BOOLEAN, LONG, LONG RAW, ROWID, UROWID.
- 'Invalid credentials: Check the database username and password.'
- 'Connection failed: Verify the database hostname and port.'
- 'Permission denied: Ensure the user has necessary grants.'
- Avro serialization failure due to negative scale in NUMBER type.
- If the connector selects a path from which the requested log data was deleted, it
  may lead to errors.
- 'ORA-01466: Attempt to select into more variables than there are columns'
- 'ORA-25191: cannot reference overflow table of an index-organized table'
- 'Error: Snapshot process interrupted due to connector failure.'
- '-1: No limit. The connector always restarts automatically, and retries the operation,
  regardless of the number of previous failures.'
- 'MISSING_SCHEMA: The connector is unable to capture the table, and an error results.'
- 'Invalid operation: Ensure that the connector is configured correctly.'
- The maximum number of retries on retriable errors (e.g. connection errors) before
  failing (-1 = no limit, 0 = disabled, > 0 = num of retries).
- 'Failed to register connector: Name already exists'
- 'Connection error: Check database credentials or URL'
- 'Task failed: Review logs for details'
- Error occurs if a field specified by the table.fields.additional.placement property
  is not found in the Outbox payload.
- 'Invalid operation: The SMT logs an error and continues to the next outbox collection
  document.'
- 'Update operation not allowed: The SMT automatically filters out delete operations.'
- 'Invalid regular expression: Ensure that your regex is correctly formatted.'
- 'Unauthorized: Ensure that the user has permission to execute the filter SMT.'
- No default value for schema.change.event.exclude.list. Must be a non-empty string.
- If a specified field is not present in a record, the SMT skips it.
- '401 Unauthorized: Recheck credentials'
- '404 Not Found: Check the endpoint path'
- 'errors.max.retries: -1'
- 'errors.retry.delay.initial.ms: 300'
- 'errors.retry.delay.max.ms: 10000'
- 'Unknown property encountered: behavior is determined by `unknown.properties.ignored`
  configuration option.'
- 'LEADER_NOT_AVAILABLE: Error while fetching metadata'
- 'Request timeout: Increase the timeout value or check your network.'
- 'LEADER_NOT_AVAILABLE: Error while fetching metadata with correlation id'
- '500: Unexpected character in JSON request'
- 'LEADER_NOT_AVAILABLE: Error while fetching metadata for the Kafka topics.'
- '{"error_code":500,"message":"Unexpected character (''n'' (code 110)): was expecting
  double-quote to start field name"}'
- '500: Unexpected character in JSON input'
- 'LEADER_NOT_AVAILABLE: New Kafka topics were created and Kafka had to assign a new
  leader for each one.'
- LEADER_NOT_AVAILABLE
- '400 Bad Request: Check your connector configuration.'
- '404 Not Found: Ensure the connector is registered.'
- '500: Unexpected character in request payload'
- '500: Unexpected character in request body'
- 'LEADER_NOT_AVAILABLE: This warning can be ignored as it means new Kafka topics
  were created and that Kafka had to assign a new leader.'
- '500: Unexpected character when posting connector configuration'
- 'LEADER_NOT_AVAILABLE: This warning can be safely ignored as it means new Kafka
  topics were created and Kafka had to assign a new leader for each one.'
- '500: Unexpected character (''n'' (code 110)): was expecting double-quote to start
  field name'
- No specific error messages provided.
- '500 Internal Server Error: Check the connector''s configuration.'
- '500: Unexpected character, check JSON format'
- 'CONTAINER_NOT_RUNNING: Ensure that the container is started before making requests.'
- 'LEADER_NOT_AVAILABLE: Error while fetching metadata.'
- 'Signal type not supported: Ensure the signal type is valid.'
- No default for offset.storage
- If you do not specify a value, the connector defaults to performing an incremental
  snapshot.
- If you omit this component from the data field, the signal stops the entire incremental
  snapshot that is in progress.
- Duplicate records may occur during blocking snapshots.
- The connector cannot successfully connect to the MySQL server.
- The connector is attempting to restart at a position in the binlog for which MySQL
  no longer has the history available.
- 'invalid resume token error: If last stream position was removed from the oplog.'
- 'guardrail.collections.max: 0'
- 'guardrail.collections.limit.action: warn'
- 'WAL_SEGMENT_DELETED: Some WAL segments are no longer available.'
- 'CONNECTION_ERROR: Connection to the database failed.'
- 'INVALID_CONFIGURATION: The connector configuration is invalid.'
- 'ORA-01555: Check UNDO_RETENTION settings.'
- 'ORA-01466: Schema modifications during snapshot.'
- If the same buffer location will be used by a new connector deployment, the files
  should be removed manually before deploying the new connector.
- Avro serialization failure may occur due to negative scale values in NUMBER type.
- 'Invalid destination: Ensure the specified archive destination is VALID and LOCAL.'
- 'LOG_MINING_ERROR: Check if the log mining process is configured correctly.'
- 'SCN_GAP_DETECTED: Adjust SCN gap detection settings.'
- 'ORA-01466: invalid cursor state'
- 'ORA-00600: internal error code, arguments: [flg0], ...'
- Duplicate events might be generated following an outage.
- 'Missing schema error: The connector cannot capture the table if the schema is not
  present in the schema history topic.'
- 'Invalid character in schema name: replaced with underscore.'
- Case sensitivity may lead to conflicts in event records.
- 'INVALID_CONFIGURATION: Check connector properties for correctness'
- 'EVENT_PROCESSING_FAILURE: Review event schema and payload'
- 'Illegal prefix ''@'' for column: x, from schema: y, table: z'
- 'Configuration and startup errors: The connectors configuration is invalid.'
- 'Missing schema error: The connector fails to capture the table if schema is not
  present.'
- 'Invalid transaction ID format: Ensure the ID is in the format txID:LSN.'
- 'Invalid operation on outbox table: UPDATE'
- Empty or null payload causes tombstone event.
- Ensure that scripting expressions can be run only by authorized users.
- No default value for schema.change.event.exclude.list
- If a specified field is not present in the event message, the SMT ignores it.
- If none of the fields exist in the message, then the transformation ignores the
  event message entirely.
- No default value for 'headers' and 'fields'.
- 'fields.null.include: Specifies how the decoding process handles fields that have
  null values in the source message.'
- The maximum number of retries on connection errors before failing (-1 = no limit,
  0 = disabled, > 0 = num of retries).
- If applications need more rigorous exactly-once behavior, then they should use the
  full Debezium platform.
- If an unknown property is encountered whether it should be silently ignored or if
  a runtime exception should be thrown.
- offset.storage.file.filename is required
- 'LEADER_NOT_AVAILABLE: This message means that new Kafka topics were created and
  that Kafka had to assign a new leader for each one.'
- Superusers have by default both of the above roles
- The connector cannot successfully connect to the MySQL server by using the specified
  connection parameters.
- 'invalid resume token error: Create a new connector to enable Debezium to continue
  capturing records.'
- '0: Disabled. The connector fails immediately, and never retries the operation.
  User intervention is required to restart the connector.'
- '> 0: The connector restarts automatically until it reaches the specified maximum
  number of retries. After the next failure, the connector stops, and user intervention
  is required to restart it.'
- '401 Unauthorized: Recheck database user or password'
- 'ORA-01466: An attempt was made to perform an operation that is not allowed'
- The connector cannot process columns that include unsupported data types such as
  BFILE, BOOLEAN, LONG, LONG RAW, ROWID, and others.
- '401 Unauthorized: Check user credentials'
- '500 Internal Server Error: Check connector configuration'
- 'ORA-01466: exception occurred'
- Exceeding the maximum number of tables that the connector can capture
- 'ORA-01555: snapshot too old'
- 'ORA-00600: internal error'
- '401 Unauthorized: Check your authorization credentials'
- 'Fail: The connector fails and reports an exception.'
- 'Warn: The connector logs a warning.'
- Commit logs are only flushed to the cdc_raw directory when full, causing delays
  in event capture.
- Schema changes are not recorded in commit logs, and changes may be skipped.
- Invalid configurations
- 'Invalid configuration: The connectors configuration is invalid.'
- 'Connection error: The connector cannot successfully connect to Spanner using the
  specified connection parameters.'
- Some database dialects might throw an exception if you set the primary.key.mode
  to kafka and set schema.evolution to basic.
- If a column maps to a data type that isnt permitted as a primary key for your target
  database, an explicit list of columns will be necessary in primary.key.fields excluding
  such columns.
- Ensure unique key across multiple physical tables when routing records to the same
  topic
- Configuration errors may occur if the SMT is not applied correctly.
- Invalid metadata fields can lead to unexpected record formats.
- SMT logs a warning and continues to the next outbox collection document.
- SMT logs an error and continues to the next outbox collection document.
- SMT logs an error and the connector stops processing.
- 'Unauthorized: Ensure proper permissions for connector creation'
- 'Invalid configuration: Ensure the schema.change.event.exclude.list is a non-empty
  string.'
- 'Invalid header name: Ensure header names are correct and exist in the record.'
- 'Invalid operation: Must be either ''move'' or ''copy''.'
- 'N/A: Determines when an unknown property is encountered whether it should be silently
  ignored or if a runtime exception should be thrown.'
- Configuration property `tracing.span.context.field` must be specified
- Configuration property `tracing.operation.name` must be specified
- 'CONNECTION_ERROR: Check Kafka broker connectivity'
- 'INVALID_CONFIG: Verify configuration settings'
auth_info:
  mentioned_objects:
  - OauthToken
  - AuthProvider
  - NamedCredential
  - Secret
  - Role
  - RoleBinding
client:
  base_url: http://localhost:8083
  auth:
    type: basic
    location: header
    header_name: Authorization
  headers:
    Accept: application/json
source_metadata: null
