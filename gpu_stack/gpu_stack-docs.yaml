resources:
- name: supported_models
  endpoint:
    path: /api/supported_models
    method: GET
    data_selector: models
- name: supported_models
  endpoint:
    path: /api/supported_models
    method: GET
    data_selector: models
    params: {}
- name: API Server
  endpoint:
    path: /api/server
    method: GET
    data_selector: data
- name: Scheduler
  endpoint:
    path: /scheduler
    method: GET
notes:
- GPUStack is an open-source GPU cluster manager for running AI models.
- Supports a wide range of models including LLMs, VLMs, image models, audio models,
  embedding models, and rerank models.
- 'Broad Hardware Compatibility: Run with different brands of GPUs in Apple Macs,
  Windows PCs, and Linux servers.'
- 'Broad Model Support: From LLMs to diffusion models, audio, embedding, and reranker
  models.'
- 'Scales with Your GPU Inventory: Easily add more GPUs or nodes to scale up your
  operations.'
- 'Distributed Inference: Supports both single-node multi-GPU and multi-node inference
  and serving.'
- 'Multiple Inference Backends: Supports llama-box (llama.cpp & stable-diffusion.cpp),
  vox-box and vLLM as the inference backends.'
- 'Lightweight Python Package: Minimal dependencies and operational overhead.'
- 'User and API key management: Simplified management of users and API keys.'
- 'GPU metrics monitoring: Monitor GPU performance and utilization in real-time.'
- 'Token usage and rate metrics: Track token usage and manage rate limits effectively.'
- 'Broad Hardware Compatibility: Run with different brands of GPUs in Apple MacBooks,
  Windows PCs, and Linux servers.'
- 'Multiple Inference Backends: Supports llama-box (llama.cpp & stable-diffusion.cpp),
  vox-box and vLLM as the inference backend.'
- The installation of GPUStack worker on a Linux system requires that the GLIBC version
  be 2.29 or higher.
- Supports a wide variety of hardware including Apple MacBooks, Windows PCs, and Linux
  servers.
- Lightweight Python package with minimal dependencies and operational overhead.
- You can only see the API key once on creation.
- When upgrading, upgrade the GPUStack server first, then upgrade the workers.
- Please DO NOT upgrade from/to the main(dev) version or a release candidate(rc) version,
  as they may contain breaking changes. Use a fresh installation if you want to try
  the main or rc versions.
- Before proceeding with an upgrade, itâ€™s strongly recommended to back up your database.
- By default, the GPUStack server uses port 80.
- You can set the worker name to a custom name using the --worker-name parameter.
- You can set the worker IP using the --worker-ip parameter when running GPUStack.
- The default data path is /var/lib/gpustack.
- The default cache path is /var/lib/gpustack/cache.
- For GGUF models, the path must point to the absolute path of the .gguf file.
- For sharded model files, use the absolute path of the first .gguf file (00001).
- For Safetensors models, the path must point to the absolute path of the model directory
  which contain *.safetensors, config.json, and other files.
errors: []
auth_info:
  mentioned_objects: []
client:
  base_url: https://docs.gpustack.ai/0.7/
source_metadata: null
