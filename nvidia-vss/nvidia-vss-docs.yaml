resources:
- name: alerts
  endpoint:
    path: /alerts
    method: GET
    data_selector: alerts
- name: chat
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: chat
- name: files
  endpoint:
    path: /files
    method: GET
    data_selector: files
- name: health
  endpoint:
    path: /health/ready
    method: GET
    data_selector: health
- name: live-stream
  endpoint:
    path: /live-stream
    method: GET
    data_selector: live_streams
- name: metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: metrics
- name: models
  endpoint:
    path: /models
    method: GET
    data_selector: models
- name: recommended_config
  endpoint:
    path: /recommended_config
    method: POST
    data_selector: recommended_config
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: summarization
- name: reviewAlert
  endpoint:
    path: /reviewAlert
    method: POST
    data_selector: review_alert
- name: generate_vlm_captions
  endpoint:
    path: /generate_vlm_captions
    method: POST
    data_selector: captions
    params: {}
- name: generate_vlm_captions
  endpoint:
    path: /generate_vlm_captions
    method: POST
- name: reviewAlert
  endpoint:
    path: /reviewAlert
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: alerts_recent
  endpoint:
    path: /alerts/recent
    method: GET
- name: vss-blueprint
  endpoint:
    path: /nvidia-blueprint-vss-2.4.0.tgz
    method: GET
- name: nim-llm
  resources:
    limits:
      nvidia.com/gpu: 4
- name: nemo-embedding
  resources:
    limits:
      nvidia.com/gpu: 1
- name: nemo-rerank
  resources:
    limits:
      nvidia.com/gpu: 1
- name: embeddings
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: ''
    params: {}
- name: ranking
  endpoint:
    path: /v1/ranking
    method: POST
    data_selector: ''
    params: {}
- name: RIVA ASR
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: ''
    params: {}
- name: NVIDIA Jetson Thor
  endpoint:
    path: /prereqs_thor.html
- name: NVIDIA DGX Spark
  endpoint:
    path: /prereqs_dgx_spark.html
- name: Deploy Using Docker Compose ARM
  endpoint:
    path: /vss_dep_docker_compose_arm.html
- name: VLM
  endpoint:
    path: /remote_vlm_deployment
    method: GET
- name: LLM
  endpoint:
    path: /remote_llm_deployment
    method: GET
- name: Embedding
  endpoint:
    path: /llama-3.2-nv-embedqa-1b-v2
    method: GET
- name: Reranker
  endpoint:
    path: /llama-3.2-nv-rerankqa-1b-v2
    method: GET
- name: summarize_video
  endpoint:
    path: /summarize/video
    method: POST
    data_selector: summary
    params: {}
- name: files
  endpoint:
    path: /files
    method: POST
    data_selector: null
    params: {}
- name: live_stream
  endpoint:
    path: /live-stream
    method: POST
    data_selector: null
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: null
    params: {}
- name: generate_vlm_captions
  endpoint:
    path: /generate_vlm_captions
    method: POST
    data_selector: null
    params: {}
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: null
    params: {}
- name: review_alert
  endpoint:
    path: /reviewAlert
    method: POST
    data_selector: null
    params: {}
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: alerts
  endpoint:
    path: /alerts
    method: POST
    data_selector: alert ID
    params: {}
- name: recent_alerts
  endpoint:
    path: /alerts
    method: GET
    data_selector: recent alerts
    params: {}
- name: health_check
  endpoint:
    path: /health/ready
    method: GET
    data_selector: server health status
    params: {}
- name: metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: server metrics
    params: {}
- name: generate_vlm_captions
  endpoint:
    path: /generate_vlm_captions
    method: POST
    data_selector: VLM captions
    params: {}
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: generate_vlm_captions
  endpoint:
    path: /generate_vlm_captions
    method: POST
- name: graph_ingestion
  endpoint:
    path: /graph/ingestion
    method: POST
    data_selector: results
    params:
      batch_size: 1
- name: vector_ingestion
  endpoint:
    path: /vector/ingestion
    method: POST
    data_selector: results
    params:
      batch_size: 1
- name: vector_retrieval
  endpoint:
    path: /vector/retrieval
    method: GET
    data_selector: results
    params:
      top_k: 5
- name: graph_retrieval
  endpoint:
    path: /graph/retrieval
    method: GET
    data_selector: results
    params:
      top_k: 5
- name: vlm_retrieval
  endpoint:
    path: /vlm/retrieval
    method: GET
    data_selector: results
    params:
      top_k: 10
- name: cot_retrieval
  endpoint:
    path: /cot/retrieval
    method: GET
    data_selector: results
    params:
      top_k: 10
      max_iterations: 3
      confidence_threshold: 0.7
- name: CV metadata
  endpoint:
    path: /cv/metadata
    method: POST
    data_selector: metadata
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: chat
  endpoint:
    path: /chat/completions
    method: POST
- name: storage_file
  endpoint:
    path: /api/v1/storage/file
    method: POST
    data_selector: ''
    params: {}
- name: storage_file_path
  endpoint:
    path: /api/v1/storage/file/path
    method: GET
    data_selector: ''
    params: {}
- name: submit_alert
  endpoint:
    path: /api/v1/alerts
    method: POST
- name: alert_health
  endpoint:
    path: /api/v1/alerts/health
    method: GET
- name: ws_alerts
  endpoint:
    path: /ws/alerts
    method: WebSocket
- name: third_party_vlm
  endpoint:
    path: /third_party/vlm
    method: GET
    data_selector: models
- name: GPT-4o
  endpoint:
    path: /v1/models/gpt-4o
    method: GET
- name: Cosmos-Reason1
  endpoint:
    path: /v1/models/cosmos-reason1
    method: GET
- name: nim-llm
  endpoint:
    path: /services/data/vXX.X/sobjects/NimLlm
    method: GET
    data_selector: records
    params: {}
- name: ca_rag_config
  endpoint:
    path: vss/configs/ca_rag_config.yaml
    method: GET
    data_selector: functions
- name: asset_storage_directory
  endpoint:
    path: /tmp/custom-asset-dir
    method: POST
    data_selector: vss.applicationSpecs.vss-deployment.containers.vss.env
    params:
      ASSET_STORAGE_DIR: /tmp/custom-asset-dir
- name: input_video_streams_directory
  endpoint:
    path: /tmp/custom-example-streams-dir
    method: POST
    data_selector: vss.applicationSpecs.vss-deployment.containers.vss.env
    params:
      EXAMPLE_STREAMS_DIR: /tmp/custom-example-streams-dir
- name: OpenAI GPT-4o
  endpoint:
    path: /api/v1/models
    method: GET
- name: Cosmos-Reason1
  endpoint:
    path: /ngc/nim/nvidia/cosmos-reason1-7b:1.1-fp8-dynamic
    method: GET
- name: openai_gpt4o
  endpoint:
    path: /v1/models/gpt-4o
    method: GET
    data_selector: data
    params: {}
- name: cosmos_reason1
  endpoint:
    path: /v1/models/cosmos-reason1
    method: GET
    data_selector: data
    params: {}
- name: qwen2.5_vl
  endpoint:
    path: /v1/models/qwen2.5-vl
    method: GET
    data_selector: data
    params: {}
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: messages
- name: arango_db
  endpoint:
    path: arango_db
    method: GET
    data_selector: records
- name: elasticsearch_db
  endpoint:
    path: elasticsearch_db
    method: GET
    data_selector: records
- name: chat_llm
  endpoint:
    path: /v1/chat
    method: POST
    data_selector: response
    params: {}
- name: summarization_llm
  endpoint:
    path: /v1/summarization
    method: POST
    data_selector: response
    params: {}
- name: nvidia_embedding
  endpoint:
    path: /v1/embedding
    method: POST
    data_selector: response
    params: {}
- name: nvidia_reranker
  endpoint:
    path: /v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking
    method: POST
    data_selector: response
    params: {}
- name: VIA_DEPLOY_AWS_DYT
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_AWS_S3B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_AWS_S3BR
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: vss_chart
  endpoint:
    repo:
      name: nvidia-blueprint
      url: https://helm.ngc.nvidia.com/nvidia/blueprint
    chart: nvidia-blueprint-vss
    version: 2.4.0
- name: vss_chart
  endpoint:
    path: https://helm.ngc.nvidia.com/nvidia/blueprint
    method: GET
    data_selector: charts
    params: {}
- name: vss
  endpoint:
    path: /vss
    method: POST
    data_selector: applicationSpecs
    params:
      startupProbe:
        failureThreshold: 360
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: response
    params: {}
- name: detector
  endpoint:
    params:
      GDINO_MODEL_PATH: /models/gdino/<your_finetuned_gdino_model>.onnx
      GDINO_INFERENCE_INTERVAL: '1'
- name: tracker
  endpoint:
    params:
      CV_PIPELINE_TRACKER_CONFIG: <path_to_config_tracker_NvDCF_accuracy_SAM2.yml>
- name: cv_pipeline_chunks
  endpoint:
    params:
      NUM_CV_CHUNKS_PER_GPU: '4'
- name: VLM Models
  endpoint:
    path: /models/vlm
    method: GET
    data_selector: models
    params: {}
- name: ASR Models
  endpoint:
    path: /models/asr
    method: GET
    data_selector: models
    params: {}
- name: CA-RAG Models
  endpoint:
    path: /models/ca-rag
    method: GET
    data_selector: models
    params: {}
- name: CV Pipeline Models
  endpoint:
    path: /models/cv-pipeline
    method: GET
    data_selector: models
    params: {}
- name: VIA Health Report
  endpoint:
    path: /tmp/via-logs
    method: GET
    data_selector: files_requestid_*
    params: {}
- name: OpenTelemetry Tracing
  endpoint:
    path: /otel-collector
    method: GET
    data_selector: traces
    params: {}
- name: alerts_recent
  endpoint:
    path: /alerts/recent
    method: GET
    data_selector: alerts
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: summary
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: results
- name: nim-llm
  endpoint:
    path: /nim-llm
    method: GET
    data_selector: records
    params: {}
- name: vss
  endpoint:
    path: /vss
    method: GET
    data_selector: records
    params: {}
- name: nemo-embedding
  endpoint:
    path: /nemo-embedding
    method: GET
    data_selector: records
    params: {}
- name: nemo-rerank
  endpoint:
    path: /nemo-rerank
    method: GET
    data_selector: records
    params: {}
- name: vss
  endpoint:
    path: /vss-deployment
    method: GET
    data_selector: containers
    params: {}
- name: nemo-embedding
  endpoint:
    path: /embedding-deployment
    method: GET
    data_selector: containers
    params: {}
- name: nemo-rerank
  endpoint:
    path: /ranking-deployment
    method: GET
    data_selector: containers
    params: {}
- name: Reranker
  endpoint:
    path: /v1/ranking
    method: POST
    data_selector: ''
    params: {}
- name: RIVA ASR
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: ''
    params: {}
- name: alerts
  endpoint:
    path: /alerts
    method: GET
- name: alerts_post
  endpoint:
    path: /alerts
    method: POST
- name: alerts_delete
  endpoint:
    path: /alerts/{alert_id}
    method: DELETE
- name: alerts_recent
  endpoint:
    path: /alerts/recent
    method: GET
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
- name: files_post
  endpoint:
    path: /files
    method: POST
- name: files_get
  endpoint:
    path: /files
    method: GET
- name: files_delete
  endpoint:
    path: /files/{file_id}
    method: DELETE
- name: files_get_by_id
  endpoint:
    path: /files/{file_id}
    method: GET
- name: files_content_get
  endpoint:
    path: /files/{file_id}/content
    method: GET
- name: health_ready
  endpoint:
    path: /health/ready
    method: GET
- name: health_live
  endpoint:
    path: /health/live
    method: GET
- name: live_stream
  endpoint:
    path: /live-stream
    method: GET
- name: live_stream_post
  endpoint:
    path: /live-stream
    method: POST
- name: live_stream_delete
  endpoint:
    path: /live-stream/{stream_id}
    method: DELETE
- name: metrics
  endpoint:
    path: /metrics
    method: GET
- name: models
  endpoint:
    path: /models
    method: GET
- name: recommended_config
  endpoint:
    path: /recommended_config
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: files
  endpoint:
    path: /files
    method: POST
- name: live-stream
  endpoint:
    path: /live-stream
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: alerts
  endpoint:
    path: /alerts
    method: POST
- name: chat
  endpoint:
    path: /chat/completions
    method: POST
- name: chat_completion
  endpoint:
    path: /chat/completions
    method: POST
- name: alerts
  endpoint:
    path: /alerts
    method: POST
- name: list_alerts
  endpoint:
    path: /alerts
    method: GET
- name: delete_alert
  endpoint:
    path: /alerts/{id}
    method: DELETE
- name: list_recent_alerts
  endpoint:
    path: /alerts
    method: GET
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: CV metadata
  endpoint:
    path: /cv/metadata
    method: POST
    data_selector: metadata
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
- name: GPT4o
  endpoint:
    path: /v1/models/GPT4o
    method: GET
- name: NVILA
  endpoint:
    path: /v1/models/NVILA
    method: GET
- name: NEVA
  endpoint:
    path: /v1/models/NEVA
    method: GET
- name: Fuyu
  endpoint:
    path: /v1/models/Fuyu
    method: GET
- name: vss-deployment
  endpoint:
    path: /v1
    method: POST
    data_selector: vss
    params: {}
- name: nim-llm
  endpoint:
    path: /services/data/vXX.X/sobjects/nim-llm
    method: GET
    data_selector: records
    params: {}
- name: ca_rag_config
  endpoint:
    path: /services/data/vXX.X/sobjects/ca_rag_config
    method: GET
    data_selector: records
    params: {}
- name: guardrails_config
  endpoint:
    path: /services/data/vXX.X/sobjects/guardrails_config
    method: GET
    data_selector: records
    params: {}
- name: assets_directory
  endpoint:
    path: /tmp/custom-asset-dir
    method: POST
    data_selector: vss.applicationSpecs.vss-deployment.containers.vss.env
    params: {}
- name: input_video_streams_directory
  endpoint:
    path: /tmp/custom-example-streams-dir
    method: POST
    data_selector: vss.applicationSpecs.vss-deployment.containers.vss.env
    params: {}
- name: 3rd-Party VLM Endpoints
  endpoint:
    path: /3rd-party-vlm-endpoints
    method: GET
    data_selector: models
- name: Community Models
  endpoint:
    path: /community-models
    method: GET
    data_selector: models
- name: chat_completions
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: model
    params: {}
- name: summarization
  endpoint:
    path: /summarization
    method: POST
- name: chat
  endpoint:
    path: /chat
    method: POST
- name: embedding
  endpoint:
    path: /embedding
    method: POST
- name: reranker
  endpoint:
    path: /reranker
    method: POST
- name: AWS_ACCESS_KEY_ID
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: AWS_SECRET_ACCESS_KEY
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_AWS_DYT
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_AWS_S3B
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_AWS_S3BR
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: VIA_DEPLOY_ENV
  endpoint:
    path: ''
    method: ''
    data_selector: ''
    params: {}
- name: vss_chart
  endpoint:
    path: /vss_chart
    method: GET
    data_selector: records
    params: {}
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: response
    params: {}
- name: Ingestion Pipeline
  endpoint:
    path: /api/ingestion
    method: POST
    data_selector: insights
    params: {}
- name: Retrieval Pipeline
  endpoint:
    path: /api/retrieval
    method: GET
    data_selector: results
    params: {}
- name: Helm Chart Deployment
  endpoint:
    path: /helm/chart/deployment
    method: GET
- name: Docker Compose Deployment
  endpoint:
    path: /docker/compose/deployment
    method: GET
- name: vss
  endpoint:
    path: /vss
    method: GET
- name: vss
  endpoint:
    path: /vss
    method: GET
    data_selector: records
    params:
      incremental: updated_at
- name: vss
  endpoint:
    path: /vss-deployment
    method: GET
    data_selector: records
    params: {}
- name: embedding
  endpoint:
    path: /embedding-deployment
    method: GET
    data_selector: records
    params: {}
- name: rerank
  endpoint:
    path: /ranking-deployment
    method: GET
    data_selector: records
    params: {}
- name: llama-3.2-nv-rerankqa-1b-v2
  endpoint:
    path: /v1/ranking
    method: POST
    data_selector: model
    params: {}
- name: RIVA ASR
  endpoint:
    path: /v1/health/ready
    method: GET
    data_selector: health
    params: {}
- name: environment_variables
  endpoint:
    path: /vss/configuration
    method: GET
    data_selector: variables
- name: alerts
  endpoint:
    path: /alerts
    method: GET
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
- name: files
  endpoint:
    path: /files
    method: POST
- name: health_ready
  endpoint:
    path: /health/ready
    method: GET
- name: live_stream
  endpoint:
    path: /live-stream
    method: GET
- name: metrics
  endpoint:
    path: /metrics
    method: GET
- name: models
  endpoint:
    path: /models
    method: GET
- name: recommended_config
  endpoint:
    path: /recommended_config
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: response
    params: {}
- name: files
  endpoint:
    path: /files
    method: POST
- name: live-stream
  endpoint:
    path: /live-stream
    method: POST
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: chat-completions
  endpoint:
    path: /chat/completions
    method: POST
- name: files
  endpoint:
    path: /files
    method: GET
- name: live-stream
  endpoint:
    path: /live-stream
    method: GET
- name: models
  endpoint:
    path: /models
    method: GET
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: chat_completions
  endpoint:
    path: /chat/completions
    method: POST
    data_selector: null
    params: {}
- name: alerts
  endpoint:
    path: /alerts
    method: POST
    data_selector: null
    params: {}
- name: list_alerts
  endpoint:
    path: /alerts
    method: GET
    data_selector: null
    params: {}
- name: delete_alert
  endpoint:
    path: /alerts/{id}
    method: DELETE
    data_selector: null
    params: {}
- name: recent_alerts
  endpoint:
    path: /alerts
    method: GET
    data_selector: null
    params: {}
- name: server_health_check
  endpoint:
    path: /health/ready
    method: GET
    data_selector: null
    params: {}
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
    data_selector: null
    params: {}
- name: GPT-4o
  endpoint:
    path: /v1/models/gpt-4o
    method: GET
- name: nim-llm
  endpoint:
    path: /services/data/vXX.X/sobjects/nim-llm
    method: GET
    data_selector: records
    params: {}
- name: chat
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: model
    params: {}
- name: Assets Directory
  endpoint:
    path: /configure/assets/directory
    method: POST
    data_selector: ASSET_STORAGE_DIR
    params: {}
- name: Input Video Streams Directory
  endpoint:
    path: /configure/input/video/streams
    method: POST
    data_selector: EXAMPLE_STREAMS_DIR
    params: {}
- name: vss_chart
  endpoint:
    path: /vss_chart
    method: GET
- name: summarize
  endpoint:
    path: /summarize
    method: POST
    data_selector: response
    params: {}
- name: tracker
  endpoint:
    path: /opt/nvidia/via/config/default_tracker_config.yml
    method: GET
- name: cv_pipeline_customization
  endpoint:
    path: /cv_pipeline/customization
    method: POST
    data_selector: results
    params: {}
- name: VILA 1.5
  endpoint:
    path: /models/vila-1.5
    method: GET
- name: NVILA 15B Lite
  endpoint:
    path: /models/nvila-15b-lite
    method: GET
- name: NVILA 15B HighRes
  endpoint:
    path: /models/nvila-15b-highres
    method: GET
- name: Parakeet-CTC-XL-0.6B
  endpoint:
    path: /models/parakeet-ctc-0_6b-asr
    method: GET
- name: LLaMA 3.1 70b Instruct
  endpoint:
    path: /models/llama-3_1-70b-instruct
    method: GET
- name: NVIDIA Retrieval QA Llama3.2 1b v2 Embedding
  endpoint:
    path: /models/llama-3_2-nv-embedqa-1b-v2
    method: GET
- name: NVIDIA Retrieval QA Llama3.2 1b v2 Reranking
  endpoint:
    path: /models/llama-3_2-nv-rerankqa-1b-v2
    method: GET
- name: SAM2
  endpoint:
    path: /models/sam2
    method: GET
- name: NVIDIA ReIdentificationNet
  endpoint:
    path: /models/reidentificationnet
    method: GET
- name: Grounding DINO
  endpoint:
    path: /models/grounding_dino
    method: GET
- name: Ingestion Pipeline
  endpoint:
    path: /ingestion/pipeline
    method: POST
    data_selector: insights
    params: {}
- name: Retrieval Pipeline
  endpoint:
    path: /retrieval/pipeline
    method: POST
    data_selector: results
    params: {}
- name: vss
  endpoint:
    path: /vss/v1
    method: GET
    data_selector: data
    params: {}
- name: vss
  endpoint:
    path: /vss-blueprint
    method: POST
    data_selector: applicationSpecs
    params: {}
- name: nemo-embedding
  endpoint:
    path: /embedding-deployment
    method: POST
    data_selector: applicationSpecs
    params: {}
- name: nemo-rerank
  endpoint:
    path: /ranking-deployment
    method: POST
    data_selector: applicationSpecs
    params: {}
- name: Summarize a Video
  endpoint:
    path: /summarize-video
    method: POST
    data_selector: summary
    params: {}
- name: files
  endpoint:
    path: /files
    method: GET
- name: live-stream
  endpoint:
    path: /live-stream
    method: GET
- name: models
  endpoint:
    path: /models
    method: GET
- name: summarize
  endpoint:
    path: /summarize
    method: POST
- name: chat
  endpoint:
    path: /chat/completions
    method: POST
- name: alerts
  endpoint:
    path: /alerts
    method: GET
- name: health
  endpoint:
    path: /health/ready
    method: GET
- name: server_metrics
  endpoint:
    path: /metrics
    method: GET
- name: traffic_camera_video
  endpoint:
    path: /traffic_camera_video
    method: GET
    data_selector: traffic_events
- name: warehouse_video_short
  endpoint:
    path: /warehouse_video_short
    method: GET
    data_selector: warehouse_events
- name: bridge_inspection_video
  endpoint:
    path: /bridge_inspection_video
    method: GET
    data_selector: bridge_condition
- name: warehouse_video_long
  endpoint:
    path: /warehouse_video_long
    method: GET
    data_selector: warehouse_events_long
- name: traffic_camera_images
  endpoint:
    path: /traffic_camera_images
    method: GET
    data_selector: image_events
- name: summarization
  endpoint:
    path: /summarization
    method: POST
    data_selector: summarization_results
    params:
      method: batch
      batch_size: define_batch_size
      summary_aggregation: define_summary_aggregation
- name: question_answering
  endpoint:
    path: /question_answering
    method: POST
    data_selector: qa_results
    params:
      method: VectorRAG
- name: alerts
  endpoint:
    path: /alerts
    method: POST
    data_selector: alert_results
    params: {}
- name: GPT-4o
  endpoint:
    path: /api/gpt-4o
    method: POST
- name: Fine-tuned VILA 1.5
  endpoint:
    path: /api/vila-1-5
    method: POST
- name: NVILA research model
  endpoint:
    path: /api/nvila
    method: POST
- name: nim-llm
  endpoint:
    path: /services/data/vXX.X/sobjects/nim-llm
    method: GET
    data_selector: records
    params: {}
- name: summarization_embedding
  endpoint:
    path: summarization.embedding.base_url
    method: UPDATE
- name: chat_embedding
  endpoint:
    path: chat.embedding.base_url
    method: UPDATE
- name: chat_reranker
  endpoint:
    path: chat.rerank.base_url
    method: UPDATE
- name: llama_model
  endpoint:
    path: models.engine
    method: UPDATE
- name: assets_directory
  endpoint:
    path: /tmp/custom-asset-dir
    method: POST
    data_selector: vss-deployment
    params: {}
- name: input_video_streams_directory
  endpoint:
    path: /tmp/custom-example-streams-dir
    method: POST
    data_selector: vss-deployment
    params: {}
- name: LLM
  endpoint:
    path: /v1/chat/completions
    method: POST
    data_selector: model
    params: {}
- name: Embedding
  endpoint:
    path: /v1/embeddings
    method: POST
    data_selector: input
    params: {}
- name: Reranker
  endpoint:
    path: /v1/ranking
    method: POST
    data_selector: query
    params: {}
- name: summarization
  endpoint:
    path: /summarize
    method: POST
    data_selector: summarization
- name: VILA 1.5
  endpoint:
    path: /orgs/nim/teams/nvidia/models/vila-1.5-40b
    method: GET
- name: GPT-4o
  endpoint:
    path: /docs/models/gpt-4o#gpt-4o
    method: GET
- name: LLaMA 3.1 70b Instruct
  endpoint:
    path: /meta/llama-3_1-70b-instruct
    method: GET
- name: NVIDIA Retrieval QA Llama3.2 1b v2 Embedding
  endpoint:
    path: /nvidia/llama-3_2-nv-embedqa-1b-v2
    method: GET
- name: NVIDIA Retrieval QA Llama3.2 1b v2 Reranking
  endpoint:
    path: /nvidia/llama-3_2-nv-rerankqa-1b-v2
    method: GET
notes:
- Multi-session Q&A is not currently supported.
- Gradio UI sometimes becomes unresponsive.
- Current VSS release also supports NVILA research model as the VLM.
- Processes video and image files as well as live streams from cameras.
- Custom VLM models can be hooked into the pipeline.
- 256 GB or more of system memory
- 500 GB of storage
- Ubuntu 22.04
- NVIDIA driver 580.65.06 (Recommended minimum version)
- CUDA 13.0+ (CUDA driver installed with NVIDIA driver)
- Kubernetes v1.31.2
- NVIDIA GPU Operator v23.9 (Recommended minimum version)
- Helm v3.x
- NGC API Key
- This project downloads and installs additional third-party open source software
  projects.
- Audio and CV metadata are not enabled by default.
- The overrides file must be created.
- The overrides file assumes a specific GPU topology (documented in the beginning
  of the overrides file).
- 'The setting nvidia.com/gpu: 0 for all microservices is 0 (means no limit in GPU
  allocation).'
- GPU allocation to each microservice is handled using env NVIDIA_VISIBLE_DEVICES.
- VSS has CV pipeline enabled by setting DISABLE_CV_PIPELINE to false. This will download
  and install additional third-party open source software projects. Review the license
  terms of these open source projects before use.
- VSS has proprietary codecs enabled by setting INSTALL_PROPRIETARY_CODECS to true.
  This will install additional open source and proprietary codecs. Review their license
  terms. This is required for Set-of-Marks overlay preview.
- Installing the additional codecs and open source packages requires root permissions.
- Guardrails has been disabled for single GPU deployment because of accuracy issues
  with the llama-3.1-8b-instruct model.
- 'If you have issues with multi-node deployment, try the following:'
- Try setting `nodeSelector` on each service as shown above when deploying.
- Try deleting existing PVCs `sudo microk8s kubectl delete pvc model-store-vss-blueprint-0
  vss-ngc-model-cache-pvc` before redeploying.
- Cosmos-Reason1 7B FP8 (default) is not supported on L40s. Use Cosmos-Reason1 7b
  FP16 instead by setting MODEL_PATH to git:https://huggingface.co/nvidia/Cosmos-Reason1-7B
  in the .env file.
- The .env file will store your private API keys in plain text. Ensure proper security
  and permission settings are in place for this file or use a secrets manager to pass
  API key environment variables in a production environment.
- Verify that you have a live embedding endpoint at port 9234 for VSS.
- Verify that you have a live reranker endpoint at port 9235 for VSS.
- Verify that you have a live RIVA ASR endpoint at port 9000 for VSS.
- For VSS container, modify the NVIDIA_VISIBLE_DEVICES variable in the .env file.
- For other containers launched using the docker run command, modify the --gpus '"device=0"'
  argument.
- When using VILA-1.5 VLM, VSS requires at least 1 GPU on an 80+ GB GPU and at least
  2 GPUs on a 48 GB GPU.
- When using Cosmos-Reason1 or NVILA VLM or a remote VLM endpoint, VSS requires at
  least 1 GPU.
- Embedding and Reranking require 1 GPU each but can share a GPU with VSS on an 80+
  GB GPU.
- RIVA ASR requires 1 GPU but can share a GPU with Embedding and Reranking on an 80+
  GB GPU.
- VSS can be deployed on NVIDIA Jetson Thor using Docker Compose only.
- NVIDIA API Key for remote model endpoints (hybrid deployment only) is optional.
- Sufficient storage space for video processing (>10GB recommended in /tmp/).
- All Docker commands must be run without sudo.
- Ensure proper security and permission settings for .env file containing API keys.
- To enable the live stream preview, set `INSTALL_PROPRIETARY_CODECS=true` in the
  `.env` file.
- VSS can be deployed on NVIDIA servers across any cloud provider using the helm or
  docker compose deployment documentation for bare-metal setups.
- 'VSS can deployed using one-click deployment on Cloud: AWS, GCP.'
- Support for VILA model and VILA related configurations are scheduled to be deprecated
  in the future.
- Default is true (disabled) for Disable CV pipeline
- 'Default: 1 for GDINO Inference Interval'
- 'Default: 2 for NUM CV Chunks Per GPU'
- Timeout in milliseconds to try TCP connection for RTP data in case UDP fails. Default
  is 2000 ms if not specified.
- Uses POST method for adding alerts and GET for listing them.
- The summarization takes from a few seconds to minutes, depending on various factors
  including video length, chunk size, prompt, VLM model, and GPUs installed on the
  host.
- The audio and CVrelated configurations are hidden by default. Audio / CV pipeline
  needs to be enabled for the options to show.
- Uses GDINO configurations for event detection in videos
- VLM prompts provided for different video scenarios
- Documents can be added with doc_index and doc_meta.
- Batcher groups documents into fixed-size batches.
- Captions generated by the Vision-Language Model are stored in Milvus DB or Elasticsearch.
- To use disable_entity_description, you will need LLM with structured output capabilities.
- Audio processing can be enabled or disabled at initialization of VSS.
- Each summarization request can be configured to enable or disable audio transcription.
- CV pipeline is supported for video files and live-streams only. Images are not supported.
- Kubernetes deployment is not currently supported.
- Using Alert Bridge entails downloading and installing additional third-party open-source
  software projects.
- Uses OpenAI API standard for third-party VLMs
- Model can ONLY be selected at initialization time
- API key required from OpenAI platform.
- Custom VLM models might not work well with GPU-sharing topology.
- The default Helm Chart will launch the Llama 3.1 70b NIM to use as the LLM.
- To use a different LLM NIM, you must adjust the repository key to the container
  path of the LLM NIM along with the tag key if needed.
- ArangoDB is not supported on aarch64 platforms.
- An overrides.yaml file will be generated in the same directory.
- Make sure the directory ASSET_STORAGE_DIR is writable by non root users.
- VLM model can ONLY be selected at initialization time
- Qwen2.5 VL based models are supported as drop-in replacements for Cosmos-Reason1
- VSS supports integrating custom VLM models.
- Ensure the model provides an OpenAI compatible REST API.
- Make sure </path/to/directory/with/inference.py> is a directory under <MODEL_ROOT_DIR_ON_HOST>.
- <MODEL_ROOT_DIR_ON_HOST> is a parent directory on the host machine for all the models.
- The first time this command is run, it will take some time to download and deploy
  the model.
- Set NVIDIA_API_KEY environment variable in the .env file.
- 'Make sure the directory ASSET_DIR_ON_HOST is writable by non root users. You can
  try running: chmod -R 777 <ASSET_DIR_ON_HOST> to make it writable.'
- Minimum version of OpenTofu must be 1.9.0.
- GCP service account is required to run the installations using one-click scripts.
- You must have a GCP cloud storage bucket to capture the metadata (state) of the
  deployment.
- This project downloads and installs additional third-party open source software
  projects. Review the license terms of these open source projects before use.
- 'The VSS container startup might be timing out on an L4 node when VILA1.5 is used
  as the VLM (default). Try increasing the startup timeout by using an overrides file
  with following values:'
- The complete setup takes around 45 minutes as it downloads the model, generates
  trt-llm engine and sets up the services.
- Exceeding frame limits will result in API errors.
- Uses NVIDIA NeMo Guardrails for user input control.
- This is an advanced use case and is an experimental feature enabled for local development
  and testing purposes.
- Uses OAuth2 with refresh token â€” requires setup of connected app in api
- The default CV pipeline is configured to run maximum `2` chunks of video on the
  same GPU.
- Increasing `NUM_CV_CHUNKS_PER_GPU` will give better performance at the cost of increased
  GPU utilization and memory consumption.
- Only NVIDIA Retrieval QA Llama3.2 1B Embedding NIM and NVIDIA Retrieval QA Llama3.2
  1B Reranking NIM models are supported for embedding and reranking respectively.
- To run VSS with a UI to graph OpenTelemetry traces, you can run with the Docker
  Compose profile perf-profiling.
- This is not required if you simply want to collect the logs and traces in static
  files.
- Users can enable VSS health evaluation reports and OTEL tracing.
- 'VSS Health evaluation reports (log files written to the container): export ENABLE_VIA_HEALTH_EVAL=true
  # required for health evaluation report and performance metrics'
- Collects comprehensive performance metrics and diagnostic files.
- Enable VIA Health Evaluation
- For optimal performance on H100, H200, or A100 GPUs, always use the GPU-optimized
  topology.
- The default topology might not fully utilize the capabilities of these high-performance
  GPUs.
- Ensure that the videos you want to evaluate are all in one directory.
- The dense caption ground truth is optional, but it is recommended to provide dense
  caption ground truth files to evaluate the accuracy of the captions generated by
  the VLM.
- 'If you want to mount only the eval directory to the container, instead of the entire
  vss-engine directory, you can do the following: In the compose.yaml file, comment
  out line 33. If you want to mount the entire vss-engine directory, you can uncomment
  line 33 and comment out line 34.'
- Audio Processing and CV pipeline do not work together for now. You can try either
  of the two at a time.
- Models are trained on specific data/use cases so if tested on other inputs then
  it might give incorrect results.
- 'VLM Model accuracy: Sometimes time stamps returned are not accurate. Also, it can
  hallucinate for certain questions. Prompt tuning might be required.'
- Summarization accuracy is heavily dependent on VLM accuracy. Also, the default configs
  have been tuned for the warehouse use case. User can supply custom VLM and summarization
  prompts to the /summarize API.
- The following harmless warnings might be seen during VSS application execution.
  These can be safely ignored.
- Due to a browser limitation, loading multiple Gradio sessions in the same browser
  can cause Gradio sessions to get stuck or appear to be slow.
- Guardrails might not reject some prompts that are expected to be rejected. This
  could be because the prompt might be relevant in other contexts and topics in the
  prompt might not be configured to be rejected. You can try tuning the guardrails
  configuration if required.
- OpenAI connection errors or 429 (too many requests) errors might be seen sometimes
  if too many requests are sent to GPT-4v or GPT-4o VLMs. It can be due to lower TPM/RPM
  limits associated with the OpenAI account.
- CA-RAG Summarization might show a truncated summary response. This is due to the
  max_tokens. Try increasing the number in the CA-RAG config file.
- CV pipeline is currently supported for video files and live-streams only. Images
  are not supported.
- 'Deleting RTSP streams can be hung sometimes. This is a GStreamer issue: https://gitlab.freedesktop.org/gstreamer/gstreamer/-/issues/1570.
  Workaround can be exporting VSS_RTSP_TIMEOUT=0. This will disable TCP transport
  after UDP timeout. But, this could cause streaming to not work at all when network
  is not good.'
- Some video encoded formats such as H.264 High 4:4:4 Profile (Hi444pp) are not supported.
  For such videos summarization request will fail and 'No summary was generated for
  given request' message will be shown.
- If you observe 'An internal error occurred' as the output, the likely cause is that
  the Initialization failed. This is because the VSS container failed to start. Check
  the logs for more details.
- In 'IMAGE FILE SUMMARIZATION & Q&A', When one of the samples is selected, VSS produce
  a summary with multiple timestamps, even though only one image is selected.
- Rarely Guardrails failure might cause VSS to stop. In such cases, either Guardrails
  can be disabled or a custom VSS container can be built with changes.
- Rarely, issuing a high number of file summarization requests back to back for long
  period of time might cause VSS to stop.
- Sometimes highlight generation can fail if the LLM is not able to generate a valid
  JSON response. You can retry the request.
- In some cases, Cosmos-Reason1 model may require to enable reasoning using 'enable_reasoning'
  in the API to extract the IDs overlayed by the CV pipeline.
- Some VSS responses may contain HTML / markdown tags which can cause the VSS UI output
  to be rendered incorrectly.
- Configuring alerts results in incorrect summaries or alerts do not get generated
  on x86 platforms and Jetson Thor. This has been fixed in VSS GitHub repository.
- In some cases, Cosmos-Reason1 model may require to enable reasoning using `enable_reasoning`
  in the API to extract the IDs overlayed by the CV pipeline.
- So VSS will automatically force reasoning when CV metadata is enabled. This behaviour
  can be disabled setting the environment variable `VSS_FORCE_CR1_REASONING_FOR_CV_METADATA=false`.
  However, even if reasoning is enabled, some object IDs may still not be extracted.
- Need to build the VSS Engine container from source to fix the issue.
- Using a smaller LLM model like Llama 3.1 8B for RAG may result in lower accuracy.
  Try using a larger model like Llama 3.1 70B
- 'VSS logs are written to the /tmp/via-logs/ directory inside the running VSS container
  in the pod: vss-vss-deployment-*.'
- By default, VSS API and UI services are assigned random node ports.
- VSS is configured to allow only 256 summarization requests to be processed in parallel.
- Make sure that the cache cleaner script from the VSS repository is running.
- This will also delete any downloaded models in the cache directory.
- Sometimes, deleting a live stream does not work.
- 'VLM Model accuracy: Sometimes time stamps returned are not accurate.'
- Summarization accuracy is heavily dependent on VLM accuracy.
- CV pipeline feature is currently at Alpha stage.
- The platform requirement can vary depending on the configuration and deployment
  topology used for VSS and dependencies like VLM, LLM, etc.
- When running on an L40 / L40S system, the default startup probe timeout might not
  be enough for the VILA model to be downloaded and its TRT-LLM engine to be built.
- Please check FAQ page for deployment failure scenarios with corresponding troubleshooting
  instructions and commands.
- Some objects like Contact may return nulls in deeply nested fields
- Uses GPU sharing topology with multiple services running on specified GPUs.
- Installation of proprietary codecs and additional packages requires root permissions.
- These models must be launched separately or configured to use remote endpoints.
- Ensure the reranker NIM works by running a sample curl command.
- For other containers launched using the docker run command, modify the --gpus 'device=0'
  argument.
- When using VILA-1.5 VLM, VSS requires at least 1 GPU on an 80+ GB GPU (A100, H100,
  H200, B200) and at least 2 GPUs on an 48 GB GPU (L40s).
- When using NVILA VLM or a remote VLM endpoint, VSS requires at least 1 GPU (A100,
  H100, H200, B200, L40s).
- Check NVIDIA NIM for Large Language Models (LLMs) documentation for LLM GPU requirements.
- GPUs can be shared even further by using the low memory modes and smaller VLM and
  LLM models.
- If using remote endpoints for any of the services, GPUs will not be used by these
  services and GPU requirements will be further reduced.
- VSS supports a variety of configuration options that can be used to customize the
  behavior of the system.
- A default configuration is provided with the VSS blueprint / docker image.
- Most of the parameters can be configured at runtime through the VSS API.
- RIVA ASR model name, not needed for NIM-based service.
- The client assumes that the VSS API server is running at http://localhost:8000.
- 'File types supported: mp4, mkv - with h264/h265 video and Opus/Vorbis audio codecs.'
- Support RTSP URLs with h264/h265 video and Opus/Vorbis audio codecs.
- The Set-Of-Marks preview only shows the frames sampled for VLM and hence will run
  at a much lower framerate. Additionally, it is delayed by `2 * chunk duration` seconds.
- VSS supports integrating custom VLM models. Depending on the model to be integrated,
  some configurations must be updated or the interface code is implemented.
- The model can ONLY be selected at initialization time.
- vllm serve errors were observed for Qwen model with latest transformers version
  (July 2025)
- Custom VLM models may not work well with GPU-sharing topology
- By default it will launch the Llama 3.1 70b NIM to use as the LLM.
- To use a different LLM NIM, you must adjust the repository key to the container
  path of the LLM NIM.
- 'Make sure the directory ASSET_STORAGE_DIR is writable by non root users. You can
  try running: chmod 777 <ASSET_DIR_ON_HOST> to make it writable.'
- Set NVIDIA_API_KEY environment variable in the .env file when using endpoints from
  build.nvidia.com
- Ensure the bucket is not public accessible but rather only to your account.
- Make sure to update config sections the (spec/infra/configs/clusters/app/master
  ; and /nodes for multi-node deployments)
- Please check your system clock - and make sure its in synch with NTP.
- VSS pod is failing and restarting on L4 node
- VSS uses Milvus vector DB to store the intermediate VLM responses per chunk before
  aggregating and summarizing the responses using CA-RAG.
- The VLM response is stored as a string, because it is and it is not parsed or stored
  as structured data.
- To modify Guardrails configuration for helm, update guardrails_config.yaml section
  in nvidia-blueprint-vss/charts/vss/values.yaml file of VSS Helm Chart.
- For deploying the new image using helm, create a new image pull secret for the custom
  container image repository.
- The engine files for the models are generated when the VSS container is initialized.
- If you are configuring a custom model, make sure to delete any stale TensorRT engines
  in NGC_MODEL_CACHE before restarting the VSS container.
- VLM that gives the best accuracy is GTP-4o.
- Some objects like Contact may return nulls in deeply nested fields.
- This is an alpha feature. Future releases may have better observability and tracing.
- Current observability logs might be deprecated in future releases.
- The logs are written into non persistent files inside the container.
- The default topology may not fully utilize the capabilities of these high-performance
  GPUs.
- Multi-stream with chat enabled in /summarize API call can result in Neo4j errors
  if number of streams >4. Neo4j Enterprise Edition is required for multi-stream processing
  for streams >4.
- Audio Processing and CV pipeline do not work together for now. You may try either
  of the two at a time.
- Due to a browser limitation, loading multiple Gradio sessions in the same browser
  may cause Gradio sessions to get stuck or appear to be slow.
- Guardrails might not reject some prompts that are expected to be rejected. This
  could be because the prompt might be relevant in other contexts as well as topics
  in the prompt might not be configured to be rejected. You can try tuning the guardrails
  configuration if required.
- By default, VSS is configured to allow only 256 summarization requests to be processed
  in parallel.
- The ingestion pipeline supports offline and batch processing of video and image
  files as well as online processing of live streams from cameras.
- VLM captions and audio transcripts are summarized together to get a final aggregated
  summary using an LLM.
- 'The default helm chart deployment topology is configured for: 8 x H100 (80 GB),
  8 x H200 (140 GB), 8 x A100 (80 GB), 8 x L40S (48 GB)'
- Full local deployment recipe on single GPU using non-default low memory modes and
  smaller LLMs are available.
- Please make sure to generate a 'Legacy API Key' at the bottom of the API Keys page.
- Uses proprietary codecs enabled by setting INSTALL_PROPRIETARY_CODECS to true.
- CV pipeline enabled by setting DISABLE_CV_PIPELINE to false.
- Uses proprietary codecs and requires root permissions for installation.
- VSS has CV pipeline enabled by setting DISABLE_CV_PIPELINE to false.
- CV and audio related features are currently not supported in Single GPU deployment.
- VSS requires at least 1 GPU on an 80+ GB GPU (A100, H100, H200) and at least 2 GPUs
  on a 48 GB GPU (L40s).
- When using NVILA VLM or a remote VLM endpoint, VSS requires at least 1 GPU (A100,
  H100, H200, L40s).
- Default is true (disabled) for CV pipeline
- Supports file and live stream summarization, Q&A, and alerts.
- Audio content in Opus and Vorbis formats is supported.
- Audio transcription is supported for offline and batch processing of video files
  as well as online processing of live streams.
- The object detector (Grounding DINO) uses the Swin-Tiny backbone by default, which
  prioritizes speed over accuracy. It is expected if you see inaccurate detection
  results, especially for uncommon object classes like 'forklift'.
- Custom VLM models may not work well with GPU-sharing topology.
- The default helm chart will launch the Llama 3.1 70b NIM to use as the LLM
- Follow the steps to install the blueprint with the overrides.
- Set NVIDIA_API_KEY environment variable in the .env file when using endpoints from
  build.nvidia.com.
- 'If you face an error like `tar: This does not look like a tar archive` while untarring
  the package, make sure you have access to NGC org/team to download the 1-click deployment
  package.'
- Use Configuration Options to override the default deployment parameters (Helm).
- Update the values.yaml file of different subcharts of VSS blueprint (Helm).
- Modify VSS deployment time configurations.
- Uses custom container image with proprietary codecs installed
- Do not change the modelEngineFile field which contains dummy paths.
- VSS CV pipeline defaults to performance mode.
- GTP-4o gives the best accuracy.
- Ensure the correct mount path in overrides_cv_accuracy_mode.yaml.
- You must use tools like `kubectl` to copy the data out of the container.
- 'Deleting RTSP streams can be hung sometimes. This is a GSTreamer issue: https://gitlab.freedesktop.org/gstreamer/gstreamer/-/issues/1570.
  Workaround can be exporting VSS_RTSP_TIMEOUT=0.'
- If you see 'An internal error occurred' as the output, the likely cause is that
  the Initialization failed. Please check the logs for more details.
- Ensure you have the correct Organization/Team selected in the top right.
- The selected Organization/Team has the NVIDIA VSS Early Access subscription and
  it is active.
- 'To configure a higher limit: For docker compose deployment, add `VSS_EXTRA_ARGS="--max-live-streams
  <max-streams>"` in the `.env` file.'
- For helm deployment, set `VSS_EXTRA_ARGS` to `"--max-live-streams <max-streams>"`
  using the overrides file.
- VLM captions are generated using the embeddings and user prompt for each chunk.
- Accelerated NeMo Retriever Embedding NIM is used for better throughput.
- GPU operator related errors with microk8s observed with certain newer NVIDIA driver
  versions.
- When running on an L40 / L40S system, the default startup probe timeout might not
  be enough for the VILA model to be downloaded and its TRT-LLM engine to be built.
  To prevent startup timeout, increase the startup timeout by passing `--set vss.applicationSpecs.vss-deployment.containers.vss.startupProbe.failureThreshold=360`
  to the helm install command or the following to the overrides file.
- It bypass the k8s device plugin resource allocation.
- Helm chart might not work in managed k8s cluster services(e.g. AKS) where this env
  is not allowed.
- For k8s where CDI is enabled this variable is ignored and GPUs are allocated randomly.
  So wonâ€™t work as expected.
- To use a different LLM NIM, you must adjust the repository key to the container
  path of the LLM NIM along with the tag key if needed
- To run this deployment, first get an NVIDIA API key from build.nvidia.com and an
  OpenAI API Key to use GPT-4o as the remote VLM.
- Inside the remote_vlm_deployment folder, edit the .env file and populate the NVIDIA_API_KEY
  and OPENAI_API_KEY fields.
- VSS requires the llama-3.2-nv-embedqa-1b-v2 embedding NIM for interactive Q&A
- VSS requires the llama-3.2-nv-rerankqa-1b-v2 reranker NIM for interactive Q&A
- This release of the OneClick scripts supports single-node deployments to AWS
- 'In case you face an error like `could not process config file: ...` while restarting/redeploying,
  try removing the temporary directory that is shown in the error logs.'
- You must wait till the deployment installation is fully complete before trying to
  access the nodes.
- The terraform scripts will install the `kubectl` utility. Users must not install
  kubernetes or kubectl manually.
- VSS uses Milvus vector DB to store the intermediate VLM responses per chunk before
  aggregating.
- The final aggregated summarization response from CA-RAG is not stored.
- You must use tools like kubectl to copy the data out of the container.
- Multi-session Q&A is not currently supported. Users should try chat only on a single
  file or live stream at a time. Trying chat on multiple files and/or live streams
  may lead to incorrect replies. This does not affect summarization.
- 'Helm deployment: VSS deployment pod fails with Error: (LLM call Exception: llm-nim-svc)'
- If this happens, please wait for additional few minutes and a pod restart fixes
  the issue. Users can monitor this using sudo watch microk8s kubectl get pod.
- Gradio UI might be slow to load the thumbnails and video preview for longer videos.
  This becomes especially noticeable over slower network connections.
- Insufficient VRAM on deployment GPUs or insufficient CPU RAM.
- OpenAI API Key not having GPT-4o model access.
- Incorrect version of NVIDIA Drivers.
- Invalid access to NGC.
errors:
- 'REQUEST_LIMIT_EXCEEDED: Throttle API calls or reduce frequency'
- 'QUERY_TIMEOUT: Break down filters or add selectivity'
- '401 Unauthorized: Recheck OAuth scopes or token expiration'
- '401 Unauthorized: Check API key or permissions.'
- 'Connection refused: Ensure the service is running and accessible.'
- '400 Bad Request: Check the request body for required fields.'
- '404 Not Found: Ensure the ID or sensor ID exists.'
- '500 Internal Server Error: Retry the request later.'
- 'API_KEY_NOT_FOUND: Ensure that the API key is set correctly'
- 'MODEL_NOT_FOUND: Check the model name and its availability'
- '401 Unauthorized: Recheck API key or token expiration'
- Container access issues could arise if the Docker container is not running.
- Missing log files may indicate that ENABLE_VIA_HEALTH_EVAL was not set before running.
- OpenAI connection errors or 429 (too many requests) errors might be seen sometimes
  if too many requests are sent to GPT-4v or GPT-4o VLMs.
- If you observe 'An internal error occurred' as the output, the likely cause is that
  the Initialization failed.
- 'Error: failed to fetch https://helm.ngc.nvidia.com/nvidia/blueprint/charts/nvidia-blueprint-vss-2.0.0.tgz
  : 403 Forbidden'
- 'Exception: [429] Too Many Requests'
- '403 Forbidden: Check NGC API key and account enablement'
- '429 Too Many Requests: Limit the number of requests in parallel to the remote endpoint'
- 'Neo.DatabaseError.Transaction.TransactionCommitFailed: Either Neo4j container/pod
  crashed or storage space is insufficient for Neo4j to commit transactions to the
  database store (~95% full).'
- Elasticsearch might fail when storage reaches approximately 90% capacity.
- '429: Too many requests'
- 'LLM call Exception: llm-nim-svc'
- CA-RAG Summarization might show a truncated summary response.
- '401 Unauthorized: Ensure the API key is valid.'
- '404 Not Found: Check the model name or endpoint.'
- 'Invalid API Key: Ensure the NVIDIA_API_KEY is set correctly.'
- 'Connection Error: Check your network connection and endpoint URL.'
- OpenAI connection errors or 429 (too many requests) errors might be seen sometimes
  if too many requests are sent to GPT-4v or GPT-4o VLMs. It can be due to lower TPM/RPM
  limits associated with the OpenAI account.
- CA-RAG Summarization might show a truncated summary response. This is due to the
  max_tokens. Try increasing the number in the CA-RAG config file.
- 'Helm deployment: VSS deployment pod fails with Error: (LLM call Exception: llm-nim-svc).
  If this happens, please wait for additional few minutes and a pod restart fixes
  the issue.'
- 'RESOURCE_LIMIT_EXCEEDED: Reduce resource allocation or increase available resources.'
- '401 Unauthorized: Recheck API key'
- 'RIVA_ASR_SERVER_API_KEY: Set in .env file as shown in documentation.'
- Neo4j errors if number of streams >4
- 'Error String : Feature not supported on this GPUError Code : 801'
- An internal error occurred
- '403 Forbidden: Check if the account has VSS EA enablement or if the wrong NGC API
  key was used.'
- 'OSError: [Errno 28] No space left on device: Increase the shared memory size of
  the VSS pod.'
- If you have issues with multi-node deployment, try setting `nodeSelector` on each
  service as shown above when deploying.
- 'vCPU Limit Exceeded: If the AWS account has a lower vCPU quota than required by
  the instance type requested, you may see a ''vCPU Limit Exceeded'' error.'
- The VSS container startup might be timing out an L40/L40S node when VILA1.5 is used
  as the VLM (default).
- '403 Forbidden: Ensure account has VSS EA enablement and correct NGC API key.'
- 'INVALID_REQUEST: Ensure correct account selection when logging in.'
- 'INTERNAL_ERROR: Avoid clicking existing accounts with owner access.'
auth_info:
  mentioned_objects:
  - NGC_API_KEY
client:
  base_url: http://localhost:8000
source_metadata: null
